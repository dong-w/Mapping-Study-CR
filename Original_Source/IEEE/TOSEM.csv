Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication_Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier,,
Runtime Verification for LTL and TLTL,A. Bauer; M. Leucker; C. Schallhart,NICTA and Australian National University,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,4,1,64,"<p>This article studies runtime verification of properties expressed either in lineartime temporal logic (LTL) or timed lineartime temporal logic (TLTL). It classifies runtime verification in identifying its distinguishing features to model checking and testing, respectively. It introduces a three-valued semantics (with truth values <i>true, false, inconclusive</i>) as an adequate interpretation as to whether a partial observation of a running system meets an LTL or TLTL property.</p> <p>For LTL, a conceptually simple monitor generation procedure is given, which is <i>optimal</i> in two respects: First, the size of the generated deterministic monitor is <i>minimal</i>, and, second, the monitor identifies a continuously monitored trace as either satisfying or falsifying a property <i>as early as possible</i>. The feasibility of the developed methodology is demontrated using a collection of real-world temporal logic specifications. Moreover, the presented approach is related to the properties monitorable in general and is compared to existing concepts in the literature. It is shown that the set of <i>monitorable properties</i> does not only encompass the <i>safety</i> and <i>cosafety</i> properties but is strictly larger.</p> <p>For TLTL, the same road map is followed by first defining a three-valued semantics. The corresponding construction of a timed monitor is more involved, yet, as shown, possible.</p>",1049-331X;1049331X,,10.1145/2000799.2000800,,,Assertion checkers;monitors;runtime verification,,,,,40,,,,,,11-Sep,,ACM,ACM Journals & Magazines,,63
Impact-Driven Process Model Repair,A. Polyvyanyy; W. M. P. V. D. Aalst; A. H. M. T. Hofstede; M. T. Wynn,"Queensland University of Technology, Brisbane, Australia",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,4,1,60,"<p>The abundance of event data in todayäó»s information systems makes it possible to äóìconfrontäóù process models with the actual observed behavior. Process mining techniques use event logs to discover process models that describe the observed behavior, and to check conformance of process models by diagnosing deviations between models and reality. In many situations, it is desirable to mediate between a preexisting model and observed behavior. Hence, we would like to repair the model while improving the correspondence between model and log as much as possible. The approach presented in this article assigns predefined costs to repair actions (allowing inserting or skipping of activities). Given a maximum degree of change, we search for models that are optimal in terms of fitnessäóîthat is, the fraction of behavior in the log not possible according to the model is minimized. To compute fitness, we need to align the model and log, which can be time consuming. Hence, finding an optimal repair may be intractable. We propose different alternative approaches to speed up repair. The number of alignment computations can be reduced dramatically while still returning near-optimal repairs. The different approaches have been implemented using the process mining framework ProM and evaluated using real-life logs.</p>",1049-331X;1049331X,,10.1145/2980764,,,Process mining;event log;process model;process model repair;repair recommendation,,,,,1,,,,,,16-Oct,,ACM,ACM Journals & Magazines,,59
Bounded satisfiability checking of metric temporal logic specifications,M. Pradella; A. Morzenti; P. S. Pietro,"DEIB, Politecnico di Milano, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,54,"<p>We introduce bounded satisfiability checking, a verification technique that extends bounded model checking by allowing also the analysis of a <i>descriptive model</i>, consisting of temporal logic formulae, instead of the more customary <i>operational model</i>, consisting of a state transition system. We define techniques for encoding temporal logic formulae into Boolean logic that support the use of bi-infinite time domain and of metric time operators. In the framework of bounded satisfiability checking, we show how a descriptive model can be refined into an operational one, and how the correctness of such a refinement can be verified for the bounded case, setting the stage for a stepwise system development method based on a bounded model refinement. Finally, we show how the adoption of a modular approach can make the bounded refinement process more manageable and efficient. All introduced concepts are extensively applied to a set of case studies, and thoroughly experimented through Zot, our SAT solver-based verification toolset.</p>",1049-331X;1049331X,,10.1145/2491509.2491514,,,Formal methods;bi-infinite time;bounded model checking;refinement;temporal logic,,,,,5,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,53
Multi-Criteria Code Refactoring Using Search-Based Software Engineering: An Industrial Case Study,A. Ouni; M. Kessentini; H. Sahraoui; K. Inoue; K. Deb,Osaka University,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,3,1,53,"<p>One of the most widely used techniques to improve the quality of existing software systems is refactoringäóîthe process of improving the design of existing code by changing its internal structure without altering its external behavior. While it is important to suggest refactorings that improve the quality and structure of the system, many other criteria are also important to consider, such as reducing the number of code changes, preserving the semantics of the software design and not only its behavior, and maintaining consistency with the previously applied refactorings. In this article, we propose a multi-objective search-based approach for automating the recommendation of refactorings. The process aims at finding the optimal sequence of refactorings that (i) improves the quality by minimizing the number of design defects, (ii) minimizes code changes required to fix those defects, (iii) preserves design semantics, and (iv) maximizes the consistency with the previously code changes. We evaluated the efficiency of our approach using a benchmark of six open-source systems, 11 different types of refactorings (move method, move field, pull up method, pull up field, push down method, push down field, inline class, move class, extract class, extract method, and extract interface) and six commonly occurring design defect types (blob, spaghetti code, functional decomposition, data class, shotgun surgery, and feature envy) through an empirical study conducted with experts. In addition, we performed an industrial validation of our technique, with 10 software engineers, on a large project provided by our industrial partner. We found that the proposed refactorings succeed in preserving the design coherence of the code, with an acceptable level of code change score while reusing knowledge from recorded refactorings applied in the past to similar contexts.</p>",1049-331X;1049331X,,10.1145/2932631,,,Search-based software engineering;multi-objective optimization;refactoring;software evolution;software maintenance,,,,,1,,,,,,16-Aug,,ACM,ACM Journals & Magazines,,52
aToucan: An Automated Framework to Derive UML Analysis Models from Use Case Models,T. Yue; L. C. Briand; Y. Labiche,"Simula Research Laboratory and University of Oslo, Olso, Norway",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,3,1,52,"<p>The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model comprising class, sequence and activity diagrams from a use case model and to automatically establish traceability links between model elements of the use case model and the generated analysis model. Note that our goal is to save effort through automated support, not to replace human abstraction and decision making.</p> <p>Seven (six) case studies were performed to compare class (sequence) diagrams generated by aToucan to the ones created by experts, Masters students, and trained, fourth-year undergraduate students. Results show that aToucan performs well regarding consistency (e.g., 88&percnt; class diagram consistency) and completeness (e.g., 80&percnt; class completeness) when comparing generated class diagrams with reference class diagrams created by experts and Masters students. Similarly, sequence diagrams automatically generated by aToucan are highly consistent with the ones devised by experts and are also rather complete, for instance, 91&percnt; and 97&percnt; message consistency and complete- ess, respectively. Further, statistical tests show that aToucan significantly outperforms fourth-year engineering students in this respect, thus demonstrating the value of automation. We also conducted two industrial case studies demonstrating the applicability of aToucan in two different industrial domains. Results showed that the vast majority of model elements generated by aToucan are correct and that therefore, in practice, such models would be good initial models to refine and augment so as to converge towards to correct and complete analysis models. A performance analysis shows that the execution time of aToucan (when generating class and sequence diagrams) is dependent on the number of simple sentences contained in the use case model and remains within a range of a few minutes. Five different software system descriptions (18 use cases altogether) were performed to evaluate the generation of activity diagrams. Results show that aToucan can generate 100&percnt; complete and correct control flow information of activity diagrams and on average 85&percnt; data flAow information completeness. Moreover, we show that aToucan outperforms three commercial tools in terms of activity diagram generation.</p>",1049-331X;1049331X,,10.1145/2699697,,,UML;Use case modeling;activity diagram;analysis model;automation;class diagram;sequence diagram;traceability;transformation,,,,,5,,,,,,15-May,,ACM,ACM Journals & Magazines,,51
An in-depth study of the potentially confounding effect of class size in fault prediction,Y. Zhou; B. Xu; H. Leung; L. Chen,Nanjing University,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,51,"<p><i>Background</i>. The extent of the potentially confounding effect of class size in the fault prediction context is not clear, nor is the method to remove the potentially confounding effect, or the influence of this removal on the performance of fault-proneness prediction models. <i>Objective</i>. We aim to provide an in-depth understanding of the effect of class size on the true associations between object-oriented metrics and fault-proneness. <i>Method</i>. We first employ statistical methods to examine the extent of the potentially confounding effect of class size in the fault prediction context. After that, we propose a linear regression-based method to remove the potentially confounding effect. Finally, we empirically investigate whether this removal could improve the prediction performance of fault-proneness prediction models. <i>Results</i>. Based on open-source software systems, we found: (a) the confounding effect of class size on the associations between object-oriented metrics and fault-proneness in general exists; (b) the proposed linear regression-based method can effectively remove the confounding effect; and (c) after removing the confounding effect, the prediction performance of fault prediction models with respect to both ranking and classification can in general be significantly improved. <i>Conclusion</i>. We should remove the confounding effect of class size when building fault prediction models.</p>",1049-331X;1049331X,,10.1145/2556777,,,Metrics;class size;confounding effect;fault;prediction,,,,,7,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,50
ADAM: External dependency-driven architecture discovery and analysis of quality attributes,D. Ganesan; M. Lindvall,"Fraunhofer Center for Experimental Software Engineering, College Park",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,51,"<p>This article introduces the Architecture Discovery and Analysis Method (ADAM). ADAM supports the discovery of module and runtime views as well as the analysis of quality attributes, such as testability, performance, and maintainability, of software systems. The premise of ADAM is that the implementation constructs, architecture constructs, concerns, and quality attributes are all influenced by the external entities (e.g., libraries, frameworks, COTS software) used by the system under analysis. The analysis uses such external dependencies to identify, classify, and review a minimal set of key source-code files supported by a knowledge base of the external entities. Given the benefits of analyzing external dependencies as a way to discover architectures and potential risks, it is demonstrated that dependencies to external entities are useful not only for architecture discovery but also for analysis of quality attributes. ADAM is evaluated using the NASA's Space Network Access System (SNAS). The results show that this method offers systematic guidelines for discovering the architecture and locating potential risks (e.g., low testability and decreased performance) that are hidden deep inside the system implementation. Some generally applicable lessons for developers and analysts, as well as threats to validity are also discussed.</p>",1049-331X;1049331X,,10.1145/2529998,,,Concerns;external entities;knowledge base;maintainability;module and runtime views;quality;reverse engineering;software architecture;testability,,,,,,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,50
D<scp>ia</scp>P<scp>ro</scp>: Unifying Dynamic Impact Analyses for Improved and Variable Cost-Effectiveness,H. Cai; R. Santelices; D. Thain,"University of Notre Dame, Notre Dame, IN",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,2,1,50,"<p>Impact analysis not only assists developers with change planning and management, but also facilitates a range of other client analyses, such as testing and debugging. In particular, for developers working in the context of specific program executions, <i>dynamic</i> impact analysis is usually more desirable than static approaches, as it produces more manageable and relevant results with respect to those concrete executions. However, existing techniques for this analysis mostly lie on two extremes: either fast, but too imprecise, or more precise, yet overly expensive. In practice, both more cost-effective techniques and variable cost-effectiveness trade-offs are in demand to fit a variety of usage scenarios and budgets of impact analysis.</p> <p>This article aims to fill the gap between these two extremes with an array of cost-effective analyses and, more broadly, to explore the cost and effectiveness dimensions in the <i>design space</i> of impact analysis. We present the development and evaluation of D<scp>ia</scp>P<scp>ro</scp>, a framework that unifies a series of impact analyses, including <i>three new hybrid techniques</i> that combine static and dynamic analyses. Harnessing both static dependencies and multiple forms of dynamic data including method-execution events, statement coverage, and dynamic points-to sets, D<scp>ia</scp>P<scp>ro</scp> prunes false-positive impacts with varying strength for variant effectiveness and overheads. The framework also facilitates an in-depth examination of the effects of various program information on the cost-effectiveness of impact analysis.</p> <p>We applied D<scp>ia</scp>P<scp>ro</scp> to ten Java applications in diverse scales and domains, evaluating it thoroughly on both arbitrary and repository-based queries from those applications. We show that the three new analyses are all significantly more effective than existing alternatives while remaining efficient, and the D<scp>ia</scp>P<scp>ro</scp> framework, as a whole- provides flexible cost-effectiveness choices for impact analysis with the best options for variable needs and budgets. Our study results also suggest that hybrid techniques tend to be much more cost-effective than purely dynamic approaches, in general, and that statement coverage has mostly stronger effects than dynamic points-to sets on the cost-effectiveness of dynamic impact analysis, while static dependencies have even stronger effects than both forms of dynamic data.</p>",1049-331X;1049331X,,10.1145/2894751,,,Impact analysis;cost-effectiveness;coverage;dependence analysis;points-to,,,,,,,,,,,16-May,,ACM,ACM Journals & Magazines,,49
Scalable Runtime Bloat Detection Using Abstract Dynamic Slicing,G. Xu; N. Mitchell; M. Arnold; A. Rountev; E. Schonberg; G. Sevitsky,"University of California, Irvine, CA",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,3,1,50,"<p>Many large-scale Java applications suffer from runtime bloat. They execute large volumes of methods and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent improvement, performance experts analyze deployed applications and regularly find gains of 2í„ or more. Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Our experience shows that, in order to identify large performance bottlenecks in a program, it is more important to understand its dynamic dataflow than traditional performance metrics, such as running time.</p> <p>This article presents a general framework for designing and implementing scalable analysis algorithms to find causes of bloat in Java programs. At the heart of this framework is a generalized form of runtime dependence graph computed by <i>abstract dynamic slicing</i>, a semantics-aware technique that achieves high scalability by performing dynamic slicing over bounded abstract domains. The framework is instantiated to create two independent dynamic analyses, <i>copy profiling</i> and <i>cost-benefit analysis</i>, that help programmers identify performance bottlenecks by identifying, respectively, high-volume copy activities and data structures that have high construction cost but low benefit for the forward execution.</p> <p>We have successfully applied these analyses to large-scale and long-running Java applications. We show that both analyses are effective at detecting inefficient operations that can be optimized for better performance. We also demonstrate that the general framework is flexible enough to be instantiated for dynamic analyses in a- variety of application domains.</p>",1049-331X;1049331X,,10.1145/2560047,,,Runtime bloat;abstract dynamic slicing;copy profiling;cost-benefit analysis;dynamic analysis,,,,,3,,,,,,14-May,,ACM,ACM Journals & Magazines,,49
Conditional Commitments: Reasoning and Model Checking,W. E. Kholy; J. Bentahar; M. E. Menshawy; H. Qu; R. Dssouli,"Concordia University, Quebec, Canada",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,2,1,49,"<p>While modeling interactions using social commitments provides a fundamental basis for capturing flexible and declarative interactions and helps in addressing the challenge of ensuring compliance with specifications, the designers of the system cannot guarantee that an agent complies with its commitments as it is supposed to, or at least an agent doesn't want to violate its commitments. They may still wish to develop efficient and scalable algorithms by which model checking conditional commitments, a natural and universal frame of social commitments, is feasible at design time. However, distinguishing between different but related types of conditional commitments, and developing dedicated algorithms to tackle the problem of model checking conditional commitments, is still an active research topic. In this article, we develop the temporal logic CTL<sup><i>cc</i></sup> that extends Computation Tree Logic (CTL) with new modalities which allow representing and reasoning about two types of communicating conditional commitments and their fulfillments using the formalism of interpreted systems. We introduce a set of rules to reason about conditional commitments and their fulfillments. The verification technique is based on developing a new symbolic model checking algorithm to address this verification problem. We analyze the computational complexity and present the full implementation of the developed algorithm on top of the MCMAS model checker. We also evaluate the algorithm's effectiveness and scalability by verifying the compliance of the NetBill protocol, taken from the business domain, and the process of breast cancer diagnosis and treatment, taken from the health-care domain, with specifications expressed in CTL<sup><i>cc</i></sup>. We finally compare the experimental results with existing proposals.</p>",1049-331X;1049331X,,10.1145/2685613,,,Reasoning rules;strong and weak conditional commitments,,,,,1,,,,,,14-Dec,,ACM,ACM Journals & Magazines,,48
The Effectiveness of Test Coverage Criteria for Relational Database Schema Integrity Constraints,P. Mcminn; C. J. Wright; G. M. Kapfhammer,"University of Sheffield, Sheffield, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,49,,1049-331X;1049331X,,10.1145/2818639,,,Software testing;automatic test data generation;coverage criteria;integrity constraints;mutation analysis;relational database schemas;schema testing;search-based software engineering,,,,,5,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,48
Does Automated Unit Test Generation Really Help Software Testers? A Controlled Empirical Study,G. Fraser; M. Staats; P. McMinn; A. Arcuri; F. Padberg,"University of Sheffield, Sheffield, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,4,1,49,"<p>Work on automated test generation has produced several tools capable of generating test data which achieves high structural coverage over a program. In the absence of a specification, developers are expected to manually construct or verify the test oracle for each test input. Nevertheless, it is assumed that these generated tests ease the task of testing for the developer, as testing is reduced to checking the results of tests. While this assumption has persisted for decades, there has been no conclusive evidence to date confirming it. However, the limited adoption in industry indicates this assumption may not be correct, and calls into question the practical value of test generation tools. To investigate this issue, we performed two controlled experiments comparing a total of 97 subjects split between writing tests manually and writing tests with the aid of an automated unit test generation tool, E<scp>vo</scp>S<scp>uite</scp>. We found that, on one hand, tool support leads to clear improvements in commonly applied quality metrics such as code coverage (up to 300&percnt; increase). However, on the other hand, there was no measurable improvement in the number of bugs actually found by developers. Our results not only cast some doubt on how the research community evaluates test generation tools, but also point to improvements and future work necessary before automated test generation tools will be widely adopted by practitioners.</p>",1049-331X;1049331X,,10.1145/2699688,,,Unit testing;automated test generation;branch coverage;empirical software engineering,,,,,7,,,,,,15-Aug,,ACM,ACM Journals & Magazines,,48
Feasibility of Stepwise Design of Multitolerant Programs,A. Ebnenasir; S. S. Kulkarni,Michigan Technological University,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,21,1,1,49,"<p>The complexity of designing programs that simultaneously tolerate multiple classes of faults, called <i>multitolerant</i> programs, is in part due to the conflicting nature of the fault tolerance requirements that must be met by a multitolerant program when different types of faults occur. To facilitate the design of multitolerant programs, we present sound and (deterministically) complete algorithms for stepwise design of two families of multitolerant programs in a high atomicity program model, where a process can read and write all program variables in an atomic step. We illustrate that if one needs to design failsafe (respectively, nonmasking) fault tolerance for one class of faults and masking fault tolerance for another class of faults, then a multitolerant program can be designed in separate polynomial-time (in the state space of the fault-intolerant program) steps regardless of the order of addition. This result has a significant methodological implication in that designers need not be concerned about unknown fault tolerance requirements that may arise due to unanticipated types of faults. Further, we illustrate that if one needs to design failsafe fault tolerance for one class of faults and nonmasking fault tolerance for a different class of faults, then the resulting problem is NP-complete in program state space. This is a counterintuitive result in that designing failsafe and nonmasking fault tolerance for the same class of faults can be done in polynomial time. We also present sufficient conditions for polynomial-time design of <i>failsafe-nonmasking</i> multitolerance. Finally, we demonstrate the stepwise design of multitolerance for a stable disk storage system, a token ring network protocol and a repetitive agreement protocol that tolerates Byzantine and transient faults. Our automatic approach decreases the design time from days to a few hours for the token ring program that is our largest example with 200 million reachable states and 8 processes.- /p>",1049-331X;1049331X,,10.1145/2063239.2063240,,,Automatic addition of fault tolerance;multitolerance,,,,,3,,,,,,11-Dec,,ACM,ACM Journals & Magazines,,48
Control Explicit--Data Symbolic Model Checking,P. Bauch; V. Havel; J. Barnat,"Masaryk University, Brno, Botanicka, Czech Republic",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,2,1,48,"<p>Automatic verification of programs and computer systems with data nondeterminism (e.g., reading from user input) represents a significant and well-motivated challenge. The case of parallel programs is especially difficult, because then also the control flow nontrivially complicates the verification process. We apply the techniques of explicit-state model checking to account for the control aspects of a program to be verified and use <i>set-based reduction</i> of the data flow, thus handling the two sources of nondeterminism separately. We build the theory of set-based reduction using first-order formulae in the bit-vector theory to encode the sets of variable evaluations representing program data. These representations are tested for emptiness and equality (state matching) during the verification, and we harness modern satisfiability modulo theory solvers to implement these tests.</p> <p>We design two methods of implementing the state matching, one using quantifiers and one that is quantifier-free, and we provide both analytical and experimental comparisons. Further experiments evaluate the efficiency of the set-based reduction method, showing the classical, explicit approach to fail to scale with the size of data domains. Finally, we propose and evaluate two heuristics to decrease the number of expensive satisfiability queries, together yielding a 10-fold speedup.</p>",1049-331X;1049331X,,10.1145/2888393,,,Model checking;modular arithmetic;static analysis,,,,,,,,,,,16-May,,ACM,ACM Journals & Magazines,,47
Less is More: Estimating Probabilistic Rewards over Partial System Explorations,E. Pavese; V. Braberman; S. Uchitel,"Departamento de Computaci&#243;n, Universidad de Buenos Aires, CABA, Argentina",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,2,1,47,"<p>Model-based reliability estimation of systems can provide useful insights early in the development process. However, computational complexity of estimating metrics such as mean time to first failure (MTTFF), turnaround time (TAT), or other domain-based quantitative measures can be prohibitive both in time, space, and precision. In this article, we present an alternative to exhaustive model exploration, as in probabilistic model checking, and partial random exploration, as in statistical model checking. Our hypothesis is that a (carefully crafted) partial systematic exploration of a system model can provide better bounds for these quantitative model metrics at lower computation cost. We present a novel automated technique for metric estimation that combines simulation, invariant inference, and probabilistic model checking. Simulation produces a probabilistically relevant set of traces from which a state invariant is inferred. The invariant characterises a partial model, which is then exhaustively explored using probabilistic model checking. We report on experiments that suggest that metric estimation using this technique (for both fully probabilistic models and those exhibiting nondeterminism) can be more effective than (full-model) probabilistic and statistical model checking, especially for system models for which the events of interest are rare.</p>",1049-331X;1049331X,,10.1145/2890494,,,Quantitative modelling;estimation;model checking;partial verification;probability,,,,,,,,,,,16-May,,ACM,ACM Journals & Magazines,,46
A framework for the checking and refactoring of crosscutting concepts,M. Shonle; W. G. Griswold; S. Lerner,University of Texas at San Antonio,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,3,1,47,"<p>Programmers employ crosscutting concepts, such as design patterns and other programming idioms, when their design ideas cannot be efficiently or effectively modularized in the underlying programming language. As a result, implementations of these crosscutting concepts can be hard to change even when the code is well structured.</p> <p>In this article, we describe Arcum, a system that supports the modular maintenance of crosscutting concepts. Arcum can be used to both check essential constraints of crosscutting concepts and to substitute crosscutting concept implementations with alternative implementations. Arcum is complementary to existing refactoring systems that focus on meaning-preserving program transformations at the programming-language-semantics level, because Arcum focuses on transformations at the conceptual level.</p> <p>We present the underpinnings of the Arcum approach and show how Arcum can be used to address several classical software engineering problems.</p>",1049-331X;1049331X,,10.1145/2211616.2211618,,,Design patterns;refactoring,,,,,,,,,,,12-Jun,,ACM,ACM Journals & Magazines,,46
Weak Alphabet Merging of Partial Behavior Models,D. Fischbein; N. Däó»Ippolito; G. Brunet; M. Chechik; S. Uchitel,Imperial College London,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2012,21,2,1,47,"<p>Constructing comprehensive operational models of intended system behavior is a complex and costly task, which can be mitigated by the construction of partial behavior models, providing early feedback and subsequently elaborating them iteratively. However, how should partial behavior models with different viewpoints covering different aspects of behavior be composed? How should partial models of component instances of the same type be put together? In this article, we propose <i>model merging</i> of modal transition systems (MTSs) as a solution to these questions. MTS models are a natural extension of labelled transition systems that support explicit modeling of what is currently unknown about system behavior. We formally define model merging based on weak alphabet refinement, which guarantees property preservation, and show that merging consistent models is a process that should result in a minimal common weak alphabet refinement (MCR). In this article, we provide theoretical results and algorithms that support such a process. Finally, because in practice MTS merging is likely to be combined with other operations over MTSs such as parallel composition, we also study the algebraic properties of merging and apply these, together with the algorithms that support MTS merging, in a case study.</p>",1049-331X;1049331X,,10.1145/2089116.2089119,,,MTS;merge;partial behavior models,,,,,4,,,,,,12-Mar,,ACM,ACM Journals & Magazines,,46
When and How to Use Multilevel Modelling,J. D. Lara; E. Guerra; J. S. Cuadrado,"Universidad Aut&#243;noma de Madrid, Madrid, Spain",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,2,1,46,"<p>Model-Driven Engineering (MDE) promotes models as the primary artefacts in the software development process, from which code for the final application is derived. Standard approaches to MDE (like those based on MOF or EMF) advocate a two-level metamodelling setting where Domain-Specific Modelling Languages (DSMLs) are defined through a metamodel that is instantiated to build models at the metalevel below.</p> <p><i>Multilevel modelling</i> (also called <i>deep metamodelling</i>) extends the standard approach to metamodelling by enabling modelling at an arbitrary number of metalevels, not necessarily two. Proposers of multilevel modelling claim this leads to simpler model descriptions in some situations, although its applicability has been scarcely evaluated. Thus, practitioners may find it difficult to discern when to use it and how to implement multilevel solutions in practice.</p> <p>In this article, we discuss those situations where the use of multilevel modelling is beneficial, and identify recurring patterns and idioms. Moreover, in order to assess how often the identified patterns arise in practice, we have analysed a wide range of existing two-level DSMLs from different sources and domains, to detect when their elements could be rearranged in more than two metalevels. The results show this scenario is not uncommon, while in some application domains (like software architecture and enterprise/process modelling) pervasive, with a high average number of pattern occurrences per metamodel.</p>",1049-331X;1049331X,,10.1145/2685615,,,Model-driven engineering;domain-specific modelling languages;metamodelling;metamodelling patterns;multilevel modelling,,,,,4,,,,,,14-Dec,,ACM,ACM Journals & Magazines,,45
A logical verification methodology for service-oriented computing,A. Fantechi; S. Gnesi; A. Lapadula; F. Mazzanti; R. Pugliese; F. Tiezzi,"Universit&#224; degli Studi di Firenze, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,3,1,46,"<p>We introduce a logical verification methodology for checking behavioral properties of service-oriented computing systems. Service properties are described by means of SocL, a branching-time temporal logic that we have specifically designed for expressing in an effective way distinctive aspects of services, such as, acceptance of a request, provision of a response, correlation among service requests and responses, etc. Our approach allows service properties to be expressed in such a way that they can be independent of service domains and specifications. We show an instantiation of our general methodology that uses the formal language COWS to conveniently specify services and the expressly developed software tool CMC to assist the user in the task of verifying SocL formulas over service specifications. We demonstrate the feasibility and effectiveness of our methodology by means of the specification and analysis of a case study in the automotive domain.</p>",1049-331X;1049331X,,10.1145/2211616.2211619,,,Service-oriented computing;Web services;formal methods;model checking;process calculi;temporal logic,,,,,,,,,,,12-Jun,,ACM,ACM Journals & Magazines,,45
Enabledness-based program abstractions for behavior validation,G. D. Caso; V. Braberman; D. Garbervetsky; S. Uchitel,Universidad de Buenos Aires,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,46,"<p>Code artifacts that have nontrivial requirements with respect to the ordering in which their methods or procedures ought to be called are common and appear, for instance, in the form of API implementations and objects. This work addresses the problem of validating if API implementations provide their intended behavior when descriptions of this behavior are informal, partial, or nonexistent. The proposed approach addresses this problem by generating abstract behavior models which resemble typestates. These models are statically computed and encode all admissible sequences of method calls. The level of abstraction at which such models are constructed has shown to be useful for validating code artifacts and identifying findings which led to the discovery of bugs, adjustment of the requirements expected by the engineer to the requirements implicit in the code, and the improvement of available documentation.</p>",1049-331X;1049331X,,10.1145/2491509.2491519,,,Source-code validation;enabledness abstractions,,,,,1,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,45
Trading obliviousness for modularity with cooperative aspect-oriented programming,K. Hoffman; P. Eugster,"Purdue University, West Lafayette, IN",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,46,"<p>The potential of aspect-oriented programming to adequately capture crosscutting concerns has yet to be fully realized. For example, authors have detailed significant challenges in creating reusable aspect component libraries. One proposed solution is to introduce Explicit Join Points (EJPs) to increase modularity by reducing obliviousness, enabling a <i>Cooperative</i> Aspect-Oriented Programming (Co-AOP) methodology where base code and aspects synergistically collaborate.</p> <p>This article explores the trade-offs between obliviousness and modularity. We briefly introduce EJPs and Co-AOP, and hypothesize how to balance obliviousness and modularity using Co-AOP. We build upon a prior empirical study to refactor three real-life Java applications to implement the exception handling concern using three distinct strategies: (1) using fully oblivious aspects in AspectJ, (2) using EJPs in a fully explicit fashion, and (3) using EJPs while following the Co-AOP methodology. We study other crosscutting concerns by refactoring a fourth application, JHotDraw. The differences in terms of common code metrics are analyzed, and the impact on modularity is assessed using design structure matrices. Results indicate that the Co-AOP methodology can in many cases significantly improve code quality attributes versus fully oblivious or fully explicit approaches. We conclude with guiding principles on the proper use of EJPs within the Co-AOP methodology.</p>",1049-331X;1049331X,,10.1145/2491509.2491516,,,Aspect-oriented programming;design structure matrix;explicit join points;modularity;obliviousness,,,,,1,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,45
Predicting Query Quality for Applications of Text Retrieval to Software Engineering Tasks,C. Mills; G. Bavota; S. Haiduc; R. Oliveto; A. Marcus; A. D. Lucia,"Florida State University, FL, USA",ACM Transactions on Software Engineering and Methodology (TOSEM),20170907,2017,26,1,1,45,"<p><b>Context:</b> Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts.</p> <p><b>Aim:</b> We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself.</p> <p><b>Method:</b> We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery.</p> <p><b>Results:</b> For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using ve- y little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average.</p> <p><b>Conclusions:</b> The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR.</p>",1049-331X;1049331X,,10.1145/3078841,,,Text retrieval;artifact traceability;concept location,,,,,,,,,,,17-Jul,,ACM,ACM Journals & Magazines,,44
Many-Objective Software Remodularization Using NSGA-III,W. Mkaouer; M. Kessentini; A. Shaout; P. Koligheu; S. Bechikh; K. Deb; A. Ouni,University of Michigan,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,3,1,45,"<p>Software systems nowadays are complex and difficult to maintain due to continuous changes and bad design choices. To handle the complexity of systems, software products are, in general, decomposed in terms of packages/modules containing classes that are dependent. However, it is challenging to automatically remodularize systems to improve their maintainability. The majority of existing remodularization work mainly satisfy one objective which is improving the structure of packages by optimizing coupling and cohesion. In addition, most of existing studies are limited to only few operation types such as move class and split packages. Many other objectives, such as the design semantics, reducing the number of changes and maximizing the consistency with development change history, are important to improve the quality of the software by remodularizing it. In this article, we propose a novel many-objective search-based approach using NSGA-III. The process aims at finding the optimal remodularization solutions that improve the structure of packages, minimize the number of changes, preserve semantics coherence, and reuse the history of changes. We evaluate the efficiency of our approach using four different open-source systems and one automotive industry project, provided by our industrial partner, through a quantitative and qualitative study conducted with software engineers.</p>",1049-331X;1049331X,,10.1145/2729974,,,Search-based software engineering;remodularization;software maintenance;software quality,,,,,6,,,,,,15-May,,ACM,ACM Journals & Magazines,,44
Solving the Search for Source Code,K. T. Stolee; S. Elbaum; D. Dobos,"Iowa State University, Ames, IA",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,3,1,45,"<p>Programmers frequently search for source code to reuse using keyword searches. The search effectiveness in facilitating reuse, however, depends on the programmer's ability to specify a query that captures how the desired code may have been implemented. Further, the results often include many irrelevant matches that must be filtered manually. More semantic search approaches could address these limitations, yet existing approaches are either not flexible enough to find approximate matches or require the programmer to define complex specifications as queries.</p> <p>We propose a novel approach to semantic code search that addresses several of these limitations and is designed for queries that can be described using a concrete input/output example. In this approach, programmers write lightweight specifications as inputs and expected output examples. Unlike existing approaches to semantic search, we use an SMT solver to identify programs or program fragments in a repository, which have been automatically transformed into constraints using symbolic analysis, that match the programmer-provided specification.</p> <p>We instantiated and evaluated this approach in subsets of three languages, the Java String library, Yahoo&excl; Pipes mashup language, and SQL select statements, exploring its generality, utility, and trade-offs. The results indicate that this approach is effective at finding relevant code, can be used on its own or to filter results from keyword searches to increase search precision, and is adaptable to find <i>approximate</i> matches and then guide modifications to match the user specifications when <i>exact</i> matches do not already exist. These gains in precision and flexibility come at the cost of performance, for which underlying factors and mitigation strategies are identified.</p>",1049-331X;1049331X,,10.1145/2581377,,,SMT solvers;Semantic code search;lightweight specification;symbolic analysis,,,,,13,,,,,,14-May,,ACM,ACM Journals & Magazines,,44
Code-Smell Detection as a Bilevel Problem,D. Sahin; M. Kessentini; S. Bechikh; K. Deb,"University of Michigan, Dearborn",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,1,1,44,,1049-331X;1049331X,,10.1145/2675067,,,Search-based software engineering;code smells;software quality,,,,,2,,,,,,14-Sep,,ACM,ACM Journals & Magazines,,43
Hierarchical Program Paths,C. Yang; S. Wu; W. K. Chan,"City University of Hong Kong, Kowloon Tong, Hong Kong",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,3,1,44,"<p><i>Complete dynamic control flow</i> is a fundamental kind of execution profile about program executions with a wide range of applications. Tracing the dynamic control flow of program executions for a brief period easily generates a trace consisting of billions of control flow events. The number of events in such a trace is large, making both path tracing and path querying to incur significant slowdowns. A major class of path tracing techniques is to design novel trace representations that can be generated efficiently, and encode the inputted sequences of such events so that the inputted sequences are still derivable from the encoded and smaller representations. The control flow semantics in such representations have, however, become obscure, which makes implementing path queries on such a representation inefficient and the design of such queries complicated. We propose a novel two-phase path tracing frameworkäóî<i>Hierarchical Program Path</i> (HPP)äóîto model the <i>complete</i> dynamic control flow of an arbitrary number of executions of a program. In Phase 1, HPP monitors each execution, and efficiently generates a stream of events, namely HPPTree, representing a novel tree-based representation of control flow for each thread of control in the execution. In Phase 2, given a set of such event streams, HPP identifies all the equivalent instances of the same exercised <i>interprocedural</i> path in all the corresponding HPPTree instances, and represents each such equivalent set of paths with a single subgraph, resulting in our compositional graph-based trace representation, namely, HPPDAG. Thus, an HPPDAG instance has the potential to be significantly smaller in size than the corresponding HPPTree instances, and still completely preserves the control flow semantics of the traced executions. Control flow queries over all the traced executions can also be directly performed on the single HPPDAG instance instead of separately proces- ing the trace representation of each execution followed by aggregating their results. We validate HPP using the SPLASH2 and SPECint 2006 benchmarks. Compared to the existing technique, named BLPT (Ball-Larus-based Path Tracing), HPP generates significantly smaller trace representations and incurs fewer slowdowns to the native executions in online tracing of Phase 1. The HPPDAG instances generated in Phase 2 are significantly smaller than their corresponding BLPT and HPPTree traces. We show that HPPDAG supports efficient backtrace querying, which is a representative path query based on complete control flow trace. Finally, we illustrate the ease of use of HPPDAG by building a novel and highly efficient path profiling technique to demonstrate the applicability of HPPDAG.</p>",1049-331X;1049331X,,10.1145/2963094,,,Path tracing;hierarchical and compositional representation;interprocedural path,,,,,,,,,,,16-Aug,,ACM,ACM Journals & Magazines,,43
Systematizing pragmatic software reuse,R. Holmes; R. J. Walker,"University of Waterloo, Waterloo, Canada",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,4,1,44,"<p>Many software reuse tasks involve reusing source code that was not designed in a manner conducive to those tasks, requiring that ad hoc modifications be applied. Such <i>pragmatic</i> reuse tasks are a reality in <i>disciplined</i> industrial practice; they arise for a variety of organizational and technical reasons. To investigate a pragmatic reuse task, a developer must navigate through, and reason about, source code dependencies in order to identify program elements that are relevant to the task and to decide how those elements should be reused. The developer must then convert his mental model of the task into a set of actions that he can perform. These steps are poorly supported by modern development tools and practices.</p> <p>We provide a model for the process involved in performing a pragmatic reuse task, including the need to capture (mentally or otherwise) the developer's decisions about how each program element should be treated: this is a <i>pragmatic-reuse plan</i>. We provide partial support for this model via a tool suite, called Gilligan; other parts of the model are supported via standard IDE tools. Using a pragmatic-reuse plan, Gilligan can semiautomatically transform the selected source code from its originating system and integrate it into the developer's system.</p> <p>We have evaluated Gilligan through a series of case studies and experiments (each involving industrial developers) using a variety of source systems and tasks; we report in particular on a previously unpublished, formal experiment. The results show that pragmatic-reuse plans are a robust metaphor for capturing pragmatic reuse intent and that, relative to standard IDE tools, Gilligan can (1) significantly decrease the time that developers require to perform pragmatic reuse tasks, (2) increase the likelihood that developers will successfully complete pragmatic reuse tasks, (3) decrease the time required by developers to identify infeasible reuse tasks, and (4) improve developer- ' sense of their ability to manage the risk in such tasks.</p>",1049-331X;1049331X,,10.1145/2377656.2377657,,,Gilligan;Software reuse;lightweight process and tooling;low commitment;planning;pragmatic software reuse;pragmatic-reuse plan enactment;pragmatic-reuse plans;source code investigation;white box reuse,,,,,5,,,,,,12-Nov,,ACM,ACM Journals & Magazines,,43
Symbolic Message Sequence Charts,A. Roychoudhury; A. Goel; B. Sengupta,National University of Singapore,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2012,21,2,1,44,"<p>Message sequence charts (MSCs) are a widely used visual formalism for scenario-based specifications of distributed reactive systems. In its conventional usage, an MSC captures an interaction snippet between <i>concrete</i> objects in the system. This leads to voluminous specifications when the system contains several objects that are behaviorally similar. MSCs also play an important role in the model-based testing of reactive systems, where they may be used for specifying (partial) system behaviors, describing test generation criteria, or representing test cases. However, since the number of processes in a MSC specification are fixed, model-based testing of systems consisting of process classes may involve a significant amount of rework: for example, reconstructing system models, or regenerating test cases for systems differing only in the number of processes of various types.</p> <p>In this article we propose a scenario-based notation, called symbolic message sequence charts (SMSCs), for modeling, simulation, and testing of process classes. SMSCs are a lightweight syntactic and semantic extension of MSCs where, unlike MSCs, a SMSC lifeline can denote some/all objects from a collection. Our extensions give us substantially more modeling power. Moreover, we present an abstract execution semantics for (structured collections of) SMSCs. This allows us to validate MSC-based system models capturing interactions between large, or even unbounded, number of objects. Finally, we describe a SMSC-based testing methodology for process classes, which allows generation of test cases for new object configurations with minimal rework.</p> <p>Since our SMSC extensions are only concerned with MSC lifelines, we believe that they can be integrated into existing standards such as UML 2.0. We illustrate our SMSC-based framework for modeling, simulation, and testing of process classes using a weather-update controller case-study from NASA.</p>",1049-331X;1049331X,,10.1145/2089116.2089122,,,Message sequence charts;symbolic execution;test generation;unified modeling language (UML),,,,,1,,,,,,12-Mar,,ACM,ACM Journals & Magazines,,43
Fixing Faults in C and Java Source Code: Abbreviated vs. Full-Word Identifier Names,G. Scanniello; M. Risi; P. Tramontana; S. Romano,"University of Basilicata, Potenza, PZ, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20170907,2017,26,2,1,43,"<p>We carried out a family of controlled experiments to investigate whether the use of abbreviated identifier names, with respect to full-word identifier names, affects fault fixing in C and Java source code. This family consists of an original (or baseline) controlled experiment and three replications. We involved 100 participants with different backgrounds and experiences in total. Overall results suggested that there is no difference in terms of effort, effectiveness, and efficiency to fix faults, when source code contains either only abbreviated or only full-word identifier names. We also conducted a qualitative study to understand the values, beliefs, and assumptions that inform and shape fault fixing when identifier names are either abbreviated or full-word. We involved in this qualitative study six professional developers with 1--3 years of work experience. A number of insights emerged from this qualitative study and can be considered a useful complement to the quantitative results from our family of experiments. One of the most interesting insights is that developers, when working on source code with abbreviated identifier names, adopt a more methodical approach to identify and fix faults by extending their focus point and only in a few cases do they expand abbreviated identifiers.</p>",1049-331X;1049331X,,10.1145/3104029,,,Controlled experiments;ethnography study;family of experiments;maintenance;qualitative investigation;replications;software testing;source code identifiers,,,,,,,,,,,17-Jul,,ACM,ACM Journals & Magazines,,42
Software Change Contracts,J. Yi; D. Qi; S. H. Tan; A. Roychoudhury,"National University of Singapore, Singapore",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,3,1,43,"<p>Software errors often originate from incorrect changes, including incorrect program fixes, incorrect feature updates, and so on. Capturing the intended program behavior explicitly via contracts is thus an attractive proposition. In our recent work, we had espoused the notion of äóìchange contractsäóù to express the intended program behavior changes across program versions. Change contracts differ from program contracts in that they do not require the programmer to describe the intended behavior of those program features which are unchanged across program versions. In this work, we present the formal semantics of our change contract language built on top of the <i>Java modeling language</i> (JML). Our change contract language can describe behavioral as well as structural changes. We evaluate the expressivity of the change contract language via a survey given to final-year undergraduate students. The survey results enable to understand the usability of our change contract language for purposes of writing contracts, comprehending written contracts, and modifying programs according to given change contracts.</p> <p>Finally, we develop both dynamic and static checkers for change contracts, and show how they can be used in maintaining software changes. We use our dynamic checker to automatically suggest tests that manifest violations of change contracts. Meanwhile, we use our static checker to verify that a program is changed as specified in its change contract. Apart from verification, our static checker also performs various other software engineering tasks, such as localizing the buggy method, detecting/debugging regression errors, and classifying the cause for a test failure as either error in production code or error in test code.</p>",1049-331X;1049331X,,10.1145/2729973,,,Software changes;dynamic checking;static checking,,,,,,,,,,,15-May,,ACM,ACM Journals & Magazines,,42
Type-Based Call Graph Construction Algorithms for Scala,K. Ali; M. Rapoport; O. Lhotíçk; J. Dolby; F. Tip,"Technische Universit&#228;t Darmstadt, Darmstadt, Germany",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,43,"<p>Call graphs have many applications in software engineering. For example, they serve as the basis for code navigation features in integrated development environments and are at the foundation of static analyses performed in verification tools. While many call graph construction algorithms have been presented in the literature, we are not aware of any that handle Scala features such as traits and abstract type members. Applying existing algorithms to the JVM bytecodes generated by the Scala compiler produces very imprecise results because type information is lost during compilation. We adapt existing type-based call graph construction algorithms to Scala and present a formalization based on Featherweight Scala. An experimental evaluation shows that our most precise algorithm generates call graphs with 1.1--3.7 times fewer nodes and 1.5--17.3 times fewer edges than a bytecode-based RTA analysis.</p>",1049-331X;1049331X,,10.1145/2824234,,,Call graphs;Scala;static analysis,,,,,,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,42
Traceability and SysML design slices to support safety inspections: A controlled experiment,L. Briand; D. Falessi; S. Nejati; M. Sabetzadeh; T. Yue,University of Luxembourg,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,43,"<p>Certifying safety-critical software and ensuring its safety requires checking the conformance between safety requirements and design. Increasingly, the development of safety-critical software relies on modeling, and the System Modeling Language (SysML) is now commonly used in many industry sectors. Inspecting safety conformance by comparing design models against safety requirements requires safety inspectors to browse through large models and is consequently time consuming and error-prone. To address this, we have devised a mechanism to establish traceability between (functional) safety requirements and SysML design models to extract design slices (model fragments) that filter out irrelevant details but keep enough context information for the slices to be easy to inspect and understand. In this article, we report on a controlled experiment assessing the impact of the traceability and slicing mechanism on inspectors' conformance decisions and effort. Results show a significant decrease in effort and an increase in decisions' correctness and level of certainty.</p>",1049-331X;1049331X,,10.1145/2559978,,,Empirical software engineering;design;requirements specification;software and system safety;software/program verification,,,,,5,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,42
Test-and-adapt: An approach for improving service interchangeability,G. Denaro; M. PezzíÂ; D. Tosi,"University of Milano-Bicocca, Milano, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,43,"<p>Service-oriented applications do not fully benefit from standard APIs yet, and many applications fail to use interchangeably all the services that implement a standard service API. This article presents an approach to develop adaptation strategies that improve service interchangeability for service-oriented applications based on standard APIs. In our approach, an adaptation strategy consists of sets of parametric adaptation plans (called test-and-adapt plans), which execute test cases to reveal the occurrence of interchangeability problems, and activate runtime adaptors according to the test results. Throughout this article, we formalize the structure of the parametric test-and-adapt plans and of their execution semantics, present an algorithm for identifying correct execution orders through sets of test-and-adapt plans, provide empirical evidence of the occurrence of interchangeability problems for sample applications and services, and discuss the effectiveness of the approach in terms of avoided failures, runtime overheads and development costs.</p>",1049-331X;1049331X,,10.1145/2522920.2522921,,,Reliability of service-oriented architectures;dynamic adaption of service-oriented applications based on standard APIs,,,,,1,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,42
Accounting for defect characteristics in evaluations of testing techniques,J. Strecker; A. M. Memon,"College of Wooster, OH",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,3,1,43,"<p>As new software-testing techniques are developed, before they can achieve widespread acceptance, their effectiveness at detecting defects must be evaluated. The most common way of evaluating testing techniques is with empirical studies, in which one or more techniques are tried out on software with known defects. However, the defects used can affect the performance of the techniques. To complicate matters, it is not even clear how to effectively describe or characterize defects. To address these problems, this article describes an experiment architecture for empirically evaluating testing techniques which takes both defect and test-suite characteristics into account. As proof of concept, an experiment on GUI-testing techniques is conducted. It provides evidence that the defect characteristics proposed do help explain defect detection, at least for GUI testing, and it explores the relationship between the coverage of defective code and the detection of defects.</p>",1049-331X;1049331X,,10.1145/2211616.2211620,,,Defects;GUI testing;faults,,,,,5,1,,,,,12-Jun,,ACM,ACM Journals & Magazines,,42
Architecture-Level Configuration of Large-Scale Embedded Software Systems,R. Behjati; S. Nejati; L. C. Briand,"Simula Research Laboratory, Norway",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,3,1,43,"<p>Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50&percnt; of the configuration decisions, and reduces the complexity of making configuration decisions.</p>",1049-331X;1049331X,,10.1145/2581376,,,Model-based product-line engineering;UML/OCL;consistent configuration;constraint satisfaction techniques;formal specification;product configuration,,,,,1,,,,,,14-May,,ACM,ACM Journals & Magazines,,42
Lattice-Based Sampling for Path Property Monitoring,M. M. Diep; M. B. Dwyer; S. Elbaum,Fraunhofer Center for Experimental Software Engineering,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,21,1,1,43,"<p>Runtime monitoring can provide important insights about a programäó»s behavior and, for simple properties, it can be done efficiently. Monitoring properties describing sequences of program states and events, however, can result in significant runtime overhead. This is particularly critical when monitoring programs deployed at user sites that have low tolerance for overhead. In this paper we present a novel approach to reducing the cost of runtime monitoring of path properties. A set of original properties are composed to form a single integrated property that is then systematically decomposed into a set of properties that encode necessary conditions for property violations. The resulting set of properties forms a lattice whose structure is exploited to select a sample of properties that can lower monitoring cost, while preserving violation detection power relative to the original properties. The lattice is then complemented with a weighting scheme that assigns each property a different priority that can be adjusted continuously to better drive the property sampling process. Our evaluation using the Hibernate API reveals that our approach produces a rich, structured set of properties that enables control of monitoring overhead, while detecting more violations more quickly than alternative techniques.</p>",1049-331X;1049331X,,10.1145/2063239.2063244,,,Runtime monitoring;deployed analysis;sequencing and path properties,,,,,0,1,,,,,11-Dec,,ACM,ACM Journals & Magazines,,42
A Large-Scale Evaluation of Automated Unit Test Generation Using EvoSuite,G. Fraser; A. Arcuri,"University of Sheffield, Sheffield, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,2,1,42,"<p>Research on software testing produces many innovative automated techniques, but because software testing is by necessity incomplete and approximate, any new technique faces the challenge of an empirical assessment. In the past, we have demonstrated scientific advance in automated unit test generation with the E<scp>VO</scp>S<scp>UITE</scp> tool by evaluating it on manually selected open-source projects or examples that represent a particular problem addressed by the underlying technique. However, demonstrating scientific advance is not necessarily the same as demonstrating practical value; even if <scp>VO</scp>S<scp>UITE</scp> worked well on the software projects we selected for evaluation, it might not scale up to the complexity of real systems. Ideally, one would use large äóìreal-worldäóù software systems to minimize the threats to external validity when evaluating research tools. However, neither choosing such software systems nor applying research prototypes to them are trivial tasks.</p> <p>In this article we present the results of a large experiment in unit test generation using the <scp>VO</scp>S<scp>UITE</scp> tool on 100 randomly chosen open-source projects, the 10 most popular open-source projects according to the SourceForge Web site, seven industrial projects, and 11 automatically generated software projects. The study confirms that <scp>VO</scp>S<scp>UITE</scp> can achieve good levels of branch coverage (on average, 71&percnt; per class) in practice. However, the study also exemplifies how the choice of software systems for an empirical study can influence the results of the experiments, which can serve to inform researchers to make more conscious choices in the selection of software system subjects. Furthermore, our experiments demonstrate how practical limitations interfere with scientific advances, branch coverage on an unbiased sample is affected by predominant environmental dependencies. - he surprisingly large effect of such practical engineering problems in unit testing will hopefully lead to a larger appreciation of work in this area, thus supporting transfer of knowledge from software testing research to practice.</p>",1049-331X;1049331X,,10.1145/2685612,,,JUnit;Java;Unit testing;automated test generation;benchmark;branch coverage;empirical software engineering,,,,,14,,,,,,14-Dec,,ACM,ACM Journals & Magazines,,41
Directed Incremental Symbolic Execution,G. Yang; S. Person; N. Rungta; S. Khurshid,"Texas State University, San Marcos, TX",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,1,1,42,"<p>The last few years have seen a resurgence of interest in the use of symbolic executionäóîa program analysis technique developed more than three decades ago to analyze program execution paths. Scaling symbolic execution to real systems remains challenging despite recent algorithmic and technological advances. An effective approach to address scalability is to <i>reduce</i> the scope of the analysis. For example, in regression analysis, <i>differences</i> between two related program versions are used to guide the analysis. While such an approach is intuitive, finding efficient and precise ways to identify program differences, and characterize their impact on how the program executes has proved challenging in practice.</p> <p>In this article, we present <i>Directed Incremental Symbolic Execution</i> (DiSE), a novel technique for detecting and characterizing the impact of program changes to scale symbolic execution. The novelty of DiSE is to combine the <i>efficiencies</i> of static analysis techniques to compute program difference information with the <i>precision</i> of symbolic execution to explore program execution paths and generate path conditions affected by the differences. DiSE complements other reduction and bounding techniques for improving symbolic execution. Furthermore, DiSE does not require analysis results to be carried forward as the software evolvesäóîonly the source code for two related program versions is required. An experimental evaluation using our implementation of DiSE illustrates its effectiveness at detecting and characterizing the effects of program changes.</p>",1049-331X;1049331X,,10.1145/2629536,,,Program differencing;software evolution;symbolic execution,,,,,4,,,,,,14-Sep,,ACM,ACM Journals & Magazines,,41
Automatic Workarounds: Exploiting the Intrinsic Redundancy of Web Applications,A. Carzaniga; A. Gorla; N. Perino; M. PezzíÂ,"University of Lugano, Switzerland",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,3,1,42,"<p>Despite the best intentions, the competence, and the rigorous methods of designers and developers, software is often delivered and deployed with faults. To cope with imperfect software, researchers have proposed the concept of <i>self-healing</i> for software systems. The ambitious goal is to create software systems capable of detecting and responding äóìautonomicallyäóù to functional failures, or perhaps even preempting such failures, to maintain a correct functionality, possibly with acceptable degradation. We believe that self-healing can only be an expression of some form of redundancy, meaning that, to automatically fix a faulty behavior, the correct behavior must be already present somewhere, in some form, within the software system either explicitly or implicitly. One approach is to <i>deliberately</i> design and develop redundant systems, and in fact this kind of deliberate redundancy is the essential ingredient of many fault tolerance techniques. However, this type of redundancy is also generally expensive and does not always satisfy the time and cost constraints of many software projects.</p> <p>With this article we take a different approach. We observe that modern software systems naturally acquire another type of redundancy that is not introduced deliberately but rather arises <i>intrinsically</i> as a by-product of modern modular software design. We formulate this notion of intrinsic redundancy and we propose a technique to exploit it to achieve some level of self-healing. We first demonstrate that software systems are indeed intrinsically redundant. Then we develop a way to express and exploit this redundancy to tolerate faults with <i>automatic workarounds.</i> In essence, a workaround amounts to replacing some failing operations with alternative operations that are semantically equivalent in their intended effect, but that execute different code and ultimately avoid the failure. The technique we propose finds such - orkarounds automatically. We develop this technique in the context of Web applications. In particular, we implement this technique within a browser extension, which we then use in an evaluation with several known faults and failures of three popular Web libraries. The evaluation demonstrates that automatic workarounds are effective: out of the nearly 150 real faults we analyzed, 100 could be overcome with automatic workarounds, and half of these workarounds found automatically were not publicly known before.</p>",1049-331X;1049331X,,10.1145/2755970,,,Automatic workarounds;Web API;Web applications,,,,,,,,,,,15-May,,ACM,ACM Journals & Magazines,,41
Achieving scalable model-based testing through test case diversity,H. Hemmati; A. Arcuri; L. Briand,"Simula Research Laboratory, Norway and University of Oslo, Norway",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,1,1,42,"<p>The increase in size and complexity of modern software systems requires scalable, systematic, and automated testing approaches. Model-based testing (MBT), as a systematic and automated test case generation technique, is being successfully applied to verify industrial-scale systems and is supported by commercial tools. However, scalability is still an open issue for large systems, as in practice there are limits to the amount of testing that can be performed in industrial contexts. Even with standard coverage criteria, the resulting test suites generated by MBT techniques can be very large and expensive to execute, especially for system level testing on real deployment platforms and network facilities. Therefore, a scalable MBT technique should be flexible regarding the size of the generated test suites and should be easily accommodated to fit resource and time constraints. Our approach is to select a subset of the generated test suite in such a way that it can be realistically executed and analyzed within the time and resource constraints, while preserving the fault revealing power of the original test suite to a maximum extent. In this article, to address this problem, we introduce a family of similarity-based test case selection techniques for test suites generated from state machines. We evaluate 320 different similarity-based selection techniques and then compare the effectiveness of the best similarity-based selection technique with other common selection techniques in the literature. The results based on two industrial case studies, in the domain of embedded systems, show significant benefits and a large improvement in performance when using a similarity-based approach. We complement these analyses with further studies on the scalability of the technique and the effects of failure rate on its effectiveness. We also propose a method to identify optimal tradeoffs between the number of test cases to run and fault detection.</p>",1049-331X;1049331X,,10.1145/2430536.2430540,,,Test case selection;model-based testing;search-based software engineering;similarity function;test case minimization,,,,,27,,,,,,13-Feb,,ACM,ACM Journals & Magazines,,41
Degree-of-knowledge: Modeling a developer's knowledge of code,T. Fritz; G. C. Murphy; E. Murphy-Hill; J. Ou; E. Hill,University of Zurich,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,42,"<p>As a software system evolves, the system's codebase constantly changes, making it difficult for developers to answer such questions as who is knowledgeable about particular parts of the code or who needs to know about changes made. In this article, we show that an externalized model of a developer's individual knowledge of code can make it easier for developers to answer such questions. We introduce a degree-of-knowledge model that computes automatically, for each source-code element in a codebase, a real value that represents a developer's knowledge of that element based on a developer's authorship and interaction data. We present evidence that shows that both authorship and interaction data of the code are important in characterizing a developer's knowledge of code. We report on the usage of our model in case studies on expert finding, knowledge transfer, and identifying changes of interest. We show that our model improves upon an existing expertise-finding approach and can accurately identify changes for which a developer should likely be aware. We discuss how our model may provide a starting point for knowledge transfer but that more refinement is needed. Finally, we discuss the robustness of the model across multiple development sites.</p>",1049-331X;1049331X,,10.1145/2512207,,,Authorship;degree-of-interest;degree-of-knowledge;development environment;expertise;onboarding;recommendation,,,,,6,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,41
Business Process Model Merging: An Approach to Business Process Consolidation,M. La Rosa; M. Dumas; R. Uba; R. Dijkman,"Queensland University of Technology and NICTA, Australia",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,2,1,42,"<p>This article addresses the problem of constructing consolidated business process models out of collections of process models that share common fragments. The article considers the construction of unions of multiple models (called <i>merged models</i>) as well as intersections (called <i>digests</i>). Merged models are intended for analysts who wish to create a model that subsumes a collection of process models -- typically representing variants of the same underlying process -- with the aim of replacing the variants with the merged model. Digests, on the other hand, are intended for analysts who wish to identify the most recurring fragments across a collection of process models, so that they can focus their efforts on optimizing these fragments. The article presents an algorithm for computing merged models and an algorithm for extracting digests from a merged model. The merging and digest extraction algorithms have been implemented and tested against collections of process models taken from multiple application domains. The tests show that the merging algorithm produces compact models and scales up to process models containing hundreds of nodes. Furthermore, a case study conducted in a large insurance company has demonstrated the usefulness of the merging and digest extraction operators in a practical setting.</p>",1049-331X;1049331X,,10.1145/2430545.2430547,,,Business process model;graph matching;model merging;variability,,,,,5,,,,,,13-Mar,,ACM,ACM Journals & Magazines,,41
Finite satisfiability of UML class diagrams with constrained class hierarchy,M. Balaban; A. Maraee,Ben-Gurion University of the Negev,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,42,"<p>Models lie at the heart of the emerging model-driven engineering approach. In order to guarantee precise, consistent, and correct models, there is a need for efficient powerful methods for verifying model correctness. Class diagram is the central language within UML. Its correctness problems involve issues of contradiction, namely the <i>consistency</i> problem, and issues of finite instantiation, namely the <i>finite satisfiability</i> problem.</p> <p>This article analyzes the problem of finite satisfiability of class diagrams with class hierarchy constraints and generalization-set constraints. The article introduces the <i>FiniteSat</i> algorithm for efficient detection of finite satisfiability in such class diagrams, and analyzes its limitations in terms of complex hierarchy structures. FiniteSat is strengthened in two directions. First, an algorithm for identification of the cause for a finite satisfiability problem is introduced. Second, a method for propagation of generalization-set constraints in a class diagram is introduced. The propagation method serves as a preprocessing step that improves FiniteSat performance, and helps developers in clarifying intended constraints. These algorithms are implemented in the <i>FiniteSatUSE</i> tool [BGU Modeling Group 2011b], as part of our ongoing effort for constructing a model-level integrated development environment [BGU Modeling Group 2010a].</p>",1049-331X;1049331X,,10.1145/2491509.2491518,,,Class hierarchy constraints;UML class diagram;class hierarchy structure;detection and cause identification;finite satisfiability;generalization set constraints;identification graph;multiplicity constraints;solvability of linear inequality system,,,,,2,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,41
Model-based synthesis of control software from system-level formal specifications,F. Mari; I. Melatti; I. Salvo; E. Tronci,"Sapienza University of Rome, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,42,"<p>Many <i>embedded systems</i> are indeed <i>software-based control systems</i>, that is, control systems whose controller consists of <i>control software</i> running on a microcontroller device. This motivates investigation on <i>formal model-based design</i> approaches for automatic synthesis of embedded systems control software. We present an algorithm, along with a tool QKS implementing it, that from a formal model (as a <i>discrete-time linear hybrid system</i>) of the controlled system (<i>plant</i>), <i>implementation specifications</i> (that is, number of bits in the <i>Analog-to-Digital</i>, AD, conversion) and <i>system-level formal specifications</i> (that is, safety and liveness requirements for the <i>closed loop system</i>) returns correct-by-construction control software that has a <i>Worst-Case Execution Time</i> (WCET) linear in the number of AD bits and meets the given specifications. We show feasibility of our approach by presenting experimental results on using it to synthesize control software for a buck DC-DC converter, a widely used mixed-mode analog circuit, and for the inverted pendulum.</p>",1049-331X;1049331X,,10.1145/2559934,,,Hybrid systems;correct-by-construction control software synthesis;model-based design of control software,,,,,6,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,41
PrIMe: A methodology for developing provenance-aware applications,S. Miles; P. Groth; S. Munroe; L. Moreau,"King's College London, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,3,1,42,"<p>Provenance refers to the past processes that brought about a given (version of an) object, item or entity. By knowing the provenance of data, users can often better understand, trust, reproduce, and validate it. A provenance-aware application has the functionality to answer questions regarding the provenance of the data it produces, by using documentation of past processes. PrIMe is a software engineering technique for adapting application designs to enable them to interact with a provenance middleware layer, thereby making them provenance-aware. In this article, we specify the steps involved in applying PrIMe, analyze its effectiveness, and illustrate its use with two case studies, in bioinformatics and medicine.</p>",1049-331X;1049331X,,10.1145/2000791.2000792,,,Methodology;Provenance,,,,,12,,,,,,11-Aug,,ACM,ACM Journals & Magazines,,41
Test Case Prioritization Using Extended Digraphs,S. S. Emam; J. Miller,"University of Alberta, Canada",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,41,"<p>Although many test case prioritization techniques exist, their performance is far from perfect. Hence, we propose a new fault-based test case prioritization technique to promote fault-revealing test cases in <i>model-based testing</i> (MBT) procedures. We seek to improve the fault detection rateäóîa measure of how fast a test suite is able to detect faults during testingäóîin scenarios such as regression testing. We propose an extended digraph model as the basis of this new technique. The model is realized using a novel <i>reinforcement-learning</i> (RL)- and hidden-Markov-model (HMM)-based technique which is able to prioritize test cases for regression testing objectives. We present a method to initialize and train an HMM based upon RL concepts applied to an application's digraph model. The model prioritizes test cases based upon forward probabilities, a new test case prioritization approach. In addition, we also propose an alternative approach to prioritizing test cases according to the amount of change they cause in applications. To evaluate the effectiveness of the proposed techniques, we perform experiments on <i>graphical user interface</i> (GUI)-based applications and compare the results with state-of-the-art test case prioritization approaches. The experimental results show that the proposed technique is able to detect faults early within test runs.</p>",1049-331X;1049331X,,10.1145/2789209,,,Fault-based test case prioritization;GUI testing;HMM;additional statement coverage;model-based testing (MBT);random prioritization;reinforcement learning,,,,,1,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,40
"An Information Foraging Theory Perspective on Tools for Debugging, Refactoring, and Reuse Tasks",S. D. Fleming; C. Scaffidi; D. Piorkowski; M. Burnett; R. Bellamy; J. Lawrance; I. Kwan,Oregon State University and University of Memphis,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,2,1,41,"<p>Theories of human behavior are an important but largely untapped resource for software engineering research. They facilitate understanding of human developersäó» needs and activities, and thus can serve as a valuable resource to researchers designing software engineering tools. Furthermore, theories abstract beyond specific methods and tools to fundamental principles that can be applied to new situations. Toward filling this gap, we investigate the applicability and utility of Information Foraging Theory (IFT) for understanding information-intensive software engineering tasks, drawing upon literature in three areas: debugging, refactoring, and reuse. In particular, we focus on software engineering tools that aim to support information-intensive activities, that is, activities in which developers spend time seeking information. Regarding applicability, we consider whether and how the mathematical equations within IFT can be used to explain why certain existing tools have proven empirically successful at helping software engineers. Regarding utility, we applied an IFT perspective to identify recurring <i>design patterns</i> in these successful tools, and consider what opportunities for future research are revealed by our IFT perspective.</p>",1049-331X;1049331X,,10.1145/2430545.2430551,,,Information foraging;software maintenance,,,,,13,,,,,,13-Mar,,ACM,ACM Journals & Magazines,,40
Path exploration based on symbolic output,D. Qi; H. D. T. Nguyen; A. Roychoudhury,"National University of Singapore, Singapore",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,41,"<p>Efficient program path exploration is important for many software engineering activities such as testing, debugging, and verification. However, enumerating all paths of a program is prohibitively expensive. In this article, we develop a partitioning of program paths based on the program output. Two program paths are placed in the same partition if they derive the output similarly, that is, the symbolic expression connecting the output with the inputs is the same in both paths. Our grouping of paths is gradually created by a smart path exploration. Our experiments show the benefits of the proposed path exploration in test-suite construction.</p> <p>Our path partitioning produces a semantic signature of a programäóîdescribing all the different symbolic expressions that the output can assume along different program paths. To reason about changes between program versions, we can therefore analyze their semantic signatures. In particular, we demonstrate the applications of our path partitioning in testing and debugging of software regressions.</p>",1049-331X;1049331X,,10.1145/2522920.2522925,,,Software testing;software evolution;symbolic execution,,,,,2,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,40
Join point interfaces for safe and flexible decoupling of aspects,E. Bodden; í_. Tanter; M. Inostroza,"Technische Universit&#228;t Darmstadt, Germany",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,41,"<p>In current aspect-oriented systems, aspects usually carry, through their pointcuts, explicit references to the base code. Those references are fragile and hinder important software engineering properties such as modular reasoning and independent evolution of aspects and base code. In this work, we introduce a novel abstraction called Join Point Interface, which, by design, aids modular reasoning and independent evolution by decoupling aspects from base code and by providing a modular type-checking algorithm. Join point interfaces can be used both with implicit announcement through pointcuts, and with explicit announcement, using closure join points. Join point interfaces further offer polymorphic dispatch on join points, with an advice-dispatch semantics akin to multimethods. To support flexible join point matching, we incorporate into our language an earlier proposal for generic advice, and introduce a mechanism for controlled global quantification. We motivate each language feature in detail, showing that it is necessary to obtain a language design that is both type safe and flexible enough to support typical aspect-oriented programming idioms. We have implemented join point interfaces as an open-source extension to AspectJ. A case study on existing aspect-oriented programs supports our design, and in particular shows the necessity of both generic interfaces and some mechanism for global quantification.</p>",1049-331X;1049331X,,10.1145/2559933,,,Aspect-oriented programming;advice dispatch;explicit announcement;implicit announcement;interfaces;join point polymorphism;modularity;typing,,,,,2,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,40
A Compiler for Multimodal Scenarios: Transforming LSCs into AspectJ,S. Maoz; D. Harel; A. Kleinbort,The Weizmann Institute of Science,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,4,1,41,"<p>We exploit the main similarity between the aspect-oriented programming paradigm and the inter-object, scenario-based approach to specification, in order to construct a new way of executing systems based on the latter. Specifically, we transform multimodal scenario-based specifications, given in the visual language of <i>live sequence charts</i> (LSC), into what we call <i>scenario aspects</i>, implemented in AspectJ. Unlike synthesis approaches, which attempt to take the inter-object scenarios and construct intra-object state-based per-object specifications or a single controller automaton, we follow the ideas behind the LSC play-out algorithm to coordinate the simultaneous monitoring and direct execution of the specified scenarios. Thus, the structure of the specification is reflected in the structure of the generated code; the high-level inter-object requirements and their structure are not lost in the translation.</p> <p>The transformation/compilation scheme is fully implemented in a UML2-compliant tool we term the <i>S2A compiler</i> (for Scenarios to Aspects), which provides full code generation of reactive behavior from inter-object multimodal scenarios. S2A supports advanced scenario-based programming features, such as multiple instances and exact and symbolic parameters. We demonstrate our work with an application whose inter-object behaviors are specified using LSCs. We discuss advantages and challenges of the compilation scheme in the context of the more general vision of scenario-based programming.</p>",1049-331X;1049331X,,10.1145/2000799.2000804,,,Aspect oriented programming;UML sequence diagrams;code generation;inter-object approach;live sequence charts;scenario-based programming;scenarios;visual formalisms,,,,,2,1,,,,,11-Sep,,ACM,ACM Journals & Magazines,,40
Verification and Validation of UML Conceptual Schemas with OCL Constraints,A. Queralt; E. Teniente,Universitat Polit&#232;cnica de Catalunya - BarcelonaTech,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2012,21,2,1,41,"<p>To ensure the quality of an information system, it is essential that the conceptual schema that represents the knowledge about its domain is semantically correct. The semantic correctness of a conceptual schema can be seen from two different perspectives. On the one hand, from the point of view of its definition, a conceptual schema must be <i>right</i>. This is ensured by means of verification techniques that check whether the schema satisfies several correctness properties. On the other hand, from the point of view of the requirements that the information system should satisfy, a schema must also be <i>the right one</i>. This is ensured by means of validation techniques, which help the designer understand the exact meaning of a schema and to see whether it corresponds to the requirements. In this article we propose an approach to verify and validate UML conceptual schemas, with arbitrary constraints formalized in OCL. We have also implemented our approach to show its feasibility.</p>",1049-331X;1049331X,,10.1145/2089116.2089123,,,Conceptual modeling;OCL;constraints,,,,,4,,,,,,12-Mar,,ACM,ACM Journals & Magazines,,40
Augmenting Field Data for Testing Systems Subject to Incremental Requirements Changes,D. Di Nardo; F. Pastore; L. Briand,"University of Luxembourg, Luxembourg",ACM Transactions on Software Engineering and Methodology (TOSEM),20170615,2017,26,1,1,40,"<p>When testing data processing systems, software engineers often use real-world data to perform system-level testing. However, in the presence of new data requirements, software engineers may no longer benefit from having real-world data with which to perform testing. Typically, new test inputs complying with the new requirements have to be manually written.</p> <p>We propose an automated model-based approach that combines data modelling and constraint solving to modify existing field data to generate test inputs for testing new data requirements. The approach scales in the presence of complex and structured data, thanks to both the reuse of existing field data and the adoption of an innovative input generation algorithm based on slicing the model into parts.</p> <p>We validated the scalability and effectiveness of the proposed approach using an industrial case study. The empirical study shows that the approach scales in the presence of large amounts of structured and complex data. The approach can produce, within a reasonable time, test input data that is over ten times larger in size than the data generated with constraint solving only. We also demonstrate that the generated test inputs achieve more code coverage than the test cases implemented by experienced software engineers.</p>",1049-331X;1049331X,,10.1145/3053430,,,Alloy;Data Processing Systems;Model Slicing;System Testing,,,,,,,,,,,17-Jul,,ACM,ACM Journals & Magazines,,39
A Continuous ASM Modelling Approach to Pacemaker Sensing,R. Banach; H. Zhu; W. Su; X. Wu,"University of Manchester, Manchester, U.K.",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,1,1,40,"<p>The cardiac pacemaker system, proposed as a problem topic in the Verification Grand Challenge, offers a range of difficulties to address for formal specification, development, and verification technologies. We focus on the sensing problem, the question of whether the heart has produced a spontaneous heartbeat or not. This question is plagued by uncertainties arising from the often unpredictable environment that a real pacemaker finds itself in. We develop a time domain tracking approach to this problem, as a complement to the usual frequency domain approach most frequently used. We develop our case study in the continuous ASM (Abstract State Machine) formalism, which is briefly summarised, through a series of refinement and retrenchment steps, each adding new levels of complexity to the model.</p>",1049-331X;1049331X,,10.1145/2610375,,,Continuous ASM;cardiac pacemakers;rigorous design and development;sensing,,,,,,,,,,,14-Sep,,ACM,ACM Journals & Magazines,,39
Automated cookie collection testing,A. F. Tappenden; J. Miller,"The King's University College, Edmonton, AB, Canada",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,40,"<p>Cookies are used by over 80&percnt; of Web applications utilizing dynamic Web application frameworks. Applications deploying cookies must be rigorously verified to ensure that the application is robust and secure. Given the intense time-to-market pressures faced by modern Web applications, testing strategies that are low cost and automatable are required. Automated Cookie Collection Testing (CCT) is presented, and is empirically demonstrated to be a low-cost and highly effective automated testing solution for modern Web applications. Automatable test oracles and evaluation metrics specifically designed for Web applications are presented, and are shown to be significant diagnostic tests. Automated CCT is shown to detect faults within five real-world Web applications. A case study of over 580 test results for a single application is presented demonstrating that automated CCT is an effective testing strategy. Moreover, CCT is found to detect security bugs in a Web application released into full production.</p>",1049-331X;1049331X,,10.1145/2559936,,,Cookies;Web application testing;adaptive random testing;automated testing;software testing;test generation;test strategies,,,,,1,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,39
Exception handlers for healing component-based systems,H. Chang; L. Mariani; M. PezzíÂ,"University of Milano Bicocca, Milano, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,40,"<p>To design effective exception handlers, developers must predict at design time the exceptional events that may occur at runtime, and must implement the corresponding handlers on the basis of their predictions. Designing exception handlers for component-based software systems is particularly difficult because the information required to build handlers is distributed between component and application developers. Component developers know the internal details of the components but ignore the applications, while application developers own the applications but cannot access the details required to implement handlers in components.</p> <p>This article addresses the problem of automatically healing the infield failures that are caused by faulty integration of OTS components. In the article, we propose a technique and a methodology to decouple the tasks of component and application developers, who will be able to share information asynchronously and independently, and communicate implicitly by developing and deploying what we call healing connectors. Component developers implement healing connectors on the basis of information about the integration problems frequently experienced by application developers. Application developers easily and safely install healing connectors in their applications without knowing the internal details of the connectors. Healing connectors heal failures activated by exceptions raised in the OTS components actually deployed in the system.</p> <p>The article defines healing connectors, introduces a methodology to develop and deploy healing connectors, and presents several case studies that indicate that healing connectors are effective, reusable and efficient.</p>",1049-331X;1049331X,,10.1145/2522920.2522923,,,COTS components;Exception handling in component-based software systems;component-based software engineering;healing connectors;healing patterns;self-healing,,,,,3,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,39
Architecture-centric support for adaptive service collaborations,R. Haesevoets; D. Weyns; T. Holvoet,"IBBT-distrinet, katholieke universiteit leuven, Belgium",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,40,"<p>In today's volatile business environments, collaboration between information systems, both within and across company borders, has become essential to success. An efficient supply chain, for example, requires the collaboration of distributed and heterogeneous systems of multiple companies. Developing such collaborative applications and building the supporting information systems poses several engineering challenges. A key challenge is to manage the ever-growing design complexity. In this article, we argue that software architecture should play a more prominent role in the development of collaborative applications. This can help to better manage design complexity by modularizing collaborations and separating concerns. State-of-the-art solutions, however, often lack proper abstractions for modeling collaborations at architectural level or do not reify these abstractions at detailed design and implementation level. Developers, on the other hand, rely on middleware, business process management, and Web services, techniques that mainly focus on low-level infrastructure.</p> <p>To address the problem of managing the design complexity of collaborative applications, we present Macodo. Macodo consists of three complementary parts: (1) a set of abstractions for modeling adaptive collaborations, (2) a set of architectural views, the main contribution of this article, that reify these abstractions at architectural level, and (3) a proof-of-concept middleware infrastructure that supports the architectural abstractions at design and implementation level. We evaluate the architectural views in a controlled experiment. Results show that the use of Macodo can reduce fault density and design complexity, and improve reuse and productivity. The main contributions of this article are illustrated in a supply chain management case.</p>",1049-331X;1049331X,,10.1145/2559937,,,Service-oriented architecture (SOA);Web services;agent organizations;architectural views;collaborative systems;empirical evaluation;middleware;role-based modeling;software architecture,,,,,4,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,39
A theoretical analysis of the risk evaluation formulas for spectrum-based fault localization,X. Xie; T. Y. Chen; F. C. Kuo; B. Xu,"Swinburne University of Technology, Australia",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,40,"<p>An important research area of Spectrum-Based Fault Localization (SBFL) is the effectiveness of risk evaluation formulas. Most previous studies have adopted an empirical approach, which can hardly be considered as sufficiently comprehensive because of the huge number of combinations of various factors in SBFL. Though some studies aimed at overcoming the limitations of the empirical approach, none of them has provided a completely satisfactory solution. Therefore, we provide a theoretical investigation on the effectiveness of risk evaluation formulas. We define two types of relations between formulas, namely, equivalent and better. To identify the relations between formulas, we develop an innovative framework for the theoretical investigation. Our framework is based on the concept that the determinant for the effectiveness of a formula is the number of statements with risk values higher than the risk value of the faulty statement. We group all program statements into three disjoint sets with risk values higher than, equal to, and lower than the risk value of the faulty statement, respectively. For different formulas, the sizes of their sets are compared using the notion of subset. We use this framework to identify the maximal formulas which should be the only formulas to be used in SBFL.</p>",1049-331X;1049331X,,10.1145/2522920.2522924,,,Debugging;risk evaluation formulas;spectrum-based fault localization;testing,,,,,44,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,39
SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization,R. M. Hierons; M. Li; X. Liu; S. Segura; W. Zheng,"Brunel University London, Middlesex, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,2,1,39,"<p>A feature model specifies the sets of features that define valid products in a software product line. Recent work has considered the problem of choosing optimal products from a feature model based on a set of user preferences, with this being represented as a many-objective optimization problem. This problem has been found to be difficult for a purely search-based approach, leading to classical many-objective optimization algorithms being enhanced either by adding in a valid product as a seed or by introducing additional mutation and replacement operators that use an SAT solver. In this article, we instead enhance the search in two ways: by providing a novel representation and by optimizing first on the number of constraints that hold and only then on the other objectives. In the evaluation, we also used feature models with realistic attributes, in contrast to previous work that used randomly generated attribute values. The results of experiments were promising, with the proposed (SIP) method returning valid products with six published feature models and a randomly generated feature model with 10,000 features. For the model with 10,000 features, the search took only a few minutes.</p>",1049-331X;1049331X,,10.1145/2897760,,,Product selection,,,,,3,,,,,,16-May,,ACM,ACM Journals & Magazines,,38
Learning Weighted Assumptions for Compositional Verification of Markov Decision Processes,F. He; X. Gao; M. Wang; B. Y. Wang; L. Zhang,"KLiss, MoE; TNList; School of Software, Tsinghua University, Beijing, China",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,3,1,39,"<p>Probabilistic models are widely deployed in various systems. To ensure their correctness, verification techniques have been developed to analyze probabilistic systems. We propose the first sound and complete learning-based compositional verification technique for probabilistic safety properties on concurrent systems where each component is an Markov decision process. Different from previous works, weighted assumptions are introduced to attain completeness of our framework. Since weighted assumptions can be implicitly represented by multiterminal binary decision diagrams (MTBDDs), we give an >i<L>/i<*-based learning algorithm for MTBDDs to infer weighted assumptions. Experimental results suggest promising outlooks for our compositional technique.</p>",1049-331X;1049331X,,10.1145/2907943,,,Compositional verification;algorithmic learning;probabilistic model checking,,,,,,,,,,,16-Aug,,ACM,ACM Journals & Magazines,,38
Some Code Smells Have a Significant but Small Effect on Faults,T. Hall; M. Zhang; D. Bowes; Y. Sun,Brunel University,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,23,4,1,39,"<p>We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness.</p>",1049-331X;1049331X,,10.1145/2629648,,,Software code smells;defects,,,,,5,,,,,,14-Aug,,ACM,ACM Journals & Magazines,,38
Type checking annotation-based product lines,C. Kí_stner; S. Apel; T. Thí_m; G. Saake,"Philipps University Marburg, Germany",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,3,1,39,"<p>Software product line engineering is an efficient means of generating a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ensure properties like type safety for the entire product line. We present a product-line-aware type system that can type check an entire software product line without generating each variant in isolation. Specifically, we extend the Featherweight Java calculus with feature annotations for product-line development and prove formally that all program variants generated from a well typed product line are well typed. Furthermore, we present a solution to the problem of typing mutually exclusive features. We discuss how results from our formalization helped implement our own product-line tool CIDE for full Java and report of our experience with detecting type errors in four existing software product line implementations.</p>",1049-331X;1049331X,,10.1145/2211616.2211617,,,#ifdef;CFJ;CIDE;Featherweight Java;conditional compilation;software product lines;type system,,,,,15,,,,,,12-Jun,,ACM,ACM Journals & Magazines,,38
Assessing the Effect of Screen Mockups on the Comprehension of Functional Requirements,F. Ricca; G. Scanniello; M. Torchiano; G. Reggio; E. Astesiano,"DIBRIS, University of Genova, Genova, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,1,1,38,"<p>Over the last few years, the software engineering community has proposed a number of modeling methods to represent functional requirements. Among them, use cases are recognized as an easy to use and intuitive way to capture and define such requirements. Screen mockups (also called user-interface sketches or user interface-mockups) have been proposed as a complement to use cases for improving the comprehension of functional requirements. In this article, we aim at quantifying the benefits achievable by augmenting use cases with screen mockups in the comprehension of functional requirements with respect to effectiveness, effort, and efficiency. For this purpose, we conducted a family of four controlled experiments, involving 139 participants having different profiles. The experiments involved comprehension tasks performed on the requirements documents of two desktop applications. Independently from the participants' profile, we found a statistically significant large effect of the presence of screen mockups on both comprehension effectiveness and comprehension task efficiency. No significant effect was observed on the effort to complete tasks. The main pragmatic lesson is that the screen mockups addition to use cases is able to almost double the efficiency of comprehension tasks.</p>",1049-331X;1049331X,,10.1145/2629457,,,Screen mockups;analysis models;controlled experiment;family of experiments;replicated experiments;use cases,,,,,3,,,,,,14-Sep,,ACM,ACM Journals & Magazines,,37
A Stack Memory Abstraction and Symbolic Analysis Framework for Executables,K. Anand; K. Elwazeer; A. Kotha; M. Smithson; R. Barua; A. Keromytis,"University of Maryland, College Park",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,2,1,38,"<p>This article makes three contributions regarding reverse-engineering of executables. First, techniques are presented for recovering a precise and correct stack-memory model in executables while addressing executable-specific challenges such as indirect control transfers. Next, the enhanced memory model is employed to define a novel symbolic analysis framework for executables that can perform the same types of program analyses as source-level tools. Third, a demand-driven framework is presented to enhance the scalability of the symbolic analysis framework. Existing symbolic analysis frameworks for executables fail to simultaneously maintain the properties of correct representation, a precise stack-memory model, and scalability. Furthermore, they ignore memory-allocated variables when defining symbolic analysis mechanisms. Our methods do not use symbolic, relocation or debug information, which are usually absent in deployed binaries. We describe our framework, highlighting the novel intellectual contributions of our approach and demonstrating its efficacy and robustness. Our techniques improve the precision of existing stack-memory models by 25&percnt;, enhance scalability of our basic symbolic analysis mechanism by 10í„, and successfully uncovers five previously undiscovered information-flow vulnerabilities in several widely used programs.</p>",1049-331X;1049331X,,10.1145/2897511,,,Executable code;information-flow security;program analysis,,,,,,,,,,,16-May,,ACM,ACM Journals & Magazines,,37
Understanding JavaScript Event-Based Interactions with Clematis,S. Alimadadi; S. Sequeira; A. Mesbah; K. Pattabiraman,"University of British Columbia, BC, Canada",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,2,1,38,"<p>Web applications have become one of the fastest-growing types of software systems today. Despite their popularity, understanding the behavior of modern web applications is still a challenging endeavor for developers during development and maintenance tasks. The challenges mainly stem from the dynamic, event-driven, and asynchronous nature of the JavaScript language. We propose a generic technique for capturing low-level event-based interactions in a web application and mapping those to a higher-level behavioral model. This model is then transformed into an interactive visualization, representing episodes of triggered causal and temporal events, related JavaScript code executions, and their impact on the dynamic DOM state. Our approach, implemented in a tool called C<scp>lematis</scp>, allows developers to easily understand the complex dynamic behavior of their application at three different semantic levels of granularity. Furthermore, C<scp>lematis</scp> helps developers bridge the gap between test cases and program code by localizing the fault related to a test assertion. The results of our industrial controlled experiment show that C<scp>lematis</scp> is capable of improving the comprehension task accuracy by 157&percnt; while reducing the task completion time by 47&percnt;. A follow-up experiment reveals that C<scp>lematis</scp> improves the fault localization accuracy of developers by a factor of two.</p>",1049-331X;1049331X,,10.1145/2876441,,,JavaScript;Program comprehension;event-based interactions;fault localization;web applications,,,,,,,,,,,16-May,,ACM,ACM Journals & Magazines,,37
Do Automatically Generated Test Cases Make Debugging Easier? An Experimental Assessment of Debugging Effectiveness and Efficiency,M. Ceccato; A. Marchetto; L. Mariani; C. D. Nguyen; P. Tonella,"Fondazione Bruno Kessler, Trento, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,38,"<p>Several techniques and tools have been proposed for the automatic generation of test cases. Usually, these tools are evaluated in terms of fault-revealing or coverage capability, but their impact on the manual debugging activity is not considered. The question is whether automatically generated test cases are equally effective in supporting debugging as manually written tests.</p> <p>We conducted a family of three experiments (five replications) with humans (in total, 55 subjects) to assess whether the features of automatically generated test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on the effectiveness and efficiency of debugging. The first two experiments compare different test case generation tools (Randoop vs. EvoSuite). The third experiment investigates the role of code identifiers in test cases (obfuscated vs. original identifiers), since a major difference between manual and automatically generated test cases is that the latter contain meaningless (obfuscated) identifiers.</p> <p>We show that automatically generated test cases are as useful for debugging as manual test cases. Furthermore, we find that, for less experienced developers, automatic tests are more useful on average due to their lower static and dynamic complexity.</p>",1049-331X;1049331X,,10.1145/2768829,,,Empirical software engineering;automatic test case generation;debugging,,,,,3,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,37
Facilitating the transition from use case models to analysis models: Approach and experiments,T. Yue; L. C. Briand; Y. Labiche,"Simula Research Laboratory, Olso, Norway",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,1,1,38,"<p>Use case modeling, including use case diagrams and use case specifications (UCSs), is commonly applied to structure and document requirements. UCSs are usually structured but unrestricted textual documents complying with a certain use case template. However, because Use Case Models (UCMods) remain essentially textual, ambiguity is inevitably introduced. In this article, we propose a use case modeling approach, called Restricted Use Case Modeling (RUCM), which is composed of a set of well-defined restriction rules and a modified use case template. The goal is two-fold: (1) restrict the way users can document UCSs in order to reduce ambiguity and (2) facilitate the manual derivation of initial analysis models which, when using the Unified Modeling Language (UML), are typically composed of class diagrams, sequence diagrams, and possibly other types of diagrams.</p> <p>Though the proposed restriction rules and template are based on a clear rationale, two main questions need to be investigated. First, do users find them too restrictive or impractical in certain situations&quest; In other words, can users express the same requirements with RUCM as with unrestricted use cases&quest; Second, do the rules and template have a positive, significant impact on the quality of the constructed analysis models&quest; To investigate these questions, we performed and report on two controlled experiments, which evaluate the restriction rules and use case template in terms of (1) whether they are easy to apply while developing UCMods and facilitate the understanding of UCSs, and (2) whether they help users manually derive higher quality analysis models than what can be generated when they are not used, in terms of correctness, completeness, and redundancy. This article reports on the first controlled experiments that evaluate the applicability of restriction rules on use case modeling and their impact on the quality of analysis models. The measures we have defined to characterize re- triction rules and the quality of analysis class and sequence diagrams can be reused to perform similar experiments in the future, either with RUCM or other approaches.</p> <p>Results show that the restriction rules are overall easy to apply and that RUCM results into significant improvements over traditional approaches (i.e., with standard templates, without restrictions) in terms of class correctness and class diagram completeness, message correctness and sequence diagram completeness, and understandability of UCSs.</p>",1049-331X;1049331X,,10.1145/2430536.2430539,,,Use case;analysis model;class diagram;controlled experiment;restriction rules;sequence diagram;use case modeling;use case template,,,,,9,,,,,,13-Feb,,ACM,ACM Journals & Magazines,,37
Generating Test Cases for Programs that Are Coded against Interfaces and Annotations,M. Islam; C. Csallner,University of Texas at Arlington,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,3,1,38,"<p>Automatic test case generation for software programs is very powerful but suffers from a key limitation. That is, most current test case generation techniques fail to cover testee code when covering that code requires additional pieces of code not yet part of the program under test. To address some of these cases, the Pex state-of-the-art test case generator can generate basic mock code. However, current test case generators cannot handle cases in which the code under test uses multiple interfaces, annotations, or reflection.</p> <p>To cover such code in an object-oriented setting, we describe a novel technique for generating test cases and mock classes. The technique consists of collecting constraints on interfaces, annotations, and reflection, combining them with program constraints collected during dynamic symbolic execution, encoding them in a constraint system, solving them with an off-the-shelf constraint solver, and mapping constraint solutions to test cases and custom mock classes. We demonstrate the value of this technique on open-source applications. Our approach covered such third-party code with generated mock classes, while competing approaches failed to cover the code and sometimes produced unintended side-effects such as filling the screen with dialog boxes and writing into the file system.</p>",1049-331X;1049331X,,10.1145/2544135,,,Dynamic symbolic execution;mock classes;stubs;test case generation,,,,,2,,,,,,14-May,,ACM,ACM Journals & Magazines,,37
A taxonomy for requirements engineering and software test alignment,M. Unterkalmsteiner; R. Feldt; T. Gorschek,Blekinge Institute of Technology,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,38,"<p>Requirements Engineering and Software Testing are mature areas and have seen a lot of research. Nevertheless, their interactions have been sparsely explored beyond the concept of traceability. To fill this gap, we propose a definition of requirements engineering and software test (REST) alignment, a taxonomy that characterizes the methods linking the respective areas, and a process to assess alignment. The taxonomy can support researchers to identify new opportunities for investigation, as well as practitioners to compare alignment methods and evaluate alignment, or lack thereof. We constructed the REST taxonomy by analyzing alignment methods published in literature, iteratively validating the emerging dimensions. The resulting concept of an information dyad characterizes the exchange of information required for any alignment to take place. We demonstrate use of the taxonomy by applying it on five in-depth cases and illustrate angles of analysis on a set of thirteen alignment methods. In addition, we developed an assessment framework (REST-bench), applied it in an industrial assessment, and showed that it, with a low effort, can identify opportunities to improve REST alignment. Although we expect that the taxonomy can be further refined, we believe that the information dyad is a valid and useful construct to understand alignment.</p>",1049-331X;1049331X,,10.1145/2523088,,,Alignment;software process assessment;software testing;taxonomy,,,,,9,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,37
Evaluating a query framework for software evolution data,M. Wí_rsch; E. Giger; H. C. Gall,"University of Zurich, Switzerland",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,38,"<p>With the steady advances in tooling to support software engineering, mastering all the features of modern IDEs, version control systems, and project trackers is becoming increasingly difficult. Answering even the most common developer questions can be surprisingly tedious and difficult. In this article we present a user study with 35 subjects to evaluate our quasi-natural language interface that provides access to various facets of the evolution of a software system but requires almost zero learning effort. Our approach is tightly woven into the Eclipse IDE and allows developers to answer questions related to source code, development history, or bug and issue management. The results of our evaluation show that our query interface can outperform classical software engineering tools in terms of correctness, while yielding significant time savings to its users and greatly advancing the state of the art in terms of usability and learnability.</p>",1049-331X;1049331X,,10.1145/2522920.2522931,,,Semantic Web;Software evolution;conceptual queries;natural language;software maintenance;source-code analysis;tool support,,,,,,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,37
Marple: Detecting faults in path segments using automatically generated analyses,W. Le; M. L. Soffa,Rochester Institute of Technology,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,38,"<p>Generally, a fault is a property violation at a program point along some execution path. To obtain the path where a fault occurs, we can either run the program or manually identify the execution paths through code inspection. In both of the cases, only a very limited number of execution paths can be examined for a program. This article presents a static framework, Marple, that automatically detects path segments where a fault occurs at a whole program scale. An important contribution of the work is the design of a demand-driven analysis that effectively addresses scalability challenges faced by traditional path-sensitive fault detection. The techniques are made general via a specification language and an algorithm that automatically generates path-based analyses from specifications. The generality is achieved in handling both <i>data-</i> and <i>control-centric</i> faults as well as both liveness and safety properties, enabling the exploitation of fault interactions for diagnosis and efficiency. Our experimental results demonstrate the effectiveness of our techniques in detecting path segments of buffer overflows, integer violations, null-pointer dereferences, and memory leaks. Because we applied an interprocedural, path-sensitive analysis, our static fault detectors generally report better precision than the tools available for comparison. Our demand-driven analyses are shown scalable to deployed applications such as <i>apache</i>, <i>putty</i>, and <i>ffmpeg</i>.</p>",1049-331X;1049331X,,10.1145/2491509.2491512,,,Path segments;demand-driven;faults;specification,,,,,2,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,37
The Minimal Failure-Causing Schema of Combinatorial Testing,C. Nie; H. Leung,Nanjing University,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,4,1,38,"<p>Combinatorial Testing (CT) involves the design of a small test suite to cover the parameter value combinations so as to detect failures triggered by the interactions among these parameters. To make full use of CT and to extend its advantages, this article first gives a model of CT and then presents a theory of the Minimal Failure-causing Schema (MFS), including the concept of the MFS, proof of its existence, some of its properties, and a method of finding the MFS. Then we propose a methodology for CT based on this MFS theory and the existing research. Our MFS-based methodology emphasizes that CT should work on accurate testing requirements, and has the following advantages: 1) Detect failure to the greatest degree with the least cost. 2) Effectiveness is improved by emphasizing mining of the information in software and making full use of the information gained from test design and execution. 3) Determine the root causes of failures and reveal related faults near the exposed ones. 4) Provide a foundation and model for regression testing and software quality evaluation of CT. A case study is presented to illustrate the MFS-based CT methodology, and an empirical study on a real software developed by us shows that the MFS really exists and the methodology based on MFS can considerably improve CT.</p>",1049-331X;1049331X,,10.1145/2000799.2000801,,,Combinatorial testing (CT);Minimal failure-causing schema (MFS);failure diagnosis;test case generation,,,,,14,,,,,,11-Sep,,ACM,ACM Journals & Magazines,,37
On the Comprehension of Program Comprehension,W. Maalej; R. Tiarks; T. Roehm; R. Koschke,University of Hamburg,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,23,4,1,37,"<p>Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension. We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge.</p> <p>We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments.</p> <p>Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support.</p>",1049-331X;1049331X,,10.1145/2622669,,,Empirical software engineering;context-aware software engineering;information needs;knowledge sharing;program comprehension,,,,,21,,,,,,14-Aug,,ACM,ACM Journals & Magazines,,36
Concurrency Debugging with Differential Schedule Projections,N. Machado; D. Quinta; B. Lucia; L. Rodrigues,"INESC-ID, Instituto Superior T&#233;cnico, Universidade de Lisboa, Portugal",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,2,1,37,"<p>We present Symbiosis: a concurrency debugging technique based on novel <i>differential schedule projections</i> (DSPs). A DSP shows the small set of memory operations and dataflows responsible for a failure, as well as a reordering of those elements that avoids the failure. To build a DSP, Symbiosis first generates a <i>full, failing, multithreaded schedule</i> via thread path profiling and symbolic constraint solving. Symbiosis selectively reorders events in the failing schedule to produce a <i>nonfailing, alternate schedule</i>. A DSP reports the ordering and dataflow <i>differences</i> between the failing and nonfailing schedules. Our evaluation on buggy real-world software and benchmarks shows that, in practical time, Symbiosis generates DSPs that both isolate the small fraction of event orders and dataflows responsible for the failure and report which event reorderings prevent failing. In our experiments, DSPs contain 90&percnt; fewer events and 96&percnt; fewer dataflows than the full failure-inducing schedules. We also conducted a user study that shows that, by allowing developers to focus on only a few events, DSPs reduce the amount of time required to understand the bugäó»s root cause and find a valid fix.</p>",1049-331X;1049331X,,10.1145/2885495,,,Concurrency;bug localization;constraint solving;differential schedule projection,,,,,,,,,,,16-May,,ACM,ACM Journals & Magazines,,36
Multi-Step Learning and Adaptive Search for Learning Complex Model Transformations from Examples,I. Baki; H. Sahraoui,"Universit&#233; de Montr&#233;al, Montr&#233;al, Canada",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,3,1,37,"<p>Model-driven engineering promotes models as main development artifacts. As several models may be manipulated during the software-development life cycle, model transformations ensure their consistency by automating model generation and update tasks. However, writing model transformations requires much knowledge and effort that detract from their benefits. To address this issue, Model Transformation by Example (MTBE) aims to learn transformation programs from source and target model pairs supplied as examples. In this article, we tackle the fundamental issues that prevent the existing MTBE approaches from efficiently solving the problem of learning model transformations. We show that, when considering complex transformations, the search space is too large to be explored by naive search techniques. We propose an MTBE process to learn complex model transformations by considering three common requirements: element context and state dependencies and complex value derivation. Our process relies on two strategies to reduce the size of the search space and to better explore it, namely, multi-step learning and adaptive search. We experimentally evaluate our approach on seven model transformation problems. The learned transformation programs are able to produce perfect target models in three transformation cases, whereas precision and recall values larger than 90&percnt; are recorded for the four remaining cases.</p>",1049-331X;1049331X,,10.1145/2904904,,,Genetic programming;model transformation;model transformation by example;model-driven engineering;simulated annealing,,,,,,,,,,,16-Aug,,ACM,ACM Journals & Magazines,,36
Combining Genetic Algorithms and Constraint Programming to Support Stress Testing of Task Deadlines,S. D. Alesio; L. C. Briand; S. Nejati; A. Gotlieb,"Simula Research Laboratory and University of Luxembourg, Lysaker, Norway",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,37,"<p>Tasks in real-time embedded systems (RTES) are often subject to hard deadlines that constrain how quickly the system must react to external inputs. These inputs and their timing vary in a large domain depending on the environment state and can never be fully predicted prior to system execution. Therefore, approaches for stress testing must be developed to uncover possible deadline misses of tasks for different input arrival times. In this article, we describe stress-test case generation as a search problem over the space of task arrival times. Specifically, we search for worst-case scenarios maximizing deadline misses, where each scenario characterizes a test case. In order to scale our search to large industrial-size problems, we combine two state-of-the-art search strategies, namely, genetic algorithms (GA) and constraint programming (CP). Our experimental results show that, in comparison with GA and CP in isolation, GA+CP achieves nearly the same effectiveness as CP and the same efficiency and solution diversity as GA, thus combining the advantages of the two strategies. In light of these results, we conclude that a combined GA+CP approach to stress testing is more likely to scale to large and complex systems.</p>",1049-331X;1049331X,,10.1145/2818640,,,Real-time systems;constraint programming;genetic algorithms;search-based software testing;stress testing;task deadline,,,,,3,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,36
Dynamite: A tool for the verification of alloy models based on PVS,M. M. Moscato; C. G. L. Pombo; M. F. Frias,Universidad de Buenos Aires (UBA),ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,37,"<p>Automatic analysis of Alloy models is supported by the Alloy Analyzer, a tool that translates an Alloy model to a propositional formula that is then analyzed using off-the-shelf SAT solvers. The translation requires user-provided bounds on the sizes of data domains. The analysis is limited by the bounds and is therefore partial. Thus, the Alloy Analyzer may not be appropriate for the analysis of critical applications where more conclusive results are necessary.</p> <p>Dynamite is an extension of PVS that embeds a complete calculus for Alloy. It also includes extensions to PVS that allow one to improve the proof effort by, for instance, automatically analyzing new hypotheses with the aid of the Alloy Analyzer. Since PVS sequents may get cluttered with unnecessary formulas, we use the Alloy unsat-core extraction feature in order to refine proof sequents. An internalization of Alloy's syntax as an Alloy specification allows us to use the Alloy Analyzer for producing witnesses for proving existentially quantified formulas.</p> <p>Dynamite complements the partial automatic analysis offered by the Alloy Analyzer with semi-automatic verification through theorem proving. It also improves the theorem proving experience by using the Alloy Analyzer for early error detection, sequent refinement, and witness generation.</p>",1049-331X;1049331X,,10.1145/2544136,,,Alloy;PVS;alloy calculus;unsat-cores,,,,,,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,36
Automated Comparison of State-Based Software Models in Terms of Their Language and Structure,N. Walkinshaw; K. Bogdanov,The University of Leicester,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,2,1,37,"<p>State machines capture the sequential behavior of software systems. Their intuitive visual notation, along with a range of powerful verification and testing techniques render them an important part of the model-driven software engineering process. There are several situations that require the ability to identify and quantify the differences between two state machines (e.g. to evaluate the accuracy of state machine inference techniques is measured by the similarity of a reverse-engineered model to its reference model). State machines can be compared from two complementary perspectives: (1) In terms of their <i>language</i> -- the externally observable sequences of events that are permitted or not, and (2) in terms of their <i>structure</i> -- the actual states and transitions that govern the behavior. This article describes two techniques to compare models in terms of these two perspectives. It shows how the difference can be quantified and measured by adapting existing binary classification performance measures for the purpose. The approaches have been implemented by the authors, and the implementation is openly available. Feasibility is demonstrated via a case study to compare two real state machine inference approaches. Scalability and accuracy are assessed experimentally with respect to a large collection of randomly synthesized models.</p>",1049-331X;1049331X,,10.1145/2430545.2430549,,,Labeled transition systems;accuracy;comparison,,,,,2,,,,,,13-Mar,,ACM,ACM Journals & Magazines,,36
Discovering Multidimensional Correlations among Regulatory Requirements to Understand Risk,R. A. Gandhi; S. W. Lee,"University of Nebraska, Omaha",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,4,1,37,"<p>Security breaches most often occur due to a cascading effect of failure among security constraints that collectively contribute to overall secure system behavior in a socio-technical environment. Therefore, during security certification activities, analysts must systematically take into account the nexus of causal chains that exist among security constraints imposed by regulatory requirements. Numerous regulatory requirements specified in natural language documents or listed in spreadsheets/databases do not facilitate such analysis. The work presented in this article outlines a stepwise methodology to discover and understand the multidimensional correlations among regulatory requirements for the purpose of understanding the potential for risk due to noncompliance during system operation. Our lattice algebraic computational model helps estimate the collective adequacy of diverse security constraints imposed by regulatory requirements and their interdependencies with each other in a bounded scenario of investigation. Abstractions and visual metaphors combine human intuition with metrics available from the methodology to improve the understanding of risk based on the level of compliance with regulatory requirements. In addition, a problem domain ontology that classifies and categorizes regulatory requirements from multiple dimensions of a socio-technical environment promotes a common understanding among stakeholders during certification and accreditation activities. A preliminary empirical investigation of our theoretical propositions has been conducted in the domain of The United States Department of Defense Information Technology Security Certification and Accreditation Process (DITSCAP). This work contributes a novel approach to understand the level of compliance with regulatory requirements in terms of the potential for risk during system operation.</p>",1049-331X;1049331X,,10.1145/2000799.2000802,,,Software requirements engineering;certification and accreditation;knowledge engineering;ontology-based domain modeling;requirements visualization;risk,,,,,3,,,,,,11-Sep,,ACM,ACM Journals & Magazines,,36
Synthesizing nonanomalous event-based controllers for liveness goals,N. D'ippolito; V. Braberman; N. Piterman; S. Uchitel,"Imperial College London, London, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,1,1,36,"<p>We present SGR(1), a novel synthesis technique and methodological guidelines for automatically constructing event-based behavior models. Our approach works for an expressive subset of liveness properties, distinguishes between controlled and monitored actions, and differentiates system goals from environment assumptions. We show that assumptions must be modeled carefully in order to avoid synthesizing anomalous behavior models. We characterize nonanomalous models and propose assumption compatibility, a sufficient condition, as a methodological guideline.</p>",1049-331X;1049331X,,10.1145/2430536.2430543,,,behavioral modeling;controller synthesis,,,,,6,,,,,,13-Feb,,ACM,ACM Journals & Magazines,,35
Documenting Design-Pattern Instances: A Family of Experiments on Source-Code Comprehensibility,G. Scanniello; C. Gravino; M. Risi; G. Tortora; G. Dodero,"University of Basilicata, Potenza, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,3,1,35,"<p>Design patterns are recognized as a means to improve software maintenance by furnishing an explicit specification of class and object interactions and their underlying intent [Gamma et al. 1995]. Only a few empirical investigations have been conducted to assess whether the kind of documentation for design patterns implemented in source code affects its comprehensibility. To investigate this aspect, we conducted a family of four controlled experiments with 88 participants having different experience (i.e., professionals and Bachelor, Master, and PhD students). In each experiment, the participants were divided into three groups and asked to comprehend a nontrivial chunk of an open-source software system. Depending on the group, each participant was, or was not, provided with graphical or textual representations of the design patterns implemented within the source code. We graphically documented design-pattern instances with UML class diagrams. Textually documented instances are directly reported source code as comments. Our results indicate that documenting design-pattern instances yields an improvement in correctness of understanding source code for those participants with an adequate level of experience.</p>",1049-331X;1049331X,,10.1145/2699696,,,Design patterns;controlled experiment;maintenance;replications;software models;source-code comprehension,,,,,2,,,,,,15-May,,ACM,ACM Journals & Magazines,,34
Automated Support for Reproducing and Debugging Field Failures,W. Jin; A. Orso,Georgia Institute of Technology,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,4,1,35,"<p>As confirmed by a recent survey conducted among developers of the Apache, Eclipse, and Mozilla projects, two extremely challenging tasks during maintenance are reproducing and debugging field failuresäóîfailures that occur on user machines after release. To help developers with these tasks, in this article we present an overall approach that comprises two different techniques: B<scp>ug</scp>R<scp>edux</scp> and F<sup>3</sup>. B<scp>ug</scp>R<scp>edux</scp> is a general technique for reproducing field failures that collects dynamic data about failing executions in the field and uses this data to synthesize executions that mimic the observed field failures. F<sup>3</sup> leverages the executions generated by B<scp>ug</scp>R<scp>edux</scp> to perform automated debugging using a set of suitably optimized fault-localization techniques. To assess the usefulness of our approach, we performed an empirical evaluation of the approach on a set of real-world programs and field failures. The results of our evaluation are promising in that, for all the failures considered, our approach was able to (1) synthesize failing executions that mimicked the observed field failures, (2) synthesize passing executions similar to the failing ones, and (3) use the synthesized executions to successfully perform fault localization with accurate results.</p>",1049-331X;1049331X,,10.1145/2774218,,,Debugging;fault localization;field failures,,,,,2,,,,,,15-Aug,,ACM,ACM Journals & Magazines,,34
Estimating Semantic Relatedness in Source Code,A. Mahmoud; G. Bradshaw,"Louisiana State University, Baton Rouge, LA",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,35,"<p>Contemporary software engineering tools exploit semantic relations between individual code terms to aid in code analysis and retrieval tasks. Such tools employ word similarity methods, often used in natural language processing (<scp>nlp</scp>), to analyze the textual content of source code. However, the notion of similarity in source code is different from natural language. Source code often includes unnatural domain-specific terms (e.g., abbreviations and acronyms), and such terms might be related due to their structural relations rather than linguistic aspects. Therefore, applying natural language similarity methods to source code without adjustment can produce low-quality and error-prone results. Motivated by these observations, we systematically investigate the performance of several semantic-relatedness methods in the context of software. Our main objective is to identify the most effective semantic schemes in capturing association relations between source code terms. To provide an unbiased comparison, different methods are compared against human-generated relatedness information using terms from three software systems. Results show that corpus-based methods tend to outperform methods that exploit external sources of semantic knowledge. However, due to inherent code limitations, the performance of such methods is still suboptimal. To address these limitations, we propose Normalized Software Distance (<scp>nsd</scp>), an information-theoretic method that captures semantic relatedness in source code by exploiting the distributional cues of code terms across the system. <scp>nsd</scp> overcomes data sparsity and lack of context problems often associated with source code, achieving higher levels of resemblance to the human perception of relatedness at the term and the text levels of code.</p>",1049-331X;1049331X,,10.1145/2824251,,,Semantic relatedness;clustering;information retrieval;information theory;latent semantics,,,,,,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,34
A two-phase approximation for model checking probabilistic unbounded until properties of probabilistic systems,P. Jennings; A. P. Ghosh; S. Basu,Iowa State University,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,3,1,35,"<p>We have developed a new approximate probabilistic model-checking method for <i>untimed</i> properties in probabilistic systems, expressed in a probabilistic temporal logic (PCTL, CSL). This method, in contrast to the existing ones, does not require the untimed until properties to be <i>bounded</i> a priori, where the bound refers to the number of discrete steps in the system required to verify the until property. The method consists of two phases. In the first phase, a suitable system- and property-dependent bound <i>k</i><sub>0</sub> is obtained automatically. In the second phase, the probability of satisfying the <i>k</i><sub>0</sub>-bounded until property is computed as the estimate of the probability of satisfying the original unbounded until property. Both phases require only verification of bounded until properties, which can be effectively performed by simulation-based methods. We prove the correctness of the proposed two-phase method and present its optimized implementation in the widely used PRISM model-checking engine. We compare this implementation with sampling-based model-checking techniques implemented in two tools: PRISM and MRMC. We show that for several models these existing tools fail to compute the result, while the two-phase method successfully computes the result efficiently with respect to time and space.</p>",1049-331X;1049331X,,10.1145/2211616.2211621,,,CSL;CTMC;DTMC;PCTL,,,,,,,,,,,12-Jun,,ACM,ACM Journals & Magazines,,34
Key factors for adopting inner source,K. J. Stol; P. Avgeriou; M. A. Babar; Y. Lucas; B. Fitzgerald,"Lero, University of Limerick",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,35,"<p>A number of organizations have adopted Open Source Software (OSS) development practices to support or augment their software development processes, a phenomenon frequently referred to as <i>Inner Source</i>. However the adoption of Inner Source is not a straightforward issue. Many organizations are struggling with the question of whether Inner Source is an appropriate approach to software development for them in the first place. This article presents a framework derived from the literature on Inner Source, which identifies nine important factors that need to be considered when implementing Inner Source. The framework can be used as a probing instrument to assess an organization on these nine factors so as to gain an understanding of whether or not Inner Source is suitable. We applied the framework in three case studies at Philips Healthcare, Neopost Technologies, and Rolls-Royce, which are all large organizations that have either adopted Inner Source or were planning to do so. Based on the results presented in this article, we outline directions for future research.</p>",1049-331X;1049331X,,10.1145/2533685,,,Case study;framework;inner source;open-source development practices,,,,,4,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,34
An algebra of design patterns,H. Zhu; I. Bayley,"Oxford Brookes University, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,35,"<p>In a pattern-oriented software design process, design decisions are made by selecting and instantiating appropriate patterns, and composing them together. In our previous work, we enabled these decisions to be formalized by defining a set of operators on patterns with which instantiations and compositions can be represented. In this article, we investigate the algebraic properties of these operators. We provide and prove a complete set of algebraic laws so that equivalence between pattern expressions can be proven. Furthermore, we define an always-terminating normalization of pattern expression to a canonical form which is unique modulo equivalence in first-order logic.</p> <p>By a case study, the pattern-oriented design of an extensible request-handling framework, we demonstrate two practical applications of the algebraic framework. First, we can prove the correctness of a finished design with respect to the design decisions made and the formal specification of the patterns. Second, we can even derive the design from these components.</p>",1049-331X;1049331X,,10.1145/2491509.2491517,,,Design patterns;algebra;equational reasoning;formal method;pattern composition;software design methodology,,,,,8,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,34
On software component co-installability,J. Vouillon; R. D. Cosmo,"CNRS, PPS, UMR 7126, Univ Paris Diderot, Sorbonne Paris Cit&#233;, Paris, France",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,35,"<p>Modern software systems are built by composing components drawn from large <i>repositories</i>, whose size and complexity is increasing at a very fast pace. A fundamental challenge for the maintainability and the scalability of such software systems is the ability to quickly identify the components that can or cannot be installed together: this is the <i>co-installability</i> problem, which is related to boolean satisfiability and is known to be algorithmically hard. This article develops a novel theoretical framework, based on formally certified semantic preserving graph-theoretic transformations, that allows us to associate to each concrete component repository a much smaller one with a simpler structure, that we call <i>strongly flat</i>, with equivalent co-installability properties. This flat repository can be displayed in a way that provides a concise view of the co-installability issues in the original repository, or used as a basis for various algorithms related to co-installability, like the efficient computation of strong conflicts between components. The proofs contained in this work have been machine checked using the Coq proof assistant.</p>",1049-331X;1049331X,,10.1145/2522920.2522927,,,Component;co-installability;conflicts;dependencies;open source;package management,,,,,2,2,,,,,13-Oct,,ACM,ACM Journals & Magazines,,34
A Generative Programming Framework for Context-Aware CSCW Applications,D. Kulkarni; T. Ahmed; A. Tripathi,"University of Minnesota, Minneapolis",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2012,21,2,1,35,"<p>We present a programming framework based on the paradigm of generative application development for building context-aware collaborative applications. In this approach, context-aware applications are implemented using a domain-specific design model, and their execution environment is generated and maintained by the middleware. The key features of this design model include support for context-based service discovery and binding, context-based access control, context-based multiuser coordination, and context-triggered automated task executions. The middleware uses the technique of <i>policy-based specialization</i> for generating application-specific middleware components from the generic middleware components. Through a case-study example, we demonstrate this approach and present the evaluations of the design model and the middleware.</p>",1049-331X;1049331X,,10.1145/2089116.2089121,,,Context-aware computing;generative middleware;pervasive computing,,,,,1,,,,,,12-Mar,,ACM,ACM Journals & Magazines,,34
QVM: An Efficient Runtime for Detecting Defects in Deployed Systems,M. Arnold; M. Vechev; E. Yahav,IBM Research,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,21,1,1,35,"<p>Coping with software defects that occur in the post-deployment stage is a challenging problem: bugs may occur only when the system uses a specific configuration and only under certain usage scenarios. Nevertheless, halting production systems until the bug is tracked and fixed is often impossible. Thus, developers have to try to reproduce the bug in laboratory conditions. Often, the reproduction of the bug takes most of the debugging effort.</p> <p>In this paper we suggest an approach to address this problem by using a specialized runtime environment called <i>Quality Virtual Machine</i> (QVM). QVM efficiently detects defects by continuously monitoring the execution of the application in a production setting. QVM enables the efficient checking of violations of user-specified correctness properties, that is, typestate safety properties, Java assertions, and heap properties pertaining to ownership. QVM is markedly different from existing techniques for continuous monitoring by using a novel overhead manager which enforces a user-specified overhead budget for quality checks. Existing tools for error detection in the field usually disrupt the operation of the deployed system. QVM, on the other hand, provides a balanced trade-off between the cost of the monitoring process and the maintenance of sufficient accuracy for detecting defects. Specifically, the overhead cost of using QVM instead of a standard JVM, is low enough to be acceptable in production environments.</p> <p>We implemented QVM on top of IBMäó»s J9 Java Virtual Machine and used it to detect and fix various errors in real-world applications.</p>",1049-331X;1049331X,,10.1145/2063239.2063241,,,Virtual machines;debugging;diagnosis;heap assertions;typestate,,,,,4,,,,,,11-Dec,,ACM,ACM Journals & Magazines,,34
Recommending Adaptive Changes for Framework Evolution,B. Dagenais; M. P. Robillard,McGill University,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,4,1,35,"<p>In the course of a frameworkäó»s evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the frameworkäó»s evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework was adapted to its own changes. In a study of the evolution of one open source framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision. In a second study of the evolution of two frameworks, we found that related change detection approaches were better at discovering systematic changes and that SemDiff was complementary to these approaches by detecting non-trivial changes such as when a functionality is imported from an external library.</p>",1049-331X;1049331X,,10.1145/2000799.2000805,,,Adaptive changes;framework;legacy study;mining software repositories;origin analysis;partial program analysis;recommendation system;software evolution,,,,,11,,,,,,11-Sep,,ACM,ACM Journals & Magazines,,34
Reducing the effort of bug report triage: Recommenders for development-oriented decisions,J. Anvik; G. C. Murphy,"Central Washington University, Ellensburg, WA",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,3,1,35,"<p>A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process.</p> <p>To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70&percnt; and 98&percnt; over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15&percnt; precision of hand-tuned developer recommenders.</p>",1049-331X;1049331X,,10.1145/2000791.2000794,,,Bug report triage;configuration assistance;machine learning;recommendation;task assignment,,,,,47,,,,,,11-Aug,,ACM,ACM Journals & Magazines,,34
Parallel Algorithms for Generating Distinguishing Sequences for Observable Non-deterministic FSMs,R. M. Hierons; U. C. Tí_rker,"Brunel University, Middlesex, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20170907,2017,26,1,1,34,"<p>A distinguishing sequence (DS) for a finite-state machine (FSM) is an input sequence that distinguishes every pair of states of the FSM. There are techniques that generate a test sequence with guaranteed fault detection power, and it has been found that shorter test sequences can be produced if DSs are used. Despite these benefits, however, until recently the only published DS generation algorithms have been for deterministic FSMs. This article develops a massively parallel algorithm, which can be used in Graphics Processing Units (GPUs) Computing, to generate DSs from partial observable non-deterministic FSMs. We also present the results of experiments using randomly generated FSMs and some benchmark FSMs. The results are promising and indicate that the proposed algorithm can derive DSs from partial observable non-deterministic FSMs with 32,000 states in an acceptable amount of time.</p>",1049-331X;1049331X,,10.1145/3051121,,,Finite state machine;distinguishing sequences,,,,,,,,,,,17-Jul,,ACM,ACM Journals & Magazines,,33
Boa: Ultra-Large-Scale Software Repository and Source-Code Mining,R. Dyer; H. A. Nguyen; H. Rajan; T. N. Nguyen,"Bowling Green State University, Bowling Green, OH",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,34,"<p>In today's software-centric world, ultra-large-scale software repositories, such as SourceForge, GitHub, and Google Code, are the new library of Alexandria. They contain an enormous corpus of software and related information. Scientists and engineers alike are interested in analyzing this wealth of information. However, systematic extraction and analysis of relevant data from these repositories for testing hypotheses is hard, and best left for <i>mining software repository</i> (MSR) experts&excl; Specifically, mining source code yields significant insights into software development artifacts and processes. Unfortunately, mining source code at a large scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse grained, or sacrifice studying the history of the code. In this article we address mining source code: (a) at a very large scale; (b) at a fine-grained level of detail; and (c) with full history information. To address these challenges, we present domain-specific language features for source-code mining in our language and infrastructure called <i>Boa</i>. The goal of <i>Boa</i> is to ease testing MSR-related hypotheses. Our evaluation demonstrates that <i>Boa</i> substantially reduces programming efforts, thus lowering the barrier to entry. We also show drastic improvements in scalability.</p>",1049-331X;1049331X,,10.1145/2803171,,,Boa;domain-specific language;ease of use;lower barrier to entry;mining software repositories;scalable,,,,,3,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,33
The Effect of Program and Model Structure on the Effectiveness of MC/DC Test Adequacy Coverage,G. Gay; A. Rajan; M. Staats; M. Whalen; M. P. E. Heimdahl,"University of South Carolina, SC, USA",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,3,1,34,"<p>Test adequacy metrics defined over the structure of a program, such as Modified Condition and Decision Coverage (MC/DC), are used to assess testing efforts. However, MC/DC can be äóìcheatedäóù by restructuring a program to make it easier to achieve the desired coverage. This is concerning, given the importance of MC/DC in assessing the adequacy of test suites for critical systems domains. In this work, we have explored the impact of implementation structure on the efficacy of test suites satisfying the MC/DC criterion using four real-world avionics systems.</p> <p>Our results demonstrate that test suites achieving MC/DC over implementations with structurally complex Boolean expressions are generally larger and more effective than test suites achieving MC/DC over functionally equivalent, but structurally simpler, implementations. Additionally, we found that test suites generated over simpler implementations achieve significantly lower MC/DC and fault-finding effectiveness when applied to complex implementations, whereas test suites generated over the complex implementation still achieve high MC/DC and attain high fault finding over the simpler implementation. By measuring MC/DC over simple implementations, we can significantly reduce the cost of testing, but in doing so, we also reduce the effectiveness of the testing process. Thus, developers have an economic incentive to äóìcheatäóù the MC/DC criterion, but this cheating leads to negative consequences. Accordingly, we recommend that organizations require MC/DC over a structurally complex implementation for testing purposes to avoid these consequences.</p>",1049-331X;1049331X,,10.1145/2934672,,,Coverage;fault finding,,,,,1,,,,,,16-Aug,,ACM,ACM Journals & Magazines,,33
Concept location using formal concept analysis and information retrieval,D. Poshyvanyk; M. Gethers; A. Marcus,College of William and Mary,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,4,1,34,"<p>The article addresses the problem of concept location in source code by proposing an approach that combines Formal Concept Analysis and Information Retrieval. In the proposed approach, Latent Semantic Indexing, an advanced Information Retrieval approach, is used to map textual descriptions of software features or bug reports to relevant parts of the source code, presented as a ranked list of source code elements. Given the ranked list, the approach selects the most relevant attributes from the best ranked documents, clusters the results, and presents them as a concept lattice, generated using Formal Concept Analysis.</p> <p>The approach is evaluated through a large case study on concept location in the source code on six open-source systems, using several hundred features and bugs. The empirical study focuses on the analysis of various configurations of the generated concept lattices and the results indicate that our approach is effective in organizing different concepts and their relationships present in the subset of the search results. In consequence, the proposed concept location method has been shown to outperform a standalone Information Retrieval based concept location technique by reducing the number of irrelevant search results across all the systems and lattice configurations evaluated, potentially reducing the programmers' effort during software maintenance tasks involving concept location.</p>",1049-331X;1049331X,,10.1145/2377656.2377660,,,Concept location;Formal Concept Analysis;Information Retrieval;feature identification;program comprehension;software evolution and maintenance,,,,,11,,,,,,12-Nov,,ACM,ACM Journals & Magazines,,33
Validation of requirements for hybrid systems: A formal approach,A. Cimatti; M. Roveri; A. Susi; S. Tonetta,"FBK-irst, Center for Information Technology, Trento-Povo, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,4,1,34,"<p>Flaws in requirements may have unacceptable consequences in the development of safety-critical applications. Formal approaches may help with a deep analysis that takes care of the precise semantics of the requirements. However, the proposed solutions often disregard the problem of integrating the formalization with the analysis, and the underlying logical framework lacks either expressive power, or automation.</p> <p>We propose a new, comprehensive approach for the validation of functional requirements of hybrid systems, where discrete components and continuous components are tightly intertwined. The proposed solution allows to tackle problems of conversion from informal to formal, traceability, automation, user acceptance, and scalability.</p> <p>We build on a new language, othello which is expressive enough to represent various domains of interest, yet allowing efficient procedures for checking the satisfiability. Around this, we propose a structured methodology where: informal requirements are fragmented and categorized according to their role; each fragment is formalized based on its category; specialized formal analysis techniques, optimized for requirements analysis, are finally applied.</p> <p>The approach was the basis of an industrial project aiming at the validation of the European Train Control System (ETCS) requirements specification. During the project a realistic subset of the ETCS specification was formalized and analyzed. The approach was positively assessed by domain experts.</p>",1049-331X;1049331X,,10.1145/2377656.2377659,,,European train control system;Requirements validation;formal languages;methodology;safety-critical applications,,,,,4,,,,,,12-Nov,,ACM,ACM Journals & Magazines,,33
Do we need to handle every temporal violation in scientific workflow systems?,X. Liu; Y. Yang; D. Yuan; J. Chen,"East China Normal University, Shanghai, China and Swinburne University of Technology, Melbourne, Australia",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,34,"<p>Scientific processes are usually time constrained with overall deadlines and local milestones. In scientific workflow systems, due to the dynamic nature of the underlying computing infrastructures such as grid and cloud, execution delays often take place and result in a large number of temporal violations. Since temporal violation handling is expensive in terms of both monetary costs and time overheads, an essential question aroused is äóìdo we need to handle every temporal violation in scientific workflow systems&quest;äóù The answer would be äóìtrueäóù according to existing works on workflow temporal management which adopt the philosophy similar to the handling of functional exceptions, that is, every temporal violation should be handled whenever it is detected. However, based on our observation, the phenomenon of self-recovery where execution delays can be automatically compensated for by the saved execution time of subsequent workflow activities has been entirely overlooked. Therefore, considering the nonfunctional nature of temporal violations, our answer is äóìnot necessarily true.äóù To take advantage of self-recovery, this article proposes a novel adaptive temporal violation handling point selection strategy where this phenomenon is effectively utilised to avoid unnecessary temporal violation handling. Based on simulations of both real-world scientific workflows and randomly generated test cases, the experimental results demonstrate that our strategy can significantly reduce the cost on temporal violation handling by over 96&percnt; while maintaining extreme low violation rate under normal circumstances.</p>",1049-331X;1049331X,,10.1145/2559938,,,Scientific workflows;quality of service;temporal constraints;temporal verification;violation handling point selection,,,,,11,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,33
A Precise Method-Method Interaction-Based Cohesion Metric for Object-Oriented Classes,J. Al Dallal; L. C. Briand,Kuwait University,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2012,21,2,1,34,"<p>The building of highly cohesive classes is an important objective in object-oriented design. Class cohesion refers to the relatedness of the class members, and it indicates one important aspect of the class design quality. A meaningful class cohesion metric helps object-oriented software developers detect class design weaknesses and refactor classes accordingly. Several class cohesion metrics have been proposed in the literature. Most of these metrics are applicable based on low-level design information such as attribute references in methods. Some of these metrics capture class cohesion by counting the number of method pairs that share common attributes. A few metrics measure cohesion more precisely by considering the degree of interaction, through attribute references, between each pair of methods. However, the formulas applied by these metrics to measure the degree of interaction cause the metrics to violate important mathematical properties, thus undermining their construct validity and leading to misleading cohesion measurement. In this paper, we propose a formula that precisely measures the degree of interaction between each pair of methods, and we use it as a basis to introduce a low-level design class cohesion metric (LSCC). We verify that the proposed formula does not cause the metric to violate important mathematical properties. In addition, we provide a mechanism to use this metric as a useful indicator for refactoring weakly cohesive classes, thus showing its usefulness in improving class cohesion. Finally, we empirically validate LSCC. Using four open source software systems and eleven cohesion metrics, we investigate the relationship between LSCC, other cohesion metrics, and fault occurrences in classes. Our results show that LSCC is one of three metrics that explains more accurately the presence of faults in classes. LSCC is the only one among the three metrics to comply with important mathematical properties, and statistical analysis shows it cap- ures a measurement dimension of its own. This suggests that LSCC is a better alternative, when taking into account both theoretical and empirical results, as a measure to guide the refactoring of classes. From a more general standpoint, the results suggest that class quality, as measured in terms of fault occurrences, can be more accurately explained by cohesion metrics that account for the degree of interaction between each pair of methods.</p>",1049-331X;1049331X,,10.1145/2089116.2089118,,,Object-oriented software quality;attribute;class cohesion;low-level design;method;method-method interaction;refactoring,,,,,5,,,,,,12-Mar,,ACM,ACM Journals & Magazines,,33
Deciding Type-Based Partial-Order Constraints for Path-Sensitive Analysis,E. Sherman; B. J. Garvin; M. B. Dwyer,Boise State University,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,3,1,33,"<p>The precision and scalability of path-sensitive program analyses depend on their ability to distinguish feasible and infeasible program paths. Analyses express path feasibility as the satisfiability of conjoined branch conditions, which is then decided by cooperating decision procedures such as those in <i>satisfiability modulo theory</i> (SMT) solvers. Consequently, efficient underlying decision procedures are key to precise, scalable program analyses.</p> <p>When we investigate the branch conditions accumulated by inter-procedural path-sensitive analyses of object-oriented programs, we find that many relate to an object's dynamic type. These conditions arise from explicit type tests and the branching implicit in dynamic dispatch and type casting. These conditions share a common form that comprises a fragment of the theory of partial orders, which we refer to as <i>type-based partial orders (TPO)</i>.</p> <p>State-of-the-art SMT solvers can heuristically instantiate the quantified formulae that axiomatize partial orders, and thereby support TPO constraints. We present two custom decision procedures with significantly better performance. On benchmarks that reflect inter-procedural path-sensitive analyses applied to significant Java systems, the custom procedures run three orders of magnitude faster. The performance of the two decision procedures varies across benchmarks, which suggests that a portfolio approach may be beneficial for solving constraints generated by program analyses.</p>",1049-331X;1049331X,,10.1145/2755971,,,SMT;decision procedure;path-sensitive analysis;subtyping;type hierarchy,,,,,,,,,,,15-May,,ACM,ACM Journals & Magazines,,32
Mining Unit Tests for Discovery and Migration of Math APIs,A. Santhiar; O. Pandita; A. Kanade,Indian Institute of Science,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,1,1,33,"<p>Today's programming languages are supported by powerful third-party APIs. For a given application domain, it is common to have many competing APIs that provide similar functionality. Programmer productivity therefore depends heavily on the programmer's ability to discover suitable APIs both during an initial coding phase, as well as during software maintenance.</p> <p>The aim of this work is to support the discovery and migration of math APIs. Math APIs are at the heart of many application domains ranging from machine learning to scientific computations. Our approach, called M<scp>ath</scp>F<scp>inder</scp>, combines executable specifications of mathematical computations with unit tests (operational specifications) of API methods. Given a math expression, M<scp>ath</scp>F<scp>inder</scp> synthesizes pseudo-code comprised of API methods to compute the expression by mining unit tests of the API methods. We present a sequential version of our unit test mining algorithm and also design a more scalable data-parallel version.</p> <p>We perform extensive evaluation of M<scp>ath</scp>F<scp>inder</scp> (1) for <i>API discovery</i>, where math algorithms are to be implemented from scratch and (2) for <i>API migration</i>, where client programs utilizing a math API are to be migrated to <i>another</i> API. We evaluated the precision and recall of M<scp>ath</scp>F<scp>inder</scp> on a diverse collection of math expressions, culled from algorithms used in a wide range of application areas such as control systems and structural dynamics. In a user study to evaluate the productivity gains obtained by using M<scp>ath</scp>F<scp>inder</scp> for API discovery, the programmers who used M<scp>ath</scp>F<scp>inder</scp> finished their programming tasks twice as fast as their counterparts who used the usual techniques like web and code search, IDE code completion, and manual inspection of library documentation. For the problem of API migration, as a case study, we used M<scp>ath</s- p>F<scp>inder</scp> to migrate Weka, a popular machine learning library. Overall, our evaluation shows that M<scp>ath</scp>F<scp>inder</scp> is easy to use, provides highly precise results across several math APIs and application domains even with a small number of unit tests per method, and scales to large collections of unit tests.</p>",1049-331X;1049331X,,10.1145/2629506,,,API discovery;API migration;mathematical computation;mining;unit tests,,,,,,,,,,,14-Sep,,ACM,ACM Journals & Magazines,,32
"Peer Review on Open-Source Software Projects: Parameters, Statistical Models, and Theory",P. C. Rigby; D. M. German; L. Cowen; M. A. Storey,Concordia University,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,23,4,1,33,"<p>Peer review is seen as an important quality-assurance mechanism in both industrial development and the open-source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, software peer reviews are not as well understood.</p> <p>To develop an empirical understanding of OSS peer review, we examine the review policies of 25 OSS projects and study the archival records of six large, mature, successful OSS projects. We extract a series of measures based on those used in traditional inspection experiments. We measure the frequency of review, the size of the contribution under review, the level of participation during review, the experience and expertise of the individuals involved in the review, the review interval, and the number of issues discussed during review. We create statistical models of the review efficiency, review interval, and effectiveness, the issues discussed during review, to determine which measures have the largest impact on review efficacy.</p> <p>We find that OSS peer reviews are conducted asynchronously by empowered experts who focus on changes that are in their area of expertise. Reviewers provide timely, regular feedback on small changes. The descriptive statistics clearly show that OSS review is drastically different from traditional inspection.</p>",1049-331X;1049331X,,10.1145/2594458,,,Peer review;inspection;mining software repositories;open-source software,,,,,11,,,,,,14-Aug,,ACM,ACM Journals & Magazines,,32
Guidelines for Coverage-Based Comparisons of Non-Adequate Test Suites,M. Gligoric; A. Groce; C. Zhang; R. Sharma; M. A. Alipour; D. Marinov,University of Illinois at Urbana-Champaign,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,4,1,33,"<p>A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques that produce those test suites. Researchers frequently compare test suites by measuring their <i>coverage</i>. A coverage criterion <i>C</i> provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100&percnt; of the feasible requirements is called <i>C-adequate</i>. Previous rigorous evaluations of coverage criteria mostly focused on such <i>adequate</i> test suites: given two criteria <i>C</i> and <i>C</i>äó_, are <i>C</i>-adequate suites on average more effective than <i>C</i>äó_-adequate suites? However, in many realistic cases, producing adequate suites is impractical or even impossible.</p> <p>This article presents the first extensive study that evaluates coverage criteria for the common case of <i>non-adequate</i> test suites: given two criteria <i>C</i> and <i>C</i>äó_, which one is better to use to compare test suites? Namely, if suites <i>T</i><sub>1</sub>, <i>T</i><sub>2</sub>,äó_,<i>T<sub>n</sub></i> have coverage values <i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>,äó_,<i>c<sub>n</sub></i> for <i>C</i> and <i>c</i><sub>1</sub>äó_, <i>c</i><sub>2</sub>äó_,äó_,<i>c</i><sub>n</sub>äó_ for <i>C</i>äó_, is it better to compare suites based on <i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>,äó_,<i>c<sub>n</sub></i> or based on <i>c</i><sub>1</sub>äó_, <i>c</i><sub>2</sub>äó_,äó_,<i>c<sub>n</sub>äó_</i>? We evaluate a large set of plausible criteria, including basic criteria such as statement and branch coverage, as well as stronger criteria used in recent studies, including criteria based on program paths, equivalenc- classes of covered statements, and predicate states. The criteria are evaluated on a set of Java and C programs with both manually written and automatically generated test suites. The evaluation uses three correlation measures. Based on these experiments, two criteria perform best: branch coverage and an intraprocedural acyclic path coverage. We provide guidelines for testing researchers aiming to evaluate test suites using coverage criteria as well as for other researchers evaluating coverage criteria for research use.</p>",1049-331X;1049331X,,10.1145/2660767,,,Coverage criteria;non-adequate test suites,,,,,2,,,,,,15-Aug,,ACM,ACM Journals & Magazines,,32
ConMem: Detecting Crash-Triggering Concurrency Bugs through an Effect-Oriented Approach,W. Zhang; C. Sun; J. Lim; S. Lu; T. Reps,University of Wisconsin -- Madison,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,2,1,33,"<p>Multicore technology is making concurrent programs increasingly pervasive. Unfortunately, it is difficult to deliver reliable concurrent programs, because of the huge and nondeterministic interleaving space. In reality, without the resources to thoroughly check the interleaving space, critical concurrency bugs can slip into production versions and cause failures in the field. Approaches to making the best use of the limited resources and exposing severe concurrency bugs before software release would be desirable.</p> <p>Unlike previous work that focuses on bugs caused by specific interleavings (e.g., races and atomicity violations), this article targets concurrency bugs that result in one type of severe effect: program crashes. Our study of the error-propagation process of real-world concurrency bugs reveals a common pattern (50&percnt; in our nondeadlock concurrency bug set) that is highly correlated with program crashes. We call this pattern concurrency-memory bugs: buggy interleavings directly cause memory bugs (NULL-pointer-dereferences, dangling-pointers, buffer-overflows, uninitialized-reads) on shared memory objects.</p> <p>Guided by this study, we built ConMem to monitor program execution, analyze memory accesses and synchronizations, and predictively detect these common and severe concurrency-memory bugs. We also built a validator,ConMem-v, to automatically prune false positives by enforcing potential bug-triggering interleavings.</p> <p>We evaluated ConMem using 7 open-source programs with 10 real-world concurrency bugs. ConMem detects more tested bugs (9 out of 10 bugs) than a lock-set-based race detector and an unserializable-interleaving detector, which detect 4 and 6 bugs, respectively, with a false-positive rate about one tenth of the compared tools. ConMem-v further prunes out all the false positives. ConMem has reasonable overhead suitable for development usage.</p>",1049-331X;1049331X,,10.1145/2430545.2430546,,,Software testing;concurrency bugs,,,,,3,,,,,,13-Mar,,ACM,ACM Journals & Magazines,,32
Path- and index-sensitive string analysis based on monadic second-order logic,T. Tateishi; M. Pistoia; O. Tripp,"IBM Research - Tokyo, Japan",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,33,"<p>We propose a novel technique for statically verifying the strings generated by a program. The verification is conducted by encoding the program in Monadic Second-order Logic (M2L). We use M2L to describe constraints among program variables and to abstract built-in string operations. Once we encode a program in M2L, a theorem prover for M2L, such as MONA, can automatically check if a string generated by the program satisfies a given specification, and if not, exhibit a counterexample. With this approach, we can naturally encode relationships among strings, accounting also for cases in which a program manipulates strings using indices. In addition, our string analysis is path sensitive in that it accounts for the effects of string and Boolean comparisons, as well as regular-expression matches.</p> <p>We have implemented our string analysis algorithm, and used it to augment an industrial security analysis for Web applications by automatically detecting and verifying <i>sanitizers</i>äóîmethods that eliminate malicious patterns from untrusted strings, making these strings safe to use in security-sensitive operations. On the 8 benchmarks we analyzed, our string analyzer discovered 128 previously unknown sanitizers, compared to 71 sanitizers detected by a previously presented string analysis.</p>",1049-331X;1049331X,,10.1145/2522920.2522926,,,String analysis;Web security;static program analysis,,,,,2,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,32
Views: Synthesizing fine-grained concurrency control,B. Demsky; P. Lam,"University of California, Irvine",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,1,1,33,"<p>Fine-grained locking is often necessary to increase concurrency. Correctly implementing fine-grained locking with today's concurrency primitives can be challengingäóîrace conditions often plague programs with sophisticated locking schemes.</p> <p>We present views, a new approach to concurrency control. Views ease the task of implementing sophisticated locking schemes and provide static checks to automatically detect many data races. A view of an object declares a partial interface, consisting of fields and methods, to the object that the view protects. A view also contains an incompatibility declaration, which lists views that may not be simultaneously held by other threads. A set of view annotations specify which code regions hold a view of an object. Our view compiler performs simple static checks that identify many data races. We pair the basic approach with an inference algorithm that can infer view incompatibility specifications for many applications.</p> <p>We have ported four benchmark applications to use views: portions of Vuze, a BitTorrent client; Mailpuccino, a graphical email client; jphonelite, a VoIP softphone implementation; and TupleSoup, a database. Our experience indicates that views are easy to use, make implementing sophisticated locking schemes simple, and can help eliminate concurrency bugs. We have evaluated the performance of a view implementation of a red-black tree and found that views can significantly improve performance over that of the lock-based implementation.</p>",1049-331X;1049331X,,10.1145/2430536.2430538,,,concurrency;language design;static verification,,,,,1,,,,,,13-Feb,,ACM,ACM Journals & Magazines,,32
Improving software modularization via automated analysis of latent topics and dependencies,G. Bavota; M. Gethers; R. Oliveto; D. Poshyvanyk; A. d. Lucia,"University of Salerno, Fisciano (SA), Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,33,"<p>Oftentimes, during software maintenance the original program modularization decays, thus reducing its quality. One of the main reasons for such architectural erosion is suboptimal placement of source-code classes in software packages. To alleviate this issue, we propose an automated approach to help developers improve the quality of software modularization. Our approach analyzes underlying latent topics in source code as well as structural dependencies to recommend (and explain) refactoring operations aiming at moving a class to a more suitable package. The topics are acquired via Relational Topic Models (RTM), a probabilistic topic modeling technique. The resulting tool, coined as <i>R</i>3 (Rational Refactoring via RTM), has been evaluated in two empirical studies. The results of the first study conducted on nine software systems indicate that <i>R</i>3 provides a coupling reduction from 10&percnt; to 30&percnt; among the software modules. The second study with 62 developers confirms that <i>R</i>3 is able to provide meaningful recommendations (and explanations) for move class refactoring. Specifically, more than 70&percnt; of the recommendations were considered meaningful from a functional point of view.</p>",1049-331X;1049331X,,10.1145/2559935,,,Software modularization;empirical studies;recommendation system;refactoring;relational topic modeling,,,,,19,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,32
FlagRemover: A testability transformation for transforming loop-assigned flags,D. W. Binkley; M. Harman; K. Lakhotia,"Loyola University in Maryland, Baltimore, MD",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,3,1,33,"<p>Search-Based Testing is a widely studied technique for automatically generating test inputs, with the aim of reducing the cost of software engineering activities that rely upon testing. However, search-based approaches degenerate to random testing in the presence of flag variables, because flags create spikes and plateaux in the fitness landscape. Both these features are known to denote hard optimization problems for all search-based optimization techniques. Several authors have studied flag removal transformations and fitness function refinements to address the issue of flags, but the problem of loop-assigned flags remains unsolved. This article introduces a testability transformation along with a tool that transforms programs with loop-assigned flags into flag-free equivalents, so that existing search-based test data generation approaches can successfully be applied. The article presents the results of an empirical study that demonstrates the effectiveness and efficiency of the testability transformation on programs including those made up of open source and industrial production code, as well as test data generation problems specifically created to denote hard optimization problems.</p>",1049-331X;1049331X,,10.1145/2000791.2000796,,,Evolutionary testing;empirical evaluation;flags;testability transformation,,,,,1,,,,,,11-Aug,,ACM,ACM Journals & Magazines,,32
Residual Investigation: Predictive and Precise Bug Detection,K. Li; C. Reichenbach; C. Csallner; Y. Smaragdakis,"University of Massachusetts, Amherst, MA",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,2,1,32,"<p>We introduce the concept of <i>residual investigation</i> for program analysis. A residual investigation is a dynamic check installed as a result of running a static analysis that reports a possible program error. The purpose is to observe conditions that indicate whether the statically predicted program fault is likely to be realizable and relevant. The key feature of a residual investigation is that it has to be much more precise (i.e., with fewer false warnings) than the static analysis alone, yet significantly more general (i.e., reporting more errors) than the dynamic tests in the program's test suite that are pertinent to the statically reported error. That is, good residual investigations encode dynamic conditions that, when considered in conjunction with the static error report, increase confidence in the existence or severity of an error without needing to directly observe a fault resulting from the error.</p> <p>We enhance the static analyzer FindBugs with several residual investigations appropriately tuned to the static error patterns in FindBugs, and apply it to nine large open-source systems and their native test suites. The result is an analysis with a low occurrence of false warnings (false positives) while reporting several actual errors that would not have been detected by mere execution of a program's test suite.</p>",1049-331X;1049331X,,10.1145/2656201,,,False warnings;RFBI;existing test cases,,,,,,,,,,,14-Dec,,ACM,ACM Journals & Magazines,,31
Platys: An Active Learning Framework for Place-Aware Application Development and Its Evaluation,P. K. Murukannaiah; M. P. Singh,"North Carolina State University, Raleigh, NC",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,3,1,32,"<p>We introduce a high-level abstraction of location called <i>place</i>. A place derives its meaning from a user's physical space, activities, or social context. In this manner, place can facilitate improved user experience compared to the traditional representation of location, which is spatial coordinates. We propose the <i>Platys</i> framework as a way to address the special challenges of place-aware application development. The core of Platys is a middleware that (1) learns a model of places specific to each user via <i>active learning</i>, a machine learning paradigm that seeks to reduce the user-effort required for training the middleware, and (2) exposes the learned user-specific model of places to applications at run time, insulating application developers from dealing with both low-level sensors and user idiosyncrasies in perceiving places.</p> <p>We evaluated Platys via two studies. First, we collected place labels and Android phone sensor readings from 10 users. We applied Platys' active learning approach to learn each user's places and found that Platys (1) requires fewer place labels to learn a user's places with a desired accuracy than do two traditional supervised approaches, and (2) learns places with higher accuracy than two unsupervised approaches.</p> <p>Second, we conducted a developer study to evaluate Platys' efficiency in assisting developers and its effectiveness in enabling usable applications. In this study, 46 developers employed either Platys or the Android location API to develop a place-aware application. Our results indicate that application developers employing Platys, when compared to those employing the Android API, (1) develop a place-aware application faster and perceive reduced difficulty and (2) produce applications that are easier to understand (for developers) and potentially more usable and privacy preserving (for application users).</p>",1049-331X;1049331X,,10.1145/2729976,,,Mobile application development;active learning;context-aware;location-aware;middleware;place recognition;place-aware;privacy;semi-supervised learning;usability,,,,,5,,,,,,15-May,,ACM,ACM Journals & Magazines,,31
Scaling Up Symbolic Analysis by Removing Z-Equivalent States,Y. Li; S. C. Cheung; X. Zhang; Y. Liu,Hong Kong University of Science and Technology,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,23,4,1,32,"<p>Path explosion is a major issue in applying path-sensitive symbolic analysis to large programs. We observe that many symbolic states generated by the symbolic analysis of a procedure are indistinguishable to its callers. It is, therefore, possible to keep only one state from each set of equivalent symbolic states without affecting the analysis result. Based on this observation, we propose an equivalence relation called z-equivalence, which is weaker than logical equivalence, to relate a large number of z-equivalent states. We prove that z-equivalence is strong enough to guarantee that paths to be traversed by the symbolic analysis of two z-equivalent states are identical, giving the same solutions to satisfiability and validity queries. We propose a sound linear algorithm to detect z-equivalence. Our experiments show that the symbolic analysis that leverages z-equivalence is able to achieve more than ten orders of magnitude reduction in terms of search space. The reduction significantly alleviates the path explosion problem, enabling us to apply symbolic analysis in large programs such as Hadoop and Linux Kernel.</p>",1049-331X;1049331X,,10.1145/2652484,,,Symbolic analysis;path explosion;state equivalence detection,,,,,1,,,,,,14-Aug,,ACM,ACM Journals & Magazines,,31
Stochastic Performance Analysis of Global Software Development Teams,R. M. Czekster; P. Fernandes; L. Lopes; A. Sales; A. R. Santos; T. Webber,UNISC - University of Santa Cruz do Sul,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,3,1,32,"<p>Measuring productivity in globally distributed projects is crucial to improve team performance. These measures often display information on whether a given project is moving forward or starts to demonstrate undesired behaviors. In this paper we are interested in showing how analytical models could deliver insights for the behavior of specific distributed software collaboration projects. We present a model for distributed projects using stochastic automata networks (SAN) formalism to estimate, for instance, the required level of coordination for specific project configurations. We focus our attention on the level of interaction among project participants and its close relation with teamäó»s productivity. The models are parameterized for different scenarios and solved using numerical methods to obtain exact solutions. We vary the teamäó»s expertise and support levels to measure the impact on the overall project performance. As results, we present our derived productivity index for all scenarios and we state implications found in order to analyze popular preconceptions in GSD area, confirming some, and refusing others. Finally, we foresee ways to extend the models to represent more intricate behaviors and communication patterns that are usually present in globally distributed software projects.</p>",1049-331X;1049331X,,10.1145/2955093,,,Global software development;analytical modeling;performance analysis;stochastic automata networks,,,,,,,,,,,16-Aug,,ACM,ACM Journals & Magazines,,31
Software effort estimation as a multiobjective learning problem,L. L. Minku; X. Yao,"The University of Birmingham, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,32,"<p>Ensembles of learning machines are promising for software effort estimation (SEE), but need to be tailored for this task to have their potential exploited. A key issue when creating ensembles is to produce diverse and accurate base models. Depending on how differently different performance measures behave for SEE, they could be used as a natural way of creating SEE ensembles. We propose to view SEE model creation as a multiobjective learning problem. A multiobjective evolutionary algorithm (MOEA) is used to better understand the tradeoff among different performance measures by creating SEE models through the simultaneous optimisation of these measures. We show that the performance measures behave very differently, presenting sometimes even opposite trends. They are then used as a source of diversity for creating SEE ensembles. A good tradeoff among different measures can be obtained by using an ensemble of MOEA solutions. This ensemble performs similarly or better than a model that does not consider these measures explicitly. Besides, MOEA is also flexible, allowing emphasis of a particular measure if desired. In conclusion, MOEA can be used to better understand the relationship among performance measures and has shown to be very effective in creating SEE models.</p>",1049-331X;1049331X,,10.1145/2522920.2522928,,,Software effort estimation;ensembles of learning machines;multi-objective evolutionary algorithms,,,,,4,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,31
The value of design rationale information,D. Falessi; L. C. Briand; G. Cantone; R. Capilla; P. Kruchten,"University of Rome Tor Vergata and Simula Research Laboratory, Norway",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,32,"<p>A complete and detailed (full) Design Rationale Documentation (DRD) could support many software development activities, such as an impact analysis or a major redesign. However, this is typically too onerous for systematic industrial use as it is not cost effective to write, maintain, or read. The key idea investigated in this article is that DRD should be developed only to the extent required to support activities particularly difficult to execute or in need of significant improvement in a particular context. The aim of this article is to empirically investigate the customization of the DRD by documenting only the information items that will probably be required for executing an activity. This customization strategy relies on the hypothesis that the value of a specific DRD information item depends on its category (e.g., assumptions, related requirements, etc.) and on the activity it is meant to support. We investigate this hypothesis through two controlled experiments involving a total of 75 master students as experimental subjects. Results show that the value of a DRD information item significantly depends on its category and, within a given category, on the activity it supports. Furthermore, on average among activities, documenting only the information items that have been required at least half of the time (i.e., the information that will probably be required in the future) leads to a customized DRD containing about half the information items of a full documentation. We expect that such a significant reduction in DRD information should mitigate the effects of some inhibitors that currently prevent practitioners from documenting design decision rationale.</p>",1049-331X;1049331X,,10.1145/2491509.2491515,,,Empirical software engineering;design decisions;software architecture;software maintenance;value-based software engineering,,,,,5,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,31
A model for spectra-based software diagnosis,L. Naish; H. J. Lee; K. Ramamohanarao,"University of Melbourne, Melbourne, Victoria, Australia",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,3,1,32,"<p>This article presents an improved approach to assist diagnosis of failures in software (fault localisation) by ranking program statements or blocks in accordance with to how likely they are to be buggy. We present a very simple single-bug program to model the problem. By examining different possible execution paths through this model program over a number of test cases, the effectiveness of different proposed spectral ranking methods can be evaluated in idealised conditions. The results are remarkably consistent to those arrived at empirically using the Siemens test suite and Space benchmarks. The model also helps identify groups of metrics that are equivalent for ranking. Due to the simplicity of the model, an optimal ranking method can be devised. This new method out-performs previously proposed methods for the model program, the Siemens test suite and Space. It also helps provide insight into other ranking methods.</p>",1049-331X;1049331X,,10.1145/2000791.2000795,,,Fault localization;program spectra;statistical debugging,,,,,58,,,,,,11-Aug,,ACM,ACM Journals & Magazines,,31
A Unified Test Case Prioritization Approach,D. Hao; L. Zhang; L. Zhang; G. Rothermel; H. Mei,"Peking University, Beijing, China",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,2,1,31,"<p>Test case prioritization techniques attempt to reorder test cases in a manner that increases the rate at which faults are detected during regression testing. Coverage-based test case prioritization techniques typically use one of two overall strategies: a <i>total strategy</i> or an <i>additional strategy</i>. These strategies prioritize test cases based on the total number of code (or code-related) elements covered per test case and the number of additional (not yet covered) code (or code-related) elements covered per test case, respectively. In this article, we present a unified test case prioritization approach that encompasses both the total and additional strategies. Our unified test case prioritization approach includes two models (<i>basic</i> and <i>extended</i>) by which a spectrum of test case prioritization techniques ranging from a purely total to a purely additional technique can be defined by specifying the value of a parameter referred to as the <i>f</i><sub><i>p</i></sub> value. To evaluate our approach, we performed an empirical study on 28 Java objects and 40 C objects, considering the impact of three internal factors (model type, choice of<i>f</i><sub><i>p</i></sub> value, and coverage type) and three external factors (coverage granularity, test case granularity, and programming/testing paradigm), all of which can be manipulated by our approach. Our results demonstrate that a wide range of techniques derived from our basic and extended models with uniform <i>f</i><sub><i>p</i></sub> values can outperform purely total techniques and are competitive with purely additional techniques. Considering the influence of each internal and external factor studied, the results demonstrate that various values of each factor have nontrivial influence on test case prioritization techniques.</p>",1049-331X;1049331X,,10.1145/2685614,,,Software testing;additional strategy;test case prioritization;total strategy,,,,,12,,,,,,14-Dec,,ACM,ACM Journals & Magazines,,30
Intelligent Code Completion with Bayesian Networks,S. Proksch; J. Lerch; M. Mezini,"Technische Universit&#228;t Darmstadt, Darmstadt, Germany",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,31,"<p>Code completion is an integral part of modern <i>Integrated Development Environments</i> (IDEs). Developers often use it to explore <i>Application Programming Interfaces</i> (APIs). It is also useful to reduce the required amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code completion systems have been proposed in prior work to reduce the list of proposed methods to relevant items.</p> <p>This work extends one of these existing approaches, the <i>Best Matching Neighbor</i> (BMN) algorithm. We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. We compare our new approach, <i>Pattern-based Bayesian Networks</i> (PBN), to the existing BMN algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed.</p> <p>Our results show that the additional context information we collect improves prediction quality, especially for queries that do not contain method calls. We also show that PBN can obtain comparable prediction quality to BMN, while model size and inference speed scale better with large input sizes.</p>",1049-331X;1049331X,,10.1145/2744200,,,Content assist;code completion;code recommender;evaluation;integrated development environments;machine learning;productivity,,,,,5,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,30
Exact scalable sensitivity analysis for the next release problem,M. Harman; J. Krinke; I. Medina-Bulo; F. Palomo-Lozano; J. Ren; S. Yoo,University College London,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,31,"<p>The nature of the requirements analysis problem, based as it is on uncertain and often inaccurate estimates of costs and effort, makes sensitivity analysis important. Sensitivity analysis allows the decision maker to identify those requirements and budgets that are particularly sensitive to misestimation. However, finding scalable sensitivity analysis techniques is not easy because the underlying optimization problem is NP-hard. This article introduces an approach to sensitivity analysis based on exact optimization. We implemented this approach as a tool, O<scp>ATSAC</scp>, which allowed us to experimentally evaluate the scalability and applicability of Requirements Sensitivity Analysis (RSA). Our results show that O<scp>ATSAC</scp> scales sufficiently well for practical applications in Requirements Sensitivity Analysis. We also show how the sensitivity analysis can yield insights into difficult and otherwise obscure interactions between budgets, requirements costs, and estimate inaccuracies using a real-world case study.</p>",1049-331X;1049331X,,10.1145/2537853,,,Next release problem;requirement engineering;sensitivity analysis,,,,,2,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,30
Use case and task models: An integrated development methodology and its formal foundation,D. Sinnig; P. Chalin; F. Khendek,"University of Rostock, Germany",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,31,"<p>User Interface (UI) development methods are poorly integrated with standard software engineering practice. The differences in terms of artifacts involved, development philosophies, and lifecycles can often result in inconsistent system and UI specifications leading to duplication of effort and increased maintenance costs. To address such shortcomings, we propose an integrated development methodology for use case and task models. Use cases are generally used to capture functional requirements whereas task models specify the detailed user interactions with the UI. Our methodology can assist practitioners in developing software processes which allow these two kinds of artifacts to be developed in a codependent and integrated manner. We present our methodology, describe its semantic foundations along with a set of formal conformance relations, and introduce an automated verification tool.</p>",1049-331X;1049331X,,10.1145/2491509.2491521,,,Use case models;conformance;integrated development methodology;task models;verification,,,,,1,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,30
Using a functional size measurement procedure to evaluate the quality of models in MDD environments,B. Marí_n; G. Giachetti; O. Pastor; T. E. J. Vos; A. Abran,"Universidad Diego Portales, Chile",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,31,"<p>Models are key artifacts in Model-Driven Development (MDD) methods. To produce high-quality software by using MDD methods, quality assurance of models is of paramount importance. To evaluate the quality of models, defect detection is considered a suitable approach and is usually applied using reading techniques. However, these reading techniques have limitations and constraints, and new techniques are required to improve the efficiency at finding as many defects as possible. This article presents a case study that has been carried out to evaluate the use of a Functional Size Measurement (FSM) procedure in the detection of defects in models of an MDD environment. To do this, we compare the defects and the defect types found by an inspection group with the defects and the defect types found by the FSM procedure. The results indicate that the FSM is useful since it finds all the defects related to a specific defect type, it finds different defect types than an inspection group, and it finds defects related to the correctness and the consistency of the models.</p>",1049-331X;1049331X,,10.1145/2491509.2491520,,,Case study;defect detection;functional size;model-driven development,,,,,1,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,30
Human Competitiveness of Genetic Programming in Spectrum-Based Fault Localisation: Theoretical and Empirical Analysis,S. Yoo; X. Xie; F. C. Kuo; T. Y. Chen; M. Harman,"Korea Advanced Institute of Science and Technology, Republic of Korea",ACM Transactions on Software Engineering and Methodology (TOSEM),20170907,2017,26,1,1,30,"<p>We report on the application of Genetic Programming to Software Fault Localisation, a problem in the area of Search-Based Software Engineering (SBSE). We give both empirical and theoretical evidence for the human competitiveness of the evolved fault localisation formulí_ under the single fault scenario, compared to those generated by human ingenuity and reported in many papers, published over more than a decade. Though there have been previous human competitive results claimed for SBSE problems, this is the first time that evolved solutions have been formally proved to be human competitive. We further prove that no future human investigation could outperform the evolved solutions. We complement these proofs with an empirical analysis of both human and evolved solutions, which indicates that the evolved solutions are not only theoretically human competitive, but also convey similar practical benefits to human-evolved counterparts.</p>",1049-331X;1049331X,,10.1145/3078840,,,Spectrum-based fault localisation;genetic programming;search-based software engineering,,,,,,,,,,,17-Jul,,ACM,ACM Journals & Magazines,,29
DIG: A Dynamic Invariant Generator for Polynomial and Array Invariants,T. Nguyen; D. Kapur; W. Weimer; S. Forrest,University of New Mexico,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,23,4,1,30,"<p>This article describes and evaluates DIG, a dynamic invariant generator that infers invariants from observed program traces, focusing on numerical and array variables. For numerical invariants, DIG supports both nonlinear equalities and inequalities of arbitrary degree defined over numerical program variables. For array invariants, DIG generates nested relations among multidimensional array variables. These properties are nontrivial and challenging for current static and dynamic invariant analysis methods. The key difference between DIG and existing dynamic methods is its generative technique, which infers invariants directly from traces, instead of using traces to filter out predefined templates. To generate accurate invariants, DIG employs ideas and tools from the mathematical and formal methods domains, including equation solving, polyhedra construction, and theorem proving; for example, DIG represents and reasons about polynomial invariants using geometric shapes. Experimental results on 27 mathematical algorithms and an implementation of AES encryption provide evidence that DIG is effective at generating invariants for these programs.</p>",1049-331X;1049331X,,10.1145/2556782,,,Program analysis;array invariants;dynamic analysis;geometric invariant inference;invariant generation;nonlinear invariants;theorem proving,,,,,2,,,,,,14-Aug,,ACM,ACM Journals & Magazines,,29
Automated Detection of Client-State Manipulation Vulnerabilities,A. MíŸller; M. Schwarz,"Aarhus University, Aarhus, Denmark",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,23,4,1,30,"<p>Web application programmers must be aware of a wide range of potential security risks. Although the most common pitfalls are well described and categorized in the literature, it remains a challenging task to ensure that all guidelines are followed. For this reason, it is desirable to construct automated tools that can assist the programmers in the application development process by detecting weaknesses. Many vulnerabilities are related to Web application code that stores references to application state in the generated HTML documents to work around the statelessness of the HTTP protocol. In this article, we show that such client-state manipulation vulnerabilities are amenable to tool-supported detection.</p> <p>We present a static analysis for the widely used frameworks Java Servlets, JSP, and Struts. Given a Web application archive as input, the analysis identifies occurrences of client state and infers the information flow between the client state and the shared application state on the server. This makes it possible to check how client-state manipulation performed by malicious users may affect the shared application state and cause leakage or modifications of sensitive information. The warnings produced by the tool help the application programmer identify vulnerabilities before deployment. The inferred information can also be applied to configure a security filter that automatically guards against attacks at runtime. Experiments on a collection of open-source Web applications indicate that the static analysis is able to effectively help the programmer prevent client-state manipulation vulnerabilities. The analysis detects a total of 4,802 client-state parameters in ten applications, whereof 4,437 are classified as safe and 241 reveal exploitable vulnerabilities.</p>",1049-331X;1049331X,,10.1145/2531921,,,Web application security;information flow analysis;static analysis,,,,,2,,,,,,14-Aug,,ACM,ACM Journals & Magazines,,29
Effective Techniques for Static Race Detection in Java Parallel Loops,C. Radoi; D. Dig,"University of Illinois, Urbana-Champaign",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,4,1,30,"<p>Despite significant progress in recent years, the important problem of static race detection remains open. Previous techniques took a general approach and looked for races by analyzing the effects induced by low-level concurrency constructs (e.g., java.lang.Thread). But constructs and libraries for expressing parallelism at a higher level (e.g., fork-join, futures, parallel loops) are becoming available in all major programming languages. We claim that specializing an analysis to take advantage of the extra semantic information provided by the use of these constructs and libraries improves precision and scalability.</p> <p>We present I<scp>te</scp>R<scp>ace</scp>, a set of techniques that are specialized to use the intrinsic thread, safety, and dataflow structure of collections and of the new loop parallelism mechanism introduced in Java 8. Our evaluation shows that I<scp>te</scp>R<scp>ace</scp> is fast and precise enough to be practical. It scales to programs of hundreds of thousands of lines of code and reports very few race warnings, thus avoiding a common pitfall of static analyses. In five out of the seven case studies, I<scp>te</scp>R<scp>ace</scp> reported no false warnings. Also, it revealed six bugs in real-world applications. We reported four of them: one had already been fixed, and three were new and the developers confirmed and fixed them.</p> <p>Furthermore, we evaluate the effect of each specialization technique on the running time and precision of the analysis. For each application, we run the analysis under 32 different configurations. This allows to analyze each technique's effect both alone and in all possible combinations with other techniques.</p>",1049-331X;1049331X,,10.1145/2729975,,,Java;Static race detection;static analysis;synchronization,,,,,1,,,,,,15-Aug,,ACM,ACM Journals & Magazines,,29
Portfolio: Searching for relevant functions and their usages in millions of lines of code,C. Mcmillan; D. Poshyvanyk; M. Grechanik; Q. Xie; C. Fu,"University of Notre Dame, South Bend, IN",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,30,"<p>Different studies show that programmers are more interested in finding definitions of functions and their uses than variables, statements, or ordinary code fragments. Therefore, developers require support in finding relevant functions and determining how these functions are used. Unfortunately, existing code search engines do not provide enough of this support to developers, thus reducing the effectiveness of code reuse. We provide this support to programmers in a code search system called <i>Portfolio</i> that retrieves and visualizes relevant functions and their usages. We have built Portfolio using a combination of models that address surfing behavior of programmers and sharing related concepts among functions. We conducted two experiments: first, an experiment with 49 C/C&plus;&plus; programmers to compare Portfolio to Google Code Search and Koders using a standard methodology for evaluating information-retrieval-based engines; and second, an experiment with 19 Java programmers to compare Portfolio to Koders. The results show with strong statistical significance that users find more relevant functions with higher precision with Portfolio than with Google Code Search and Koders. We also show that by using PageRank, Portfolio is able to rank returned relevant functions more efficiently.</p>",1049-331X;1049331X,,10.1145/2522920.2522930,,,Pagerank;Source-code search;information retrieval;natural language processing;user studies,,,,,5,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,29
Understanding Integer Overflow in C/C++,W. Dietz; P. Li; J. Regehr; V. Adve,University of Illinois at Urbana-Champaign,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,25,1,1,29,"<p>Integer overflow bugs in C and C++ programs are difficult to track down and may lead to fatal errors or exploitable vulnerabilities. Although a number of tools for finding these bugs exist, the situation is complicated because not all overflows are bugs. Better tools need to be constructed, but a thorough understanding of the issues behind these errors does not yet exist. We developed IOC, a dynamic checking tool for integer overflows, and used it to conduct the first detailed empirical study of the prevalence and patterns of occurrence of integer overflows in C and C++ code. Our results show that intentional uses of wraparound behaviors are more common than is widely believed; for example, there are over 200 distinct locations in the SPEC CINT2000 benchmarks where overflow occurs. Although many overflows are intentional, a large number of accidental overflows also occur. Orthogonal to programmers' intent, overflows are found in both well-defined and undefined flavors. Applications executing undefined operations can be, and have been, broken by improvements in compiler optimizations. Looking beyond SPEC, we found and reported undefined integer overflows in SQLite, PostgreSQL, SafeInt, GNU MPC and GMP, Firefox, LLVM, Python, BIND, and OpenSSL; many of these have since been fixed.</p>",1049-331X;1049331X,,10.1145/2743019,,,Integer overflow;integer wraparound;undefined behavior,,,,,3,,,,,,15-Dec,,ACM,ACM Journals & Magazines,,28
Combining Centralised and Distributed Testing,R. M. Hierons,"Brunel University, Uxbridge, Middlesex, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,1,1,29,"<p>Many systems interact with their environment at distributed interfaces (ports) and sometimes it is not possible to place synchronised local testers at the ports of the system under test (SUT). There are then two main approaches to testing: having independent local testers or a single centralised tester that interacts asynchronously with the SUT. The power of using independent testers has been captured using implementation relation dioco. In this article, we define implementation relation diococ for the centralised approach and prove that dioco and dioco<sub><i>c</i></sub> are incomparable. This shows that the frameworks detect different types of faults and so we devise a hybrid framework and define an implementation relation diocos for this. We prove that the hybrid framework is more powerful than the distributed and centralised approaches. We then prove that the Oracle problem is NP-complete for diococ and diocos but can be solved in polynomial time if we place an upper bound on the number of ports. Finally, we consider the problem of deciding whether there is a test case that is guaranteed to force a finite state model into a particular state or to distinguish two states, proving that both problems are undecidable for the centralised and hybrid frameworks.</p>",1049-331X;1049331X,,10.1145/2661296,,,Centralised testing;distributed testing;model-based testing,,,,,3,,,,,,14-Sep,,ACM,ACM Journals & Magazines,,28
Inflow and Retention in OSS Communities with Commercial Involvement: A Case Study of Three Hybrid Projects,M. Zhou; A. Mockus; X. Ma; L. Zhang; H. Mei,"Peking University, Beijing",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,2,1,29,"<p><b>Motivation:</b> Open-source projects are often supported by companies, but such involvement often affects the robust contributor inflow needed to sustain the project and sometimes prompts key contributors to leave. To capture user innovation and to maintain quality of software and productivity of teams, these projects need to attract and retain contributors. <b>Aim:</b> We want to understand and quantify how inflow and retention are shaped by policies and actions of companies in three application server projects. <b>Method:</b> We identified three hybrid projects implementing the same JavaEE specification and used published literature, online materials, and interviews to quantify actions and policies companies used to get involved. We collected project repository data, analyzed affiliation history of project participants, and used generalized linear models and survival analysis to measure contributor inflow and retention. <b>Results:</b> We identified coherent groups of policies and actions undertaken by sponsoring companies as three models of community involvement and quantified tradeoffs between the inflow and retention each model provides. We found that full control mechanisms and high intensity of commercial involvement were associated with a decrease of external inflow and with improved retention. However, a shared control mechanism was associated with increased external inflow contemporaneously with the increase of commercial involvement. <b>Implications:</b> Inspired by a natural experiment, our methods enabled us to quantify aspects of the balance between community and private interests in open- source software projects and provide clear implications for the structure of future open-source communities.</p>",1049-331X;1049331X,,10.1145/2876443,,,Hybrid project;commercial involvement;contributor inflow;contributor retention;extent and intensity of involvement;natural experiment,,,,,2,,,,,,16-May,,ACM,ACM Journals & Magazines,,28
A web-centred approach to end-user software engineering,D. Lizcano; F. Alonso; J. Soriano; G. Lopez,"Universidad a Distancia de Madrid, UDIMA, Spain",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,29,"<p>This article addresses one of the major end-user software engineering (EUSE) challenges, namely, how to motivate end users to apply unfamiliar software engineering techniques and activities to achieve their goal: translate requirements into software that meets their needs. EUSE activities are secondary to the goal that the program is helping to achieve and end-user programming is opportunistic. The challenge is then to find ways to incorporate EUSE activities into the existing workflow without users having to make substantial changes to the type of work they do or their priorities. In this article, we set out an approach to EUSE for web-based applications. We also propose a software lifecycle that is consistent with the conditions and priorities of end users without programming skills and is well-aligned with EUSE's characteristic informality, ambiguity and opportunisticness. Users applying this lifecycle manage to find solutions that they would otherwise be unable to identify. They also develop quality products. Users of this approach will not have to be acquainted with software engineering, as a framework will take them through the web-centred EUSE lifecycle step-by-step. We also report a statistical experiment in which users develop web software with and without a framework to guide them through the lifecycle. Its aim is to validate the applicability of our framework-driven lifecycle.</p>",1049-331X;1049331X,,10.1145/2522920.2522929,,,End-user programming;end-user development;human-computer interaction;visual programming,,,,,,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,28
A formal model for automated software modularity and evolvability analysis,Y. Cai; K. Sullivan,"Drexel University, Philadelphia, PA",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,4,1,29,"<p>Neither the nature of modularity in software design, characterized as a property of the structure of dependencies among design decisions, or its economic value are adequately well understood. One basic problem is that we do not even have a sufficiently clear definition of what it means for one design decision to depend on another. The main contribution of this work is one possible mathematically precise definition of <i>dependency</i> based on an <i>augmented constraint network</i> model. The model provides an end-to-end account of the connection between modularity and its value in terms of options to make adaptive changes in uncertain and changing design spaces. We demonstrate the validity and theoretical utility of the model, showing that it is consistent with, and provides new insights into, several previously published results in design theory.</p>",1049-331X;1049331X,,10.1145/2377656.2377658,,,Design modeling;augmented constraint network;design analysis;design structure matrix;evolution;modularity;software economics,,,,,,,,,,,12-Nov,,ACM,ACM Journals & Magazines,,28
DARWIN: An approach to debugging evolving programs,D. Qi; A. Roychoudhury; Z. Liang; K. Vaswani,"National University of Singapore, Singapore",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,3,1,29,"<p>Bugs in programs are often introduced when programs evolve from a stable version to a new version. In this article, we propose a new approach called <i>DARWIN</i> for automatically finding potential root causes of such bugs. Given two programsäóîa reference program and a modified programäóîand an input that fails on the modified program, our approach uses symbolic execution to automatically synthesize a new input that (a) is very similar to the failing input and (b) does not fail. We find the potential cause(s) of failure by comparing control-flow behavior of the passing and failing inputs and identifying code fragments where the control flows diverge.</p> <p>A notable feature of our approach is that it handles hard-to-explain bugs, like code missing errors, by pointing to code in the reference program. We have implemented this approach and conducted experiments using several real-world applications, such as the Apache Web server, libPNG (a library for manipulating PNG images), and TCPflow (a program for displaying data sent through TCP connections). In each of these applications, <i>DARWIN</i> was able to localize bugs with high accuracy. Even though these applications contain several thousands of lines of code, <i>DARWIN</i> could usually narrow down the potential root cause(s) to less than ten lines. In addition, we find that the inputs synthesized by <i>DARWIN</i> provide additional value by revealing other undiscovered errors.</p>",1049-331X;1049331X,,10.1145/2211616.2211622,,,Software debugging;software evolution;symbolic execution,,,,,12,,,,,,12-Jun,,ACM,ACM Journals & Magazines,,28
Modeling and verifying hierarchical real-time systems using stateful timed CSP,J. Sun; Y. Liu; J. S. Dong; Y. Liu; L. Shi; í_. Andrí©,Singapore University of Technology and Design,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,1,1,29,"<p>Modeling and verifying complex real-time systems are challenging research problems. The <i>de facto</i> approach is based on Timed Automata, which are finite state automata equipped with clock variables. Timed Automata are deficient in modeling hierarchical complex systems. In this work, we propose a language called <i>Stateful Timed CSP</i> and an automated approach for verifying Stateful Timed CSP models. Stateful Timed CSP is based on Timed CSP and is capable of specifying hierarchical real-time systems. Through dynamic zone abstraction, <i>finite-state</i> zone graphs can be generated automatically from Stateful Timed CSP models, which are subject to model checking. Like Timed Automata, Stateful Timed CSP models suffer from Zeno runs, that is, system runs that take infinitely many steps within finite time. Unlike Timed Automata, model checking with non-Zenoness in Stateful Timed CSP can be achieved based on the zone graphs. We extend the PAT model checker to support system modeling and verification using Stateful Timed CSP and show its usability/scalability via verification of real-world systems.</p>",1049-331X;1049331X,,10.1145/2430536.2430537,,,Non-Zenoness;PAT;Stateful Timed CSP;Zone Abstraction,,,,,8,,,,,,13-Feb,,ACM,ACM Journals & Magazines,,28
Fault localization prioritization: Comparing information-theoretic and coverage-based approaches,S. Yoo; M. Harman; D. Clark,"University College London, UK",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,29,"<p>Test case prioritization techniques seek to maximize early fault detection. Fault localization seeks to use test cases already executed to help find the fault location. There is a natural interplay between the two techniques; once a fault is detected, we often switch focus to fault fixing, for which localization may be a first step. In this article we introduce the Fault Localization Prioritization (FLP) problem, which combines prioritization and localization. We evaluate three techniques: a novel FLP technique based on information theory, FLINT (Fault Localization using INformation Theory), that we introduce in this article, a standard Test Case Prioritization (TCP) technique, and a äóìtest similarity techniqueäóù used in previous work. Our evaluation uses five different releases of four software systems. The results indicate that FLP and TCP can statistically significantly reduce fault localization costs for 73&percnt; and 76&percnt; of cases, respectively, and that FLINT significantly outperforms similarity-based localization techniques in 52&percnt; of the cases considered in the study.</p>",1049-331X;1049331X,,10.1145/2491509.2491513,,,Test case prioritization;fault localization;information theory,,,,,17,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,28
An Approach for Modeling Architectural Design Rules in UML and its Application to Embedded Software,A. Mattsson; B. Fitzgerald; B. Lundell; B. Lings,"Combitech AB, Sweden and University of Limerick, Ireland",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2012,21,2,1,29,"<p>Current techniques for modeling software architecture do not provide sufficient support for modeling architectural design rules. This is a problem in the context of model-driven development in which it is assumed that major design artifacts are represented as formal or semi-formal models. This article addresses this problem by presenting an approach to modeling architectural design rules in UML at the abstraction level of the meaning of the rules. The high abstraction level and the use of UML makes the rules both amenable to automation and easy to understand for both architects and developers, which is crucial to deployment in an organization. To provide a proof-of-concept, a tool was developed that validates a system model against the architectural rules in a separate UML model. To demonstrate the feasibility of the approach, the architectural design rules of an existing live industrial-strength system were modeled according to the approach.</p>",1049-331X;1049331X,,10.1145/2089116.2089120,,,Model-driven development (MDD);embedded software development;model-driven engineering (MDE),,,,,0,,,,,,12-Mar,,ACM,ACM Journals & Magazines,,28
Amplifying Tests to Validate Exception Handling Code: An Extended Study in the Mobile Application Domain,P. Zhang; S. Elbaum,University of Nebraska-Lincoln,ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,23,4,1,28,"<p>Validating code handling exceptional behavior is difficult, particularly when dealing with external resources that may be noisy and unreliable, as it requires (1) systematic exploration of the space of exceptions that may be thrown by the external resources, and (2) setup of the context to trigger specific patterns of exceptions. In this work, we first present a study quantifying the magnitude of the problem by inspecting the bug repositories of a set of popular applications in the increasingly relevant domain of Android mobile applications. The study revealed that 22&percnt; of the confirmed and fixed bugs have to do with poor exceptional handling code, and half of those correspond to interactions with external resources. We then present an approach that addresses this challenge by performing an systematic amplification of the program space explored by a test by manipulating the behavior of external resources. Each amplification attempts to expose a program's exception handling constructs to new behavior by mocking an external resource so that it returns normally or throws an exception following a predefined set of patterns. Our assessment of the approach indicates that it can be fully automated, is powerful enough to detect 67&percnt; of the faults reported in the bug reports of this kind, and is precise enough that 78&percnt; of the detected anomalies are fixed, and it has a great potential to assist developers.</p>",1049-331X;1049331X,,10.1145/2652483,,,Test transformation;exception handling;mobile applications;test amplification;test case generation,,,,,1,,,,,,14-Aug,,ACM,ACM Journals & Magazines,,27
Using Cohesion and Coupling for Software Remodularization: Is It Enough?,I. Candela; G. Bavota; B. Russo; R. Oliveto,"University of Molise, Pesche (IS), Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,3,1,28,"<p>Refactoring and, in particular, remodularization operations can be performed to repair the design of a software system and remove the erosion caused by software evolution. Various approaches have been proposed to support developers during the remodularization of a software system. Most of these approaches are based on the underlying assumption that developers pursue an optimal balance between cohesion and coupling when modularizing the classes of their systems. Thus, a remodularization recommender proposes a solution that implicitly provides a (near) optimal balance between such quality attributes. However, there is still no empirical evidence that such a balance is the <i>desideratum</i> by developers. This article aims at analyzing both objectively and subjectively the aforementioned phenomenon. Specifically, we present the results of (1) a large study analyzing the modularization quality, in terms of package cohesion and coupling, of 100 open-source systems, and (2) a survey conducted with 29 developers aimed at understanding the driving factors they consider when performing modularization tasks. The results achieved have been used to distill a set of lessons learned that might be considered to design more effective remodularization recommenders.</p>",1049-331X;1049331X,,10.1145/2928268,,,Remodularization;software quality,,,,,2,,,,,,16-Aug,,ACM,ACM Journals & Magazines,,27
A Methodology for Exposing Risk in Achieving Emergent System Properties,L. Layman; V. R. Basili; M. V. Zelkowitz,"Fraunhofer Center for Experimental Software Engineering, College Park, MD",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,3,1,28,"<p>Determining whether systems achieve desired emergent properties, such as safety or reliability, requires an analysis of the system as a whole, often in later development stages when changes are difficult and costly to implement. In this article we propose the Process Risk Indicator (PRI) methodology for analyzing and evaluating emergent properties early in the development cycle. A fundamental assumption of system engineering is that risk mitigation processes reduce system risks, yet these processes may also be a source of risk: (1) processes may not be appropriate for achieving the desired emergent property; or (2) processes may not be followed appropriately. PRI analyzes development process artifacts (e.g., designs pertaining to reliability or safety analysis reports) to quantify process risks that may lead to higher system risk. We applied PRI to the hazard analysis processes of a network-centric, Department of Defense system-of-systems and two NASA spaceflight projects to assess the risk of not achieving one such emergent property, software safety, during the early stages of the development lifecycle. The PRI methodology was used to create measurement baselines for process indicators of software safety risk, to identify risks in the hazard analysis process, and to provide feedback to projects for reducing these risks.</p>",1049-331X;1049331X,,10.1145/2560048,,,Process risk;risk measurement;software safety,,,,,2,,,,,,14-May,,ACM,ACM Journals & Magazines,,27
Required behavior of sequence diagrams: Semantics and conformance,L. Lu; D. K. Kim,Oakland University,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,28,"<p>Many reusable software artifacts such as design patterns and design aspects make use of UML sequence diagrams to describe interaction behaviors. When a pattern or an aspect is reused in an application, it is important to ensure that the sequence diagrams for the application conform to the corresponding sequence diagrams for the pattern or aspect. Reasoning about conformance relationship between sequence diagrams has not been addressed adequately in literature. In this article, we focus on required behaviors specified by a UML sequence diagram and provide a semantic-based formalization of conformance relationships between sequence diagrams. A novel trace semantics is first given that captures precisely required behaviors. A refinement relation between sequence diagrams is then defined based on the semantics. The refinement relation allows a sequence diagram to be refined by changing its structure so long as its required behaviors are preserved. A conformance relation between sequence diagrams is finally given that includes the refinement relation as a special case. It allows one to introduce and rename lifelines, messages, and system variables when reusing sequence diagrams. Properties of the semantics, refinement, and conformance relations are studied. Two case studies are provided to illustrate the efficacy of semantic-based conformance reasoning.</p>",1049-331X;1049331X,,10.1145/2523108,,,Sequence diagram;conformance;design aspect;design pattern;refinement;required behavior;semantics;trace semantics,,,,,1,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,27
"HAMPI: A solver for word equations over strings, regular expressions, and context-free grammars",A. Kiezun; V. Ganesh; S. Artzi; P. J. Guo; P. Hooimeijer; M. D. Ernst,Brigham and Women's Hospital/Harvard Medical School,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,4,1,28,"<p>Many automatic testing, analysis, and verification techniques for programs can be effectively reduced to a constraint-generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable software reliability tools. The increasing efficiency of off-the-shelf constraint solvers makes this approach even more compelling. However, there are few effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis of string-manipulating programs, so researchers end up implementing their own ad-hoc solvers.</p> <p>To fulfill this need, we designed and implemented Hampi, a solver for string constraints over bounded string variables. Users of Hampi specify constraints using regular expressions, context-free grammars, equality between string terms, and typical string operations such as concatenation and substring extraction. Hampi then finds a string that satisfies all the constraints or reports that the constraints are unsatisfiable.</p> <p>We demonstrate Hampi's expressiveness and efficiency by applying it to program analysis and automated testing. We used Hampi in static and dynamic analyses for finding SQL injection vulnerabilities in Web applications with hundreds of thousands of lines of code. We also used Hampi in the context of automated bug finding in C programs using dynamic systematic testing (also known as concolic testing). We then compared Hampi with another string solver, CFGAnalyzer, and show that Hampi is several times faster. Hampi's source code, documentation, and experimental data are available at http://people.csail.mit.edu/akiezun/hampi<sup>1</sup></p>",1049-331X;1049331X,,10.1145/2377656.2377662,,,String constraints;concolic testing;context-free languages;program analysis;regular languages;word equations,,,,,4,,,,,,12-Nov,,ACM,ACM Journals & Magazines,,27
Validating software metrics: A spectrum of philosophies,A. Meneely; B. Smith; L. Williams,North Carolina State University,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2012,21,4,1,28,"<p><i>Context</i>. Researchers proposing a new metric have the burden of proof to demonstrate to the research community that the metric is acceptable in its intended use. This burden of proof is provided through the multi-faceted, scientific, and objective process of software metrics validation. Over the last 40 years, however, researchers have debated what constitutes a äóìvalidäóù metric.</p> <p><i>Aim</i>. The debate over what constitutes a valid metric centers on software metrics validation criteria. The objective of this article is to guide researchers in making sound contributions to the field of software engineering metrics by providing a practical summary of the metrics validation criteria found in the academic literature.</p> <p><i>Method</i>. We conducted a systematic literature review that began with 2,288 papers and ultimately focused on 20 papers. After extracting 47 unique validation criteria from these 20 papers, we performed a comparative analysis to explore the relationships amongst the criteria.</p> <p><i>Results</i>. Our 47 validation criteria represent a diverse view of what constitutes a valid metric. We present an analysis of the criteria's categorization, conflicts, common themes, and philosophical motivations behind the validation criteria.</p> <p><i>Conclusions</i>. Although the 47 validation criteria are not conflict-free, the diversity of motivations and philosophies behind the validation criteria indicates that metrics validation is complex. Researchers proposing new metrics should consider the applicability of the validation criteria in terms of our categorization and analysis. Rather than arbitrarily choosing validation criteria for each metric, researchers should choose criteria that can confirm that the metric is appropriate for its intended use. We conclude that metrics validation criteria provide answers to questions that researchers have about the merits and limitations of a metric.</p>",1049-331X;1049331X,,10.1145/2377656.2377661,,,Software metrics;systematic literature review;validation criterion,,,,,9,,,,,,12-Nov,,ACM,ACM Journals & Magazines,,27
Precise memory leak detection for java software using container profiling,G. Xu; A. Rountev,"University of California, Irvine, CA",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,3,1,28,"<p>A memory leak in a Java program occurs when object references that are no longer needed are unnecessarily maintained. Such leaks are difficult to detect because static analysis typically cannot precisely identify these redundant references, and existing dynamic leak detection tools track and report fine-grained information about individual objects, producing results that are usually hard to interpret and lack precision.</p> <p>In this article we introduce a novel <i>container-based</i> heap-tracking technique, based on the fact that many memory leaks in Java programs occur due to incorrect uses of containers, leading to containers that keep references to unused data entries. The novelty of the described work is twofold: (1) instead of tracking arbitrary objects and finding leaks by analyzing references to unused objects, the technique tracks only containers and directly identifies the source of the leak, and (2) the technique computes a confidence value for each container based on a combination of its memory consumption and its elements' staleness (time since last retrieval), while previous approaches do not consider such combined metrics. Our experimental results show that the reports generated by the proposed technique can be very precise: for two bugs reported by Sun, a known bug in SPECjbb 2000, and an example bug from IBM developerWorks, the top containers in the reports include the containers that leak memory.</p>",1049-331X;1049331X,,10.1145/2491509.2491511,,,Memory leaks;container profiling;leaking confidence,,,,,2,,,,,,13-Jul,,ACM,ACM Journals & Magazines,,27
Prevalence of coincidental correctness and mitigation of its impact on fault localization,W. Masri; R. A. Assi,"American University of Beirut, Beirut, Lebanon",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,1,1,28,"<p>Researchers have argued that for failure to be observed the following three conditions must be met: <i>C</i><sub><i>R</i></sub> &equals; the defect was reached; <i>C</i><sub><i>I</i></sub> &equals; the program has transitioned into an infectious state; and <i>C</i><sub><i>P</i></sub> &equals; the infection has propagated to the output. <i>Coincidental Correctness</i> (CC) arises when the program produces the correct output while condition C<sub>R</sub> is met but not C<sub>P</sub>. We recognize two forms of coincidental correctness, weak and strong. In <i>weak CC</i>, C<sub>R</sub> is met, whereas C<sub>I</sub> might or might not be met, whereas in <i>strong</i> <i>CC</i>, both C<sub>R</sub> and C<sub>I</sub> are met. In this work we first show that CC is prevalent in both of its forms and demonstrate that it is a safety reducing factor for <i>Coverage-Based Fault Localization</i> (CBFL). We then propose two techniques for cleansing test suites from coincidental correctness to enhance CBFL, given that the test cases have already been classified as failing or passing. We evaluated the effectiveness of our techniques by empirically quantifying their accuracy in identifying weak CC tests. The results were promising, for example, the better performing technique, using 105 test suites and statement coverage, exhibited 9&percnt; false negatives, 30&percnt; false positives, and no false negatives nor false positives in 14.3&percnt; of the test suites. Also using 73 test suites and more complex coverage, the numbers were 12&percnt;, 19&percnt;, and 15&percnt;, respectively.</p>",1049-331X;1049331X,,10.1145/2559932,,,Coincidental correctness;cluster analysis;coverage-based fault localization;fuzzy sets;strong coincidental correctness;weak coincidental correctness,,,,,10,,,,,,14-Feb,,ACM,ACM Journals & Magazines,,27
Ensuring the Consistency of Adaptation through Inter- and Intra-Component Dependency Analysis,A. Sadeghi; N. Esfahani; S. Malek,"University of California, Irvine, CA",ACM Transactions on Software Engineering and Methodology (TOSEM),20170615,2017,26,1,1,27,"<p>Dynamic adaptation should not leave a software system in an inconsistent state, as it could lead to failure. Prior research has used inter-component dependency models of a system to determine a safe interval for the adaptation of its components, where the most important tradeoff is between disruption in the operations of the system and reachability of safe intervals. This article presents Savasana, which automatically analyzes a software systemäó»s code to extract both inter- and intra-component dependencies. In this way, Savasana is able to obtain more fine-grained models compared to previous approaches. Savasana then uses the detailed models to find safe adaptation intervals that cannot be determined using techniques from prior research. This allows Savasana to achieve a better tradeoff between disruption and reachability. The article demonstrates how Savasana infers safe adaptation intervals for components of a software system under various use cases and conditions.</p>",1049-331X;1049331X,,10.1145/3063385,,,Adaptive software;component-based software;update criteria,,,,,,,,,,,17-Jul,,ACM,ACM Journals & Magazines,,26
"Data Model Property Inference, Verification, and Repair for Web Applications",J. Nijjar; I. BociÛˆ; T. Bultan,"University of California, Santa Barbara",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,4,1,27,"<p>Most software systems nowadays are Web-based applications that are deployed over compute clouds using a three-tier architecture, where the persistent data for the application is stored in a backend datastore and is accessed and modified by the server-side code based on the user interactions at the client-side. The data model forms the foundation of these three tiers, and identifies the sets of objects (object classes) and the relations among them (associations among object classes) stored by the application. In this article, we present a set of property patterns to specify properties of a data model, as well as several heuristics for automatically inferring them. We show that the specified or inferred data model properties can be automatically verified using bounded and unbounded verification techniques. For the properties that fail, we present techniques that generate fixes to the data model that establish the failing properties. We implemented this approach for Web applications built using the Ruby on Rails framework and applied it to ten open source applications. Our experimental results demonstrate that our approach is effective in automatically identifying and fixing errors in data models of real-world web applications.</p>",1049-331X;1049331X,,10.1145/2699691,,,Data models;Web applications;automated repair;automated verification,,,,,1,,,,,,15-Aug,,ACM,ACM Journals & Magazines,,26
Guided test generation for database applications via synthesized database interactions,K. Pan; X. Wu; T. Xie,"University of North Carolina at Charlotte, Charlotte, NC",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,27,"<p>Testing database applications typically requires the generation of tests consisting of both program inputs and database states. Recently, a testing technique called Dynamic Symbolic Execution (DSE) has been proposed to reduce manual effort in test generation for software applications. However, applying DSE to generate tests for database applications faces various technical challenges. For example, the database application under test needs to physically connect to the associated database, which may not be available for various reasons. The program inputs whose values are used to form the executed queries are not treated symbolically, posing difficulties for generating valid database states or appropriate database states for achieving high coverage of query-result-manipulation code. To address these challenges, in this article, we propose an approach called <i>SynDB</i> that synthesizes new database interactions to replace the original ones from the database application under test. In this way, we bridge various constraints within a database application: query-construction constraints, query constraints, database schema constraints, and query-result-manipulation constraints. We then apply a state-of-the-art DSE engine called Pex for .NET from Microsoft Research to generate both program inputs and database states. The evaluation results show that tests generated by our approach can achieve higher code coverage than existing test generation approaches for database applications.</p>",1049-331X;1049331X,,10.1145/2491529,,,Automatic test generation;database application testing;dynamic symbolic execution;synthesized database interactions,,,,,6,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,26
The Choice Calculus: A Representation for Software Variation,M. Erwig; E. Walkingshaw,Oregon State University,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,21,1,1,27,"<p>Many areas of computer science are concerned with some form of variation in software---from managing changes to software over time to supporting families of related artifacts. We present the choice calculus, a fundamental representation for software variation that can serve as a common language of discourse for variation research, filling a role similar to the lambda calculus in programming language research. We also develop an associated theory of software variation, including sound transformations of variation artifacts, the definition of strategic normal forms, and a design theory for variation structures, which will support the development of better algorithms and tools.</p>",1049-331X;1049331X,,10.1145/2063239.2063245,,,Variation;representation,,,,,13,,,,,,11-Dec,,ACM,ACM Journals & Magazines,,26
Gaia-PL: A Product Line Engineering Approach for Efficiently Designing Multiagent Systems,J. Dehlinger; R. R. Lutz,Towson University,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,4,1,27,"<p>Agent-oriented software engineering (AOSE) has provided powerful and natural, high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of AOSE partially depends on whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious development methods. Specifically, AOSE does not adequately address requirements specifications as reusable assets. Software product line engineering is a reuse technology that supports the systematic development of a set of similar software systems through understanding, controlling, and managing their common, core characteristics and their differing variation points. In this article, we present an extension to the Gaia AOSE methodology, named Gaia-PL (Gaia-Product Line), for agent-based distributed software systems that enables requirements specifications to be easily reused. We show how our methodology uses a product line perspective to promote reuse in agent-based software systems early in the development life cycle so that software assets can be reused throughout system development and evolution. We also present results from an application to show how Gaia-PL provided reuse that reduced the design and development effort for a large, multiagent system.</p>",1049-331X;1049331X,,10.1145/2000799.2000803,,,Agent-oriented software engineering;software product line engineering,,,,,1,,,,,,11-Sep,,ACM,ACM Journals & Magazines,,26
On the impact of UML analysis models on source-code comprehensibility and modifiability,G. Scanniello; C. Gravino; M. Genero; J. A. Cruz-Lemus; G. Tortora,University of Basilicata,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,2,1,26,<p>We carried out a family of experiments to investigate whether the use of UML models produced in the requirements analysis process helps in the comprehensibility and modifiability of source code. The family consists of a controlled experiment and 3 external replications carried out with students and professionals from Italy and Spain. 86 participants with different abilities and levels of experience with UML took part. The results of the experiments were integrated through the use of meta-analysis. The results of both the individual experiments and meta-analysis indicate that UML models produced in the requirements analysis process influence neither the comprehensibility of source code nor its modifiability.</p>,1049-331X;1049331X,,10.1145/2491912,,,Analysis models;UML;comprehensibility;controlled experiment;family of experiments;maintenance;modifiability;replicated experiments,,,,,3,,,,,,14-Mar,,ACM,ACM Journals & Magazines,,25
A methodology for testing CPU emulators,L. Martignoni; R. Paleari; A. Reina; G. F. Roglia; D. Bruschi,"Universit&#224; degli Studi di Udine, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,4,1,26,"<p>A CPU emulator is a software system that simulates a hardware CPU. Emulators are widely used by computer scientists for various kind of activities (e.g., debugging, profiling, and malware analysis). Although no theoretical limitation prevents developing an emulator that faithfully emulates a physical CPU, writing a fully featured emulator is a very challenging and error prone task. Modern CISC architectures have a very rich instruction set, some instructions lack proper specifications, and others may have undefined effects in corner cases. This article presents a testing methodology specific for CPU emulators, based on fuzzing. The emulator is äóìstressedäóù with specially crafted test cases, to verify whether the CPU is properly emulated or not. Improper behaviors of the emulator are detected by running the same test case concurrently on the emulated and on the physical CPUs and by comparing the state of the two after the execution. Differences in the final state testify defects in the code of the emulator. We implemented this methodology in a prototype (named as EmuFuzzer), analyzed five state-of-the-art IA-32 emulators (QEMU, Valgrind, Pin, BOCHS, and JPC), and found several defects in each of them, some of which can prevent proper execution of programs.</p>",1049-331X;1049331X,,10.1145/2522920.2522922,,,Software testing;automatic test generation;emulation;fuzzing,,,,,2,,,,,,13-Oct,,ACM,ACM Journals & Magazines,,25
GreASE: A Tool for Efficient äóìNonequivalenceäóù Checking,N. d. Francesco; G. Lettieri; A. Santone; G. Vaglini,"University of Pisa, Italy",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2014,23,3,1,26,"<p>Equivalence checking plays a crucial role in formal verification to ensure the correctness of concurrent systems. However, this method cannot be scaled as easily with the increasing complexity of systems due to the state explosion problem. This article presents an efficient procedure, based on heuristic search, for checking Milner's strong and weak equivalence; to achieve higher efficiency, we actually search for a difference between two processes to be discovered as soon as possible, thus the heuristics aims to find a counterexample, even if not the minimum one, to prove nonequivalence. The presented algorithm builds the system state graph on-the-fly, during the checking, and the heuristics promotes the construction of the more promising subgraph. The heuristic function is syntax based, but the approach can be applied to different specification languages such as CCS, LOTOS, and CSP, provided that the language semantics is based on the concept of transition. The algorithm to explore the search space of the problem is based on a greedy technique; GreASE (Greedy Algorithm for System Equivalence), the tool supporting the approach, is used to evaluate the achieved reduction of both state-space size and time with respect to other verification environments.</p>",1049-331X;1049331X,,10.1145/2560563,,,Formal methods;equivalence checking;heuristic searches,,,,,2,,,,,,14-May,,ACM,ACM Journals & Magazines,,25
Expressive and Extensible Parameter Passing for Distributed Object Systems,E. Tilevich; S. Gopal,Virginia Tech,ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,21,1,1,26,"<p>In modern distributed object systems, reference parameters to a remote method are passed according to their runtime type. This design choice limits the expressiveness, readability, and maintainability of distributed applications. Further, to extend the built-in set of parameter passing semantics of a distributed object system, the programmer has to understand and modify the underlying middleware implementation. To address these design shortcomings, this article presents (i) a declarative and extensible approach to remote parameter passing that decouples parameter passing semantics from parameter types, and (ii) a plugin-based framework, <i>DeXteR</i>, which enables the programmer to extend the built-in set of remote parameter passing semantics, without having to understand or modify the underlying middleware implementation. DeXteR treats remote parameter passing as a distributed cross-cutting concern and uses aspect-oriented and generative techniques. DeXteR enables the implementation of different parameter passing semantics as reusable application-level plugins, applicable to application, system, and third-party library classes. The expressiveness, flexibility, and extensibility of the approach is validated by adding several nontrivial remote parameter passing semantics (i.e., copy-restore, lazy, streaming) to Java Remote Method Invocation (RMI) as DeXteR plugins.</p>",1049-331X;1049331X,,10.1145/2063239.2063242,,,Extensible middleware;aspect-oriented programming (AOP);declarative programming;generative programming;metadata,,,,,0,,,,,,11-Dec,,ACM,ACM Journals & Magazines,,25
Detecting missing method calls as violations of the majority rule,M. Monperrus; M. Mezini,Technische Universit&#228;t Darmstadt,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,1,1,25,"<p>When using object-oriented frameworks it is easy to overlook certain important method calls that are required at particular places in code. In this article, we provide a comprehensive set of empirical facts on this problem, starting from traces of missing method calls in a bug repository. We propose a new system that searches for missing method calls in software based on the other method calls that are observable. Our key insight is that the voting theory concept of majority rule holds for method calls: a call is likely to be missing if there is a majority of similar pieces of code where this call is present. The evaluation shows that the system predictions go further missing method calls and often reveal different kinds of code smells (e.g., violations of API best practices).</p>",1049-331X;1049331X,,10.1145/2430536.2430541,,,Bugdetection;data mining;static analysis,,,,,2,,,,,,13-Feb,,ACM,ACM Journals & Magazines,,24
Personalized Reliability Prediction of Web Services,Z. Zheng; M. R. Lyu,"Shenzhen Research Institute, The Chinese University of Hong Kong",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,2,1,25,"<p>Service Oriented Architecture (SOA) is a business-centric IT architectural approach for building distributed systems. Reliability of service-oriented systems heavily depends on the remote Web services as well as the unpredictable Internet connections. Designing efficient and effective reliability prediction approaches of Web services has become an important research issue. In this article, we propose two personalized reliability prediction approaches of Web services, that is, neighborhood-based approach and model-based approach. The neighborhood-based approach employs past failure data of similar neighbors (either service users or Web services) to predict the Web service reliability. On the other hand, the model-based approach fits a factor model based on the available Web service failure data and use this factor model to make further reliability prediction. Extensive experiments are conducted with our real-world Web service datasets, which include about 23 millions invocation results on more than 3,000 real-world Web services. The experimental results show that our proposed reliability prediction approaches obtain better reliability prediction accuracy than other competing approaches.</p>",1049-331X;1049331X,,10.1145/2430545.2430548,,,Reliability prediction;user-collaboration;web service,,,,,18,,,,,,13-Mar,,ACM,ACM Journals & Magazines,,24
How Well Do Search Engines Support Code Retrieval on the Web?,S. E. Sim; M. Umarji; S. Ratanotayanon; C. V. Lopes,"University of California, Irvine",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,21,1,1,25,"<p>Software developers search the Web for various kinds of source code for diverse reasons. In a previous study, we found that searches varied along two dimensions: the size of the search target (e.g., block, subsystem, or system) and the motivation for the search (e.g., reference example or as-is reuse). Would each of these kinds of searches require different search technologies? To answer this question, we conducted an experiment with 36 participants to evaluate three diverse approaches (general purpose information retrieval, source code search, and component reuse), as represented by five Web sites (Google, Koders, Krugle, Google Code Search, and SourceForge). The independent variables were search engine, size of search target, and motivation for search. The dependent variable was the participants judgement of the relevance of the first ten hits. We found that it was easier to find reference examples than components for as-is reuse and that participants obtained the best results using a general-purpose information retrieval site. However, we also found an interaction effect: code-specific search engines worked better in searches for subsystems, but Google worked better on searches for blocks. These results can be used to guide the creation of new tools for retrieving source code from the Web.</p>",1049-331X;1049331X,,10.1145/2063239.2063243,,,Empirical study;open source;opportunistic development;search archetypes,,,,,8,1,,,,,11-Dec,,ACM,ACM Journals & Magazines,,24
Formal Verification of Software Countermeasures against Side-Channel Attacks,H. Eldib; C. Wang; P. Schaumont,"Virginia Polytechnic Institute and State University, Blacksburg, VA",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2014,24,2,1,24,"<p>A common strategy for designing countermeasures against power-analysis-based side-channel attacks is using <i>random masking</i> techniques to remove the statistical dependency between sensitive data and side-channel emissions. However, this process is both labor intensive and error prone and, currently, there is a lack of automated tools to formally assess how secure a countermeasure really is. We propose the first SMT-solver-based method for formally verifying the security of a masking countermeasure against such attacks. In addition to checking whether the sensitive data are <i>masked</i> by random variables, we also check whether they are <i>perfectly masked</i>, that is, whether the intermediate computation results in the implementation of a cryptographic algorithm are independent of the secret key. We encode this verification problem using a series of quantifier-free first-order logic formulas, whose satisfiability can be decided by an off-the-shelf SMT solver. We have implemented the proposed method in a software verification tool based on the LLVM compiler frontend and the Yices SMT solver. Our experiments on a set of recently proposed masking countermeasures for cryptographic algorithms such as AES and MAC-Keccak show the method is both effective in detecting power side-channel leaks and scalable for practical use.</p>",1049-331X;1049331X,,10.1145/2685616,,,AES;MAC-Keccak;Side-channel attack;countermeasure;cryptographic software;differential power analysis;perfect masking;satisfiability modulo theory (SMT),,,,,2,,,,,,14-Dec,,ACM,ACM Journals & Magazines,,23
Mining Privacy Goals from Privacy Policies Using Hybridized Task Recomposition,J. Bhatia; T. D. Breaux; F. Schaub,"Carnegie Mellon University, Pittsburgh, Pennsylvania",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2016,25,3,1,24,"<p>Privacy policies describe high-level goals for corporate data practices; regulators require industries to make available conspicuous, accurate privacy policies to their customers. Consequently, software requirements must conform to those privacy policies. To help stakeholders extract privacy goals from policies, we introduce a semiautomated framework that combines crowdworker annotations, natural language typed dependency parses, and a reusable lexicon to improve goal-extraction coverage, precision, and recall. The framework evaluation consists of a five-policy corpus governing web and mobile information systems, yielding an average precision of 0.73 and recall of 0.83. The results show that no single framework element alone is sufficient to extract goals; however, the overall framework compensates for elemental limitations. Human annotators are highly adaptive at discovering annotations in new texts, but those annotations can be inconsistent and incomplete; dependency parsers lack sophisticated, tacit knowledge, but they can perform exhaustive text search for prospective requirements indicators; and while the lexicon may never completely saturate, the lexicon terms can be reliably used to improve recall. Lexical reuse reduces false negatives by 41&percnt;, increasing the average recall to 0.85. Last, crowd workers were able to identify and remove false positives by around 80&percnt;, which improves average precision to 0.93.</p>",1049-331X;1049331X,,10.1145/2907942,,,Requirements extraction;crowdsourcing;natural language processing;privacy,,,,,1,,,,,,16-Aug,,ACM,ACM Journals & Magazines,,23
Temporal dependency-based checkpoint selection for dynamic verification of temporal constraints in scientific workflow systems,J. Chen; Y. Yang,"Swinburne University of Technology, Melbourne, Australia",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,3,1,23,"<p>In a scientific workflow system, a checkpoint selection strategy is used to select checkpoints along scientific workflow execution for verifying temporal constraints so that we can identify any temporal violations and handle them in time in order to ensure overall temporal correctness of the execution that is often essential for the usefulness of execution results. The problem of existing representative strategies is that they do not differentiate temporal constraints as, once a checkpoint is selected, they verify all temporal constraints. However, such a checkpoint does not need to be taken for those constraints whose consistency can be deduced from others. The corresponding verification of such constraints is consequently unnecessary and can severely impact overall temporal verification efficiency while the efficiency determines whether temporal violations can be identified quickly for handling in time. To address the problem, in this article, we develop a new temporal-dependency based checkpoint selection strategy which can select checkpoints in accordance with different temporal constraints. With our strategy, the corresponding unnecessary verification can be avoided. The comparison and experimental simulation further demonstrate that our new strategy can improve the efficiency of overall temporal verification significantly over the existing representative strategies.</p>",1049-331X;1049331X,,10.1145/2000791.2000793,,,Scientific workflows;checkpoint selection;temporal constraints;temporal verification,,,,,8,,,,,,11-Aug,,ACM,ACM Journals & Magazines,,22
Scaling predictive analysis of concurrent programs by removing trace redundancy,J. Huang; J. Zhou; C. Zhang,"Hong Kong University of Science and Technology, Kowloon, Hong Kong",ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,1,1,21,"<p>Predictive trace analysis (PTA) of concurrent programs is powerful in finding concurrency bugs unseen in past program executions. Unfortunately, existing PTA solutions face considerable challenges in scaling to large traces. In this article, we identify that a large percentage of events in the trace are redundant for presenting useful analysis results to the end user. Removing them from the trace can significantly improve the scalability of PTA without affecting the quality of the results. We present a trace redundancy theorem that specifies a redundancy criterion and the soundness guarantee that the PTA results are preserved after removing the redundancy. Based on this criterion, we design and implement TraceFilter, an efficient algorithm that automatically removes redundant events from a trace for the PTA of general concurrency access anomalies. We evaluated TraceFilter on a set of popular concurrent benchmarks as well as real world large server programs. Our experimental results show that TraceFilter is able to significantly improve the scalability of PTA by orders of magnitude, without impairing the analysis result.</p>",1049-331X;1049331X,,10.1145/2430536.2430542,,,,,,,,5,1,,,,,13-Feb,,ACM,ACM Journals & Magazines,,20
Verification across Intellectual Property Boundaries,S. Chaki; C. Schallhart; H. Veith,Software Engineering Institute,ACM Transactions on Software Engineering and Methodology (TOSEM),20160129,2013,22,2,1,12,"<p>In many industries, the importance of software components provided by third-party suppliers is steadily increasing. As the suppliers seek to secure their intellectual property (IP) rights, the customer usually has no direct access to the suppliersäó» source code, and is able to enforce the use of verification tools only by legal requirements. In turn, the supplier has no means to convince the customer about successful verification without revealing the source code. This article presents an approach to resolve the conflict between the IP interests of the supplier and the quality interests of the customer. We introduce a protocol in which a dedicated server (called the äóìamanatäóù) is controlled by both parties: the customer controls the verification task performed by the amanat, while the supplier controls the communication channels of the amanat to ensure that the amanat does not leak information about the source code. We argue that the protocol is both practically useful and mathematically sound. As the protocol is based on well-known (and relatively lightweight) cryptographic primitives, it allows a straightforward implementation on top of existing verification tool chains. To substantiate our security claims, we establish the correctness of the protocol by cryptographic reduction proofs.</p>",1049-331X;1049331X,,10.1145/2430545.2430550,,,Intellectual property;supply chain,,,,,1,,,,,,13-Mar,,ACM,ACM Journals & Magazines,,11
A Baseline Model for Software Effort Estimation,P. A. Whigham; C. A. Owen; S. G. Macdonell,"University of Otago, Dunedin, New Zealand",ACM Transactions on Software Engineering and Methodology (TOSEM),20161111,2015,24,3,1,11,"<p><i>Software effort estimation</i> (SEE) is a core activity in all software processes and development lifecycles. A range of increasingly complex methods has been considered in the past 30 years for the prediction of effort, often with mixed and contradictory results. The comparative assessment of effort prediction methods has therefore become a common approach when considering how best to predict effort over a range of project types. Unfortunately, these assessments use a variety of sampling methods and error measurements, making comparison with other work difficult. This article proposes an <i>automatically transformed linear model</i> (ATLM) as a suitable baseline model for comparison against SEE methods. ATLM is simple yet performs well over a range of different project types. In addition, ATLM may be used with mixed numeric and categorical data and requires no parameter tuning. It is also deterministic, meaning that results obtained are amenable to replication. These and other arguments for using ATLM as a baseline model are presented, and a reference implementation described and made available. We suggest that ATLM should be used as a baseline of effort prediction quality for all future model comparisons in SEE.</p>",1049-331X;1049331X,,10.1145/2738037,,,Baseline model;transformed linear model,,,,,3,,,,,,15-May,,ACM,ACM Journals & Magazines,,10
A revisit of fault class hierarchies in general boolean specifications,Z. Chen; T. Y. Chen; B. Xu,"Nanjing University, Nanjing, China",ACM Transactions on Software Engineering and Methodology (TOSEM),20121018,2011,20,3,1,11,"<p>Recently, Kapoor and Bowen [2007] have extended the works by Kuhn [1999], Tsuchiya and Kikuno [2002], and Lau and Yu [2005]. However, their proofs overlook the possibility that a mutant of the Boolean specifications under test may be equivalent. Hence, each of their fault relationships is either incorrect or has an incorrect proof. In this article, we give counterexamples to the incorrect fault relationships and provide new proofs for the valid fault relationships. Furthermore, a co-stronger fault relation is introduced to establish a new fault class hierarchy for general Boolean specifications.</p>",1049-331X;1049331X,,10.1145/2000791.2000797,,,Boolean specifications;Fault-based testing;fault class,,,,,9,,,,,,11-Aug,,ACM,ACM Journals & Magazines,,10