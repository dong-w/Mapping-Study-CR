Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication_Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Organizational volatility and its effects on software defects,A. Mockus,"Avaya Labs Research, Basking Ridge, NJ, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,117,126,"<p>The key premise of an organization is to allow more efficient production, including production of high quality software. To achieve that, an organization defines roles and reporting relationships. Therefore, changes in organization's structure are likely to affect product's quality. We propose and investigate a relationship between developer-centric measures of organizational change and the probability of customer-reported defects in the context of a large software project. We find that the proximity to an organizational change is significantly associated with reductions in software quality. We also replicate results of several prior studies of software quality supporting findings that code, change, and developer characteristics affect fault-proneness. In contrast to prior studies we find that distributed development decreases quality. Furthermore, recent departures from an organization were associated with increased probability of customer-reported defects, thus demonstrating that in the observed context the organizational change reduces product quality.</p>",,,10.1145/1882291.1882311,,,organizational volatility;software defects,,,,,19,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Evolution of a bluetooth test application product line: a case study,N. Ramasubbu; R. K. Balan,"Singapore Management University, Singapore, Singapore",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,107,116,"<p>In this paper, we study the decision making process involved in the five year lifecycle of a Bluetooth software product produced by a large, multi-national test and measurement firm. In this environment, customer change requests either have to be added as a standard feature in the product, or developed as a special customized version of the product. We first discuss the influential factors, such as evolving standards, market share, installed-base, and complexity, which collectively determined how the firm responded to product change requests. We then develop a predictive decision model to test the collective impact of these factors on determining whether to standardize or customize a customer's change request. Finally, we develop and test a customization cost estimation model, for use by software product teams, which specifically accounts for factors unique to the customization stage of a product lifecycle.</p>",,,10.1145/1882291.1882309,,,complexity;product development;product life cycle;software engineering economics;software evolution;software process,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Using dynamic analysis to create trace-focused user interfaces for IDEs,D. Myers; M. A. Storey,"University of Victoria, Victoria, BC, Canada",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,367,368,"<p>This research demonstration presents the tool, <i>Dynamic Interactive Views for Reverse Engineering</i> (Diver). Diver supports software understanding through a trace focused user interface. The <i>trace focused user interface</i> is a method of re-organizing the user interface of integrated development environments so that developers can focus their attention on artifacts related to the run-time behaviour of the software that they are investigating. The tool combines concepts from research in software visualization, dynamic analysis, software reconnaissance, and task focused user interfaces.</p>",,,10.1145/1882291.1882351,,,integrated development environments;reverse engineering;software reconnaissance;visualization,,,,,2,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
The missing links: bugs and bug-fix commits,A. Bachmann; C. Bird; F. Rahman; P. Devanbu; A. Bernstein,"University of Zurich, Zurich, Switzerland",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,97,106,"<p>Empirical studies of software defects rely on links between bug databases and program code repositories. This <i>linkage</i> is typically based on bug-fixes identified in developer-entered commit logs. Unfortunately, developers do not always report which commits perform bug-fixes. Prior work suggests that such links can be a biased sample of the entire population of fixed bugs. The validity of statistical hypotheses-testing based on linked data could well be affected by bias. Given the wide use of linked defect data, it is vital to gauge the nature and extent of the bias, and try to develop testable theories and models of the bias. To do this, <i>we must establish ground truth</i>: manually analyze a complete version history corpus, and nail down those commits that fix defects, and those that do not. This is a diffcult task, requiring an expert to compare versions, analyze changes, find related bugs in the bug database, reverse-engineer missing links, and finally record their work for use later. This effort must be repeated for hundreds of commits to obtain a useful sample of reported and unreported bug-fix commits. We make several contributions. First, we present Linkster, a tool to facilitate link reverse-engineering. Second, we evaluate this tool, engaging a core developer of the Apache HTTP web server project to exhaustively annotate 493 commits that occurred during a six week period. Finally, we analyze this comprehensive data set, showing that there are serious and consequential problems in the data.</p>",,,10.1145/1882291.1882308,,,apache;bias;case study;manual annotation;tool,,,,,36,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
A study of applying a research prototype tool in industrial practice,O. Badreddin; T. C. Lethbridge,"University of Ottawa, Ottawa, ON, Canada",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,353,356,"<p>Our research tool, Umple, has the objective of raising the abstraction level of programming languages by adding modeling abstractions such as UML associations, attributes and state machines. My research focuses on the syntax and semantics of state machines in Umple plus the empirical validation of Umple. The latter consists of a grounded theory study of Umple users, and action research involving the use of Umple in an industrial sitting. The grounded theory study is guiding our research and development of Umple. The action research is enlarging the pool of Umple users and contributing to the validation of Umple's approach.</p>",,,10.1145/1882291.1882345,,,action research;grounded theory;modeling;uml,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Analyzing hierarchical complex real-time systems,Y. Liu; J. Sun; J. S. Dong,"National University of Singapore, Singapore, Singapore",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,365,366,"<p>Specification and verification of real-time systems are important research topics which have practical implications. In this work, we present a self-contained toolkit to analyze real-time systems, which supports system modeling, animated simulation and automatic verification (based on advanced model checking techniques like dynamic zone abstraction). In this tool, we adopt an event-based modeling language for describing real-time systems with hierarchical structure. Experiments show that our tool has compatible performance with the state-of-the-art verifiers, and complement them with additional capabilities like LTL model checking, timed refinement checking.</p>",,,10.1145/1882291.1882350,,,real-time system;zone abstraction,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Locating need-to-translate constant strings in web applications,X. Wang; L. Zhang; T. Xie; H. Mei; J. Sun,"Peking University, Beijing, China",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,87,96,"<p>Software internationalization aims to make software accessible and usable by users all over the world. For a Java application that does not consider internationalization at the beginning of its develop- ment stage, our previous work proposed an approach to locating need-to-translate constant strings in the Java code. However, when being applied on web applications, it can identify only constant strings that may go to the generated HTML texts, but cannot further distinguish constant strings visible at the browser side (need-to-translate) from other constant strings (not need-to-translate). In this paper, to address significant challenges in internationalizing web applications, we propose a novel approach to locating need-to-translate constant strings in web applications. Among those constant strings that may go to the generated HTML texts, our approach further distinguishes strings visible at the browser side from non-visible strings via a novel technique called flag propagation. We evaluated our approach on three real-world open source PHP-based web applications (in total near 17 KLOC): Squirrel Mail, Lime Survey, and Mrbs. The empirical results demonstrate that our approach accurately distinguishes visible strings from non-visible strings among all the constant strings that may go to the generated HTML texts, and is effective for locating need-to-translate constant strings in web applications.</p>",,,10.1145/1882291.1882306,,,flag propagation;software internationalization;web application,,,,,11,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Towards behavior elaboration and synthesis using modes,H. Shokry,"University of Limerick, Limerick, Ireland",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,349,352,"<p>Early system requirements are often captured by declarative and property-based artifacts, such as scenarios and goals. While such artifacts are intuitive and useful, they are partial and typically lack an overarching structure to allow systematic elaboration of the fragmented behaviors they denote. I aim to develop a design technique for structuring the partial specifications by partitioning the state-space based on Parnas' notions of 'modes' and 'mode-classes'. A mode is set of states, characterized by a predicate. A mode-class is a set of disjoint modes completely covering the state space. The structuring framework supports early elaboration of partial specification, and facilitates improved synthesis of integrated system behavioral prototype.</p>",,,10.1145/1882291.1882344,,,behavior synthesis;modes;state-space partitioning.,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
HI-C: diagnosing object churn in framework-based applications,M. Fisher; L. Marrs; B. G. Ryder,"University of Memphis, Memphis, TN, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,379,380,"<p>In prior work we have developed an escape analysis to help developers identify sources of object churn (i.e., excessive use of temporaries) in large framework-based applications. We have developed Hi-C, an Eclipse plug-in that allows users to visualize, filter, and explore analysis results to aid them in diagnosis of object churn and in program comprehension in general.</p>",,,10.1145/1882291.1882358,,,blended analysis;object churn;performance diagnosis,,,,,1,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Realizability analysis for message-based interactions using shared-state projections,S. Hall’©; T. Bultan,"Universit&#233; du Qu&#233;bec &#224; Chicoutimi, Chicoutimi, PQ, Canada",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,27,36,"<p>The global interaction behavior in message-based systems can be specified as a finite-state machine defining acceptable sequences of messages exchanged by a group of peers. Realizability analysis determines if there exist local implementations for each peer, such that their composition produces exactly the intended global behavior. Although there are existing sufficient conditions for realizability, we show that these earlier results all fail for a particular class of specifications called arbitrary-initiator protocols. We present a novel algorithm for deciding realizability by computing a finite-state model that keeps track of the information about the global state of a conversation protocol that each peer can deduce from the messages it sends and receives. By searching for disagreements between each peer's deduced states, we provide a sound analysis for realizability that correctly classifies realizability of arbitrary-initiator protocols.</p>",,,10.1145/1882291.1882298,,,asynchronous communication;choreography;realizability,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Basset: a tool for systematic testing of actor programs,S. Lauterburg; R. K. Karmani; D. Marinov; G. Agha,"University of Illinois at Urbana-Champaign, Urbana, IL, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,363,364,"<p>This paper presents Basset, a tool for systematic testing of JVM-based actor programs. The actor programming model offers a promising approach for developing reliable concurrent and distributed systems. Since the actor model is based on message passing and disallows shared state, it avoids some of the problems inherent in shared-memory programming, e.g., low-level dataraces involving access to shared data. However, actor programs can still have bugs that result from incorrect orders of messages among actors or processing of messages by individual actors. To systematically test an actor program, it is necessary to explore different message delivery schedules that might occur during execution. Basset facilitates such exploration and provides a generic platform that can support actor systems that compile to Java bytecode. Our current implementation of Basset supports testing of programs developed using the ActorFoundry library and the Scala programming language.</p>",,,10.1145/1882291.1882349,,,actors;jpf;scala;state-space exploration,,,,,2,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Synthesis of live behaviour models,N. R. D'Ippolito; V. Braberman; N. Piterman; S. Uchitel,"Imperial College London, London, United Kingdom &#38; Universidad de Buenos Aires, Buenos Aires, Argentina",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,77,86,"<p>We present a novel technique for synthesising behaviour models that works for an expressive subset of liveness properties and conforms to the foundational requirements engineering World/Machine model, dealing explicitly with assumptions on environment behaviour and distinguishing controlled and monitored actions. This is the first technique that conforms to what is considered best practice in requirements specifications: distinguishing prescriptive and descriptive assertions. Most previous attempts at using synthesis of behavioural models were restricted to handling only safety properties. Those that did support liveness were inadequate for synthesis of operational event based models as they did not include the bespoke distinction between system goals and environment assumptions.</p>",,,10.1145/1882291.1882305,,,behavioural modelling;controller synthesis,,,,,11,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
LEAP: lightweight deterministic multi-processor replay of concurrent java programs,J. Huang; P. Liu; C. Zhang,"Hong Kong University of Science and Technology, Hong Kong, Hong Kong",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,207,216,"<p>The technique of deterministic record and replay aims at faithfully reenacting an earlier program execution. For concurrent programs, it is one of the most important techniques for program understanding and debugging. The state of the art deterministic replay techniques face challenging efficiency problems in supporting multi-processor executions due to the unoptimized treatment of shared memory accesses. We propose LEAP: a deterministic record and replay technique that uses a new type of local order w.r.t. the shared memory locations and concurrent threads. Compared to the related work, our technique records much less information without losing the replay determinism. The correctness of our technique is underpinned by formal models and a replay theorem that we have developed in this work. Through our evaluation using both benchmarks and real world applications, we show that LEAP is more than 10x faster than conventional global-order based approaches and, in most cases, 2x to 10x faster than other local-order based approaches. Our recording overhead on the two large open source multi-threaded applications Tomcat and Derby is less than 10%. Moreover, as the evidence of the deterministic replay, LEAP is able to deterministically reproduce 7 out of 8 real bugs in Tomcat and Derby, 13 out of 16 benchmark bugs in IBM ConTest benchmark suite, and 100% of the randomly injected concurrency bugs.</p>",,,10.1145/1882291.1882323,,,concurrency bug reproduction;deterministic replay;multi-processor,,,,,7,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
A learning-based approach for engineering feature-oriented self-adaptive software systems,A. Elkhodary,"George Mason University, Fairfax, VA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,345,348,,,,10.1145/1882291.1882343,,,feature-orientation;learning;qos analysis;self-adaptation,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
RT-simex: retro-analysis of execution traces,J. DeAntoni; F. Mallet; F. Thomas; G. Reydet; J. P. Babau; C. Mraidha; L. Gauthier; L. Rioux; N. Sordon,"Universit&#233; Nice Sophia Antipolis, Sophia Antipolis, France",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,377,378,"<p>This presentation demonstrates the early results from the French ANR project RT-Simex. RT-Simex proposes a set of tools to analyze parallel embedded code and trace the simulation results back to the initial models from which the code was generated. The whole tool-set relies on standard formats (UML MARTE, Open Trace Format) to ensure a perennial use. The main difficulty is to reconcile different execution traces extracted from codes running concurrently on different unsynchronized platforms. This is achieved through the polychronous logical time model of MARTE.</p>",,,10.1145/1882291.1882357,,,marte;polychronous logical time,,,,,3,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Finding latent performance bugs in systems implementations,C. Killian; K. Nagaraj; S. Pervez; R. Braud; J. W. Anderson; R. Jhala,"Purdue University, West Lafayette, IN, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,17,26,"<p>Robust distributed systems commonly employ high-level recovery mechanisms enabling the system to recover from a wide variety of problematic environmental conditions such as node failures, packet drops and link disconnections. Unfortunately, these recovery mechanisms also effectively mask additional serious design and implementation errors, disguising them as <i>latent performance bugs</i> that severely degrade end-to-end system performance. These bugs typically go unnoticed due to the challenge of distinguishing between a bug and an intermittent environmental condition that must be tolerated by the system. We present techniques that can automatically pinpoint latent performance bugs in systems implementations, in the spirit of recent advances in model checking by systematic state space exploration. The techniques proceed by automating the process of conducting random simulations, identifying performance anomalies, and analyzing anomalous executions to pinpoint the circumstances leading to performance degradation.</p> <p>By focusing our implementation on the MACE toolkit, MACEPC can be used to test our implementations directly, without modification. We have applied MACEPC to five thoroughly tested and trusted distributed systems implementations. MACEPC was able to find significant, previously unknown, long-standing performance bugs in each of the systems, and led to fixes that significantly improved the end-to-end performance of the systems.</p>",,,10.1145/1882291.1882297,,,debugging;distributed systems;mace;macepc;performance,,,,,9,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Developer fluency: achieving true mastery in software projects,M. Zhou; A. Mockus,"Peking University, Beijing, China",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,137,146,"<p>Outsourcing and offshoring lead to a rapid influx of new developers in software projects. That, in turn, manifests in lower productivity and project delays. To address this common problem we study how the developers become fluent in software projects. We found that developer productivity in terms of number of tasks per month increases with project tenure and plateaus within a few months in three small and medium projects and it takes up to 12 months in a large project. When adjusted for the task difficulty, developer productivity did not plateau but continued to increase over the entire three year measurement interval. We also discovered that tasks vary according to their importance(centrality) to a project. The increase in task centrality along four dimensions: customer, system-wide, team, and future impact was approximately linear over the entire period. By studying developer fluency we contribute by determining dimensions along which developer expertise is acquired, finding ways to measure them, and quantifying the trajectories of developer learning.</p>",,,10.1145/1882291.1882313,,,developer fluency;developer learning;productivity;task centrality;task difficulty,,,,,15,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
BERT: a tool for behavioral regression testing,W. Jin; A. Orso; T. Xie,"Georgia Institute of Technology, Atlanta, GA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,361,362,"<p>During maintenance, software is modified and evolved to enhance its functionality, eliminate faults, and adapt it to changed or new platforms. In this demo, we present BERT, a tool for helping developers identify regression faults that they may have introduced when modifying their code. BERT is based on the concept of behavioral regression testing: given two versions of a program, BERT identifies behavioral differences between the two versions through dynamic analysis, in three steps. First, it generates a large number of test inputs that focus on the changed parts of the code. Second, it runs the generated test inputs on the old and new versions of the code and identifies differences in the tests' behavior. Third, it analyzes the identified differences and presents them to the developers. By focusing on a subset of the code and leveraging differential behavior, BERT can provide developers with more detailed information than traditional regression testing approaches---approaches that rely exclusively on existing test suites, which may be limited in scope and may not adequately test the changes in a program. BERT is implemented as a plug-in for Eclipse, a popular Integrated Development Environment, and is freely available. This demo presents BERT, its underlying technology, and examples of its usage.</p>",,,10.1145/1882291.1882348,,,differential testing;regression testing,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Staged concurrent program analysis,N. Sinha; C. Wang,"NEC Labs America, Princeton, NJ, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,47,56,"<p>Concurrent program verification is challenging because it involves exploring a large number of possible thread interleavings together with complex sequential reasoning. As a result, concurrent program verifiers resort to bi-modal reasoning, which alternates between reasoning over intra-thread (sequential) semantics and inter-thread (concurrent) semantics. Such reasoning often involves repeated intra-thread reasoning for exploring each interleaving (inter-thread reasoning) and leads to inefficiency. In this paper, we present a new two-stage analysis which completely separates intra- and inter-thread reasoning. The first stage uses sequential program semantics to obtain a precise summary of each thread in terms of the global accesses made by the thread. The second stage performs inter-thread reasoning by composing these thread-modular summaries using the notion of sequential consistency. Assertion violations and other concurrency errors are then checked in this composition with the help of an off-the-shelf SMT solver. We have implemented our approach in the FUSION framework for checking concurrent C programs shows that avoiding redundant bi-modal reasoning makes the analysis more scalable.</p>",,,10.1145/1882291.1882301,,,axiomatic composition;interference abstraction;interference skeleton;sequential consistency;smt solvers;staged analysis;thread-modular summarization,,,,,3,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Change-point detection for black-box services,I. Epifani; C. Ghezzi; G. Tamburrelli,"Politecnico di Milano, Milano, Italy",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,227,236,"<p>Modern software systems are increasingly built out of services that are developed, deployed, and operated by independent organizations, which expose them for use by potential clients. Services may be directly invoked by clients. They may also be composed by service integrators, who in turn expose the composite artifact as a new service. Continuous change is typical of this world. Providers may change services and the deployment infrastructure to meet continuously changing requirements and be more competitive. Clients may change their operational profiles. Changes have a severe impact on the quality of services.</p> <p>In this paper we address the problem of identifying changes concerning the non-functional behavior of software services managed by external organizations, and consequently considered as black-box artifacts. We define the concept of <i>change-point</i> and provide a statistical technique aimed at identifying it, given an execution trace produced by client invocations. Change-point detection is key to reasoning about changes, diagnosing their cause, and suitably reacting to their occurrence.</p>",,,10.1145/1882291.1882326,,,bayesian theory;discrete time markov chains;performance;reliability,,,,,5,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Leveraging usage similarity for effective retrieval of examples in code repositories,S. K. Bajracharya; J. Ossher; C. V. Lopes,"University of California, Irvine, Irvine, CA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,157,166,"<p>Developers often learn to use APIs (Application Programming Interfaces) by looking at existing examples of API usage. Code repositories contain many instances of such usage of APIs. However, conventional information retrieval techniques fail to perform well in retrieving API usage examples from code repositories. This paper presents Structural Semantic Indexing (SSI), a technique to associate words to source code entities based on similarities of API usage. The heuristic behind this technique is that entities (classes, methods, etc.) that show similar uses of APIs are semantically related because they do similar things. We evaluate the effectiveness of SSI in code retrieval by comparing three SSI based retrieval schemes with two conventional baseline schemes. We evaluate the performance of the retrieval schemes by running a set of 20 candidate queries against a repository containing 222,397 source code entities from 346 jars belonging to the Eclipse framework. The results of the evaluation show that SSI is effective in improving the retrieval of examples in code repositories.</p>",,,10.1145/1882291.1882316,,,api usage;code search;software information retrieval;ssi;structural semantic indexing,,,,,25,3,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Combining hardware and software instrumentation to classify program executions,C. Yilmaz; A. Porter,"Sabanci University, Tuzla, Istanbul, Turkey",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,67,76,"<p>Several research efforts have studied ways to infer properties of software systems from program spectra gathered from the running systems, usually with software-level instrumentation. One specific application of this general approach, which is the focus of this paper, is distinguishing failed executions from successful executions. While existing efforts appear to produce accurate classifications, detailed understanding of their costs and potential cost-benefit tradeoffs is lacking. In this work, we present a hybrid instrumentation approach which uses hardware performance counters to gather program spectra at very low cost. This underlying data is further augmented with data captured by minimal amounts of software-level instrumentation. We also evaluate this hybrid approach by comparing it to other existing approaches. We conclude that these hybrid spectra can reliably distinguish failed executions from successful executions at a fraction of the runtime overhead cost of using software-only spectra.</p>",,,10.1145/1882291.1882304,,,fault detection;hardware performance counters;software quality assurance,,,,,5,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Golden implementation driven software debugging,A. Banerjee; A. Roychoudhury; J. A. Harlie; Z. Liang,"National University of Singapore, Singapore, Singapore",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,177,186,"<p>The presence of a functionally correct golden implementation has a significant advantage in the software development life cycle. Such a golden implementation is exploited for software development in several domains, including embedded software --- a low resource-consuming version of the golden implementation. The golden implementation gives the functionality that the program is supposed to implement, and is used as a guide during the software development process. In this paper, we investigate the possibility of using the golden implementation as a reference model in software debugging. We perform a substantial case study involving the Busybox embedded Linux utilities while treating the GNU Core Utilities as the golden or reference implementation. Our debugging method consists of dynamic slicing with respect to the observable error in both the implementations (the golden implementation as well as the buggy software). During dynamic slicing we also perform a step-by-step weakest precondition computation of the observable error with respect to the statements in the dynamic slice. The formulae computed as weakest pre-condition in the two implementations are then compared to accurately locate the root cause of a given observable error. Experimental results obtained from Busybox suggest that our method performs well in practice and is able to pinpoint all the bugs recently published in [8] that could be reproduced on Busybox version 1.4.2. The bug report produced by our approach is concise and pinpoints the program locations inside the Busybox source that contribute to the difference in behavior.</p>",,,10.1145/1882291.1882319,,,embedded linux;software evolution;symbolic execution,,,,,11,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Language-based replay via data flow cut,M. Wu; F. Long; X. Wang; Z. Xu; H. Lin; X. Liu; Z. Guo; H. Guo; L. Zhou; Z. Zhang,"Microsoft Research Asia, Beijing, China",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,197,206,"<p>A replay tool aiming to reproduce a program's execution interposes itself at an appropriate replay interface between the program and the environment. During recording, it logs all non-deterministic side effects passing through the interface from the environment and feeds them back during replay. The replay interface is critical for correctness and recording overhead of replay tools.</p> <p>iTarget is a novel replay tool that uses programming language techniques to automatically seek a replay interface that both ensures correctness and minimizes recording overhead. It performs static analysis to extract data flows, estimates their recording costs via dynamic profiling, computes an optimal replay interface that minimizes the recording overhead, and instruments the program accordingly for interposition. Experimental results show that iTarget can successfully replay complex C programs, including Apache web server and Berkeley DB, and that it can reduce the log size by up to two orders of magnitude and slowdown by up to 50%.</p>",,,10.1145/1882291.1882322,,,data flow;graph cut;instrumentation;replay,,,,,0,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
From requirements to partial behavior models: an iterative approach to incremental specification refinement,I. Krka,"University of Southern California, Los Angeles, CA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,341,344,"<p>In this thesis, I will improve the state-of-the-art for capturing, analyzing, and refining functional requirements by providing support for synthesizing, analyzing, and refining Modal Transition Systems.</p>",,,10.1145/1882291.1882342,,,elicitation;modal transition systems;refinement;scenarios;synthesis,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Building and using pluggable type systems,M. D. Ernst; M. Ali,"University of Washington, Seattle, WA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,375,376,"<p>Are you a practitioner who is tired of null pointer exceptions, unintended side effects, SQL injections, concurrency errors, mistaken equality tests, and other run-time errors that appear during testing or in the field? A pluggable type system can guarantee the absence of these errors, and many more.</p> <p>Are you a researcher who wants to be able to quickly and easily implement a type system, giving you the ability to evaluate it in practice and to field it? You need a framework that supports these essential activities.</p> <p>This demo is aimed at both audiences. It describes both the theory of pluggable types and also the practice of implementing them. A simple pluggable type-checker can be implemented in 2 minutes, and can be enhanced thereafter. A type checkers is easy to create, easy for programmers to use, and effective in finding real, important bugs.</p> <p>The demo uses the Checker Framework, which enables you to create pluggable type systems for Java, while your code remains backward-compatible with all versions of Java. The ideas translate to other languages and toolsets. The tools are freely available at http://types.cs.washington.edu/checker-framework/.</p>",,,10.1145/1882291.1882356,,,checker framework;java;pluggable type-system;type annotation;type checking;user-defined type system,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
FUSION: a framework for engineering self-tuning self-adaptive software systems,A. Elkhodary; N. Esfahani; S. Malek,"George Mason University, Fairfax, VA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,7,16,"<p>Self-adaptive software systems are capable of adjusting their behavior at run-time to achieve certain objectives. Such systems typically employ analytical models specified at design-time to assess their characteristics at run-time and make the appropriate adaptation decisions. However, prior to system's deployment, engineers often cannot foresee the changes in the environment, requirements, and system's operational profile. Therefore, any analytical model used in this setting relies on underlying assumptions that if not held at run-time make the analysis and hence the adaptation decisions inaccurate. We present and evaluate FeatUre-oriented Self-adaptatION (FUSION) framework, which aims to solve this problem by learning the impact of adaptation decisions on the system's goals. The framework (1) allows for automatic online fine-tuning of the adaptation logic to unanticipated conditions, (2) reduces the upfront effort required for building such systems, and (3) makes the run-time analysis of such systems very efficient.</p>",,,10.1145/1882291.1882296,,,feature-orientation;learning;qos analysis;self-adaptation,,,,,25,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
An effective dynamic analysis for detecting generalized deadlocks,P. Joshi; M. Naik; K. Sen; D. Gay,"University of California, Berkeley, Berkeley, CA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,327,336,"<p>We present an effective dynamic analysis for finding a broad class of deadlocks, including the well-studied lock-only deadlocks as well as the less-studied, but no less widespread or insidious, deadlocks involving condition variables. Our analysis consists of two stages. In the first stage, our analysis observes a multi-threaded program execution and generates a simple multi-threaded program, called a <i>trace program</i>, that only records operations observed during the execution that are deemed relevant to finding deadlocks. Such operations include lock acquire and release, wait and notify, thread start and join, and change of values of user-identified synchronization predicates associated with condition variables. In the second stage, our analysis uses an off-the-shelf model checker to explore all possible thread interleavings of the trace program and check if any of them deadlocks. A key advantage of our technique is that it discards most of the program logic which usually causes state-space explosion in model checking, and retains only the relevant synchronization logic in the trace program, which is sufficient for finding deadlocks. We have implemented our analysis for Java, and have applied it to twelve real-world multi-threaded Java programs. Our analysis is effective in practice, finding thirteen previously known as well as four new deadlocks.</p>",,,10.1145/1882291.1882339,,,concurrency;deadlock detection;dynamic program analysis,,,,,8,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Creating and evolving developer documentation: understanding the decisions of open source contributors,B. Dagenais; M. P. Robillard,"McGill University, Montreal, PQ, Canada",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,127,136,"<p>Developer documentation helps developers learn frameworks and libraries. To better understand how documentation in open source projects is created and maintained, we performed a qualitative study in which we interviewed core contributors who wrote developer documentation and developers who read documentation. In addition, we studied the evolution of 19 documents by analyzing more than 1500 document revisions. We identified the decisions that contributors make, the factors influencing these decisions and the consequences for the project. Among many findings, we observed how working on the documentation could improve the code quality and how constant interaction with the projects' community positively impacted the documentation.</p>",,,10.1145/1882291.1882312,,,developer documentation;framework;grounded theory;open source projects;qualitative studies;software documentation,,,,,18,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
LINKSTER: enabling efficient manual inspection and annotation of mined data,C. Bird; A. Bachmann; F. Rahman; A. Bernstein,"University of California, Davis, Davis, CA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,369,370,"<p>While many uses of mined software engineering data are automatic in nature, some techniques and studies either require, or can be improved, by manual methods. Unfortunately, manually inspecting, analyzing, and annotating mined data can be difficult and tedious, especially when information from multiple sources must be integrated. Oddly, while there are numerous tools and frameworks for automatically mining and analyzing data, there is a dearth of tools which facilitate manual methods. To fill this void, we have developed LINKSTER, a tool which integrates data from bug databases, source code repositories, and mailing list archives to allow manual inspection and annotation. LINKSTER has already been used successfully by an OSS project lead to obtain data for one empirical study.</p>",,,10.1145/1882291.1882352,,,mining,,,,,6,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Generating integration test cases automatically,K. Rubinov,"University of Lugano, Lugano, Switzerland",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,357,360,"<p>In this thesis, I investigate the problem of automatically generating test cases. In particular, I focus on the problem of automatic generation of integration test cases from unit test cases. I start from the observation that software is usually provided with many unit test cases, and that unit test cases carry a lot of information about the unit execution that can be used to generate integration test cases. This paper illustrates the approach that I am investigating and that consists in capturing information in unit test cases with static analysis techniques to automatically merge unit test cases to produce useful integration test cases. The preliminary results reported in this paper provide evidence of the effectiveness of the approach. My current research is developing the approach further and producing additional experimental evidence. I expect to complete the research by defining a set of design for testability rules to produce software that facilitates the automatic generation of integration test cases.</p>",,,10.1145/1882291.1882346,,,automatic test generation;design for testability;software testing;unit and integration testing,,,,,1,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
WhoselsThat: finding software engineers with codebook,A. Begel; K. Y. Phang; T. Zimmermann,"Microsoft Research, Redmond, WA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,381,382,"<p>In this demo, we describe WhoseIsThat, a social search portal which we built using the Codebook framework. We improve the search experience in two ways: first, we search across multiple software repositories at once with a single query; second, we return not just a list of artifacts in the results, but also engineers.</p>",,,10.1145/1882291.1882359,,,inter-team coordination;knowledge management;mining software repositories;social networking,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
PENELOPE: weaving threads to expose atomicity violations,F. Sorrentino; A. Farzan; P. Madhusudan,"University of Illinois at Urbana-Champaign, Urbana, IL, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,37,46,"<p>Testing concurrent programs is challenged by the <i>interleaving explosion problem</i>--- the problem of exploring the large number of interleavings a program exhibits, even under a single test input. Rather than try all interleavings, we propose to test wisely: to exercise only those schedules that lead to interleavings that are typical error patterns. In particular, in this paper we select schedules that exercise patterns of interaction that correspond to atomicity violations. Given an execution of a program under a test harness, our technique is to <i>algorithmically</i> mine from the execution a small set of alternate schedules that cause atomicity violations. The program is then re-executed under these predicted atomicity-violating schedules, and verified by the test harness. The salient feature of our tool is the efficient algorithmic prediction and synthesis of alternate schedules that cover all possible atomicity violations at program locations. We implement the tool PENELOPE that realizes this testing framework and show that the monitoring, prediction, and rescheduling (with precise repro) are efficient and effective in finding bugs related to atomicity violations.</p>",,,10.1145/1882291.1882300,,,atomicity violation;concurrency;dynamic analysis;predictive analysis;schedule selection;testing,,,,,23,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Memory indexing: canonicalizing addresses across executions,W. N. Sumner; X. Zhang,"Purdue University, West Lafayette, IN, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,217,226,"<p>Fine-grained program execution comparison examines different executions generated by different program versions, different inputs, or by perturbations. It has a wide range of applications in debugging, regression testing, program comprehension, and security. Meaningful comparison demands that executions are aligned before they are compared, otherwise the resulting differences do not reflect semantic differences. Prior work has focused on aligning executions along the control flow dimension. In this paper, we observe that the memory dimension is also critical and propose a novel solution to align memory locations across different executions. We introduce a canonical representation for memory locations and pointer values called memory indexing. Aligned memory locations across runs share the same index. We formally define the semantics of memory indexing and present a cost-effective design. We also show that memory indexing overcomes an important challenge in automated debugging by enabling robust state replacement across runs.</p>",,,10.1145/1882291.1882324,,,aliasing;automated debugging;execution indexing;memory indexing,,,,,3,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
A study of the uniqueness of source code,M. Gabel; Z. Su,"University of California at Davis, Davis, CA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,147,156,"<p>This paper presents the results of the first study of the <i>uniqueness of source code</i>. We define the uniqueness of a unit of source code with respect to the entire body of written software, which we approximate with a corpus of 420 million lines of source code. Our high-level methodology consists of examining a collection of 6,000 software projects and measuring the degree to which each project can be `assembled' solely from portions of this corpus, thus providing a precise measure of `uniqueness' that we call <i>syntactic redundancy</i>. We parameterized our study over a variety of variables, the most important of which being the level of granularity at which we view source code. Our suite of experiments together consumed approximately four months of CPU time, providing quantitative answers to the following questions: at <i>what</i> levels of granularity is software unique, and at a <i>given</i> level of granularity, <i>how</i> unique is software? While we believe these questions to be of intrinsic interest, we discuss possible applications to genetic programming and developer productivity tools.</p>",,,10.1145/1882291.1882315,,,large scale study;software uniqueness;source code,,,,,29,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
A trace simplification technique for effective debugging of concurrent programs,N. Jalbert; K. Sen,"UC Berkeley, Berkeley, CA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,57,66,"<p>Concurrent programs are notoriously difficult to debug. We see two main reasons for this: 1) concurrency bugs are often difficult to reproduce, 2) traces of buggy concurrent executions can be complicated by fine-grained thread interleavings. Recently, a number of efficient techniques have tried to address the former reproducibility problem; however, there is no effective solution for the latter trace simplification problem. In this paper, we formalize and prove the trace simplification problem is NP-hard. We then propose a heuristic algorithm, <b>Tinertia</b>, that transforms a buggy execution trace into an easier-to-understand simplified trace. <b>Tinertia</b> works by automatically and iteratively increasing the granularity of the thread interleavings in the buggy trace. <b>Tinertia</b> cannot guarantee optimal simplification; however, we empirically show that our algorithm often generates optimally simplified traces. Moreover, we show that in the simplified trace, the locations of preemptive context switches point to the cause of the bug. We have implemented <b>Tinertia</b> in a tool for C/C++ programs using Pthreads and applied it to 11 benchmarks with up to 37,000 lines of code.</p>",,,10.1145/1882291.1882302,,,automated debugging;concurrency;trace simplification,,,,,9,2,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Automatic workarounds for web applications,A. Carzaniga; A. Gorla; N. Perino; M. Pezz’å,"University of Lugano, Lugano, Switzerland",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,237,246,"<p>We present a technique that finds and executes workarounds for faulty Web applications automatically and at runtime. Automatic workarounds exploit the inherent redundancy of Web applications, whereby a functionality of the application can be obtained through different sequences of invocations of Web APIs. In general, runtime workarounds are applied in response to a failure, and require that the application remain in a consistent state before and after the execution of a workaround. Therefore, they are ideally suited for interactive Web applications, since those allow the user to act as a failure detector with minimal effort, and also either use read-only state or manage their state through a transactional data store. In this paper we focus on faults found in the access libraries of widely used Web applications such as Google Maps. We start by classifying a number of reported faults of the Google Maps and YouTube APIs that have known workarounds. From those we derive a number of general and API-specific program-rewriting rules, which we then apply to other faults for which no workaround is known. Our experiments show that workarounds can be readily deployed within Web applications, through a simple client-side plug-in, and that program-rewriting rules derived from elementary properties of a common library can be effective in finding valid and previously unknown workarounds.</p>",,,10.1145/1882291.1882327,,,automatic workarounds;web api;web applications,,,,,15,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Instant code clone search,M. W. Lee; J. W. Roh; S. w. Hwang; S. Kim,"Pohang University of Science and Technology (POSTECH), Pohang, South Korea",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,167,176,"<p>In this paper, we propose a scalable instant code clone search engine for large-scale software repositories. While there are commercial code search engines available, they treat software as text and often fail to find semantically related code. Meanwhile, existing tools for semantic code clone searches take a ""post-mortem"" approach involving the detection of clones ""after"" the code development is completed, and hence, fail to return the results instantly. In clear contrast, we combine the strength of these two lines of existing research, by supporting instant code clone detection. To achieve this goal, we propose scalable indexing structures on vector abstractions of code. Our proposed algorithms allow developers to detect clones of a given code segment among the 1.7 million code segments from 492 open source projects in sub-second response times, without compromising the accuracy obtained by a state-of-the-art tool.</p>",,,10.1145/1882291.1882317,,,clone detection;code search,,,,,12,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Scalable SMT-based verification of GPU kernel functions,G. Li; G. Gopalakrishnan,"University of Utah, Salt Lake City, UT, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,187,196,"<p>Interest in Graphical Processing Units (GPUs) is skyrocketing due to their potential to yield spectacular performance on many important computing applications. Unfortunately, writing such efficient GPU kernels requires painstaking manual optimization effort which is very error prone. We contribute the first comprehensive symbolic verifier for kernels written in CUDA C. Called the 'Prover of User GPU programs (PUG),' our tool efficiently and automatically analyzes real-world kernels using Satisfiability Modulo Theories (SMT) tools, detecting bugs such as data races, incorrectly synchronized barriers, bank conflicts, and wrong results. PUG's innovative ideas include a novel approach to symbolically encode thread interleavings, exact analysis for correct barrier placement, special methods for avoiding interleaving generation, dividing up the analysis over barrier intervals, and handling loops through three approaches: loop normalization, overapproximation, and invariant finding. PUG has analyzed over a hundred CUDA kernels from public distributions and in-house projects, finding bugs as well as subtle undocumented assumptions.</p>",,,10.1145/1882291.1882320,,,concurrency;cuda;formal verification;gpu;satisfiability modulo theories (decision procedures),,,,,14,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Updating requirements from tests during maintenance and evolution,E. B. Charrada,"University of Zurich, Zurich, Switzerland",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,337,340,"<p>Keeping requirements specification up-to-date during the evolution of a software system is an expensive task. Consequently, specifications are usually not updated and rapidly become obsolete and unreliable. The goal of our research is to preserve the alignment between requirements and the implementation by supporting the maintenance of the specification. In this proposal, we explore the idea of using tests to automatically generate hints about the evolution of requirements. We discuss the main research questions that need to be addressed, and propose ideas to approach them.</p>",,,10.1145/1882291.1882341,,,change propagation;requirements maintenance;software evolution,,,,,1,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Ref-Finder: a refactoring reconstruction tool based on logic query templates,M. Kim; M. Gee; A. Loh; N. Rachatasumrit,"The University of Texas at Austin, Austin, TX, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,371,372,"<p>Knowing which parts of a system underwent which types of refactoring between two program versions can help programmers better understand code changes. Though there are a number of techniques that automatically find refactorings from two input program versions, these techniques are inadequate in terms of coverage by handling only a subset of refactoring types---mostly simple rename and move refactorings at the level of classes, methods, and fields. This paper presents a Ref-Finder Eclipse plug-in that automatically identifies both atomic and composite refactorings using a template-based refactoring reconstruction approach---it expresses each refactoring type in terms of template logic queries and uses a logic programming engine to infer concrete refactoring instances. Ref-Finder currently supports sixty three types in the Fowler's catalog, showing the most comprehensive coverage among existing techniques.</p>",,,10.1145/1882291.1882353,,,logic-based program representation;program differencing;refactoring;software evolution,,,,,11,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
iMuse: interactive model-based use-case and storytelling environment,K. Winbladh; H. Ziv; D. J. Richardson,"University of Delaware, Newark, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,383,384,"<p>Requirements specification is an important problem in software engineering. Key challenges in Requirements Engineering (RE) include enabling different stakeholders to understand and validate the requirements, and enabling collaboration among different types of stakeholders with different skills and expertise and potentially conflicting needs and expectations. We contend that collaboration among both technical and non-technical stakeholders is improved by a requirements specification technique that provides both precision and usability - and a better balance of the two. We implement our specification technique in iMuse - Interactive Model-based Use-case and Storytelling Environment. In this demo, we show how iMuse can be used by stakeholders to express and view narrative functional requirements.</p>",,,10.1145/1882291.1882360,,,precision;usability,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Guided recovery for web service applications,J. Simmonds; S. Ben-David; M. Chechik,"University of Toronto, Toronto, ON, Canada",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,247,256,"<p>Web service applications are dynamic, highly distributed, and loosely coupled orchestrations of services which are notoriously difficult to debug. In this paper, we describe a user-guided recovery framework for web services. When behavioural correctness properties (safety and bounded liveness) of an application are violated at runtime, we automatically propose and rank recovery plans which users can then select for execution. For safety violations, such plans essentially involve ""going back"" -- compensating the occurred actions until an alternative behavior of the application is possible. For bounded liveness violations, such plans include both ""going back"" and ""re-planning"" --guiding the application towards a desired behavior. We report on the implementation and our experience with the recovery system.</p>",,,10.1145/1882291.1882328,,,behavioral properties;lts;planning;runtime monitoring;sat solving;web services,,,,,9,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Phantm: PHP analyzer for type mismatch,E. Kneuss; P. Suter; V. Kuncak,"Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,373,374,"<p>We present Phantm, a static analyzer that uses a flow-sensitive analysis to detect type errors in PHP applications. Phantm can infer types for nested arrays, and can leverage runtime information and procedure summaries for more precise results. Phantm found over 200 true problems when applied to three applications with over 50'000 lines of code, including the popular DokuWiki code base.</p>",,,10.1145/1882291.1882355,,,abstract interpretation;php;type inference,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
LEAP: lightweight deterministic multi-processor replay of concurrent java programs,J. Huang; P. Liu; C. Zhang,"Hong Kong University of Science and Technology, Hong Kong, China",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,385,386,"<p>The technique of deterministic record and replay aims at faithfully reenacting an earlier program execution. For concurrent programs, it is one of the most important techniques for program understanding and debugging. This demo presents LEAP: an efficient technique as well as a tool prototype to deterministically replay concurrent Java programs on multi-processors without any changes to the host's environment. During execution, LEAP records the thread access orders w.r.t. each shared memory location. The same thread access orders are then enforced in the replay execution to drive the program to the same states. The replay determinism of this approach is underpinned by formal models and a replay theorem developed in this work. Compared to the related approaches, LEAP records much less information, and thus much more efficient.</p>",,,10.1145/1882291.1882361,,,concurrency bug reproduction;deterministic replay;multi-processor,,,,,6,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Directed test suite augmentation: techniques and tradeoffs,Z. Xu; Y. Kim; M. Kim; G. Rothermel; M. B. Cohen,"University of Nebraska - Lincoln, Lincoln, NE, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,257,266,"<p>Test suite augmentation techniques are used in regression testing to identify code elements affected by changes and to generate test cases to cover those elements. Our preliminary work suggests that several factors influence the cost and effectiveness of test suite augmentation techniques. These include the order in which affected elements are considered while generating test cases, the manner in which existing regression test cases and newly generated test cases are used, and the algorithm used to generate test cases. In this work, we present the results of an empirical study examining these factors, considering two test case generation algorithms (concolic and genetic). The results of our experiment show that the primary factor affecting augmentation is the test case generation algorithm utilized; this affects both cost and effectiveness. The manner in which existing and newly generated test cases are utilized also has a substantial effect on efficiency but a lesser effect on effectiveness. The order in which affected elements are considered turns out to have relatively few effects when using concolic test case generation, but more substantial effects when using genetic test case generation.</p>",,,10.1145/1882291.1882330,,,concolic testing;empirical studies;genetic algorithms;regression testing;test suite augmentation,,,,,18,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Supercomputing in biology: towards understanding living systems in atomic detail,K. Y. Sanbonmatsu,"Los Alamos National Laboratory, Los Alamos, NM, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,1,2,"<p>Biotechnology has achieved exquisite control over many biological systems. However, to design drugs computationally, we need to understand these systems at the atomic level of detail well enough to predict their behavior. While we have not achieved this goal, advances in high-performance computing are helping considerably. The advent of petaflop-scale computing has opened the door to realistic simulations of biomolecular systems, making it possible to simulate major drug targets, such as the ribosome, in atomic detail. In addition to being the target of approximately half of antibiotics used in US hospitals, the ribosome is also interesting from an evolutionary point of view. Because many regions of the ribosome are identical in every organism studied to date, the ribosome is arguably the oldest molecular fossil, and represents a bridge between the RNA-world and the modern DNA-RNA-protein world. Analogous to the 'CPU of the living cell', the ribosome performs a look-up table operation, converting the 4-letter alphabet of genetic instructions into the 20-letter alphabet of proteins, the cell's workhorse molecules that perform biochemistry and constitute structures of the cell. We have used high performance computing to delve into the inner workings of this molecular factory, applying Newton's equations of motion to every atom in the ribosome and its surrounding environment, iterating for billions of time steps. In addition to revealing mechanistic insight unattainable by experimental studies, these simulations present an excellent system to push the limits of supercomputing - even exa-scale simulations are insufficient to simulate the entire process of protein synthesis.</p>",,,10.1145/1882291.1882292,,,computational biology;high-performance computing;molecular machines,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
An enhanced test case selection approach for model-based testing: an industrial case study,H. Hemmati; L. Briand; A. Arcuri; S. Ali,"Simula Research Laboratory and University of Oslo, Oslo, Norway",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,267,276,"<p>In recent years, Model-Based Testing (MBT) has attracted an increasingly wide interest from industry and academia. MBT allows automatic generation of a large and comprehensive set of test cases from system models (e.g., state machines), which leads to the systematic testing of the system. However, even when using simple test strategies, applying MBT in large industrial systems often leads to generating large sets of test cases that cannot possibly be executed within time and cost constraints. In this situation, test case selection techniques are employed to select a subset from the entire test suite such that the selected subset conforms to available resources while maximizing fault detection. In this paper, we propose a new similarity-based selection technique for state machine-based test case selection, which includes a new similarity function using triggers and guards on transitions of state machines and a genetic algorithm-based selection algorithm. Applying this technique on an industrial case study, we show that our proposed approach is more effective in detecting real faults than existing alternatives. We also assess the overall benefits of model-based test case selection in our case study by comparing the fault detection rate of the selected subset with the maximum possible fault detection rate of the original test suite.</p>",,,10.1145/1882291.1882331,,,genetic algorithms;model-based testing;similarity-based selection;test case selection,,,,,8,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
"Big data, global development, and complex social systems",N. Eagle,"Santa Fe Institute / Massachusetts Institute of Technology, Boston, MA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,3,4,"<p>Petabytes of data about human movements, transactions, and communication patterns are continuously being generated by everyday technologies such as mobile phones and credit cards. In collaboration with the mobile phone, internet, and credit card industries, Eagle and colleagues are aggregating and analyzing behavioral data from over 250 million people from North and South America, Europe, Asia and Africa. Eagle discusses projects arising from these collaborations that involve inferring behavioral dynamics on a broad spectrum of scales from risky behavior in a group of MIT freshman to population-level behavioral signatures, including cholera outbreaks in Rwanda and wealth in the UK. The research group is developing a range of large-scale network analysis and machine learning algorithms that will provide deeper insight into human behavior.</p>",,,10.1145/1882291.1882293,,,behavioral dynamics;data analysis;machine learning;network analysis,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Representation dependence testing using program inversion,A. Kanade; R. Alur; S. Rajamani; G. Ramanlingam,"Indian Institute of Science, Bangalore, India",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,277,286,"<p>The definition of a data structure may permit many different concrete representations of the same logical content. A (client) program that accepts such a data structure as input is said to have a <i>representation dependence</i> if its behavior differs for logically equivalent input values. In this paper, we present a methodology and tool for automated testing of clients of a data structure for representation dependence. In the proposed methodology, the developer expresses the logical equivalence by writing a normalization program <i>f</i> that maps each concrete representation to a canonical one. Our solution relies on automatically synthesizing the one-to-many inverse function of <i>f</i>: given an input value <i>x</i>, we can generate multiple test inputs logically equivalent to <i>x</i> by executing the inverse with the canonical value <i>f</i>(<i>x</i>) as input repeatedly. We present an inversion algorithm for restricted classes of normalization programs including programs mapping arrays to arrays in a typical iterative manner. We present a prototype implementation of the algorithm, and demonstrate how our methodology reveals bugs due to representation dependence in open source software such as Open Office and Picasa using the widely used image format TIFF. TIFF is a challenging case study for our approach.</p>",,,10.1145/1882291.1882332,,,data structures;program inversion;testing,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Avoiding the classic catastrophic computer science failure mode: 2010 acm sigsoft outstanding research award talk,R. E. Johnson,"University of Illinois at Urbana-Champaign, Urbana, IL, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,5,6,"<p>Many computer science research efforts fail. Some of this is inevitable, since research is risky. But sometimes the agenda of a group of researchers fails because there is a part of the problem that everyone agrees is crucial, but that nobody works on. Often this is because there are not enough rewards for working on it; it is hard to publish and/or there is no funding. This is more common than you might think; I call it the classic catastrophic computer science failure mode.</p> <p>Design Patterns could have fallen into this trap. Our hypothesis was ""A standard description of design techniques will enable new designers to become expert faster and will improve the design process for experts."" I don't believe we have proven this hypothesis yet, though there is a lot of anecdotal evidence for it. A key step in proving it is developing the standard description of design techniques, and ""Design Patterns"" is part of that. There were lots of reasons not to write the book. Who were we to try to standardize design? How did we know we had the right patterns? These are unanswerable questions, so we focused on writing the catalog and let other people decide whether it was worthwhile. Other people publicized it for us. Without those people, it would not have made much of an impact.</p> <p>Other projects have not been so fortunate. I've seen several projects fail because they needed something like a catalog, but nobody worked on developing it. This talk will describe warning signs of the classic catastrophic computer science failure mode and how to avoid it.</p>",,,10.1145/1882291.1882294,,,design patterns;failure mode;research;risk,,,,,0,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Field-sensitive program dependence analysis,S. Litvak; N. Dor; R. Bodik; N. Rinetzky; M. Sagiv,"Tel Aviv University &#38; Panaya Inc., Tel-Aviv, Israel",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,287,296,"<p>Statement st transitively depends on statement <i>st<sub>seed</sub></i> if the execution of <i>st<sub>seed</sub></i> may affect the execution of st. Computing transitive program dependences is a fundamental operation in many automatic software analysis tools. Existing tools find it challenging to compute transitive dependences for programs manipulating large aggregate structure variables, and their limitations adversely affect analysis of certain important classes of software systems, e.g., large-scale <i>enterprise resource planning</i> (ERP) systems.</p> <p>This paper presents an efficient conservative interprocedural static analysis algorithm for computing field-sensitive transitive program dependences in the presence of large aggregate structure variables. Our key insight is that program dependences coming from operations on whole substructures can be precisely (i.e., field-sensitively) represented at the granularity of substructures instead of individual fields. Technically, we adapt the interval domain to concisely record dependences between <i>multiple pairs</i> of fields of aggregate structure variables by exploiting the fields' spatial arrangement.</p> <p>We <i>prove</i> that our algorithm is as precise as any algorithm which works at the granularity of individual fields, the most-precise known approach for this problem. Our empirical study, in which we analyzed industrial ERP programs with over 100,000 lines of code in average, shows significant improvements in both the running times and memory consumption over existing approaches: The baseline is an efficient field-insensitive <i>whole-structure</i> that incurs a 62% false error rate. An <i>atomization</i>-based algorithm, which disassemble every aggregate structure variable into the collection of its individual fields, can remove all these false errors at the cost of doubling the average analysis time, from 30 to 60 minutes. In contrast, our new precise algorithm removes all false errors by increasin- - g the time only to 35 minutes. In terms of memory consumption, our algorithm increases the footprint by less than 10%, compared to 50% overhead of the atomizing algorithm.</p>",,,10.1145/1882291.1882334,,,adas;aggregate structure variables;erp;field-sensitivity;transitive program dependences,,,,,1,1,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
"DSketch: lightweight, adaptable dependency analysis",B. Cossette; R. J. Walker,"University of Calgary, Calgary, AB, Canada",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,297,306,"<p>Software developers who extend or repair existing software systems spend considerable effort in understanding how their modifications will require follow-on changes in order to work correctly. Tool support for this process is available for single, popular languages, but does not suffice for less popular languages, uncommon language variants, or arbitrary combinations of languages and connection technologies. We have created the DSketch tool so that developers can create a lightweight pattern specification for how dependencies can be heuristically identified in their systems. We performed two case studies involving industrial developers who applied our tool for conducting polylingual dependency analysis in software systems; the developers found it easy to configure the tool for their needs, were able to adapt their patterns to new contexts, and had sufficiently accurate dependency predictions for their work.</p>",,,10.1145/1882291.1882335,,,approximation;case study;dependency analysis;developer feedback;dsketch;lightweight tool support;pattern matching;polylingual systems,,,,,6,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Path-based fault correlations,W. Le; M. L. Soffa,"University of Virginia, Charlottesville, VA, USA",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,307,316,"<p>Although a number of automatic tools have been developed to detect faults, much of the diagnosis is still being done manually. To help with the diagnostic tasks, we formally introduce <i>fault correlation</i>, a causal relationship between faults. We statically determine correlations based on the expected dynamic behavior of a fault. If the occurrence of one fault causes another fault to occur, we say they are correlated. With the identification of the correlated faults, we can better understand fault behaviors and the risks of faults. If one fault is uniquely correlated with another, we know fixing the first fault will fix the other. Correlated faults can be grouped, enabling prioritization of diagnoses of the fault groups. In this paper, we develop an interprocedural, path-sensitive, and scalable algorithm to automatically compute correlated faults in a program. In our approach, we first statically detect faults and determine their error states. By propagating the effects of the error state along a path, we detect the correlation of pairs of faults. We automatically construct a correlation graph which shows how correlations occur among multiple faults and along different paths. Guided by a correlation graph, we can reduce the number of faults required for diagnosis to find root causes. We implemented our correlation algorithm and found through experimentation that faults involved in the correlations can be of different types and located in different procedures. Using correlation information, we are able to automate diagnostic tasks that previously had to be done manually.</p>",,,10.1145/1882291.1882336,,,demand-driven;error state;fault correlation;path-sensitive,,,,,6,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Practical and effective symbolic analysis for buffer overflow detection,L. Li; C. Cifuentes; N. Keynes,"Oracle, Brisbane, Australia",Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering,20110708,2010,,,317,326,"<p>Although buffer overflow detection has been studied for more than 20 years, it is still the most common source of security vulnerabilities in systems code. Different approaches using symbolic analysis have been proposed to detect this vulnerability. However, existing symbolic analysis techniques are either too complex to scale to millions of lines of code (MLOC), or too simple to effectively handle loops and complex program structures.</p> <p>In this paper, we present a novel symbolic analysis algorithm for buffer overflow detection that applies simple rules to solve relevant control and data dependencies. Our approach is path-sensitive and effectively handles loops and complex program structures. Scalability is achieved by using a simple symbolic value representation, filtering out irrelevant dependencies in symbolic value computation and computing symbolic values on demand.</p> <p>Evaluation of our approach shows that it is both practical and effective:the analysis runs over 8.6 MLOC of the OpenSolaris<sup>TM</sup> Operating system/Networking (ON)codebase in 11 minutes and finds hundreds of buffer overflows with a false positive rate of less than 10%.</p>",,,10.1145/1882291.1882338,,,demand-driven;scalability;static program analysis,,,,,4,,,,,,7-11 Nov. 2010,,ACM,ACM Conferences
Dione: an integrated measurement and defect prediction solution,B. Caglayan; A. T. Misirli; G. Calikli; A. Bener; T. Aytac; B. Turhan,"Bogazici University, Turkey",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,2,"<p>We present an integrated measurement and defect prediction tool: <i>Dione</i>. Our tool enables organizations to measure, monitor, and control product quality through learning based defect prediction. Similar existing tools either provide data collection and analytics, or work just as a prediction engine. Therefore, companies need to deal with multiple tools with incompatible interfaces in order to deploy a complete measurement and prediction solution. Dione provides a fully integrated solution where data extraction, defect prediction and reporting steps fit seamlessly. In this paper, we present the major functionality and architectural elements of <i>Dione</i> followed by an overview of our demonstration.</p>",,,10.1145/2393596.2393619,,,measurement;software defect prediction;software tool,,,,,2,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
How do developers react to API deprecation?: the case of a smalltalk ecosystem,R. Robbes; M. Lungu; D. R’_thlisberger,University of Chile,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation---known as a ripple effect---is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes.</p> <p>Although studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on an empirical study of API deprecations that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating around the Squeak and Pharo software ecosystems: seven years of evolution, more than 3,000 contributors, and more than 2,600 distinct systems. We analyzed 577 methods and 186 classes that were deprecated, and answer research questions regarding the frequency, magnitude, duration, adaptation, and consistency of the ripple effects triggered by API changes.</p>",,,10.1145/2393596.2393662,,,ecosystems;empirical studies;mining software repositories,,,,,8,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
SecuriTAS: a tool for engineering adaptive security,L. Pasquale; C. Menghi; M. Salehie; L. Cavallaro; I. Omoronyia; B. Nuseibeh,"University of Limerick, Ireland",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>This paper presents SecuriTAS, a tool to engineer adaptive security. It allows software designers to model security concerns together with the requirements of a system. This model is then used at runtime to analyze changes in security concerns and select the best set of security controls necessary to protect the system.</p>",,,10.1145/2393596.2393618,,,adaptive software;dynamic access control;goals;security,,,,,1,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Searching connected API subgraph via text phrases,W. K. Chan; H. Cheng; D. Lo,The Chinese University of Hong Kong,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Reusing APIs of existing libraries is a common practice during software development, but searching suitable APIs and their usages can be time-consuming [6]. In this paper, we study a new and more practical approach to help users find usages of APIs given only simple text phrases, when users have limited knowledge about an API library. We model API invocations as an API graph and aim to find an optimum connected subgraph that meets users' search needs.</p> <p>The problem is challenging since the search space in an API graph is very huge. We start with a greedy subgraph search algorithm which returns a connected subgraph containing nodes with high textual similarity to the query phrases. Two refinement techniques are proposed to improve the quality of the returned subgraph. Furthermore, as the greedy subgraph search algorithm relies on online query of shortest path between two graph nodes, we propose a space-efficient compressed shortest path indexing scheme that can efficiently recover the exact shortest path. We conduct extensive experiments to show that the proposed subgraph search approach for API recommendation is very effective in that it boosts the average F<sub>1</sub>-measure of the state-of-the-art approach, <i>Portfolio</i> [15], on two groups of real-life queries by 64% and 36% respectively.</p>",,,10.1145/2393596.2393606,,,API graph;API recommendation;subgraph searching,,,,,12,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
History slicing: assisting code-evolution tasks,F. Servant; J. A. Jones,"University of California, Irvine, CA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Many software-engineering tasks require developers to understand the history and evolution of source code. However, today's software-development techniques and tools are not well suited for the easy and efficient procurement of such information. In this paper, we present an approach called <i>history slicing</i> that can automatically identify a minimal number of code modifications, across any number of revisions, for any arbitrary segment of source code at fine granularity. We also present our implementation of history slicing, Chronos, that includes a novel visualization of the entire evolution for the code of interest. We provide two experiments: one experiment automatically computes 16,000 history slices to determine the benefit brought by various levels of automation, and another experiment that assesses the practical implications of history slicing for actual developers using the technique for actual software-maintenance tasks that involve code evolution. The experiments show that history slicing offered drastic improvements over the conventional techniques in three ways: (1) the amount of information needed to be examined and traced by developers was reduced by up to three orders of magnitude; (2) the correctness of developers attempting to solve software-maintenance tasks was more than doubled; and (3) the time to completion of these software-maintenance tasks was almost halved.</p>",,,10.1145/2393596.2393646,,,mining software repositories;program comprehension;software evolution;software visualization,,,,,8,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Seeking the ground truth: a retroactive study on the evolution and migration of software libraries,B. E. Cossette; R. J. Walker,"University of Calgary, Calgary, Alberta, Canada",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Application programming interfaces (APIs) are a common and industrially-relevant means for third-party software developers to reuse external functionality. Several techniques have been proposed to help migrate client code between library versions with incompatible APIs, but it is not clear how well these perform in an absolute sense. We present a retroactive study into the presence and nature of API incompatibilities between several versions of a set of Java-based software libraries; for each, we perform a detailed, manual analysis to determine what the correct adaptations are to migrate from the older to the newer version. In addition, we investigate whether any of a set of adaptation recommender techniques is capable of identifying the correct adaptations for library migration. We find that a given API incompatibility can typically be addressed by only one or two recommender techniques, but sometimes none serve. Furthermore, those techniques give correct recommendations, on average, in only about 20% of cases.</p>",,,10.1145/2393596.2393661,,,API;adaptive change;recommendation systems,,,,,14,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Multi-layered approach for recovering links between bug reports and fixes,A. T. Nguyen; T. T. Nguyen; H. A. Nguyen; T. N. Nguyen,"Iowa State University, Ames, IA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar.</p> <p>This paper introduces MLink, a multi-layered approach that takes into account not only textual features but also <i>source code features</i> of the changed code corresponding to the commit logs. It is also capable of learning the <i>association</i> relations between the <i>terms</i> in bug reports and the <i>names</i> of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that MLink can improve the state-of-the-art bug-to-fix link recovery methods by 11--18%, 13--17%, and 8--17% in F-score, recall, and precision, respectively.</p>",,,10.1145/2393596.2393671,,,bug-to-fix links;bugs;fixes;mining software repository,,,,,12,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Mining the execution history of a software system to infer the best time for its adaptation,K. R. Canavera; N. Esfahani; S. Malek,George Mason University,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>An important challenge in dynamic adaptation of a software system is to prevent inconsistencies (failures) and disruptions in its operations during and after change. Several prior techniques have solved this problem with various tradeoffs. All of them, however, assume the availability of detailed component dependency models. This paper presents a complementary technique that solves this problem in settings where such models are either not available, difficult to build, or outdated due to the evolution of the software. Our approach first mines the execution history of a software system to infer a <i>stochastic component dependency model</i>, representing the probabilistic sequence of interactions among the system's components. We then demonstrate how this model could be used at runtime to infer the ""best time"" for adaptation of the system's components. We have thoroughly evaluated this research on a multi-user real world software system and under varying conditions.</p>",,,10.1145/2393596.2393616,,,component dependency;data mining;dynamic adaptation,,,,,4,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Automating adaptive maintenance changes with SrcML and LINQ,V. Augustine,"ABB Corporate Research, Raleigh, NC",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,2,"<p>Automated program transformation can significantly improve the speed and accuracy of adaptive maintenance tasks. However, developers typically eschew program transformation because of the difficulty in creating, using, and maintaining transformations. They may also be unwilling to learn a new technique that will be used infrequently. We present an approach that leverages programmers' existing knowledge of C# and LINQ to drastically reduce the learning curve associated with program transformation.</p>",,,10.1145/2393596.2393604,,,program transformation,,,,,,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Improving software developers' fluency by recommending development environment commands,E. Murphy-Hill; R. Jiresal; G. C. Murphy,"North Carolina State University, Raleigh, North Carolina",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Software developers interact with the development environments they use by issuing commands that execute various programming tools, from source code formatters to build tools. However, developers often only use a small subset of the commands offered by modern development environments, reducing their overall development fluency. In this paper, we use several existing command recommender algorithms to suggest new commands to developers based on their existing command usage history, and also introduce several new algorithms. By running these algorithms on data submitted by several thousand Eclipse users, we describe two studies that explore the feasibility of automatically recommending commands to software developers. The results suggest that, while recommendation is more difficult in development environments than in other domains, it is still feasible to automatically recommend commands to developers based on their usage history, and that using patterns of past discovery is a useful way to do so.</p>",,,10.1145/2393596.2393645,,,IDEs;commands;discovery;software developers,,,,,8,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Concept-based failure clustering,N. DiGiuseppe; J. A. Jones,"University of California, Irvine",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>When attempting to determine the number and set of execution failures that are caused by particular faults, developers must perform an arduous task of investigating and diagnosing each individual failure. Researchers proposed failure-clustering techniques to automatically categorize failures, with the intention of isolating each culpable fault. The current techniques utilize dynamic control flow to characterize each failure to then cluster them. These existing techniques, however, are blind to the intent or purpose of each execution, other than what can be inferred by the control-flow profile. We hypothesize that semantically rich execution information can aid clustering effectiveness by categorizing failures according to which functionality they exhibit in the software. This paper presents a novel clustering method that utilizes latent-semantic-analysis techniques to categorize each failure by the semantic concepts that are expressed in the executed source code. We present an experiment comparing this new technique to traditional control-flow-based clustering. The results of the experiment showed that the semantic-concept clustering was more precise in the number of clusters produced than the traditional approach, without sacrificing cluster accuracy.</p>",,,10.1145/2393596.2393629,,,debugging;fault clustering;testing,,,,,7,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Rubicon: bounded verification of web applications,J. P. Near; D. Jackson,"Massachusetts Institute of Technology, Cambridge, MA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Rubicon is a verifier for web applications. Specifications are written in an embedded domain-specific language and are checked fully automatically. Rubicon is designed to fit with current practices: its language is based on RSpec, a popular testing framework, and its analysis leverages the standard Ruby interpreter to perform symbolic execution (generating verification conditions that are checked by the Alloy Analyzer). Rubicon has been evaluated on five open-source applications; in one, a widely used customer relationship management system, a previously unknown security flaw was revealed.</p>",,,10.1145/2393596.2393667,,,Alloy analyzer;Alloy language;RSpec;Ruby on rails;lightweight formal methods;programming languages;symbolic execution;web applications,,,,,3,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Predicting null-pointer dereferences in concurrent programs,A. Farzan; P. Madhusudan; N. Razavi; F. Sorrentino,University of Toronto,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>We propose null-pointer dereferences as a target for finding bugs in concurrent programs using testing. A null-pointer dereference prediction engine observes an execution of a concurrent program under test and predicts alternate interleavings that are likely to cause null-pointer dereferences. Though accurate scalable prediction is intractable, we provide a carefully chosen novel set of techniques to achieve reasonably accurate and scalable prediction. We use an abstraction to the shared-communication level, take advantage of a static lock-set based pruning, and finally, employ precise and relaxed constraint solving techniques that use an SMT solver to predict schedules. We realize our techniques in a tool, ExceptioNULL, and evaluate it over 13 benchmark programs and find scores of null-pointer dereferences by using only a single test run as the prediction seed for each benchmark.</p>",,,10.1145/2393596.2393651,,,SMT;concurrency;data-races;null-pointers;testing,,,,,4,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
How do developers use parallel libraries?,S. Okur; D. Dig,"University of Illinois at Urbana-Champaign, IL",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Parallel programming is hard. The industry leaders hope to convert the hard problem of <i>using parallelism</i> into the easier problem of <i>using a parallel library</i>. Yet, we know little about how programmers adopt these libraries in practice. Without such knowledge, other programmers cannot educate themselves about the state of the practice, library designers are unaware of API misusage, researchers make wrong assumptions, and tool vendors do not support common usage of library constructs.</p> <p>We present the first study that analyzes the usage of parallel libraries in a large scale experiment. We analyzed 655 open-source applications that adopted Microsoft's new parallel libraries -- Task Parallel Library (TPL) and Parallel Language Integrated Query (PLINQ) -- comprising 17.6M lines of code written in C#. These applications are developed by 1609 programmers. Using this data, we answer 8 research question and we uncover some interesting facts. For example, (i) for two of the fundamental parallel constructs, in at least 10% of the cases developers misuse them so that the code runs sequentially instead of concurrently, (ii) developers make their parallel code unnecessarily complex, (iii) applications of different size have different adoption trends. The library designers confirmed that our finding are useful and will influence the future development of the libraries.</p>",,,10.1145/2393596.2393660,,,C#;empirical study;multi-core;parallel libraries,,,,,12,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Software change contracts,D. Qi; J. Yi; A. Roychoudhury,National University of Singapore,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Incorrect program changes including regression bugs, incorrect bug-fixes, incorrect feature updates are pervasive in software. These incorrect program changes affect software quality and are difficult to detect/correct. In this paper, we propose the notion of ""change contracts"" to avoid incorrect program changes. Change contracts formally specify the intended effect of program changes. Incorrect program changes are detected when they are checked with respect to the change contracts. We design a change contract language for Java programs and a dynamic checking system for our change contract language. We conduct a preliminary user study to check the expressiveness of our change contract language and find that the language is expressive enough to capture a wide variety of real-life changes in three large software projects (i.e., Ant, JMeter, log4j). Finally, our contract checking system detects several real-life incorrect changes in these three software projects via runtime checking of the change contracts.</p>",,,10.1145/2393596.2393622,,,JML;change contract;regression testing;software evolution,,,,,2,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Automated extraction of security policies from natural-language software documents,X. Xiao; A. Paradkar; S. Thummalapenta; T. Xie,"North Carolina State University, Raleigh, NC",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Access Control Policies (ACP) specify which principals such as users have access to which resources. Ensuring the correctness and consistency of ACPs is crucial to prevent security vulnerabilities. However, in practice, ACPs are commonly written in Natural Language (NL) and buried in large documents such as requirements documents, not amenable for automated techniques to check for correctness and consistency. It is tedious to manually extract ACPs from these NL documents and validate NL functional requirements such as use cases against ACPs for detecting inconsistencies. To address these issues, we propose an approach, called Text2Policy, to automatically extract ACPs from NL software documents and resource-access information from NL scenario-based functional requirements. We conducted three evaluations on the collected ACP sentences from publicly available sources along with use cases from both open source and proprietary projects. The results show that Text2Policy effectively identifies ACP sentences with the precision of 88.7% and the recall of 89.4%, extracts ACP rules with the accuracy of 86.3%, and extracts action steps with the accuracy of 81.9%.</p>",,,10.1145/2393596.2393608,,,access control;natural language processing;requirements analysis,,,,,11,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
From pixels to bytes: evolutionary scenario based design with video,H. Xu; O. Creighton; N. Boulila; B. Bruegge,"Technische Universit&#228;t M&#252;nchen, Munich, Germany",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Change and user involvement are two major challenges in agile software projects. As change and user involvement usually arise spontaneously, reaction to change, validation and communication are thereby expected to happen in a continuous way in the project lifecycle. We propose Evolutionary Scenario Based Design, which employs video in fulfilling this goal, and present a new idea that supports video production using SecondLife-like virtual world technology.</p>",,,10.1145/2393596.2393631,,,requirements engineering;scenario based design;video prototyping;virtual world,,,,,2,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
An industrial study on the risk of software changes,E. Shihab; A. E. Hassan; B. Adams; Z. M. Jiang,"Queen's University, Canada",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., <i>changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing</i>. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67%, and a precision improvement of 87% (using developer specific models) and 37% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects.</p>",,,10.1145/2393596.2393670,,,bug inducing changes;change metrics;change risk;code metrics,,,,,15,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Do crosscutting concerns cause modularity problems?,R. J. Walker; S. Rawal; J. Sillito,"University of Calgary, Calgary, Canada",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>It has been claimed that crosscutting concerns are pervasive and problematic, leading to difficulties in program comprehension, evolution, and long-term design degradation. To consider whether this theory bears out, we examine the patch history of the Mozilla project over a period of a decade to consider whether crosscutting concerns exist therein and whether we can see evidence of problems arising from them. Mozilla is an interesting case, due to its longevity; size; polylingual nature; and use of a patch review process, which maintains strong connections between issue reports and the patches that are intended to address each. We perform several statistical analyses of the over 200,000 patches submitted to address over 90,000 issues reported in this time period. We find that 90% of patches show little or no evidence of scattering, that the scattering of a patch tends to decrease slightly upon review on average, and that the system shows at worst a slow increase of average scattering over time.</p>",,,10.1145/2393596.2393654,,,AOSD;Mozilla;power laws;retroactive study;scattering,,,,,3,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
MayPar: a may-happen-in-parallel analyzer for concurrent objects,E. Albert; A. Flores-Montoya; S. Genaim,Complutense University of Madrid,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,2,"<p>We present the concepts, usage and prototypical implementation of MayPar, a <i>may-happen-in-parallel</i> (MHP) static analyzer for a distributed asynchronous language based on <i>concurrent objects</i>. Our tool allows analyzing an application and finding out the pairs of statements that can execute in parallel. The information can be displayed by means of a graphical representation of the MHP analysis graph or, in a textual way, as a set of pairs which identify the program points that may run in parallel. The information yield by MayPar can be relevant (1) to spot bugs in the program related to fragments of code which should not run in parallel and also (2) to improve the precision of other analyses which infer more complex properties (e.g., termination and cost).</p>",,,10.1145/2393596.2393611,,,concurrent objects;parallelism;resource guarantees;static analysis,,,,,,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Automating presentation changes in dynamic web applications via collaborative hybrid analysis,X. Wang; L. Zhang; T. Xie; Y. Xiong; H. Mei,"Peking University, MOE, China",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Web applications are becoming increasingly popular nowadays. During the development and evolution of a web application, a typical type of tasks is to change the presentation of the web application, such as correcting display errors, adding user-interface controls, or changing appearance styles. To change the presentation of a static web page, developers are able to modify the HTML text of the web page using a graphical web-page editor. However, to change the presentation of a dynamic web application, instead of using a graphical web-page editor to directly modify generated web pages, developers need to modify the code that generates the web pages. As manually performing presentation changes in dynamic web applications is tedious and error-prone, we propose a novel approach based on <i>collaborative hybrid analysis</i> that combines static analysis and dynamic analysis to facilitate developers to perform presentation changes in dynamic web applications. Our approach includes two parts. The first part takes as input the presentation change to be performed on a generated web page (with proper runtime information), and uses <i>dynamic string-origin analysis</i> to locate the source-code segment that generates the changed part of the web page. The second part checks unexpected impact of directly performing the change on the source-code segment, and asks for human intervention when unexpected impact exists. We implemented our approach for the PHP language and carried out an empirical study on 39 presentation-change tasks identified from 600 bug reports of three real-world dynamic web applications (in total more than 148 KLOC). Among the 39 tasks, our approach is able to correctly locate the place to modify in each presentation-change task and correctly perform the presentation change on the source code in more than half of the tasks.</p>",,,10.1145/2393596.2393614,,,dynamic string-origin analysis;presentation change;web application,,,,,5,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
ACM SIGSOFT Impact Paper Award 2012: Systematic Software Testing: The Korat Approach,C. Boyapati; S. Khurshid; D. Marinov,"Google Inc. Mountain View, CA 94043",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,,,"At ISSTA 2002, the three authors (then Ph.D. students) published the paper ""Korat: Automated Testing Based on Java Predicates"", which won one of the fi rst ACM SIGSOFT Distinguished paper awards. In 2012, the paper won the ACM SIGSOFT Impact Paper Award. The authors briefly recount the motivation behind Korat research, the ideas presented in the original paper, and some work it inspired.",,,10.1145/2393596.2407117,,,,,,,,,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
REPERTOIRE: a cross-system porting analysis tool for forked software projects,B. Ray; C. Wiley; M. Kim,The University of Texas at Austin,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>To create a new variant of an existing project, developers often copy an existing codebase and modify it. This process is called software forking. After forking software, developers often port new features or bug fixes from peer projects. Repertoire analyzes repeated work of cross-system porting among forked projects. It takes the version histories as input and identifies ported edits by comparing the content of individual patches. It also shows users the extent of ported edits, where and when the ported edits occurred, which developers ported code from peer projects, and how long it takes for patches to be ported.</p>",,,10.1145/2393596.2393603,,,code clones;forking;porting;repetitive changes;software evolution,,,,,3,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Efficiently scripting change-resilient tests,S. Thummalapenta; N. Singhania; P. Devaki; S. Sinha; S. Chandra; A. K. Das; S. Mangipudi,IBM Research -- India,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,2,"<p>In industrial practice, test cases often start out as steps described in natural language and are intended to be executed by a human. Since tests are executed repeatedly, they go through an automation process, in which they are converted to automated test scripts (or programs) that perform the test steps mechanically. Conventional test-automation techniques can be time-consuming, require specialized skills, and can produce fragile scripts. To address these limitations, we present a tool, called ata, for automating the test-automation task. Using a novel combination of natural-language processing, backtracking exploration, and learning, ata can significantly improve tester productivity in automating manual tests. ata also produces <i>change-resilient</i> scripts, which automatically adapt themselves in the presence of certain common types of user-interface changes.</p>",,,10.1145/2393596.2393643,,,natural-language processing;test automation;test repair,,,,,2,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Has this bug been reported?,K. Liu; H. B. K. Tan; M. Chandramohan,"Nanyang Technological University, Singapore",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Bug reporting is an uncoordinated process that is often the cause of redundant workload in triaging and fixing bugs due to many duplicated bug reports. Furthermore, quite often, same bugs are repeatedly reported as users or testers are unaware of whether they have been reported from the search query results. In order to reduce both the users and developers' efforts, the quality of search in a bug tracking system is crucial. However, all existing search functions in a bug tracking system produce results with undesired relevance and ranking. Hence, it is essential to provide an effective search function to any bug tracking system.</p> <p>Learning to rank (LTR) is a supervised machine learning technique that is used to construct a ranking model from training data. We propose a novel approach by using LTR to search for potentially related bug reports in a bug tracking system. Our method uses a set of proposed features of bug reports and queries. A preliminary evaluation shows that our approach can enhance the quality of searching for similar bug reports, therefore, relieving the burden of developers in dealing with duplicate bug reports.</p>",,,10.1145/2393596.2393628,,,bug report;bug tracking system;duplicate;learning to rank;search engine;search quality,,,,,4,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Automated concolic testing of smartphone apps,S. Anand; M. Naik; M. J. Harrold; H. Yang,Georgia Tech,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>We present an algorithm and a system for generating input events to exercise smartphone apps. Our approach is based on concolic testing and generates sequences of events automatically and systematically. It alleviates the path-explosion problem by checking a condition on program executions that identifies subsumption between different event sequences. We also describe our implementation of the approach for Android, the most popular smartphone app platform, and the results of an evaluation that demonstrates its effectiveness on five Android apps.</p>",,,10.1145/2393596.2393666,,,Android;GUI testing;testing event-driven programs,,,,,47,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
DTAM: dynamic taint analysis of multi-threaded programs for relevancy,M. Ganai; D. Lee; A. Gupta,NEC Labs America,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Testing and debugging multi-threaded programs are notoriously difficult due to non-determinism not only in inputs but also in OS schedules. In practice, dynamic analysis and failure replay systems instrument the program to record events of interest in the test execution, e.g., program inputs, accesses to shared objects, synchronization operations, context switches, etc. To reduce the overhead of logging during runtime, these testing and debugging efforts have proposed tradeoffs for sampling or selective logging, at the cost of reducing coverage or performing more expensive search offline.</p> <p>We propose to identify a subset of input sources and shared objects that are, in a sense, <i>relevant</i> for covering program behavior. We classify various types of relevancy in terms of how an input source or a shared object can affect control flow (e.g., a conditional branch) or dataflow (e.g., state of the shared objects) in the program. Such relevancy data can be used by testing and debugging methods to reduce their recording overhead and to guide coverage.</p> <p>To conduct relevancy analysis, we propose a novel framework based on <u>d</u>ynamic <u>t</u>aint <u>a</u>nalysis for <u>m</u>ulti-threaded programs, called <i>DTAM</i>. It performs thread-modular taint analysis for each thread in parallel during runtime, and then aggregates the thread-modular results offline. This approach has many advantages: (a) it is faster than conducting taint analysis for serialized multi-threaded executions, (b) it can compute results for alternate thread interleavings by generalizing the observed execution, and (c) it provides a knob to tradeoff precision with coverage, depending on how thread-modular results are aggregated to account for alternate interleavings. We have implemented DTAM and performed an experimental evaluation on publicly available benchmarks for relevancy analysis. Our experiments show that most shared accesses and - onditional branches are dependent on some program input sources. Interestingly in our test runs, on average, only about 25% input sources and 3% shared objects affect other shared accesses through conditional branches. Thus, it is important to identify such relevant input sources and shared objects for testing and debugging.</p>",,,10.1145/2393596.2393650,,,generalization;relevancy;taint analysis,,,,,1,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Retargeting Android applications to Java bytecode,D. Octeau; S. Jha; P. McDaniel,Pennsylvania State University,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>The Android OS has emerged as the leading platform for SmartPhone applications. However, because Android applications are compiled from Java source into platform-specific Dalvik bytecode, existing program analysis tools cannot be used to evaluate their behavior. This paper develops and evaluates algorithms for retargeting Android applications received from markets to Java class files. The resulting Dare tool uses a new intermediate representation to enable fast and accurate retargeting. Dare further applies strong constraint solving to infer typing information and translates the 257 DVM opcodes using only 9 translation rules. It also handles cases where the input Dalvik bytecode is unverifiable. We evaluate Dare on 1,100 of the top applications found in the free section of the Android market and successfully retarget 99.99% of the 262,110 associated classes. Further, whereas existing tools can only fully retarget about half of these applications, Dare can recover over 99% of them. In this way, we open the door to users, developers and markets to use the vast array of program analysis tools to ensure the correct operation of Android applications.</p>",,,10.1145/2393596.2393600,,,Android;Dalvik bytecode;Dalvik retargeting,,,,,19,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
TouchDevelop: app development on mobile devices,N. Tillmann; M. Moskal; J. de Halleux; M. Fahndrich; S. Burckhardt,"Microsoft Research, Redmond WA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,2,"<p>Mobile devices are becoming the prevalent computing platform for most people. TouchDevelop is a new mobile development environment that enables anyone with a Windows Phone to create new apps directly on the smartphone, without a PC or a traditional keyboard. At the core is a new mobile programming language and editor that was designed with the touchscreen as the only input device in mind. Programs written in TouchDevelop can leverage all phone sensors such as GPS, cameras, accelerometer, gyroscope, and stored personal data such as contacts, songs, pictures. Thousands of programs have already been written and published with TouchDevelop.</p>",,,10.1145/2393596.2393641,,,cloud;evolution;mobile devices;touchscreen;type inference,,,,,3,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Inference and checking of context-sensitive pluggable types,A. Milanova; W. Huang,"Rensselaer Polytechnic Institute, Troy NY",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Pluggable types can help find bugs such as null-pointer dereference or unwanted mutation (or they can prove the absence of such bugs). Unfortunately, pluggable types require annotations, which imposes a burden on programmers.</p> <p>We propose a framework for specifying, <i>inferring</i> and checking of context-sensitive pluggable types. Programmers can use the framework to plug existing context-sensitive type systems (e.g., Immutability, Ownership) as well as to build new systems.</p>",,,10.1145/2393596.2393626,,,type inference,,,,,1,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
A case study of cross-system porting in forked projects,B. Ray; M. Kim,"The University of Texas at Austin, Austin, TX",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Software forking---creating a variant product by copying and modifying an existing product---is often considered an ad hoc, low cost alternative to principled product line development. To maintain such forked products, developers often need to port an existing feature or bug-fix from one product variant to another. As a first step towards assessing whether forking is a sustainable practice, we conduct an in-depth case study of 18 years of the BSD product family history. Our study finds that maintaining forked projects involves significant effort of porting patches from other projects. Cross-system porting happens periodically and the porting rate does not necessarily decrease over time. A significant portion of active developers participate in porting changes from peer projects. Surprisingly, ported changes are less defect-prone than non-ported changes. Our work is the first to comprehensively characterize the temporal, spatial, and developer dimensions of cross-system porting in the BSD family, and our tool Repertoire is the first automated tool for detecting ported edits with high accuracy of 94% precision and 84% recall. Our study finds that the upkeep work of porting changes from peer projects is significant and currently, porting practice seems to heavily depend on developers doing their porting job on time. This result calls for new techniques to automate cross-system porting to reduce the maintenance cost of forked projects.</p>",,,10.1145/2393596.2393659,,,code clones;forking;porting;repetitive changes;software evolution,,,,,12,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Scalable test data generation from multidimensional models,E. Torlak,U.C. Berkeley,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Multidimensional data models form the core of modern decision support software. The need for this kind of software is significant, and it continues to grow with the size and variety of datasets being collected today. Yet real multidimensional instances are often unavailable for testing and benchmarking, and existing data generators can only produce a limited class of such structures. In this paper, we present a new framework for scalable generation of test data from a rich class of multidimensional models. The framework provides a small, expressive language for specifying such models, and a novel solver for generating sample data from them. While the satisfiability problem for the language is NP-hard, we identify a polynomially solvable fragment that captures most practical modeling patterns. Given a model and, optionally, a statistical specification of the desired test dataset, the solver detects and instantiates a maximal subset of the model within this fragment, generating data that exhibits the desired statistical properties. We use our framework to generate a variety of high-quality test datasets from real industrial models, which cannot be correctly instantiated by existing data generators, or as effectively solved by general-purpose constraint solvers.</p>",,,10.1145/2393596.2393637,,,constraint solving;multidimensional models;specification;test data generation,,,,,1,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Dealing with uncertainty in early software architecture,N. Esfahani; K. Razavi; S. Malek,George Mason University,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Changing early architectural decisions of a system is both difficult and costly. It is very important for the architect to get them <i>""right""</i>. However, in early design, the architect is often forced to make these decisions under uncertainty, i.e., not knowing the precise impact of those decisions on system's properties (e.g., scalability) as well as stakeholder concerns (e.g., cost). In this paper, we provide an overview of <i>GuideArch</i>, a framework aimed at systematic exploration of the architectural solution space under uncertainty to help with making early architectural decisions.</p>",,,10.1145/2393596.2393621,,,decision making;software architecture;uncertainty,,,,,6,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Conditional model checking: a technique to pass information between verifiers,D. Beyer; T. A. Henzinger; M. E. Keremoglu; P. Wendler,"University of Passau, Germany",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Software model checking, as an undecidable problem, has three possible outcomes: (1) the program satisfies the specification, (2) the program does not satisfy the specification, and (3) the model checker fails. The third outcome usually manifests itself in a space-out, time-out, or one component of the verification tool giving up; in all of these failing cases, significant computation is performed by the verification tool before the failure, but no result is reported. We propose to reformulate the model-checking problem as follows, in order to have the verification tool report a summary of the performed work even in case of failure: given a program and a specification, the model checker returns a condition Ôå ---usually a state predicate--- such that the program satisfies the specification under the condition Ôå ---that is, as long as the program does not leave the states in which Ôå is satisfied. In our experiments, we investigated as one major application of conditional model checking the sequential combination of model checkers with information passing. We give the condition that one model checker produces, as input to a second conditional model checker, such that the verification problem for the second is restricted to the part of the state space that is not covered by the condition, i.e., the second model checker works on the problems that the first model checker could not solve. Our experiments demonstrate that repeated application of conditional model checkers, passing information from one model checker to the next, can significantly improve the verification results and performance, i.e., we can now verify programs that we could not verify before.</p>",,,10.1145/2393596.2393664,,,coverage;formal verification;model checking;program analysis;sequential combination;testing,,,,,3,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
AUSUM: approach for unsupervised bug report summarization,S. Mani; R. Catherine; V. S. Sinha; A. Dubey,IBM Research - India,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>In most software projects, resolved bugs are archived for future reference. These bug reports contain valuable information on the reported problem, investigation and resolution. When bug triaging, developers look for how similar problems were resolved in the past. Search over bug repository gives the developer a set of recommended bugs to look into. However, the developer still needs to manually peruse the contents of the recommended bugs which might vary in size from a couple of lines to thousands. Automatic summarization of bug reports is one way to reduce the amount of data a developer might need to go through. Prior work has presented learning based approaches for bug summarization. These approaches have the disadvantage of requiring large training set and being biased towards the data on which the model was learnt. In fact, maximum efficacy was reported when the model was trained and tested on bug reports from the same project. In this paper, we present the results of applying four unsupervised summarization techniques for bug summarization. Industrial bug reports typically contain a large amount of noise---email dump, chat transcripts, core-dump---useless sentences from the perspective of summarization. These derail the unsupervised approaches, which are optimized to work on more well-formed documents. We present an approach for noise reduction, which helps to improve the precision of summarization over the base technique (4% to 24% across subjects and base techniques). Importantly, by applying noise reduction, two of the unsupervised techniques became scalable for large sized bug reports.</p>",,,10.1145/2393596.2393607,,,bug report;summarization;unsupervised,,,,,14,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Who is going to mentor newcomers in open source projects?,G. Canfora; M. Di Penta; R. Oliveto; S. Panichella,"University of Sannio, Benevento, Italy",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>When newcomers join a software project, they need to be properly trained to understand the technical and organizational aspects of the project. Inadequate training could likely lead to project delay or failure.</p> <p>In this paper we propose an approach, named Yoda (<b>Y</b>oung and newc<b>O</b>mer <b>D</b>eveloper <b>A</b>ssistant) aimed at identifying and recommending mentors in software projects by mining data from mailing lists and versioning systems. Candidate mentors are identified among experienced developers who actively interact with newcomers. Then, when a newcomer joins the project, Yoda recommends her a mentor that, among the available ones, has already discussed topics relevant for the newcomer.</p> <p>Yoda has been evaluated on software repositories of five open source projects. We have also surveyed some developers of these projects to understand whether mentoring was actually performed in their projects, and asked them to evaluate the mentoring relations Yoda identified. Results indicate that top committers are not always the most appropriate mentors, and show the potential usefulness of Yoda as a recommendation system to aid project managers in supporting newcomers joining a software project.</p>",,,10.1145/2393596.2393647,,,developer mentoring;empirical studies;mining software repositories,,,,,21,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Seeing the forest and the trees: focusing team interaction on value and effort drivers,M. Book; S. Grapenthin; V. Gruhn,"University of Duisburg-Essen, Essen, Germany",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Large-scale information system development is often plagued by defects and deadline overruns that can be traced to insufficient communication within the project team, particularly between stakeholders from the business, technical and management side. Although agile process models put a strong emphasis on team communication, they provide only little support for focusing the communication on the most relevant issues. We therefore introduce the concept of so-called ""interaction rooms"", where teams work with a pragmatic combination of model sketches and annotations to foster understanding of the system and its business domain, to reveal risks and uncertainties, and discuss those system aspects that are most critical for project success.</p>",,,10.1145/2393596.2393630,,,communication;risks;teams;value-orientation,,,,,6,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
"Recalling the ""imprecision"" of cross-project defect prediction",F. Rahman; D. Posnett; P. Devanbu,"University of California Davis, Davis, CA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>There has been a great deal of interest in <i>defect prediction:</i> using prediction models trained on historical data to help focus quality-control resources in ongoing development. Since most new projects don't have historical data, there is interest in <i>cross-project prediction</i>: using data from one project to predict defects in another. Sadly, results in this area have largely been disheartening. Most experiments in cross-project defect prediction report poor performance, using the standard measures of <i>precision, recall and F-score</i>. We argue that these IR-based measures, while broadly applicable, are not as well suited for the quality-control settings in which defect prediction models are used. Specifically, these measures are taken at <i>specific threshold settings</i> (typically thresholds of the predicted probability of defectiveness returned by a logistic regression model). However, in practice, software quality control processes choose from a <i>range</i> of time-and-cost <i>vs</i> quality tradeoffs: how many files shall we test? how many shall we inspect? Thus, we argue that measures based on a variety of tradeoffs, <i>viz</i>., 5%, 10% or 20% of files tested/inspected would be more suitable. We study cross-project defect prediction from this perspective. <i>We find that cross-project prediction performance is no worse than within-project performance, and substantially better than random prediction!</i></p>",,,10.1145/2393596.2393669,,,empirical software engineering;fault prediction;inspection,,,,,27,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Asynchronous programs with prioritized task-buffers,M. Emmi; A. Lal; S. Qadeer,"LIAFA, Universit&#233; Paris Diderot, France",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>We consider the algorithmic analysis of asynchronous software systems as a means for building reliable software. A key challenge in designing such analyses is identifying a concurrency model which does not extraneously introduce behaviors infeasible in the actual system, does not extraneously exclude actual behaviors, and isolates the challenging features for analyses to focus on.</p> <p>Guided by real-world asynchronous software, we propose a concurrency model which enriches the existing serial task-buffer asynchrony model [29] with task-priorities and multiple task-buffers. Our model allows non-serial execution: tasks with higher priority preempt lower-priority tasks, and tasks drawn from distinct buffers freely interleave with one another. Modeling these features allows analysis algorithms to detect otherwise uncaught programming errors in asynchronous programs due to inter-buffer interleaving and task-interruption, while correctly ignoring false errors due to infeasible out-of-priority-order executions.</p> <p>Besides more precisely capturing real-world systems, our concurrency model inspires the design of a novel analysis algorithm. Given parameters <i>K</i><sub>1</sub>, <i>K</i><sub>2</sub> Ôµ N that restrict inter-buffer task interleaving and intra-buffer task reordering, we give a code-to-code translation to sequential programs, which can then be analyzed by off-the-shelf program analysis tools. For any given parameter values, the resulting sequential program encodes a subset of possible behaviors, and in the limit as both parameters approach infinity, the sequential program encodes all behaviors. We demonstrate the viability of our technique by experimenting with a prototype implementation. Our prototype is competitive with state-of-the-art concurrent program verification tools, and is able to correctly identify errors in simplified Windows device driver code, while ignoring infeasible executions.</p>",,,10.1145/2393596.2393652,,,asynchronous programs;concurrency;sequentialization;testing,,,,,2,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Semantic fault diagnosis: automatic natural-language fault descriptions,N. DiGiuseppe; J. A. Jones,"University of California, Irvine",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Before a fault can be fixed, it first must be understood. However, understanding <i>why</i> a system fails is often a difficult and time consuming process. While current automated-debugging techniques provide assistance in knowing <i>where</i> a fault is, developers are left unaided in understanding <i>what</i> a fault is, and <i>why</i> the system is failing. We present <i>Semantic Fault Diagnosis</i> (SFD), a technique that leverages lexicographic and dynamic information to automatically capture natural-language fault descriptors. SFD utilizes class names, method names, variable expressions, developer comments, and keywords from the source code to describe a fault. SFD can be used immediately after observing a failing execution and requires no input from developers or bug reports. In addition we present motivating examples and results from a SFD prototype to serve as a proof of concept.</p>",,,10.1145/2393596.2393623,,,maintenance;program comprehension;testing,,,,,2,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Practical change impact analysis based on static program slicing for industrial software systems,M. Acharya; B. Robinson,"ABB Corporate Research, Raleigh NC",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,2,"<p>Change impact analysis, i.e., knowing the potential consequences of a software change, is critical for the risk analysis, developer effort estimation, and regression testing of evolving software. Static program slicing is an attractive option for enabling routine change impact analysis for newly committed changesets during daily software build. However, static program slicing faces accuracy and scalability problems when applied routinely on large and evolving industrial software systems. In this paper, we present a tool called <i>Imp</i>, used within ABB, to address these problems. Imp transparently integrates with the version control and the daily build environments and is also available as a Visual Studio plugin for quick <i>what-if</i> analysis by developers for potential changes in software written in C/C++.</p>",,,10.1145/2393596.2393610,,,Imp;evolving software;routine change impact analysis;static program slicing;tool demonstration,,,,,,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Service selection for happy users: making user-intuitive quality abstractions,D. Athanasopoulos; A. V. Zarras; P. Vassiliadis,Univ. of Ioannina Greece,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>The state of the art service search engines allow the users to pick the services they need, based on the quality properties, offered by these services. To this end, the users should interact with the search engines based on the quality models that are imposed by the engines. This is a significant restriction towards making the service-oriented paradigm attractive to the general public. In this paper, we propose an approach that allows a user to specify his perception of quality in terms of a simple, user-defined quality model. The proposed approach automatically maps the user-defined quality model to the search engine's quality model. This mapping forms the basis for ordering, grouping and, in general manipulating, the results of the user's service discovery requests.</p>",,,10.1145/2393596.2393632,,,quality of service;service selection,,,,,1,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
A field study of refactoring challenges and benefits,M. Kim; T. Zimmermann; N. Nagappan,"The University of Texas at Austin, TX",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of <i>semantics-preserving code transformations</i> and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds that the binary modules refactored by this team experienced significant reduction in the number of inter-module dependencies and post-release defects, indicating a visible benefit of refactoring.</p>",,,10.1145/2393596.2393655,,,churn;component dependencies;defects;empirical study;refactoring;software evolution,,,,,27,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Sando: an extensible local code search framework,D. Shepherd; K. Damevski; B. Ropski; T. Fritz,"ABB, Inc., Raleigh, NC",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,2,"<p>Developers heavily rely on Local Code Search (LCS)---the execution of a text-based search on a single code base---to find starting points in software maintenance tasks. While LCS approaches commonly used by developers are based on lexical matching and often result in failed searches or irrelevant results, developers have not yet migrated to the various research approaches that have made significant advancements in LCS. We hypothesize that two of the major reasons for this lack of migration are as follows. First, developers do not know which approach is the best, due to a lack of comparative field studies and the discrepancies in the underlying LCS process that these research approaches address. Second, developers lack access to a stable implementation of most of the research approaches. To address these issues, we studied a number of LCS approaches, distilled the general component structure underlying these approaches and, based on this structure, developed a LCS tool and framework, called Sando. Currently used by developers at ABB, Inc. and elsewhere, Sando also supports the flexible extension of its components to rapidly disseminate research advancements, and allows for user-based evaluation of competing approaches.</p>",,,10.1145/2393596.2393612,,,code search;feature location,,,,,17,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Testing mined specifications,M. Gabel; Z. Su,The University of Texas at Dallas,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Specifications are necessary for nearly every software engineering task, but they are often missing or incomplete. ""Specification mining"" is a line of research promising to solve this problem through automated tools that infer specifications directly from existing programs. The standard practice is one of inductive learning: mining tools make observations about software and inductively generalize them into specifications. Inductive reasoning is unsound, however, and existing tools commonly grapple with the problem of inferring ""false"" specifications, which must be manually checked.</p> <p>In this work, we introduce a new technique for automatically validating mined specifications that lessens this manual burden. Our technique is not based on heuristics; it rather uses a general, semantic definition of a ""true"" specification. We perform systematic, targeted program transformations to test a mined specification's <i>necessity for overall correctness</i>. If a ""violating"" program is correct, the specification is false. We have implemented our technique in a prototype tool that validates temporal properties of Java programs, and we demonstrate it to be effective through a large-scale case study on the DaCapo benchmarks.</p>",,,10.1145/2393596.2393598,,,reverse engineering;software tools;specification inference,,,,,5,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
SelfMotion: a declarative language for adaptive service-oriented mobile apps,G. Cugola; C. Ghezzi; L. S. Pinto; G. Tamburrelli,"DEEPSE Group @ DEI, Politecnico di Milano, Italy",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>In this demo we present SelfMotion: a declarative language and a run-time system conceived to support the development of adaptive, mobile applications, built as compositions of ad-hoc components, existing services and third party applications. The advantages of the approach and the adaptive capabilities of SelfMotion are demonstrated in the demo by designing and executing a mobile application inspired by an existing, worldwide distributed, mobile application.</p>",,,10.1145/2393596.2393602,,,declarative orchestration language;mobile applications;self-adaptive systems,,,,,1,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
FaultTracer: a change impact and regression fault analysis tool for evolving Java programs,L. Zhang; M. Kim; S. Khurshid,"The University of Texas at Austin, Austin TX",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Keeping evolving software fault-free is hard. In our previous work, we proposed FaultTracer, a change impact and regression fault analysis tool for evolving programs. It takes the old and new versions of a program and a regression test suite as inputs, and then identifies affected tests---a subset of tests relevant to the program differences between the two versions and affecting changes---a subset of atomic changes relevant to each affected test. It adapts spectrum-based fault localization techniques and applies them in tandem with an enhanced change impact analysis to identify and rank failure-inducing program edits. We have shown that FaultTracer, compared to existing techniques (e.g., Chianti), achieves improvement in selecting influenced tests, determining suspicious failure-inducing edits, and ranking failure-inducing program edits. In this paper, we show the design, implementation, and demonstration of our FaultTracer approach as a publicly available toolkit for testing and debugging Java programs.</p>",,,10.1145/2393596.2393642,,,fault localization;regression testing;software evolution,,,,,4,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Scalable malware clustering through coarse-grained behavior modeling,M. Chandramohan; H. B. K. Tan; L. K. Shar,"Nanyang Technological University, Singapore",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Anti-malware vendors receive several thousand new malware (malicious software) variants per day. Due to large volume of malware samples, it has become extremely important to group them based on their malicious characteristics. Grouping of malware variants that exhibit similar behavior helps to generate malware signatures more efficiently. Unfortunately, exponential growth of new malware variants and huge-dimensional feature space, as used in existing approaches, make the clustering task very challenging and difficult to scale. Furthermore, malware behavior modeling techniques proposed in the literature do not scale well, where malware feature space grows in proportion with the number of samples under examination.</p> <p>In this paper, we propose a scalable malware behavior modeling technique that models the interactions between malware and sensitive system resources in a coarse-grained manner. Coarse-grained behavior modeling enables us to generate malware feature space that does not grow in proportion with the number of samples under examination. A preliminary study shows that our approach generates 289 times less malware features and yet improves the average clustering accuracy by 6.20% comparing to a state-of-the-art malware clustering technique.</p>",,,10.1145/2393596.2393627,,,coarse-grained behavior modeling;malware behavior modeling;malware clustering,,,,,1,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
UMLtoCSP (UOST): a tool for efficient verification of UML/OCL class diagrams through model slicing,A. Shaikh; U. K. Wiil,University of Southern Denmark (Denmark) and International Islamic University (Malaysia),Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Model errors are a major concern in the paradigm of Model-Driven Development (MDD) because of model transformations and code generation. It is important to detect model errors before transformation as in the later stages it is harder to trace and fix such errors. Formal verification tools and techniques can check the correctness of a model, but their high computational complexity can limit their scalability. In this research, we present a tool named UMLtoCSP (UOST) that uses a UML/OCL Slicing Technique (UOST) to verify complex UML/OCL class diagram. The tool accepts UML class diagrams annotated with OCL constraints as input, breaks the original model <i>m</i> into <i>m</i><sub>1</sub>, <i>m</i><sub>2</sub>, <i>m</i><sub>3</sub>,...,<i>m</i><sub><i>n</i></sub> sub-models while abstracting unnecessary model elements.</p>",,,10.1145/2393596.2393639,,,UML/OCL model verification;complex model verification;verification through slicing,,,,,,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
"Green: reducing, reusing and recycling constraints in program analysis",W. Visser; J. Geldenhuys; M. B. Dwyer,"Stellenbosch University, Stellenbosch, South Africa",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>The analysis of constraints plays an important role in many aspects of software engineering, for example constraint satisfiability checking is central to symbolic execution. However, the norm is to recompute results in each analysis. We propose a different approach where every call to the solver is wrapped in a check to see if the result is not already available. While many tools use some form of results caching, the novelty of our approach is the persistence of results across runs, across programs being analyzed, across different analyses and even across physical location. Achieving such reuse requires that constraints be distilled into their essential parts and represented in a canonical form.</p> <p>In this paper, we describe the key ideas of our approach and its implementation, the Green solver interface, which reduces constraints to a simple form, allows for reuse of constraint solutions within an analysis run, and allows for recycling constraint solutions produced in one analysis run for use in other analysis runs. We describe how we integrated Green into two existing symbolic execution tools and demonstrate the reuse we achieve in the different settings.</p>",,,10.1145/2393596.2393665,,,NoSQL;constraint solving;path feasibility;symbolic execution,,,,,14,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Assessing the value of branches with what-if analysis,C. Bird; T. Zimmermann,"Microsoft Research, Redmond, WA",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Branches within source code management systems (SCMs) allow a software project to divide work among its teams for concurrent development by isolating changes. However, this benefit comes with several costs: increased time required for changes to move through the system and pain and error potential when integrating changes across branches. In this paper, we present the results of a survey to characterize how developers use branches in a large industrial project and common problems that they face. One of the major problems mentioned was the long delay that it takes changes to move from one team to another, which is often caused by having too many branches (branchmania). To monitor branch health, we introduce a novel what-if analysis to assess alternative branch structures with respect to two properties, isolation and liveness. We demonstrate with several scenarios how our what-if analysis can support branch decisions. By removing high-cost-low-benefit branches in Windows based on our what-if analysis, changes would each have saved 8.9 days of delay and only introduced 0.04 additional conflicts on average.</p>",,,10.1145/2393596.2393648,,,branch refactoring;branches;concurrent development;coordination;teams;what-if analysis,,,,,20,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Toward measuring program comprehension with functional magnetic resonance imaging,J. Siegmund; A. Brechmann; S. Apel; C. K’_stner; J. Liebig; T. Leich; G. Saake,"University of Magdeburg, Germany",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Program comprehension is an often evaluated, internal cognitive process. In neuroscience, <i>functional magnetic resonance imaging (fMRI)</i> is used to visualize such internal cognitive processes. We propose an experimental design to measure program comprehension based on fMRI. In the long run, we hope to answer questions like <i>What distinguishes good programmers from bad programmers?</i> or <i>What makes a good programmer?</i></p>",,,10.1145/2393596.2393624,,,controlled experiments;fMRI;program comprehension,,,,,2,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Understanding myths and realities of test-suite evolution,L. S. Pinto; S. Sinha; A. Orso,Politecnico di Milano,Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Test suites, once created, rarely remain static. Just like the application they are testing, they evolve throughout their lifetime. Test obsolescence is probably the most known reason for test-suite evolution---test cases cease to work because of changes in the code and must be suitably repaired. Repairing existing test cases manually, however, can be extremely time consuming, especially for large test suites, which has motivated the recent development of automated test-repair techniques. We believe that, for developing effective repair techniques that are applicable in real-world scenarios, a fundamental prerequisite is a thorough understanding of how test cases evolve in practice. Without such knowledge, we risk to develop techniques that may work well for only a small number of tests or, worse, that may not work at all in most realistic cases. Unfortunately, to date there are no studies in the literature that investigate how test suites evolve. To tackle this problem, in this paper we present a technique for studying test-suite evolution, a tool that implements the technique, and an extensive empirical study in which we used our technique to study many versions of six real-world programs and their unit test suites. This is the first study of this kind, and our results reveal several interesting aspects of test-suite evolution. In particular, our findings show that test repair is just one possible reason for test-suite evolution, whereas most changes involve refactorings, deletions, and additions of test cases. Our results also show that test modifications tend to involve complex, and hard-to-automate, changes to test cases, and that existing test-repair techniques that focus exclusively on assertions may have limited practical applicability. More generally, our findings provide initial insight on how test cases are added, removed, and modified in practice, and can guide future research efforts in the area of test-suite evolution.</p>",,,10.1145/2393596.2393634,,,test-suite evolution;test-suite maintenance;unit testing,,,,,16,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
How do software engineers understand code changes?: an exploratory study in industry,Y. Tao; Y. Dang; T. Xie; D. Zhang; S. Kim,"The Hong Kong University of Science and Technology, Hong Kong, China",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Software evolves with continuous source-code changes. These code changes usually need to be understood by software engineers when performing their daily development and maintenance tasks. However, despite its high importance, such change-understanding practice has not been systematically studied. Such lack of empirical knowledge hinders attempts to evaluate this fundamental practice and improve the corresponding tool support.</p> <p>To address this issue, in this paper, we present a large-scale quantitative and qualitative study at Microsoft. The study investigates the role of understanding code changes during software-development process, explores engineers' information needs for understanding changes and their requirements for the corresponding tool support. The study results reinforce our beliefs that understanding code changes is an indispensable task performed by engineers in software-development process. A number of insufficiencies in the current practice also emerge from the study results. For example, it is difficult to acquire important information needs such as a change's completeness, consistency, and especially the risk imposed by it on other software components. In addition, for understanding a composite change, it is valuable to decompose it into sub-changes that are aligned with individual development issues; however, currently such decomposition lacks tool support.</p>",,,10.1145/2393596.2393656,,,code change;code review;information needs;tool support,,,,,18,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Detecting and analyzing insecure component usage,T. Kwon; Z. Su,"University of California, Davis",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Software is commonly built from reusable components that provide desired functionalities. Although component reuse significantly improves software productivity, <i>insecure component usage</i> can lead to security vulnerabilities in client applications. For example, we noticed that widely-used IE-based browsers, such as IE Tab, do not enable important security features that IE enables by default, even though they all use the same browser components. This insecure usage renders these IE-based browsers vulnerable to the attacks blocked by IE. To our knowledge, this important security aspect of component reuse has largely been unexplored.</p> <p>This paper presents the first practical framework for <i>detecting and analyzing vulnerabilities of insecure component usage</i>. Its goal is to enforce and support secure component reuse. Our core approach is based on differential testing and works as follows. Suppose that component <i>C</i> maintains a security policy configuration to block certain malicious behavior. If two clients of component <i>C</i>, say a reference and a test subject, handle the malicious behavior inconsistently, the test subject uses <i>C</i> insecurely. In particular, we model component usage related to a policy based on 1) accesses to the configuration state inside the component and 2) the conditional jumps affected by the data read from the state. We utilize this model to detect inconsistent policy evaluations, which can lead to insecure component usage. We have implemented our technique for Windows applications and used it to detect and analyze insecure usage of popular software components. Our evaluation results show that 1) insecure component usage is a general concern and frequently occurs in widely-used software, and 2) our detection framework is practical and effective at detecting and analyzing insecure component usage. In particular, it detected several serious, new vulnerabilities and helped perform detailed analysis of insecure compone- t usage. We have reported these to the affected software vendors, some of whom have already acknowledged our findings and are actively addressing them.</p>",,,10.1145/2393596.2393599,,,differential testing;insecure component usage;testing and analysis of real-world software,,,,,,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
ReImInfer: method purity inference for Java,W. Huang; A. Milanova,"Rensselaer Polytechnic Institute, Troy NY",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Method purity inference, also known as side-effect analysis, is an important problem. It has many applications including compiler optimization, model checking, memoization of function calls, atomicity, etc. Surprisingly, despite the long history of this problem, we know of no purity inference tool that scales to large codes and analyzes both whole programs and libraries.</p> <p>We build a purity inference tool called ReImInfer on top of a type inference and checking framework. ReImInfer infers method purity for Java. It is modular and compositional, produces precise results and scales to large programs.</p>",,,10.1145/2393596.2393640,,,method purity;type inference,,,,,2,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Toward semantic search via SMT solver,K. T. Stolee; S. Elbaum,"University of Nebraska -- Lincoln, Lincoln, NE",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,4,"<p>Searching for code is a common task among programmers, with the ultimate goal of reuse. While the process of searching for code -- issuing a query and selecting a relevant match -- is straightforward, several costs must be balanced, including the costs of specifying the query, examining the results to find desired code, and <i>not</i> finding a relevant result. For syntactic searches the query cost is quite low, but the results are often irrelevant, so the examination cost is high and matches may be missed. Semantic searches may return more relevant results, but current techniques that involve writing complex specifications or executing code against test cases are costly to the developer. We propose an approach for semantic search in which developers specify lightweight specifications and an SMT solver identifies matching programs from a repository. A program repository is automatically encoded offline so the search is efficient. Programs are encoded at various abstraction levels to enable partial matches when no, or few, exact matches exist. We instantiate this approach on a subset of the Yahoo! Pipes mashup language. Preliminary results show promise for the feasibility of the approach.</p>",,,10.1145/2393596.2393625,,,SMT solvers;lightweight specifications;semantic code search,,,,,6,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Test input generation using dynamic programming,R. Nokhbeh Zaeem; S. Khurshid,"The University of Texas at Austin, Austin, TX",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Constraint-based input generation is an effective technique for testing programs, such as compilers and web browsers, which have complex inputs. However, efficient generation of such inputs remains a challenging problem. We present a novel input generation technique that takes constraints written as recursive predicates in the underlying programming language and uses <i>dynamic programming</i> to solve the constraints efficiently. Our key insight is to leverage the recursive structure of desired inputs and partition the problem of generating an input into several sub-problems of generating smaller inputs that exhibit the same structure, and then to use dynamic programming -- a well-known problem solving methodology designed to exploit common sub-problems -- to combine them. A lazy initialization strategy and symbolic execution optimize our basic technique. Our technique provides not only bounded exhaustive input generation but also enables random input generation. We show the correctness of our technique. Furthermore, we present an experimental evaluation, which shows that our technique can provide over an order of magnitude performance improvement for input generation compared to Korat (an efficient solver for structural constraints) and Pex (a state-of-the-art tool for symbolic execution). Finally, we use our technique to effectively find bugs in production versions of Google Chrome and Apple Safari web browsers.</p>",,,10.1145/2393596.2393635,,,dynamic programming;lazy initialization;recursive test generation;symbolic execution,,,,,2,1,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
Variability points and design pattern usage in architectural tactics,M. Mirakhorli; P. M’_der; J. Cleland-Huang,"DePaul University, Chicago, IL",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Architectural tactics are important building blocks of software architecture. Tactics come in many shapes and sizes, describe solutions for addressing specific quality concerns, and are prevalent across high-performance fault-tolerant systems. Once a decision is made to utilize a tactic, the developer must generate a concrete plan for realizing the tactic in the design and code. Unfortunately, the variability points found in individual tactics can make this a challenging task. To address this knowledge gap, we conducted a study to investigate how design patterns were used to implement various tactics. Data mining techniques were used to identify potential pattern instances within tactic implementations. Our manual analysis of the retrieved data identified a distinct set of variability points for each tactic, as well as corresponding design patterns used to address them. From these observations we construct tactic-level decision trees depicting variability points of a tactic and generate a reference model which provides implementation guidance.</p>",,,10.1145/2393596.2393657,,,architecture;design patterns;tactics,,,,,8,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
CarFast: achieving higher statement coverage faster,S. Park; B. M. M. Hossain; I. Hussain; C. Csallner; M. Grechanik; K. Taneja; C. Fu; Q. Xie,"Georgia Institute of Technology, Atlanta, Georgia",Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering,20160129,2012,,,1,11,"<p>Test coverage is an important metric of software quality, since it indicates thoroughness of testing. In industry, test coverage is often measured as statement coverage. A fundamental problem of software testing is how to achieve <i>higher</i> statement coverage <i>faster</i>, and it is a difficult problem since it requires testers to cleverly find input data that can steer execution sooner toward sections of application code that contain more statements.</p> <p>We created a novel fully automatic approach for <i>aChieving higher stAtement coveRage FASTer (CarFast)</i>, which we implemented and evaluated on twelve generated Java applications whose sizes range from 300 LOC to one million LOC. We compared CarFast with several popular test case generation techniques, including pure random, adaptive random, and Directed Automated Random Testing (DART). Our results indicate with strong statistical significance that when execution time is measured in terms of the number of runs of the application on different input test data, CarFast outperforms the evaluated competitive approaches on most subject applications.</p>",,,10.1145/2393596.2393636,,,experimentation;statement coverage;testing,,,,,8,,,,,,11-16 Nov. 2012,,ACM,ACM Conferences
A logical revolution (keynote),M. Y. Vardi,"Rice University, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,1,1,"<p> Mathematical logic was developed in an effort to provide formal foundations for mathematics. In this quest, which ultimately failed, logic begat computer science, yielding both computers and theoretical computer science. But then logic turned out to be a disappointment as foundations for computer science, as almost all decision problems in logic are either undecidable or intractable. Starting from the mid 1970s, however, there has been a quiet revolution in logic in computer science, and problems that are theoretically undecidable or intractable were shown to be quite feasible in practice. This talk describes the rise, fall, and rise of logic in computer science, describing several modern applications of logic to computing, include databases, hardware design, and software engineering. </p>",,,10.1145/2491411.2505427,,,Logic,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Towards open architecture system,B. Vogel,"Linnaeus University, Sweden",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,731,734,"<p> The use of diverse standards while developing web and mobile technologies brings new challenges when it comes to flexibility, interoperability, customizability and extensibility of the software systems. In addition, such systems in most of the cases are closed, thus make the development and customization process for system designers, developers and end-users a challenging effort. All these developments require further research attention. This work addresses these challenges from open system architecture perspective. The proposed approach is based on practical development efforts, and theoretical research including state of the art projects and definitions related to open architectures that we surveyed. The initial results indicate that a combination of service-oriented approaches with open source components and open standard data formats pave the way towards an open, extensible architecture. The core contribution of this research will be (a) an open architecture model and (b) the developed system itself based on the model, and (c) the benefits of applying open architecture approaches throughout the development processes. </p>",,,10.1145/2491411.2492407,,,Open architecture;customizability;evolvability;extensibility;flexibility;model;validation;web and mobile software,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Automotive architecture description and its quality,Y. Dajsuren,"Eindhoven University of Technology, Netherlands",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,727,730,"<p> This research is part of the Hybrid Innovations for Trucks (HIT), an ongoing multi-disciplinary project with the objectives of CO$_2$ emission reduction and fuel saving for long haul vehicles. Achieving this goal necessitates definition of a proper architecture and quality techniques to enable the development of a new and more efficient control software. Therefore, this research covers automotive architecture description language and quality of automotive software. </p>",,,10.1145/2491411.2492403,,,architectural quality;architecture description language;automotive architecture;quality metrics,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Synthesis of component and connector models from crosscutting structural views,,,Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,444,454,"<p> We present component and connector (C&C) views, which specify structural properties of component and connector models in an expressive and intuitive way. C&C views provide means to abstract away direct hierarchy, direct connectivity, port names and types, and thus can crosscut the traditional boundaries of the implementation-oriented hierarchical decomposition of systems and sub-systems, and reflect the partial knowledge available to different stakeholders involved in a system's design. As a primary application for C&C views we investigate the synthesis problem: given a C&C views specification, consisting of mandatory, alternative, and negative views, construct a concrete satisfying C&C model, if one exists. We show that the problem is NP-hard and solve it, in a bounded scope, using a reduction to SAT, via Alloy. We further extend the basic problem with support for library components, specification patterns, and architectural styles. The result of synthesis can be used for further exploration, simulation, and refinement of the C&C model or, as the complete, final model itself, for direct code generation. A prototype tool and an evaluation over four example systems with multiple specifications show promising results and suggest interesting future research directions towards a comprehensive development environment for the structure of component and connector designs. </p>",,,10.1145/2491411.2491414,,,component and connector models;synthesis,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Z3-str: a z3-based string solver for web application analysis,Y. Zheng; X. Zhang; V. Ganesh,"Purdue University, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,114,124,"<p> Analyzing web applications requires reasoning about strings and non-strings cohesively. Existing string solvers either ignore non-string program behavior or support limited set of string operations. In this paper, we develop a general purpose string solver, called Z3-str, as an extension of the Z3 SMT solver through its plug-in interface. Z3-str treats strings as a primitive type, thus avoiding the inherent limitations observed in many existing solvers that encode strings in terms of other primitives. The logic of the plug-in has three sorts, namely, bool, int and string. The string-sorted terms include string constants and variables of arbitrary length, with functions such as concatenation, sub-string, and replace. The int-sorted terms are standard, with the exception of the length function over string terms. The atomic formulas are equations over string terms, and (in)-equalities over integer terms. Not only does our solver have features that enable whole program symbolic, static and dynamic analysis, but also it performs better than other solvers in our experiments. The application of Z3-str in remote code execution detection shows that its support of a wide spectrum of string operations is key to reducing false positives. </p>",,,10.1145/2491411.2491456,,,String Analysis;String Constraint Solver;Web Application,,,,,15,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Inferring project-specific bug patterns for detecting sibling bugs,G. Liang; Q. Wang; T. Xie; H. Mei,"Peking University, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,565,575,"<p> Lightweight static bug-detection tools such as FindBugs, PMD, Jlint, and Lint4j detect bugs with the knowledge of generic bug patterns (e.g., objects of java.io.InputStream are not closed in time after used). Besides generic bug patterns, different projects under analysis may have some project-specific bug patterns. For example, in a revision of the Xerces project, the class field ""fDTDHandler"" is dereferenced without proper null-checks, while it could actually be null at runtime. We name such bug patterns directly related to objects instantiated in specific projects as Project-Specific Bug Patterns (PSBPs). Due to lack of such PSBP knowledge, existing tools usually fail in effectively detecting most of this kind of bugs. We name bugs belonging to the same project and sharing the same PSBP as sibling bugs. If some sibling bugs are fixed in a fix revision but some others remain, we treat such fix as an incomplete fix. To address such incomplete fixes, we propose a PSBP-based approach for detecting sibling bugs and implement a tool called Sibling-Bug Detector (SBD). Given a fix revision, SBD first infers the PSBPs implied by the fix revision. Then, based on the inferred PSBPs, SBD detects their related sibling bugs in the same project. To evaluate SBD, we apply it to seven popular open-source projects. Among the 108 warnings reported by SBD, 63 of them have been confirmed as real bugs by the project developers, while two existing popular static detectors (FindBugs and PMD) cannot report most of them. </p>",,,10.1145/2491411.2491422,,,Project-specific bug patterns;incomplete fixes;sibling-bug detection,,,,,2,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Jalangi: a selective record-replay and dynamic analysis framework for JavaScript,K. Sen; S. Kalasapur; T. Brutch; S. Gibbs,"UC Berkeley, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,488,498,"<p> JavaScript is widely used for writing client-side web applications and is getting increasingly popular for writing mobile applications. However, unlike C, C++, and Java, there are not that many tools available for analysis and testing of JavaScript applications. In this paper, we present a simple yet powerful framework, called Jalangi, for writing heavy-weight dynamic analyses. Our framework incorporates two key techniques: 1) selective record-replay, a technique which enables to record and to faithfully replay a user-selected part of the program, and 2) shadow values and shadow execution, which enables easy implementation of heavy-weight dynamic analyses. Our implementation makes no special assumption about JavaScript, which makes it applicable to real-world JavaScript programs running on multiple platforms. We have implemented concolic testing, an analysis to track origins of nulls and undefined, a simple form of taint analysis, an analysis to detect likely type inconsistencies, and an object allocation profiler in Jalangi. Our evaluation of Jalangi on the SunSpider benchmark suite and on five web applications shows that Jalangi has an average slowdown of 26X during recording and 30X slowdown during replay and analysis. The slowdowns are comparable with slowdowns reported for similar tools, such as PIN and Valgrind for x86 binaries. We believe that the techniques proposed in this paper are applicable to other dynamic languages. </p>",,,10.1145/2491411.2491447,,,Concolic Testing;Dynamic Analysis;JavaScript;Record and Repla,,,,,27,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Using topic models to understand the evolution of a software ecosystem,N. Lopez,"UC Irvine, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,723,726,"<p> The development of a software system is now ever more frequently a part of a larger development effort, including multiple software systems that co-exist in the same environment: a software ecosystem. Though most studies of the evolution of software have focused on a single software system, there is much that we can learn from the analysis of a set of interrelated systems. Topic modeling techniques show promise for mining the data stored in software repositories to understand the evolution of a system. In my research I seek to explore how topic modeling techniques can aid in understanding the evolution of a software ecosystem. The results of this research have the potential to improve how topic modeling techniques are used to predict, plan, and understand the evolution of software, and will inform the design of tools that support software engineering activities such as feature location, expertise identification, and bug detection. </p>",,,10.1145/2491411.2492402,,,Latent Dirichlet Allocation;Mining software repositories;software ecosys-tems;software evolution;topic modeling,,,,,2,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Incrementally synthesizing controllers from scenario-based product line specifications,J. Greenyer; C. Brenner; M. Cordy; P. Heymans; E. Gressi,"Leibniz Universit&#228;t Hannover, Germany",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,433,443,"<p> Many software-intensive systems consist of components that interact to fulfill complex functionality. Moreover, often many variants of such systems have to be designed at once. This adds complexity to the design task. Recently, we proposed a scenario-based approach to design product lines, which combines feature diagrams and Modal Sequence Diagrams. We proposed a consistency-checking technique based on a dedicated product line model checker. One limitation of this technique is that it is incomplete, i.e., it may fail to show the consistency of some consistent specifications. In this paper we propose a new game-based approach that overcomes this incompleteness and, in addition, automatically synthesizes controllers for the consistent product specifications. We exploit the fact that many variants are similar and efficiently synthesize product controllers incrementally. We provide a prototype tool and evaluate the efficiency of the approach. </p>",,,10.1145/2491411.2491445,,,Controller synthesis;Features;Message sequence diagrams,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Second-order constraints in dynamic invariant inference,K. Li; C. Reichenbach; Y. Smaragdakis; M. Young,"University of Massachusetts at Amherst, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,103,113,"<p> The current generation of dynamic invariant detectors often produce invariants that are inconsistent with program semantics or programmer knowledge. We improve the consistency of dynamically discovered invariants by taking into account higher-level constraints. These constraints encode knowledge about invariants, even when the invariants themselves are unknown. For instance, even though the invariants describing the behavior of two functions f1 and f2 may be unknown, we may know that any valid input for f1 is also valid for f2, i.e., the precondition of f1 implies that of f2. We explore techniques for expressing and employing such consistency constraints to improve the quality of produced invariants. We further introduce techniques for dynamically discovering potential second-order constraints that the programmer can subsequently approve or reject. </p> <p> Our implementation builds on the Daikon tool, with a vocabulary of constraints that the programmer can use to enhance and constrain DaikonŠ—Ès inference. We show that dynamic inference of second-order constraints together with minimal human effort can significantly influence the produced (first-order) invariants even in systems of substantial size, such as the Apache Commons Collections and the AspectJ compiler. We also find that 99% of the dynamically inferred second-order constraints we sampled are true. </p>",,,10.1145/2491411.2491457,,,Daikon;dynamic invariant inference;second-order constraints,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Using fault history to improve mutation reduction,L. Inozemtseva; H. Hemmati; R. Holmes,"University of Waterloo, Canada",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,639,642,"<p> Mutation testing can be used to measure test suite quality in two ways: by treating the kill score as a quality metric, or by treating each surviving, non-equivalent mutant as an indicator of an inadequacy in the test suite. The first technique relies on the assumption that the mutation score is highly correlated with the suite's real fault detection rate, which is not well supported by the literature. The second technique relies only on the weaker assumption that the ""interesting"" mutants (i.e., the ones that indicate an inadequacy in the suite) are in the set of surviving mutants. Using the second technique also makes improving the suite straightforward. </p> <p> Unfortunately, mutation testing has a performance problem. At least part of the test suite must be run on every mutant, meaning mutation testing can be too slow for practical use. Previous work has addressed this by reducing the number of mutants to evaluate in various ways, including selecting a random subset of them. However, reducing the set of mutants by random reduction is suboptimal for developers using the second technique described above, since random reduction will eliminate many of the interesting mutants. </p> <p> We propose a new reduction method that supports the use of the second technique by reducing the set of mutants to those generated by altering files that have contained many faults in the past. We performed a pilot study that suggests that this reduction method preferentially chooses mutants that will survive mutation testing; that is, it preserves a greater number of interesting mutants than random reduction does. </p>",,,10.1145/2491411.2494586,,,Mutation testing;fault history;mutant reduction;test suite quality,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Scalable and incremental software bug detection,S. McPeak; C. H. Gros; M. K. Ramanathan,"Coverity, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,554,564,"<p> An important, but often neglected, goal of static analysis for detecting bugs is the ability to show defects to the programmer quickly. Unfortunately, existing static analysis tools scale very poorly, or are shallow and cannot find complex interprocedural defects. Previous attempts at reducing the analysis time by adding more resources (CPU, memory) or by splitting the analysis into multiple sub-analyses based on defect detection capabilities resulted in limited/negligible improvements. </p> <p> We present a technique for parallel and incremental static analysis using top-down, bottom-up and global specification inference based around the concept of a work unit, a self-contained atom of analysis input, that deterministically maps to its output. A work unit contains both abstract and concrete syntax to analyze, a supporting fragment of the class hierarchy, summarized interprocedural behavior, and defect reporting information, factored to ensure a high level of reuse when analyzing successive versions incrementally. Work units are created and consumed by an analysis master process that coordinates the multiple analysis passes, the flow of information among them, and incrementalizes the computation. Meanwhile, multiple analysis worker processes use abstract interpretation to compute work unit results. Process management and interprocess communication is done by a general-purpose computation distributor layer. </p> <p> We have implemented our approach and our experimental results show that using eight processor cores, we can perform complete analysis of code bases with millions of lines of code in a few hours, and even faster after incremental changes to that code. The analysis is thorough and accurate: it usually reports about one crash-causing defect per thousand lines of code, with a false positive rate of 10--20% </p>",,,10.1145/2491411.2501854,,,Bug Detection;Incremental;Parallel;Static Analysis,,,,,2,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Good technology makes the difficult task easy,A. Terekhov,"Saint-Petersburg State University, Russia",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,683,686,<p> A new language for chip design is presented. The main advantages of the language are explicit conveyer and parallel features fully controlled by the author of chip design. Non trivial industrial example is under discussion. There are run-time estimations and comparison with traditional programming in C. </p>,,,10.1145/2491411.2494571,,,CoDesign;Hardware design;VHDL;Verilog;pipeline,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
API change and fault proneness: a threat to the success of Android apps,M. Linares-V’squez; G. Bavota; C. Bernal-C’rdenas; M. Di Penta; R. Oliveto; D. Poshyvanyk,"College of William and Mary, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,477,487,"<p> During the recent years, the market of mobile software applications (apps) has maintained an impressive upward trajectory. Many small and large software development companies invest considerable resources to target available opportunities. As of today, the markets for such devices feature over 850K+ apps for Android and 900K+ for iOS. Availability, cost, functionality, and usability are just some factors that determine the success or lack of success for a given app. Among the other factors, reliability is an important criteria: users easily get frustrated by repeated failures, crashes, and other bugs; hence, abandoning some apps in favor of others. </p> <p> This paper reports a study analyzing how the fault- and change-proneness of APIs used by 7,097 (free) Android apps relates to applications' lack of success, estimated from user ratings. Results of this study provide important insights into a crucial issue: making heavy use of fault- and change-prone APIs can negatively impact the success of these apps. </p>",,,10.1145/2491411.2491428,,,API changes;Android;Empirical Studies;Mining Software Repositories,,,,,38,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Lexical statistical machine translation for language migration,A. T. Nguyen; T. T. Nguyen; T. N. Nguyen,"Iowa State University, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,651,654,"<p> Prior research has shown that source code also exhibits naturalness, i.e. it is written by humans and is likely to be repetitive. The researchers also showed that the n-gram language model is useful in predicting the next token in a source file given a large corpus of existing source code. In this paper, we investigate how well statistical machine translation (SMT) models for natural languages could help in migrating source code from one programming language to another. We treat source code as a sequence of lexical tokens and apply a phrase-based SMT model on the lexemes of those tokens. Our empirical evaluation on migrating two Java projects into C# showed that lexical, phrase-based SMT could achieve high lexical translation accuracy (BLEU from 81.3-82.6%). Users would have to manually edit only 11.9-15.8% of the total number of tokens in the resulting code to correct it. However, a high percentage of total translation methods (49.5-58.6%) is syntactically incorrect. Therefore, our result calls for a more program-oriented SMT model that is capable of better integrating the syntactic and semantic information of a program to support language migration. </p>",,,10.1145/2491411.2494584,,,Language Migration;Statistical Machine Translation,,,,,6,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
PHRT: a model and programmable tool for hardware reengineering automation,O. Nenashev,"Saint Petersburg State Polytechnical University, Russia",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,719,722,"<p> Hardware reengineering is a highly resource-consuming process of development cycle, so it is important to automate reengineering in order to reduce costs and provide reusable solutions. There are many specialized electronic design automation (EDA) tools for specific cases, but only few programmable tools supporting implementation of user-specific reengineering operations. </p> <p> This paper presents PhD research, which aims development of such Programmable Hardware Reengineering Tool (PHRT), which can be useful for small hardware-design companies and research groups, who have specific recurrent tasks and cannot afford development of automation tools Š—“from scratchŠ—. </p> <p> We propose HDL-independent Š—“hybridŠ— device representation model for automated analysis and transformation, which combines low-level structural descriptions (netlists) with features from high-level hardware description languages (HDLs). Such model supports parallel analysis and transformation of multiple description layers at once. In our research we present PHRT prototype, which is an extendable core, which provides basic functionality for import/export, analysis, editing and transformation of hybrid models. Its functionality can be extended by extensions and script programs. </p> <p> At the current state, PHRT prototype is being successfully used by several Russian hardware-design companies. Test results have proven applicability of PHRT as a good framework for user-specific reengineering cases like testing instrumentation and reliability assurance (memory replacement, structural redundancy insertion, etc.). </p>",,,10.1145/2491411.2492406,,,Hardware reengineering;electronic design automation;extensible toolkit;hybrid device model;test insertion;verification platform,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Producing software by integration: challenges and research directions (keynote),P. Inverardi; M. Autili; D. Di Ruscio; P. Pelliccione; M. Tivoli,"University of l&#039;Aquila, Italy",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,2,12,"<p> Software is increasingly produced according to a certain goal and by integrating existing software produced by third-parties, typically black-box, and often provided without a machine readable documentation. This implies that development processes of the next future have to explicitly deal with an inherent incompleteness of information about existing software, notably on its behaviour. Therefore, on one side a software producer will less and less know the precise behaviour of a third party software service, on the other side she will need to use it to build her own application. In this paper we present an innovative development process to automatically produce dependable software systems by integrating existing services under uncertainty and according to the specied goal. Moreover, we (i) discuss important challenges that must be faced while producing the kind of systems we are targeting, (ii) give an overview of the state of art related to the identied challenges, and finally (iii) provide research directions to address these challenges. </p>",,,10.1145/2491411.2505428,,,Dependable software systems;automated synthesis;model elicitation;model-driven engineering,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Mining behavior models from enterprise web applications,M. Schur; A. Roth; A. Zeller,"SAP, Germany",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,422,432,"<p> Today's enterprise web applications demand very high release cycles---and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. As specifying such behavior models and writing the necessary scripts manually is a hard task, a possible alternative could be to extract them from existing applications. However, mining such models can be a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present ProCrawl (PROcess CRAWLer), a generic approach to mine behavior models from (multi-user) enterprise web applications. ProCrawl observes the behavior of the application through its user interface, generates and executes tests to explore unobserved behavior. In our evaluation of three non-trivial web applications (an open-source shop system, an SAP product compliance application, and an open-source conference manager), ProCrawl produces models that precisely abstract application behavior and which can be directly used for effective model-based regression testing. </p>",,,10.1145/2491411.2491426,,,Specification mining;dynamic analysis;model-based testing,,,,,7,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
SPLat: lightweight dynamic analysis for reducing combinatorics in testing configurable systems,C. H. P. Kim; D. Marinov; S. Khurshid; D. Batory; S. Souto; P. Barros; M. DeAmorim,"University of Texas at Austin, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,257,267,"<p> Many programs can be configured through dynamic and/or static selection of configuration variables. A software product line (SPL), for example, specifies a family of programs where each program is defined by a unique combination of features. Systematically testing SPL programs is expensive as it can require running each test against a combinatorial number of configurations. Fortunately, a test is often independent of many configuration variables and need not be run against every combination. Configurations that are not required for a test can be pruned from execution. This paper presents SPLat, a new way to dynamically prune irrelevant configurations: the configurations to run for a test can be determined during test execution by monitoring accesses to configuration variables. SPLat achieves an optimal reduction in the number of configurations and is lightweight compared to prior work that used static analysis and heavyweight dynamic execution. Experimental results on 10 SPLs written in Java show that SPLat substantially reduces the total test execution time in many cases. Moreover, we demonstrate the scalability of SPLat by applying it to a large industrial code base written in Ruby on Rails. </p>",,,10.1145/2491411.2491459,,,Automated testing;Configurable Systems;Efficiency;Software Product Lines,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Bayesian inference using data flow analysis,G. Claret; S. K. Rajamani; A. V. Nori; A. D. Gordon; J. Borgstrom,"INRIA, France",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,92,102,"<p> We present a new algorithm for Bayesian inference over probabilistic programs, based on data flow analysis techniques from the program analysis community. Unlike existing techniques for Bayesian inference on probabilistic programs, our data flow analysis algorithm is able to perform inference directly on probabilistic programs with loops. Even for loop-free programs, we show that data flow analysis offers better precision and better performance benefits over existing techniques. We also describe heuristics that are crucial for our inference to scale, and present an empirical evaluation of our algorithm over a range of benchmarks. </p>",,,10.1145/2491411.2491423,,,algebraic decision diagrams;data flow analysis;probabilistic programming,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Iterative test suites refinement for elastic computing systems,A. Gambi; A. Filieri; S. Dustdar,"University of Lugano, Switzerland",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,635,638,"<p> Elastic computing systems can dynamically scale to continuously and cost-effectively provide their required Quality of Service in face of time-varying workloads, and they are usually implemented in the cloud. Despite their wide-spread adoption by industry, a formal definition of elasticity and suitable procedures for its assessment and verification are still missing. Both academia and industry are trying to adapt established testing procedures for functional and non-functional properties, with limited effectiveness with respect to elasticity. In this paper we propose a new methodology to automatically generate test-suites for testing the elastic properties of systems. Elasticity, plasticity, and oscillations are first formalized through a convenient behavioral abstraction of the elastic system and then used to drive an iterative test suite refinement process. The outcomes of our approach are a test suite tailored to the violation of elasticity properties and a human-readable abstraction of the system behavior to further support diagnosis and fix. </p>",,,10.1145/2491411.2494579,,,Cloud;behavioral modeling;model-based testing,,,,,2,3,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Crossing the gap from imperative to functional programming through refactoring,A. Gyori; L. Franklin; D. Dig; J. Lahoda,"University of Illinois, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,543,553,"<p> Java 8 introduces two functional features: lambda expressions and functional operations like map or filter that apply a lambda expression over the elements of a Collection. Refactoring existing code to use these new features enables explicit but unobtrusive parallelism and makes the code more succinct. However, refactoring is tedious: it requires changing many lines of code. It is also error-prone: the programmer must reason about the control-, data-flow, and side-effects. Fortunately, refactorings can be automated. We designed and implemented LambdaFicator, a tool which automates two refactorings. The first refactoring converts anonymous inner classes to lambda expressions. The second refactoring converts for loops that iterate over Collections to functional operations that use lambda expressions. Using 9 open-source projects, we have applied these two refactorings 1263 and 1709 times, respectively. The results show that LambdaFicator is useful: (i) it is widely applicable, (ii) it reduces the code bloat, (iii) it increases programmer productivity, and (iv) it is accurate. </p>",,,10.1145/2491411.2491461,,,Fucntional Programming;Imperative Programming;Java 8;Lambda Expressions,,,,,6,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Agreements for software reuse in corporations,T. De Gooijer; H. Koziolek,"ABB Research, Sweden",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,679,682,"<p> Agreements for sharing of software between entities in a corporation have to be tailored to fit the situation. Such agreements are not legal documents and must address different issues than traditional software licenses. We found that these agreements should cover what is granted, payment, support, ownership and liability. In a case study we learned that an agreement should list its assumptions on the structure and processes of the software organization. The presented work enables others to create guidelines for software sharing agreements tailored to their organization and shares lessons about the differences between software product lines and corporate software sharing and reuse. </p>",,,10.1145/2491411.2494576,,,Software reuse;corporate agreements;software licensing,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Boosting concolic testing via interpolation,J. Jaffar; V. Murali; J. A. Navas,"National University of Singapore, Singapore",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,48,58,"<p> Concolic testing has been very successful in automatically generating test inputs for programs. However one of its major limitations is path-explosion that limits the generation of high coverage inputs. Since its inception several ideas have been proposed to attack this problem from various angles: defining search heuristics that increase coverage, caching of function summaries, pruning of paths using static/dynamic information etc. </p> <p> We propose a new and complementary method based on interpolation, that greatly mitigates path-explosion by subsuming paths that can be guaranteed to not hit a bug. We discuss new challenges in using interpolation that arise specifically in the context of concolic testing. We experimentally evaluate our method with different search heuristics using Crest, a publicly available concolic tester. </p>",,,10.1145/2491411.2491425,,,Concolic testing;interpolation;symbolic execution,,,,,5,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
The economics of static analysis tools,R. Kumar; A. V. Nori,"Microsoft Research, India",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,707,710,"<p> Static analysis tools have experienced a dichotomy over the span of the last decade. They have proven themselves to be useful in many domains, but at the same time have not (in general) experienced any notable concrete integration into a development environment. This is partly due to the inherent complexity of the tools themselves, as well as due to other intangible factors. Such factors usually tend to include questions about the return on investment of the tool and the value the tool provides in a development environment. In this paper, we present an empirical model for evaluating static analysis tools from the perspective of the economic value they provide. We further apply this model to a case study of the Static Driver Verier (SDV) tool that ships with the Windows Driver Kit and show the usefulness of the model and the tool. </p>",,,10.1145/2491411.2494574,,,cost;defects;economics;quality;sever-;static analysis tools,,,,,2,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Diversity in software engineering research,M. Nagappan; T. Zimmermann; C. Bird,"Queen&#8217;s University, Canada",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,466,476,"<p> One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space). </p>",,,10.1145/2491411.2491415,,,Coverage;Diversity;Representativeness;Sampling,,,,,16,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Automated oracles: an empirical study on cost and effectiveness,C. D. Nguyen; A. Marchetto; P. Tonella,"Fondazione Bruno Kessler, Italy",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,136,146,"<p> Software testing is an effective, yet expensive, method to improve software quality. Test automation, a potential way to reduce testing cost, has received enormous research attention recently, but the so-called Š—“oracle problemŠ— (how to decide the PASS/FAIL outcome of a test execution) is still a major obstacle to such cost reduction. We have extensively investigated state-of-the-art works that contribute to address this problem, from areas such as specification mining and model inference. In this paper, we compare three types of automated oracles: Data invariants, Temporal invariants, and Finite State Automata. More specifically, we study the training cost and the false positive rate; we evaluate also their fault detection capability. Seven medium to large, industrial application subjects and real faults have been used in our empirical investigation. </p>",,,10.1145/2491411.2491434,,,Automated Testing Oracles;Empirical Study;Specification Mining,,,,,8,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
SocialCDE: a social awareness tool for global software teams,F. Calefato; F. Lanubile,"University of Bari, Italy",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,587,590,"<p> We present SocialCDE, a tool that aims at augmenting Application Lifecycle Management (ALM) platforms with social awareness to facilitate the establishment of interpersonal connections and increase the likelihood of successful interactions by disclosing developersŠ—È personal interests and contextual information. </p>",,,10.1145/2491411.2494592,,,ALM;Application Lifecycle Management;Social awareness;Social networks;Trust,,,,,4,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
BugMap: a topographic map of bugs,J. Gong; H. Zhang,"Tsinghua University, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,647,650,"<p> A large and complex software system could contain a large number of bugs. It is desirable for developers to understand how these bugs are distributed across the system, so they could have a better overview of software quality. In this paper, we describe BugMap, a tool we developed for visualizing large-scale bug location information. Taken source code and bug data as the input, BugMap can display bug localizations on a topographic map. By examining the topographic map, developers can understand how the components and files are affected by bugs. We apply this tool to visualize the distribution of Eclipse bugs across components/files. The results show that our tool is effective for understanding the overall quality status of a large-scale system and for identifying the problematic areas of the system. </p>",,,10.1145/2491411.2494582,,,Software visualization;bug;bug location;topographic map,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Understanding gamification mechanisms for software development,D. J. Dubois; G. Tamburrelli,"Massachusetts Institute of Technology, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,659,662,"<p> In this paper we outline the idea to adopt gamification techniques to engage, train, monitor, and motivate all the players involved in the development of complex software artifacts, from the inception to the deployment and maintenance. The paper introduces the concept of gamification and proposes a research approach to understand how its principles may be successfully applied to the process of software development. Applying gamification to software engineering is not as straightforward as it may appear since it has to be casted to the peculiarities of this domain. Existing literature in the area has already recognized the possible use of such technology in the context of software development, however how to design and use gamification in this context is still an open question. This leads to several research challenges which are organized in a fascinating research agenda that is part of the contribution of this paper. Finally, to support the proposed ideas we present a preliminary experiment that shows the effect of gamification on the performance of students involved in a software engineering project. </p>",,,10.1145/2491411.2494589,,,Software development;gamification;reward;serious games,,,,,28,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Empirical answers to fundamental software engineering problems (panel),B. Meyer; H. Gall; M. Harman; G. Succi,"ETH Zurich, Switzerland / ITMO, Russia / Eiffel Software, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,14,18,<p> Can the methods of empirical software engineering give us answers to the truly important open questions in the field? </p>,,,10.1145/2491411.2505430,,,Empirical Software Engineering,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Fuzzy service matching in on-the-fly computing,M. C. Platenius,"University of Paderborn, Germany",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,715,718,"<p> In the future vision of software engineering, services from world-wide markets are composed automated in order to build custom-made systems. Supporting such scenarios requires an adequate service matching approach. Many existing approaches do not fulfill two key requirements of emerging concepts like On-The-Fly-Computing, namely (1) comprehensiveness, i.e., the consideration of different service views that cover not only functional properties, but also non-functional properties and (2) fuzzy matching, i.e., the ability to deliver gradual results in order to cope with a certain extent of uncertainty, incompleteness, and tolerance ranges. In this paper, I present a fuzzy matching process that distinguishes between different fuzziness sources and leverages fuzziness in different matching steps which consider several service views, e.g., behavior and quality properties. </p>",,,10.1145/2491411.2492405,,,Fuzzy Matching;On-The-Fly Computing;Service Matching;Service Specifications,,,,,2,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Enhancing symbolic execution with built-in term rewriting and constrained lazy initialization,P. Braione; G. Denaro; M. Pezz’å,"University of Milano-Bicocca, Italy",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,411,421,"<p> Symbolic execution suffers from problems when analyzing programs that handle complex data structures as their inputs and take decisions over non-linear expressions. For these programs, symbolic execution may incur invalid inputs or unidentified infeasible traces, and may raise large amounts of false alarms. Some symbolic executors tackle these problems by introducing executable preconditions to exclude invalid inputs, and some solvers exploit rewrite rules to address non linear problems. In this paper, we discuss the core limitations of executable preconditions, and address these limitations by proposing invariants specifically designed to harmonize with the lazy initialization algorithm. We exploit rewrite rules applied within the symbolic executor, to address simplifications of inverse relationships fostered from either program-specific calculations or the logic of the verification tasks. We present a symbolic executor that integrates the two techniques, and validate our approach against the verification of a relevant set of properties of the Tactical Separation Assisted Flight Environment. The empirical data show that the integrated approach can improve the effectiveness of symbolic execution. </p>",,,10.1145/2491411.2491433,,,Software analysis;Symbolic execution,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Termination proofs from tests,A. V. Nori; R. Sharma,"Microsoft Research, India",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,246,256,"<p> We show how a test suite for a sequential program can be profitably used to construct a termination proof. In particular, we describe an algorithm TpT for proving termination of a program based on information derived from testing it. TpT iteratively calls two phases: (a) an infer phase, and (b) a validate phase. In the infer phase, machine learning, in particular, linear regression is used to efficiently compute a candidate loop bound for every loop in the program. These loop bounds are verified for correctness by an off-the-shelf checker. If a loop bound is invalid, then the safety checker provides a test or a counterexample that is used to generate more data which is subsequently used by the next infer phase to compute better estimates for loop bounds. On the other hand, if all loop bounds are valid, then we have a proof of termination. We also describe a simple extension to our approach that allows us to infer polynomial loop bounds automatically. We have evaluated TpT on two benchmark sets, micro-benchmarks obtained from recent literature on program termination, and Windows device drivers. Our results are promising -- on the micro-benchmarks, we show that TpT is able to prove termination on 15% more benchmarks than any previously known technique, and our evaluation on Windows device drivers demonstrates TpT's ability to analyze and scale to real world applications. </p>",,,10.1145/2491411.2491413,,,Machine learning;Software model checking;Termination,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Precision reuse for efficient regression verification,D. Beyer; S. L’_we; E. Novikov; A. Stahlbauer; P. Wendler,"University of Passau, Germany",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,389,399,"<p> Continuous testing during development is a well-established technique for software-quality assurance. Continuous model checking from revision to revision is not yet established as a standard practice, because the enormous resource consumption makes its application impractical. Model checkers compute a large number of verification facts that are necessary for verifying if a given specification holds. We have identified a category of such intermediate results that are easy to store and efficient to reuse: abstraction precisions. The precision of an abstract domain specifies the level of abstraction that the analysis works on. Precisions are thus a precious result of the verification effort and it is a waste of resources to throw them away after each verification run. In particular, precisions are reasonably small and thus easy to store; they are easy to process and have a large impact on resource consumption. We experimentally show the impact of precision reuse on industrial verification problems created from 62 Linux kernel device drivers with 1119 revisions. </p>",,,10.1145/2491411.2491429,,,Formal Verification;Regression Checking,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Dynodroid: an input generation system for Android apps,A. Machiry; R. Tahiliani; M. Naik,"Georgia Tech, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,224,234,"<p> We present a system Dynodroid for generating relevant inputs to unmodified Android apps. Dynodroid views an app as an event-driven program that interacts with its environment by means of a sequence of events through the Android framework. By instrumenting the framework once and for all, Dynodroid monitors the reaction of an app upon each event in a lightweight manner, using it to guide the generation of the next event to the app. Dynodroid also allows interleaving events from machines, which are better at generating a large number of simple inputs, with events from humans, who are better at providing intelligent inputs. </p> <p> We evaluated Dynodroid on 50 open-source Android apps, and compared it with two prevalent approaches: users manually exercising apps, and Monkey, a popular fuzzing tool. Dynodroid, humans, and Monkey covered 55%, 60%, and 53%, respectively, of each app's Java source code on average. Monkey took 20X more events on average than Dynodroid. Dynodroid also found 9 bugs in 7 of the 50 apps, and 6 bugs in 5 of the top 1,000 free apps on Google Play. </p>",,,10.1145/2491411.2491450,,,Android;GUI testing;testing event-driven programs,,,,,76,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Identifying message flow in distributed event-based systems,J. Garcia; D. Popescu; G. Safi; W. G. J. Halfond; N. Medvidovic,"University of Southern California, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,367,377,"<p> Distributed event-based (DEB) systems contain highly-decoupled components that interact by exchanging messages. This enables flexible system composition and adaptation, but also makes DEB systems difficult to maintain. Most existing program analysis techniques to support maintenance are not well suited to DEB systems, while those that are tend to suffer from inaccuracy or make assumptions that limit their applicability. This paper presents Eos, a static analysis technique that identifies message information useful for maintaining a DEB system, namely, message types and message flow within a system. Eos has been evaluated on six off-the-shelf DEB systems spanning five different middleware platforms, and has exhibited excellent accuracy and efficiency. Furthermore, a case study involving a range of maintenance activities undertaken on three existing DEB systems shows that, on average, Eos enables an engineer to identify the scope and impact of required changes more accurately than existing alternatives. </p>",,,10.1145/2491411.2491462,,,distributed event-based systems;maintenance;message flow,,,,,4,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Scalable analysis of variable software,J. Liebig; A. von Rhein; C. Kestner; S. Apel; J. Dorre; C. Lengauer,"University of Passau, Germany",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,81,91,"<p> The advent of variability management and generator technology enables users to derive individual variants from a variable code base based on a selection of desired configuration options. This approach gives rise to the generation of possibly billions of variants that, however, cannot be efficiently analyzed for errors with classic analysis techniques. To address this issue, researchers and practitioners usually apply sampling heuristics. While sampling reduces the analysis effort significantly, the information obtained is necessarily incomplete and it is unknown whether sampling heuristics scale to billions of variants. Recently, researchers have begun to develop variability-aware analyses that analyze the variable code base directly exploiting the similarities among individual variants to reduce analysis effort. However, while being promising, so far, variability-aware analyses have been applied mostly only to small academic systems. To learn about the mutual strengths and weaknesses of variability-aware and sampling-based analyses of software systems, we compared the two strategies by means of two concrete analysis implementations (type checking and liveness analysis), applied them to three subject systems: Busybox, the x86 Linux kernel, and OpenSSL. Our key finding is that variability-aware analysis outperforms most sampling heuristics with respect to analysis time while preserving completeness. </p>",,,10.1145/2491411.2491437,,,C Preprocessor;Liveness Analysis;Software Product Lines;Type Checking;Variability-aware Analysis,,,,,17,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Data debugging with continuous testing,K. Mu™lu; Y. Brun; A. Meliou,"University of Washington, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,631,634,"<p> Today, systems rely as heavily on data as on the software that manipulates those data. Errors in these systems are incredibly costly, annually resulting in multi-billion dollar losses, and, on multiple occasions, in death. While software debugging and testing have received heavy research attention, less effort has been devoted to data debugging: discovering system errors caused by well-formed but incorrect data. In this paper, we propose continuous data testing: using otherwise-idle CPU cycles to run test queries, in the background, as a user or database administrator modifies a database. This technique notifies the user or administrator about a data bug as quickly as possible after that bug is introduced, leading to at least three benefits: (1) The bug is discovered quickly and can be fixed before it is likely to cause a problem. (2) The bug is discovered while the relevant change is fresh in the user's or administrator's mind, increasing the chance that the underlying cause of the bug, as opposed to only the discovered side-effect, is fixed. (3) When poor documentation or company policies contribute to bugs, discovering the bug quickly is likely to identify these contributing factors, facilitating updating documentation and policies to prevent similar bugs in the future. We describe the problem space and potential benefits of continuous data testing, our vision for the technique, challenges we encountered, and our prototype implementation for PostgreSQL. The prototype's low overhead shows promise that continuous data testing can address the important problem of data debugging. </p>",,,10.1145/2491411.2494580,,,Database testing;continuous testing;data debugging,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
A statistical semantic language model for source code,T. T. Nguyen; A. T. Nguyen; H. A. Nguyen; T. N. Nguyen,"Iowa State University, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,532,542,"<p> Recent research has successfully applied the statistical n-gram language model to show that source code exhibits a good level of repetition. The n-gram model is shown to have good predictability in supporting code suggestion and completion. However, the state-of-the-art n-gram approach to capture source code regularities/patterns is based only on the lexical information in a local context of the code units. To improve predictability, we introduce SLAMC, a novel statistical semantic language model for source code. It incorporates semantic information into code tokens and models the regularities/patterns of such semantic annotations, called sememes, rather than their lexemes. It combines the local context in semantic n-grams with the global technical concerns/functionality into an n-gram topic model, together with pairwise associations of program elements. Based on SLAMC, we developed a new code suggestion method, which is empirically evaluated on several projects to have relatively 18-68% higher accuracy than the state-of-the-art approach. </p>",,,10.1145/2491411.2491458,,,Code Completion;Statistical Semantic Language Model,,,,,16,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Precise range analysis on large industry code,S. Kumar; B. Chimdyalwar; U. Shrotri,"Tata Consultancy Services, India",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,675,678,"<p> Abstract interpretation is widely used to perform static code analysis with non-relational (interval) as well as relational (difference-bound matrices, polyhedral) domains. Analysis using non-relational domains is highly scalable but delivers imprecise results, whereas, use of relational domains produces precise results but does not scale up. We have developed a tool that implements K-limited path sensitive interval domain analysis to get precise results without losing on scalability. The tool was able to successfully analyse 10 million lines of embedded code for different properties such as division by zero, array index out of bound (AIOB), overflow-underflow and so on. This paper presents details of the tool and results of our experiments for detecting AIOB property. A comparison with the existing tools in the market demonstrates that our tool is more precise and scales better. </p>",,,10.1145/2491411.2494569,,,Abstract Interpretation;Interval Domain;Range Analysis,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Con2colic testing,A. Farzan; A. Holzer; N. Razavi; H. Veith,"University of Toronto, Canada",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,37,47,"<p> In this paper, we describe (con)2colic testing - a systematic testing approach for concurrent software. Based on concrete and symbolic executions of a concurrent program, (con)2colic testing derives inputs and schedules such that the execution space of the program under investigation is systematically explored. We introduce interference scenarios as key concept in (con)2colic testing. Interference scenarios capture the flow of data among different threads and enable a unified representation of path and interference constraints. We have implemented a (con)2colic testing engine and demonstrate the effectiveness of our approach by experiments. </p>",,,10.1145/2491411.2491453,,,Concolic;Concurrency;Interference;Testing,,,,,5,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Design and optimization of multi-clocked embedded systems using formal technique,Y. Jiang; Z. Li; H. Zhang; Y. Deng; X. Song; M. Gu; J. Sun,"Tsinghua University, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,703,706,"<p> TodayŠ—Ès system-on-chip and distributed systems are commonly equipped with multiple clocks. The key challenge in designing such systems is that heterogenous control-oriented and data-oriented behaviors within one clock domain, and asynchronous communications between two clock domains have to be captured and evaluated in a single framework. In this paper, we propose to use timed automata and synchronous dataflow to capture the dynamic behaviors of multi-clock embedded systems. A timed automata and synchronous dataflow based modeling and analyzing framework is constructed to evaluate and optimize the performance of multiclock embedded systems. Data-oriented behaviors are captured by synchronous dataflow, while synchronous control-oriented behaviors are captured by timed automata, and inter clock-domain asynchronous communication can be modeled in an interface timed automaton or a synchronous dataflow module with the CSP mechanism. The behaviors of synchronous dataflow are interpreted by some equivalent timed automata to maintain the semantic consistency of the mixed model. Then, various functional properties can be simulated and verified within the framework. We apply this framework in the design process of a sub-system that is used in real world subway communication control system </p>",,,10.1145/2491411.2494575,,,control- oriented behavior;data-oriented behavior;embedded system;multi-clock;synchronous data flow;timed automata,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Convergent contemporary software peer review practices,P. C. Rigby; C. Bird,"Concordia University, Canada",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,202,212,"<p> Software peer review is practiced on a diverse set of software projects that have drastically different settings, cultures, incentive systems, and time pressures. In an effort to characterize and understand these differences we examine two Google-led projects, Android and Chromium OS, three Microsoft projects, Bing, Office, and MS SQL, and projects internal to AMD. We contrast our findings with data taken from traditional software inspection conducted on a Lucent project and from open source software peer review on six projects, including Apache, Linux, and KDE. Our measures of interest include the review interval, the number of developers involved in review, and proxy measures for the number of defects found during review. We find that despite differences among projects, many of the characteristics of the review process have independently converged to similar values which we think indicate general principles of code review practice. We also introduce a measure of the degree to which knowledge is shared during review. This is an aspect of review practice that has traditionally only had experiential support. Our knowledge sharing measure shows that conducting peer review increases the number of distinct files a developer knows about by 66% to 150% depending on the project. This paper is one of the first studies of contemporary review in software firms and the most diverse study of peer review to date. </p>",,,10.1145/2491411.2491444,,,Empirical Software Engineering;Inspection;Open source software;Peer code review;Software firms,,,,,27,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Differential assertion checking,S. K. Lahiri; K. L. McMillan; R. Sharma; C. Hawblitzel,"Microsoft Research, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,345,355,"<p> Previous version of a program can be a powerful enabler for program analysis by defining new relative specifications and making the results of current program analysis more relevant. In this paper, we describe the approach of differential assertion checking (DAC) for comparing different versions of a program with respect to a set of assertions. DAC provides a natural way to write relative specifications over two programs. We introduce a novel modular approach to DAC by reducing it to safety checking of a composed program, which can be accomplished by standard program verifiers. In particular, we leverage automatic invariant generation to synthesize relative specifications for pairs of loops and procedures. We provide a preliminary evaluation of a prototype implementation within the SymDiff tool along two directions (a) soundly verifying bug fixes in the presence of loops and (b) providing a knob for suppressing alarms when checking a new version of a program. </p>",,,10.1145/2491411.2491452,,,differential analysis;regressions;verification,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
RSA-MBT: a test tool for generating test artifacts based on models,A. D. d. Costa; R. Venieris; G. Carvalho; C. J. P. De Lucena,"PUC-Rio, Brazil",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,619,622,"<p> Model-Based Testing (MBT) has attracted the attention of many industries and, hence, it has provided several approaches reported in the literature. The Software Engineering Lab (LES) at the Pontifical Catholic University of Rio de Janeiro has worked extensively on coordinating and carrying out tests of large-scale software systems developed (for web and desktop) for different domains (e.g. petroleum, e-commerce, etc). Based on this experience, an LES test group created a new test modeling language called UML Testing Profile for Coordination (UTP-C) to model relevant test data. However, to use the advantages of the new modeling, an appropriate test-modeling tool became necessary. Thus, this paper presents the RSA-MBT, a new plug-in developed for the Rational Software Architecture (RSA) tool, to generate a set of test artifacts from UTP-C diagrams. </p>",,,10.1145/2491411.2494593,,,Model Based Test;Multi-agent System;Rational Tool;Software Testing;UML Testing Profile,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
A framework for defining the dynamic semantics of DSLs,U. Tikhonova,"Eindhoven University of Technology, Netherlands",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,735,738,"<p> In this research abstract we describe our project on a common reference framework for defining domain specific languages (DSLs). The framework is meant for defining the dynamic semantics of DSLs and allows for mapping the DSL definition to the various platforms, such as verification, validation and simulation. The objectives of the project are to make a DSL dynamic semantics definition explicit and to use this definition for bridging technological diversity of various platforms, used in the DSLs development. </p>",,,10.1145/2491411.2492404,,,Domain Specific Languages;dynamic semantics;model transformations;validation and verification,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Searching for better configurations: a rigorous approach to clone evaluation,,,Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,455,465,"<p> Clone detection finds application in many software engineering activities such as comprehension and refactoring. However, the confounding configuration choice problem poses a widely-acknowledged threat to the validity of previous empirical analyses. We introduce desktop and parallelised cloud-deployed versions of a search based solution that finds suitable configurations for empirical studies. We evaluate our approach on 6 widely used clone detection tools applied to the Bellon suite of 8 subject systems. Our evaluation reports the results of 9.3 million total executions of a clone tool; the largest study yet reported. Our approach finds significantly better configurations (p < 0.05) than those currently used, providing evidence that our approach can ameliorate the confounding configuration choice problem. </p>",,,10.1145/2491411.2491420,,,Clone Detection;Genetic Algorithms;SBSE,,,,,30,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
An empirical analysis of the co-evolution of schema and code in database applications,D. Qiu; B. Li; Z. Su,"Southeast University, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,125,135,"<p> Modern database applications are among the most widely used and complex software systems. They constantly evolve, responding to changes to data, database schemas, and code. It is challenging to manage these changes and ensure that everything co-evolves consistently. For example, when a database schema is modified, all the code that interacts with the database must be changed accordingly. Although database evolution and software evolution have been extensively studied in isolation, the co-evolution of schema and code has largely been unexplored. </p> <p> This paper presents the first comprehensive empirical analysis of the co-evolution of database schemas and code in ten popular large open-source database applications, totaling over 160K revisions. Our major findings include: 1) Database schemas evolve frequently during the application lifecycle, exhibiting a variety of change types with similar distributions across the studied applications; 2) Overall, schema changes induce significant code-level modifications, while certain change types have more impact on code than others; and 3) Co-change analyses can be viable to automate or assist with database application evolution. We have also observed that: 1) 80% of the schema changes happened in 20-30% of the tables, while nearly 40% of the tables did not change; and 2) Referential integrity constraints and stored procedures are rarely used in our studied subjects. We believe that our study reveals new insights into how database applications evolve and useful guidelines for designing assistive tools to aid their evolution. </p>",,,10.1145/2491411.2491431,,,Co-evolution;database application;empirical analysis,,,,,13,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Mining succinct predicated bug signatures,C. Sun; S. C. Khoo,"National University of Singapore, Singapore",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,576,586,"<p> A bug signature is a set of program elements highlighting the cause or effect of a bug, and provides contextual information for debugging. In order to mine a signature for a buggy program, two sets of execution profiles of the program, one capturing the correct execution and the other capturing the faulty, are examined to identify the program elements contrasting faulty from correct. Signatures solely consisting of control flow transitions have been investigated via discriminative sequence and graph mining algorithms. These signatures might be handicapped in cases where the effect of a bug is not manifested by any deviation in control flow transitions. In this paper, we introduce the notion of predicated bug signature/ that aims to enhance the predictive power of bug signatures by utilizing both data predicates and control-flow information. We introduce a novel ``discriminative itemset generator'' mining technique to generate succinct/ signatures which do not contain redundant or irrelevant program elements. Our case studies demonstrate that predicated signatures can hint at more scenarios of bugs where traditional control-flow signatures fail. </p>",,,10.1145/2491411.2491449,,,bug signature;feature selection;statistical debugging,,,,,5,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Practical static analysis of JavaScript applications in the presence of frameworks and libraries,M. Madsen; B. Livshits; M. Fanning,"Aarhus University, Denmark",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,499,509,"<p> JavaScript is a language that is widely-used for both web- based and standalone applications such as those in the upcoming Windows 8 operating system. Analysis of JavaScript has long been known to be challenging due to its dynamic nature. On top of that, most JavaScript applications rely on large and complex libraries and frameworks, often written in a combination of JavaScript and native code such as C and C++. Stubs have been commonly employed as a partial specification mechanism to address the library problem; however, they are tedious to write, incomplete, and occasionally incorrect. </p> <p> However, the manner in which library code is used within applications often sheds light on what library APIs return or consume as parameters. In this paper, we propose a technique which combines pointer analysis with use analysis to handle many challenges posed by large JavaScript libraries. Our approach enables a variety of applications, ranging from call graph discovery to auto-complete to supporting runtime optimizations. Our techniques have been implemented and empirically validated on a set of 25 Windows 8 JavaScript applications, averaging 1,587 lines of code, demonstrating a combination of scalability and precision. </p>",,,10.1145/2491411.2491417,,,JavaScript;frameworks;libraries;points-to analysis;use analysis,,,,,9,4,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
A cost-effectiveness criterion for applying software defect prediction models,H. Zhang; S. C. Cheung,"Tsinghua University, China / ISCAS, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,643,646,"<p> Ideally, software defect prediction models should help organize software quality assurance (SQA) resources and reduce cost of finding defects by allowing the modules most likely to contain defects to be inspected first. In this paper, we study the cost-effectiveness of applying defect prediction models in SQA and propose a basic cost-effectiveness criterion. The criterion implies that defect prediction models should be applied with caution. We also propose a new metric FN/(FN+TN) to measure the cost-effectiveness of a defect prediction model. </p>",,,10.1145/2491411.2494581,,,Defect prediction;cost effectiveness;evaluation metrics,,,,,2,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
ShAir: extensible middleware for mobile peer-to-peer resource sharing,D. J. Dubois; Y. Bando; K. Watanabe; H. Holtzman,"Massachusetts Institute of Technology, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,687,690,"<p> ShAir is a middleware infrastructure that allows mobile applications to share resources of their devices (e.g., data, storage, connectivity, computation) in a transparent way. The goals of ShAir are: (i) abstracting the creation and maintenance of opportunistic delay-tolerant peer-to-peer networks; (ii) being decoupled from the actual hardware and network platform; (iii) extensibility in terms of supported hardware, protocols, and on the type of resources that can be shared; (iv) being capable of self-adapting at run-time; (v) enabling the development of applications that are easier to design, test, and simulate. In this paper we discuss the design, extensibility, and maintainability of the ShAir middleware, and how to use it as a platform for collaborative resource-sharing applications. Finally we show our experience in designing and testing a file-sharing application. </p>",,,10.1145/2491411.2494573,,,Resource sharing;middleware;mobile devices;peer-to-peer,,,,,5,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Code fragment summarization,A. T. T. Ying; M. P. Robillard,"McGill University, Canada",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,655,658,"<p> Current research in software engineering has mostly focused on the retrieval accuracy aspect but little on the presentation aspect of code examples, e.g., how code examples are presented in a result page. We investigate the feasibility of summarizing code examples for better presenting a code example. Our algorithm based on machine learning could approximate summaries in an oracle manually generated by humans with a precision of 0.71. This result is promising as summaries with this level of precision achieved the same level of agreement as human annotators with each other. </p>",,,10.1145/2491411.2494587,,,Machine Learning;source code analysis;summarization,,,,,10,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Software engineering for mathematics (keynote),G. Gonthier,"Microsoft Research, UK",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,13,13,"<p> Since Turing, we have wanted to use computers to store, process, and check mathematics. However even with the assistance of modern software tools, the formalization of research-level mathematics remains a daunting task, not least because of the talent with which working mathematicians combine diverse theories to achieve their ends. By drawing on tools and techniques from type theory, language design, and software engineering we captured enough of these practices to formalize the proof of the Odd Order theorem, a landmark result in Group Theory, which ultimately lead to the monumental Classification of finite simple groups. This involved recasting the software component concept in the setting of higher-order, higher-kinded Type Theory to create a library of mathematical components covering most of the undergraduate Algebra and graduate Group Theory syllabus. This library then allowed us to write a formal proof comparable in size and abstraction level to the 250-page textbook proof of the Odd Order theorem. </p>",,,10.1145/2491411.2505429,,,Theorem Proving,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Cachetor: detecting cacheable data to remove bloat,,,Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,268,278,"<p> Modern object-oriented software commonly suffers from runtime bloat that significantly affects its performance and scalability. Studies have shown that one important pattern of bloat is the work repeatedly done to compute the same data values. Very often the cost of computation is very high and it is thus beneficial to memoize the invariant data values for later use. While this is a common practice in real-world development, manually finding invariant data values is a daunting task during development and tuning. To help the developers quickly find such optimization opportunities for performance improvement, we propose a novel run-time profiling tool, called Cachetor, which uses a combination of dynamic dependence profiling and value profiling to identify and report operations that keep generating identical data values. The major challenge in the design of Cachetor is that both dependence and value profiling are extremely expensive techniques that cannot scale to large, real-world applications for which optimizations are important. To overcome this challenge, we propose a series of novel abstractions that are applied to run-time instruction instances during profiling, yielding significantly improved analysis time and scalability. We have implemented Cachetor in Jikes Research Virtual Machine and evaluated it on a set of 14 large Java applications. Our experimental results suggest that Cachetor is effective in exposing caching opportunities and substantial performance gains can be achieved by modifying a program to cache the reported data. </p>",,,10.1145/2491411.2491416,,,Runtime bloat;cacheable data;dynamic dependence analysis;performance optimization,,,,,5,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Adequate monitoring of service compositions,A. Bertolino; E. Marchetti; A. Morichetta,"ISTI-CNR, Italy",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,59,69,"<p> Monitoring is essential to validate the runtime behaviour of dynamic distributed systems. However, monitors can inform of relevant events as they occur, but by their very nature they will not report about all those events that are not happening. In service-oriented applications it would be desirable to have means to assess the thoroughness of the interactions among the services that are being monitored. In case some events or message sequences or interaction patterns have not been observed for a while, in fact, one could timely check whether this happens because something is going wrong. In this paper, we introduce the novel notion of monitoring adequacy, which is generic and can be defined on different entities. We then define two adequacy criteria for service compositions and implement a proof-of-concept adequate monitoring framework. We validate the approach on two case studies, the Travel Reservation System and the Future Market choreographies. </p>",,,10.1145/2491411.2491441,,,Adequacy criteria;Branch coverage;Choreography;Monitoring;Operation coverage,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Automatically describing software faults,N. DiGiuseppe,"UC Irvine, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,711,714,"<p> A developers ability to successfully debug a fault is directly related to their ability to comprehend the fault. Notwithstanding improvements in software-maintenance automation, this fault comprehension task remains largely manual and time consuming. I propose an automated approach to describe software faults, thus ameliorating comprehension and reducing manual effort. My approach leverages dynamic analysis, fault localization, and source-code mining to produce a succinct, natural-language fault summary. </p>",,,10.1145/2491411.2492401,,,Automated Testing;Debugging;Fault Comprehension;Latent Semantic Analysis,,,,,,10,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Sample size vs. bias in defect prediction,F. Rahman; D. Posnett; I. Herraiz; P. Devanbu,"UC Davis, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,147,157,"<p> Most empirical disciplines promote the reuse and sharing of datasets, as it leads to greater possibility of replication. While this is increasingly the case in Empirical Software Engineering, some of the most popular bug-fix datasets are now known to be biased. This raises two significant concerns: first, that sample bias may lead to underperforming prediction models, and second, that the external validity of the studies based on biased datasets may be suspect. This issue has raised considerable consternation in the ESE literature in recent years. However, there is a confounding factor of these datasets that has not been examined carefully: size. Biased datasets are sampling only some of the data that could be sampled, and doing so in a biased fashion; but biased samples could be smaller, or larger. Smaller data sets in general provide less reliable bases for estimating models, and thus could lead to inferior model performance. In this setting, we ask the question, what affects performance more, bias, or size? We conduct a detailed, large-scale meta-analysis, using simulated datasets sampled with bias from a high-quality dataset which is relatively free of bias. Our results suggest that size always matters just as much bias direction, and in fact much more than bias direction when considering information-retrieval measures such as AUCROC and F-score. This indicates that at least for prediction models, even when dealing with sampling bias, simply finding larger samples can sometimes be sufficient. Our analysis also exposes the complexity of the bias issue, and raises further issues to be explored in the future. </p>",,,10.1145/2491411.2491418,,,bias;defect prediction;size,,,,,10,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
REDACT: preventing database deadlocks from application-based transactions,B. M. M. Hossain; M. Grechanik; U. Buy; H. Wang,"University of Illinois at Chicago, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,591,594,"<p> In this demonstration, we will present a database deadlocks prevention system that visualizes our algorithm for detecting hold-and-wait cycles that specify how resources (e.g., database tables) are locked and waited on to be locked during executions of SQL statements and utilizes those cycles information to prevent database deadlocks automatically. </p>",,,10.1145/2491411.2494594,,,Database deadlocks;concurrency,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Toward understanding the causes of unanswered questions in software information sites: a case study of stack overflow,R. K. Saha; A. K. Saha; D. E. Perry,"University of Texas at Austin, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,663,666,"<p> Stack Overflow is a highly successful question-answering website in the programming community, which not only provide quick solutions to programmersŠ—È questions but also is considered as a large repository of valuable software engineering knowledge. However, despite having a very engaged and active user community, Stack Overflow currently has more than 300K unanswered questions. In this paper, we perform an initial investigation to understand why these questions remain unanswered by applying a combination of statistical and data mining techniques. Our preliminary results indicate that although there are some topics that were never answered, most questions remained unanswered because they apparently are of little interest to the user community. </p>",,,10.1145/2491411.2494585,,,Mining software repositories;Q&A site;Stack Overflow,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
A publication culture in software engineering (panel),S. Fraser; L. Baresi; J. Cleland-Huang; C. A. Furia; G. Gonthier; P. Inverardi; M. Y. Vardi,"CISCO, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,19,23,"<p> This panel will discuss what characterizes the publication process in the software engineering community and debate how it serves the needs of the community, whether it is fair - e.g. valuable work gets published and mediocre work rejected - and highlight the obstacles for young scientists. The panel will conclude with a discussion on suggested next steps. </p>",,,10.1145/2491411.2505431,,,Community;Publication;Research;Reviews,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Cascading verification: an integrated method for domain-specific model checking,F. Zervoudakis; D. S. Rosenblum; S. Elbaum; A. Finkelstein,"University College London, UK",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,400,410,"<p> Model checking is an established method for verifying behavioral properties of system models. But model checkers tend to support low-level modeling languages that require intricate models to represent even the simplest systems. Modeling complexity arises in part from the need to encode domain knowledge at relatively low levels of abstraction. </p> <p> In this paper, we demonstrate that formalized domain knowledge can be reused to raise the abstraction level of model and property specifications, and hence the effectiveness of model checking. We describe a novel method for domain-specific model checking called cascading verification that uses composite reasoning over high-level system specifications and formalized domain knowledge to synthesize both low-level system models and their behavioral properties for verification. In particular, model builders use a high-level domain-specific language (DSL) based on YAML to express system specifications that can be verified with probabilistic model checking. Domain knowledge is encoded in the Web Ontology Language (OWL), the Semantic Web Rule Language (SWRL) and Prolog, which are used in combination to overcome their individual limitations. A compiler then synthesizes models and properties for verification by the probabilistic model checker PRISM. We illustrate cascading verification for the domain of uninhabited aerial vehicles (UAVs), for which we have constructed a prototype implementation. An evaluation of this prototype reveals nontrivial reductions in the size and complexity of input specifications compared to the artifacts synthesized for PRISM. </p>",,,10.1145/2491411.2491454,,,Composite reasoning;OWL;PRISM;Prolog;SWRL;UAVs;domain model;model checking,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
KATCH: high-coverage testing of software patches,P. D. Marinescu; C. Cadar,"Imperial College London, UK",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,235,245,"<p> One of the distinguishing characteristics of software systems is that they evolve: new patches are committed to software repositories and new versions are released to users on a continuous basis. Unfortunately, many of these changes bring unexpected bugs that break the stability of the system or affect its security. In this paper, we address this problem using a technique for automatically testing code patches. Our technique combines symbolic execution with several novel heuristics based on static and dynamic program analysis which allow it to quickly reach the code of the patch. We have implemented our approach in a tool called KATCH, which we have applied to all the patches written in a combined period of approximately six years for nineteen mature programs from the popular GNU diffutils, GNU binutils and GNU findutils utility suites, which are shipped with virtually all UNIX-based distributions. Our results show that KATCH can automatically synthesise inputs that significantly increase the patch coverage achieved by the existing manual test suites, and find bugs at the moment they are introduced. </p>",,,10.1145/2491411.2491438,,,Patch Testing;Symbolic Execution,,,,,9,2,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Improving trace accuracy through data-driven configuration and composition of tracing features,S. Lohar; S. Amornborvornwong; A. Zisman; J. Cleland-Huang,"DePaul University, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,378,388,"<p> Software traceability is a sought-after, yet often elusive quality in large software-intensive systems primarily because the cost and effort of tracing can be overwhelming. State-of-the art solutions address this problem through utilizing trace retrieval techniques to automate the process of creating and maintaining trace links. However, there is no simple one- size-fits all solution to trace retrieval. As this paper will show, finding the right combination of tracing techniques can lead to significant improvements in the quality of generated links. We present a novel approach to trace retrieval in which the underlying infrastructure is configured at runtime to optimize trace quality. We utilize a machine-learning approach to search for the best configuration given an initial training set of validated trace links, a set of available tracing techniques specified in a feature model, and an architecture capable of instantiating all valid configurations of features. We evaluate our approach through a series of experiments using project data from the transportation, healthcare, and space exploration domains, and discuss its implementation in an industrial environment. Finally, we show how our approach can create a robust baseline against which new tracing techniques can be evaluated. </p>",,,10.1145/2491411.2491432,,,Trace retrieval;configuration;trace configuration,,,,,12,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Do all task dependencies require coordination? the role of task properties in identifying critical coordination needs in software projects,K. Blincoe; G. Valetto; D. Damian,"Drexel University, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,213,223,"<p> Several methods exist to detect the coordination needs within software teams. Evidence exists that developersŠ—È awareness about coordination needs improves work performance. Distinguishing with certainty between critical and trivial coordination needs and identifying and prioritizing which specific tasks a pair of developers should coordinate about remains an open problem. We investigate what work dependencies should be considered when establishing coordination needs within a development team. We use our conceptualization of work dependencies named Proximity and leverage machine learning techniques to analyze what additional task properties are indicative of coordination needs. In a case study of the Mylyn project, we were able to identify from all potential coordination requirements a subset of 17% that are most critical. We define critical coordination requirements as those that can cause the most disruption to task duration when left unmanaged. These results imply that coordination awareness tools could be enhanced to make developers aware of only the coordination needs that can bring about the highest performance benefit. </p>",,,10.1145/2491411.2491440,,,Awareness;Collaborative Software Development;Coordination Requirements;Machine Learning;Proximity;Task Dependencies,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Preventing database deadlocks in applications,M. Grechanik; B. M. M. Hossain; U. Buy; H. Wang,"University of Illinois at Chicago, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,356,366,"<p> Many organizations deploy applications that use databases by sending Structured Query Language (SQL) statements to them and obtaining data that result from the execution of these statements. Since applications often share the same databases concurrently, database deadlocks routinely occur in these databases resulting in major performance degradation in these applications. Database engines do not prevent database deadlocks for the same reason that the schedulers of operating system kernels do not preempt processes in a way to avoid race conditions and deadlocks - it is not feasible to find an optimal context switching schedule quickly for multiple processes (and SQL statements), and the overhead of doing it is prohibitive. </p> <p> We created a novel approach that combines run-time monitoring, which automatically prevents database deadlocks, with static analysis, which detects hold-and-wait cycles that specify how resources (e.g., database tables) are held in contention during executions of SQL statements. We rigorously evaluated our approach. For a realistic case of over 1,200 SQL statements, our algorithm detects all hold-and-wait cycles in less than two seconds. We built a toolset and experimented with three applications. Our tool prevented all existing database deadlocks in these applications and increased their throughputs by up to three orders of magnitude. </p>",,,10.1145/2491411.2491412,,,Petri net;database deadlock;hold-and-wait cycle,,,,,4,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
USMMC: a self-contained model checker for UML state machines,S. Liu; Y. Liu; J. Sun; M. Zheng; B. Wadhwa; J. S. Dong,"National University of Singapore, Singapore",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,623,626,"<p> UML diagrams are gaining increasing usage in Object-Oriented system designs. UML state machines are specifically used in modeling dynamic behaviors of classes. It has been widely agreed that verification of system designs at an early stage will dramatically reduce the development cost. Tool support for verification UML designs can also encourage consistent usage of UML diagrams throughout the software development procedure. In this work, we present a tool, named USMMC, which turns model checking of UML state machines into practice. USMMC is a self-contained toolkit, which provides editing, interactive simulation as well as powerful model checking support for UML state machines. The evaluation results show the effectiveness and scalability of our tool. </p>",,,10.1145/2491411.2494595,,,UML state machines;model checking;semantics,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Server interface descriptions for automated testing of JavaScript web applications,C. S. Jensen; A. M’Ùller; Z. Su,"Aarhus University, Denmark",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,510,520,"<p> Automated testing of JavaScript web applications is complicated by the communication with servers. Specifically, it is difficult to test the JavaScript code in isolation from the server code and database contents. We present a practical solution to this problem. First, we demonstrate that formal server interface descriptions are useful in automated testing of JavaScript web applications for separating the concerns of the client and the server. Second, to support the construction of server interface descriptions for existing applications, we introduce an effective inference technique that learns communication patterns from sample data. </p> <p> By incorporating interface descriptions into the testing tool Artemis, our experimental results show that we increase the level of automation for high-coverage testing on a collection of JavaScript web applications that exchange JSON data between the clients and servers. Moreover, we demonstrate that the inference technique can quickly and accurately learn useful server interface descriptions. </p>",,,10.1145/2491411.2491421,,,Web applications;automated testing;interface descriptions,,,,,4,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Risky files: an approach to focus quality improvement effort,A. Mockus; R. Hackbarth; J. Palframan,"Avaya Labs Research, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,691,694,"<p> As the development of software products frequently transitions among globally distributed teams, the knowledge about the source code, design decisions, original requirements, and the history of troublesome areas gets lost. A new team faces tremendous challenges to regain that knowledge. In numerous projects we observed that only 1% of project files are involved in more than 60% of the customer reported defects (CFDs), thus focusing quality improvement on such files can greatly reduce the risk of poor product quality. We describe a mostly automated approach that annotates the source code at the file and module level with the historic information from multiple version control, issue tracking, and an organization's directory systems. Risk factors (e.g, past changes and authors who left the project) are identified via a regression model and the riskiest areas undergo a structured evaluation by experts. The results are presented via a web-based tool and project experts are then trained how to use the tool in conjunction with a checklist to determine risk remediation actions for each risky file. We have deployed the approach in seven projects in Avaya and are continuing deployment to the remaining projects as we are evaluating the results of earlier deployments. The approach is particularly helpful to focus quality improvement effort for new releases of deployed products in a resource-constrained environment. </p>",,,10.1145/2491411.2494572,,,Software quality assurance (SQA);defect prevention;defect tracking;risky file analysis;version control,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Effective dynamic detection of alias analysis errors,J. Wu; G. Hu; Y. Tang; J. Yang,"Columbia University, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,279,289,"<p> Alias analysis is perhaps one of the most crucial and widely used analyses, and has attracted tremendous research efforts over the years. Yet, advanced alias analyses are extremely difficult to get right, and the bugs in these analyses are one key reason that they have not been adopted to production compilers. This paper presents NeonGoby, a system for effectively detecting errors in alias analysis implementations, improving their correctness and hopefully widening their adoption. NeonGoby detects the worst type of bugs where the alias analysis claims that two pointers never alias, but they actually alias at runtime. NeonGoby works by dynamically observing pointer addresses during the execution of a test program and then checking these addresses against an alias analysis for errors. It is explicitly designed to (1) be agnostic to the alias analysis it checks for maximum applicability and ease of use and (2) detect alias analysis errors that manifest on real-world programs and workloads. It emits no false positives as long as test programs do not have undefined behavior per ANSI C specification or call external functions that interfere with our detection algorithm. It reduces performance overhead using a practical selection of techniques. Evaluation on three popular alias analyses and real-world programs Apache and MySQL shows that NeonGoby effectively finds 29 alias analysis bugs with zero false positives and reasonable overhead; the most serious four bugs have been patched by the developers. To enable alias analysis builders to start using NeonGoby today, we have released it open-source at https://github.com/columbia/neongoby, along with our error detection results and proposed patches. </p>",,,10.1145/2491411.2491439,,,Alias Analysis;Dynamic Analysis;Error Detection,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Prediction of atomic web services reliability based on k-means clustering,M. Silic; G. Delac; S. Srbljic,"University of Zagreb, Croatia",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,70,80,"<p> Contemporary web applications are often designed as composite services built by coordinating atomic services with the aim of providing the appropriate functionality. Although functional properties of each atomic service assure correct functionality of the entire application, nonfunctional properties such as availability, reliability, or security might significantly influence the user-perceived quality of the application. In this paper, we present CLUS, a model for reliability prediction of atomic web services that improves state-of-the-art approaches used in modern recommendation systems. CLUS predicts the reliability for the ongoing service invocation using the data collected from previous invocations. We improve the accuracy of the current state-of-the-art prediction models by considering user-, service- and environment-specific parameters of the invocation context. To address the computational performance related to scalability issues, we aggregate the available previous invocation data using K-means clustering algorithm. We evaluated our model by conducting experiments on services deployed in different regions of the Amazon cloud. The evaluation results suggest that our model improves both performance and accuracy of the prediction when compared to the current state-of-the-art models. </p>",,,10.1145/2491411.2491424,,,Cloud computing;K-means clustering;Prediction model;Reliability;Web services,,,,,11,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Finding incorrect compositions of atomicity,P. Liu; J. Dolby; C. Zhang,"Hong Kong University of Science and Technology, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,158,168,"<p> In object-oriented code, atomicity is ideally isolated in a library which encapsulates shared program state and provides atomic APIs for access. The library provides a convenient way for programmers to reason about the needed synchronization. However, as the library exports a limited set of APIs, it cannot satisfy every unplanned atomicity demand; therefore, clients may have to compose invocations of the library APIs to obtain new atomic functionality. This process is error-prone due to the complexity of reasoning required, hence tool support for uncovering incorrect compositions (i.e., atomic compositions that are implemented incorrectly) would be very helpful. A key difficulty is how to determine the intended atomic compositions, which are rarely documented. Existing inference techniques cannot be used to infer the atomic compositions because they cannot recognize the library and the client, which requires understanding the related program state. Even if extended to support the library/client, they lead to many false positives or false negatives because they miss the key program logic which reflects programmersŠ—È coding paradigms for atomic compositions. </p> <p> We define a new inference technique which identifies intended atomic compositions using two key symptoms based on program dependence. We then check dynamically whether these atomic compositions are implemented incorrectly as non-atomic. Evaluation on thirteen large applications shows that our approach finds around 50 previously unknown incorrect compositions. Further study on Tomcat shows that almost half (5 out of 12) of discovered incorrect compositions are confirmed as bugs by the developers. Given that Tomcat is heavily used in 250,000 sites including Linkedin.com and Ebay.com, we believe finding multiple new bugs in it automatically with relatively few false positives supports our heuristics for determining intended atomicity. </p>",,,10.1145/2491411.2491435,,,Atomic compositions;concurrent programming;predictive analysis;program dependence;static analysis,,,,,4,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
aPET: a test case generation tool for concurrent objects,E. Albert; P. Arenas; M. G’_mez-Zamalloa; P. Y. H. Wong,"Complutense University of Madrid, Spain",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,595,598,"<p> We present the concepts, usage and prototypical implementation of aPET, a test case generation tool for a distributed asynchronous language based on concurrent objects. The system receives as input a program, a selection of methods to be tested, and a set of parameters that include a selection of a coverage criterion. It yields as output a set of test cases which guarantee that the selected coverage criterion is achieved. aPET is completely integrated within the language's IDE via Eclipse. The generated test cases can be displayed in textual mode and, besides, it is possible to generate ABSUnit code (i.e., code runnable in a simple framework similar to JUnit to write repeatable tests). The information yield by aPET can be relevant to spot bugs during program development and also to perform regression testing. </p>",,,10.1145/2491411.2494590,,,Concurrency;Concurrent objects;Software testing;Symbolic execution;Test case generation,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Where is the business logic?,Y. Dubinsky; Y. Feldman; M. Goldstein,"IBM Research, Israel",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,667,670,"<p> One of the challenges in maintaining legacy systems is to be able to locate business logic in the code, and isolate it for different purposes, including implementing requested changes, refactoring, eliminating duplication, unit testing, and extracting business logic into a rule engine. Our new idea is an iterative method to identify the business logic in the code and visualize this information to gain better understanding of the logic distribution in the code, as well as developing a domain-specific business vocabulary. This new method combines and extends several existing technologies, including search, aggregation, and visualization. We evaluated the visualization method on a large-scale application and found that it yields useful results, provided an appropriate vocabulary is available. </p>",,,10.1145/2491411.2494588,,,Business logic;business glossary;context-based search;tree-map visualization,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Artifact evaluation (summary),A. Bergel; L. Bettini,"University of Chile, Chile",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,24,25,<p> The artifact evaluation committee (AEC) is in charge of evaluating data and software that accompany papers accepted at the ESEC/FSE'13 research track. Authors of more than 40% of the accepted papers have submitted an artifact (22 out of the 51 accepted papers). The AEC has positively evaluated more than 50% of the submitted artifacts. 12 out of the 22 artifacts have been graded as ``met expectations'' or ``exceeded expectations''. </p>,,,10.1145/2491411.2508114,,,Artifact evaluation process;artifact evaluation committee;data;software,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Extracting URLs from JavaScript via program analysis,Q. Wang; J. Zhou; Y. Chen; Y. Zhang; J. Zhao,"Shanghai Jiao Tong University, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,627,630,"<p> With the extensive use of client-side JavaScript in web applications, web contents are becoming more dynamic than ever before. This poses significant challenges for search engines, because more web URLs are now embedded or hidden inside JavaScript code and most web crawlers are script-agnostic, significantly reducing the coverage of search engines. We present a hybrid approach that combines static analysis with dynamic execution, overcoming the weakness of a purely static or dynamic approach that either lacks accuracy or suffers from huge execution cost. We also propose to integrate program analysis techniques such as statement coverage and program slicing to improve the performance of URL mining. </p>",,,10.1145/2491411.2494583,,,Dynamic URL;JavaScript;Program Analysis,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Explaining inconsistent code,M. Sch’_f; D. Schwartz-Narbonne; T. Wies,"United Nations University, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,521,531,"<p> A code fragment is inconsistent if it is not part of any normally terminating execution. Examples of such inconsistencies include code that is unreachable, code that always fails due to a run-time error, and code that makes conflicting assumptions about the program state. In this paper, we consider the problem of automatically explaining inconsistent code. This problem is difficult because traditional fault localization techniques do not apply. Our solution relies on a novel algorithm that takes an infeasible code fragment as input and generates a so-called error invariant automaton. The error invariant automaton is an abstraction of the input code fragment that only mentions program statements and facts that are relevant for understanding the cause of the inconsistency. We conducted a preliminary usability study which demonstrated that error invariant automata can help programmers better understand inconsistencies in code taken from real-world programs. In particular, access to an error invariant automata tripled the speed at which programmers could diagnose the cause of a code inconsistency. </p>",,,10.1145/2491411.2491448,,,Craig interpolation;Inconsistent code;error detection;fault lo- calization;static analysis,,,,,2,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
System reliability calculation based on the run-time analysis of ladder program,Y. Jiang; H. Zhang; H. Liu; X. Song; W. N. N. Hung; M. Gu; J. Sun,"Tsinghua University, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,695,698,"<p> Programmable logic controller (PLC) system is a typical kind of embedded system that is widely used in industry. The complexity of reliability analysis of safety critical PLC systems arises in handling the temporal correlations among the system components caused by the run-time execution logic of the embedded ladder program. In this paper, we propose a novel probabilistic model for the reliability analysis of PLC systems, called run-time reliability model (RRM). It is constructed based on the structure and run-time execution of the embedded ladder program, automatically. Then, we present some custom-made conditional probability distribution (CPD) tables according to the execution semantics of the RRM nodes, and insert the reliability probability of each system component referenced by the node into the corresponding CPD table. The proposed model is accurate and fast compared to previous work as described in the experiment results. </p>",,,10.1145/2491411.2494570,,,Ladder Program;Run-time Reliability,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Feature model extraction from large collections of informal product descriptions,J. M. Davril; E. Delfosse; N. Hariri; M. Acher; J. Cleland-Huang; P. Heymans,"University of Namur, Belgium",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,290,300,"<p> Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia. </p>",,,10.1145/2491411.2491455,,,Domain Analysis;Feature Models;Product Lines,,,,,14,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Tightfit: adaptive parallelization with foresight,O. Tripp; N. Rinetzky,"Tel Aviv University, Israel",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,169,179,"<p> Irregular applications often exhibit data-dependent parallelism: Different inputs, and sometimes also different execution phases, enable different levels of parallelism. These changes in available parallelism have motivated work on adaptive concurrency control mechanisms. Existing adaptation techniques mostly learn about available parallelism indirectly, through runtime monitors that detect pathologies (e.g. excessive retries in speculation or high lock contention in mutual exclusion). </p> <p> We present a novel approach to adaptive parallelization, whereby the effective level of parallelism is predicted directly based on input features, rather than through circumstantial indicators over the execution environment (such as retry rate). This enables adaptation with foresight, based on the input data and not the run prefix. For this, the user specifies input features, which our system then correlates with the amount of available parallelism through offline learning. The resulting prediction rule serves in deployment runs to foresee the available parallelism for a given workload and tune the parallelization system accordingly. </p> <p> We have implemented our approach in TIGHTFIT, a general framework for input-centric offline adaptation. Our experimental evaluation of TIGHTFIT over two adaptive runtime systems and eight benchmarks provides positive evidence regarding TIGHTFIT's efficacy and accuracy. </p>",,,10.1145/2491411.2491443,,,STM;adaptive software parallelization;data-dependent parallelism;irregular applications;offline learning,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
RUBRIC: a flexible tool for automated checking of conformance to requirement boilerplates,C. Arora; M. Sabetzadeh; L. Briand; F. Zimmer; R. Gnaga,"University of Luxembourg, Luxembourg",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,599,602,"<p> Using requirement boilerplates is an effective way to mit- igate many types of ambiguity in Natural Language (NL) requirements and to enable more automated transformation and analysis of these requirements. When requirements are expressed using boilerplates, one must check, as a first qual- ity assurance measure, whether the requirements actually conform to the boilerplates. If done manually, boilerplate conformance checking can be laborious, particularly when requirements change frequently. We present RUBRIC (Re- qUirements BoileRplate sanIty Checker), a flexible tool for automatically checking NL requirements against boilerplates for conformance. RUBRIC further provides a range of di- agnostics to highlight potentially problematic syntactic con- structs in NL requirement statements. RUBRIC is based on a Natural Language Processing (NLP) technique, known as text chunking. A key advantage of RUBRIC is that it yields highly accurate results even in early stages of requirements writing, where a requirements glossary may be unavailable or only partially specified. RUBRIC is scalable and can be applied repeatedly to large sets of requirements as they evolve. The tool has been validated through an industrial case study which we outline briefly in the paper. </p>",,,10.1145/2491411.2494591,,,Natural Language Processing (NLP);Requirement Boilerplates;Text Chunking,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Towards emotional awareness in software development teams,E. Guzman; B. Bruegge,"TU Munich, Germany",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,671,674,"<p> Emotions play an important role in determining work results and how team members collaborate within a project. When working in large, distributed teams, members can lose awareness of the emotional state of the project. We propose an approach to improve emotional awareness in software development teams by means of quantitative emotion summaries. Our approach automatically extracts and summarizes emotions expressed in collaboration artifacts by combining probabilistic topic modeling with lexical sentiment analysis techniques. We applied the approach to 1000 collaboration artifacts produced by three development teams in a three month period. Interviews with the teams' project leaders suggest that the proposed emotion summaries have a good correlation with the emotional state of the project, and could be useful for improving emotional awareness. However, the interviews also indicate that the current state of the summaries is not detailed enough and further improvements are needed. </p>",,,10.1145/2491411.2494578,,,Sentiment analysis;collaborative software engineering;topic extraction,,,,,14,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Efficiency and early fault detection with lower and higher strength combinatorial interaction testing,J. Petke; S. Yoo; M. B. Cohen; M. Harman,"University College London, UK",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,26,36,"<p> Combinatorial Interaction Testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. However, in order to be practically applicable, it must be able to cater for soft and hard real-world constraints and should, ideally, report a test priority order that maximises earliest fault detection. We show that we can achieve the highest strength CIT in 5.65 minutes on average. This was previously thought to be too computationally expensive to be feasible. Furthermore, we show that higher strength suites find more faults, while prioritisations using lower strengths are no worse at achieving early fault revelation. </p>",,,10.1145/2491411.2491436,,,Combinatorial Interaction Testing;Empirical Studies;Prioritisation;Software Testing,,,,,19,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
h-ubu: an industrial-strength service-oriented component framework for JavaScript applications,C. Escoffier; P. Lalanda; N. Rempulski,"Grenoble University, France",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,699,702,"<p> In the last years, we developed web applications requiring a large amount of JavaScript code. These web applications present adaptation requirements. In addition to platform-centric adaptation, applications have to dynamically react to external events like connectivity disruptions. Building such applications is complex and we faced sharp maintainability challenges. This paper presents h-ubu, a service-oriented component framework for JavaScript allowing building adaptive applications. h-ubu is used in industrial web applications and mobile applications. h-ubu is available in open source, as part of the OW2 Nanoko project. </p>",,,10.1145/2491411.2494577,,,Adaptation;Dynamism;JavaScript;Service-orientation,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
N-way model merging,J. Rubin; M. Chechik,"IBM Research, Israel / University of Toronto, Canada",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,301,311,"<p> Model merging is widely recognized as an essential step in a variety of software development activities. During the process of combining a set of related products into a product line or consolidating model views of multiple stakeholders, we need to merge multiple input models into one; yet, most of the existing approaches are applicable to merging only two models. In this paper, we define the n-way merge problem. We show that it can be reduced to the known and widely studied NP-hard problem of weighted set packing. Yet, the approximation solutions for that problem do not scale for real-sized software models. We thus evaluate alternative approaches of merging models that incrementally process input models in small subsets and propose our own algorithm that considerably improves precision over such approaches without sacrificing performance. </p>",,,10.1145/2491411.2491446,,,Model merging;combining multiple models;weighted set packing,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Distributed program tracing,D. Saha; P. Dhoolia; G. Paul,"IBM Research, India",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,180,190,"<p> Dynamic program analysis techniques depend on accurate program traces. Program instrumentation is commonly used to collect these traces, which causes overhead to the program execution. Various techniques have addressed this problem by minimizing the number of probes/witnesses used to collect traces. In this paper, we present a novel distributed trace collection framework wherein, a program is executed multiple times with the same input for different sets of witnesses. The partial traces such obtained are then merged to create the whole program trace. Such divide-and-conquer strategy enables parallel collection of partial traces, thereby reducing the total time of collection. The problem is particularly challenging as arbitrary distribution of witnesses cannot guarantee correct formation of traces. We provide and prove a necessary and sufficient condition for distributing the witnesses which ensures correct formation of trace. Moreover, we describe witness distribution strategies that are suitable for parallel collection. We use the framework to collect traces of field SAP-ABAP programs using breakpoints as witnesses as instrumentation cannot be performed due to practical constraints. To optimize such collection, we extend Ball-Larus' optimal edge-based profiling algorithm to an optimal node-based algorithm. We demonstrate the effectiveness of the framework for collecting traces of SAP-ABAP programs. </p>",,,10.1145/2491411.2491451,,,Distributed;Divide and Conquer;Parallel;Path Collection;Program Tracing,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
RiTHM: a tool for enabling time-triggered runtime verification for C programs,S. Navabpour; Y. Joshi; W. Wu; S. Berkovich; R. Medhat; B. Bonakdarpour; S. Fischmeister,"University of Waterloo, Canada",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,603,606,<p> We introduce the tool RiTHM (Runtime Time-triggered Heterogeneous Monitoring). RiTHM takes a C program under inspection and a set of LTL properties as input and generates an instrumented C program that is verified at run time by a time-triggered monitor. RiTHM provides two techniques based on static analysis and control theory to minimize instrumentation of the input C program and monitoring intervention. The monitor's verification decision procedure is sound and complete and exploits the GPU many-core technology to speedup and encapsulate monitoring tasks. </p>,,,10.1145/2491411.2494596,,,Embedded systems;Overhead predictability and containment;Real-time systems;Runtime monitoring,,,,,3,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Compiling mockups to flexible UIs,N. Sinha; R. Karim,"IBM Research, India",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,312,322,"<p> As the web becomes ubiquitous, developers are obliged to develop web applications for a variety of desktop and mobile platforms. Re- designing the user interface for every such platform is clearly cumbersome. We propose a new framework based on model-based compilation to assist the designer in solving this problem. Starting from an under-specified visual design mockup drawn by the designer, we show how faithful and flexible web pages can be obtained with virtually no manual effort. Our framework, in sharp contrast to existing web design tools, overcomes the tough challenges involved in mockup compilation by (a) employing combinatorial search to infer hierarchical layouts and (b) mechanizing adhoc principles for CSS design into a modular, extensible rule-based architecture. We believe ours is the first disciplined effort to solve the problem and will inspire rapid, low-effort web design. </p>",,,10.1145/2491411.2491427,,,CSS;HTML;Layout inference;Mockup-based design,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Will you still compile me tomorrow? static cross-version compiler validation,C. Hawblitzel; S. K. Lahiri; K. Pawar; H. Hashmi; S. Gokbulut; L. Fernando; D. Detlefs; S. Wadsworth,"Microsoft, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,191,201,"<p> This paper describes a cross-version compiler validator and measures its effectiveness on the CLR JIT compiler. The validator checks for semantically equivalent assembly language output from various versions of the compiler, including versions across a seven-month time period, across two architectures (x86 and ARM), across two compilation scenarios (JIT and MDIL), and across optimizations levels. For month-to-month comparisons, the validator achieves a false alarm rate of just 2.2%. To help understand reported semantic differences, the validator performs a root-cause analysis on the counterexample traces generated by the underlying automated theorem proving tools. This root-cause analysis groups most of the counterexamples into a small number of buckets, reducing the number of counterexamples analyzed by hand by anywhere from 53% to 96%. The validator ran on over 500,000 methods across a large suite of test programs, finding 12 previously unknown correctness and performance bugs in the CLR compiler. </p>",,,10.1145/2491411.2491442,,,Compilers;Translation Validation;Verification,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
PoMMaDe: pushdown model-checking for malware detection,F. Song; T. Touili,"East China Normal University, China",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,607,610,"<p> We present PoMMaDe, a Pushd own Model-checking based M alware D etector. In PoMMaDe, a binary program is modeled as a pushdown system (PDS) which allows to track the stack of the program, and malicious behaviors are specified in SCTPL or SLTPL, where SCTPL (resp. SLTPL) is an extension of CTL (resp. LTL) with variables, quantifiers, and predicates over the stack (needed for malware specification). The malware detection problem is reduced to SCTPL/SLTPL model-checking for PDSs. PoMMaDe allows us to detect 600 real malwares, 200 new malwares generated by two malware generators NGVCK and VCL32, and prove benign programs are benign. In particular, PoMMaDe was able to detect several malwares that could not be detected by well-known anti-viruses such as Avira, Avast, Kaspersky, McAfee, AVG, BitDefender, Eset Nod32, F-Secure, Norton, Panda, Trend Micro and Qihoo 360. </p>",,,10.1145/2491411.2494599,,,Malware Detection;Model-Checking;Pushdown Systems,,,,,4,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Making offline analyses continuous,K. Muslu; Y. Brun; M. D. Ernst; D. Notkin,"University of Washington, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,323,333,"<p> Developers use analysis tools to help write, debug, and understand software systems under development. A developer's change to the system source code may affect analysis results. Typically, to learn those effects, the developer must explicitly initiate the analysis. This may interrupt the developer's workflow and/or the delay until the developer learns the implications of the change. The situation is even worse for impure analyses Š—” ones that modify the code on which it runs Š—” because such analyses block the developer from working on the code. </p> <p> This paper presents Codebase Replication, a novel approach to easily convert an offline analysis Š—” even an impure one Š—” into a continuous analysis that informs the developer of the implications of recent changes as quickly as possible after the change is made. Codebase Replication copies the developer's codebase, incrementally keeps this copy codebase in sync with the developer's codebase, makes that copy codebase available for offline analyses to run without disturbing the developer and without the developer's changes disturbing the analyses, and makes analysis results available to be presented to the developer. </p> <p> We have implemented Codebase Replication in Solstice, an open-source, publicly-available Eclipse plug-in. We have used Solstice to convert three offline analyses Š—” FindBugs, PMD, and unit testing Š—” into continuous ones. Each conversion required on average 436 NCSL and took, on average, 18 hours. Solstice-based analyses experience no more than 2.5 milliseconds of runtime overhead per developer action. </p>",,,10.1145/2491411.2491460,,,Codebase Replication;Continuous analysis;Solstice,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
RADA: a tool for reasoning about algebraic data types with abstractions,T. H. Pham; M. W. Whalen,"University of Minnesota, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,611,614,"<p> We present RADA, a portable, scalable tool for reasoning about formulas containing algebraic data types using catamorphism (fold) functions. It can work as a back-end for reasoning about recursive programs that manipulate algebraic types. RADA operates by successively unrolling catamorphisms and uses either CVC4 and Z3 as reasoning engines. We have used RADA for reasoning about functional implementations of complex data structures and to reason about guard applications that determine whether XML messages should be allowed to cross network security domains. Promising experimental results demonstrate that RADA can be used in several practical contexts. </p>",,,10.1145/2491411.2494597,,,Decision procedures;satisfiability;unrolling,,,,,,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Regression tests to expose change interaction errors,M. Bohme; B. C. d. S. Oliveira; A. Roychoudhury,"National University of Singapore, Singapore",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,334,344,"<p> Changes often introduce program errors, and hence recent software testing literature has focused on generating tests which stress changes. In this paper, we argue that changes cannot be treated as isolated program artifacts which are stressed via testing. Instead, it is the complex dependency across multiple changes which introduce subtle errors. Furthermore, the complex dependence structures, that need to be exercised to expose such errors, ensure that they remain undiscovered even in well tested and deployed software. We motivate our work based on empirical evidence from a well tested and stable project - Linux GNU Coreutils - where we found that one third of the regressions take more than two (2) years to be fixed, and that two thirds of such long-standing regressions are introduced due to change interactions for the utilities we investigated. </p> <p> To combat change interaction errors, we first define a notion of change interaction where several program changes are found to affect the result of a program statement via program dependencies. Based on this notion, we propose a change sequence graph (CSG) to summarize the control-flow and dependencies across changes. The CSG is then used as a guide during program path exploration via symbolic execution - thereby efficiently producing test cases which witness change interaction errors. Our experimental infra-structure was deployed on various utilities of GNU Coreutils, which have been distributed with Linux for almost twenty years. Apart from finding five (5) previously unknown errors in the utilities, we found that only one in five generated test cases exercises a sequence that is critical to exposing a change-interaction error, while being an order of magnitude more likely to expose an error. On the other hand, stressing changes in isolation only exposed half of the change interaction errors. These results demonstrate the importance and difficulty of change dependence-aware regression testing. </p>",,,10.1145/2491411.2491430,,,Graph-Based Search;Test Generation,,,,,5,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
"Jalangi: a tool framework for concolic testing, selective record-replay, and dynamic analysis of JavaScript",K. Sen; S. Kalasapur; T. Brutch; S. Gibbs,"UC Berkeley, USA",Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,20160129,2013,,,615,618,"<p> We describe a tool framework, called Jalangi, for dynamic analysis and concolic testing of JavaScript programs. The framework is written in JavaScript and allows implementation of various heavy-weight dynamic analyses for JavaScript. Jalangi incorporates two key techniques: 1) selective record-replay, a technique which enables to record and to faithfully replay a user-selected part of the program, and 2) shadow values and shadow execution, which enables easy implementation of heavy-weight dynamic analyses such as concolic testing and taint tracking. Jalangi works through source-code instrumentation which makes it portable across platforms. Jalangi is available at https://github.com/SRA-SiliconValley/jalangi under Apache 2.0 license. Our evaluation of Jalangi on the SunSpider benchmark suite and on five web applications shows that Jalangi has an average slowdown of 26X during recording and 30X slowdown during replay and analysis. The slowdowns are comparable with slowdowns reported for similar tools, such as PIN and Valgrind for x86 binaries. </p>",,,10.1145/2491411.2494598,,,Concolic Testing;Dynamic Analysis;JavaScript;Record and Repla,,,,,1,,,,,,18-26 Aug. 2013,,ACM,ACM Conferences
Omlet: a revolution against big-brother social networks (invited talk),M. S. Lam,"Stanford University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,1,1,"<p> With the wide-spread adoption of proprietary social networks like Facebook and mobile chat platforms like Wechat, we may be heading to a future where all our communication are monetized and our online transactions are mediated by monopolistic big-data companies. This talk describes a new anti-data monetization movement led by Omlet, an open messaging service and distributed computing platform that spun out of 4 years of research at Stanford University. With Omlet, (1) users can own their data and have them hosted on cloud services of their choice and (2) distributed ""p2p webapps"" enable phones and other internet of things to interact with each other without having its communication be monetized. Introduced in March 2014, Omlet is already seeing traction, as it is being distributed on millions of Android phones, by Asus and other yet-to-be-announced device makers. This paradigm shift to decentralized computation not only safeguards users' data privacy, it fosters open competition and innovation, and provides an efficient and scalable foundation to handle the billions of phones and devices. Software engineering researchers can help make this a reality by making distributed mobile app development on such a platform accessible. </p>",,,10.1145/2635868.2684426,,,Social Networks,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
A large scale study of programming languages and code quality in github,B. Ray; D. Posnett; V. Filkov; P. Devanbu,"University of California at Davis, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,155,165,"<p> What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also find that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages. </p>",,,10.1145/2635868.2635922,,,bug fix;code quality;empirical research;programming language;regression analysis;software domain;type system,,,,,24,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Let's talk about it: evaluating contributions through discussion in GitHub,J. Tsay; L. Dabbish; J. Herbsleb,"Carnegie Mellon University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,144,154,"<p> Open source software projects often rely on code contributions from a wide variety of developers to extend the capabilities of their software. Project members evaluate these contributions and often engage in extended discussions to decide whether to integrate changes. These discussions have important implications for project management regarding new contributors and evolution of project requirements and direction. We present a study of how developers in open work environments evaluate and discuss pull requests, a primary method of contribution in GitHub, analyzing a sample of extended discussions around pull requests and interviews with GitHub developers. We found that developers raised issues around contributions over both the appropriateness of the problem that the submitter attempted to solve and the correctness of the implemented solution. Both core project members and third-party stakeholders discussed and sometimes implemented alternative solutions to address these issues. Different stakeholders also influenced the outcome of the evaluation by eliciting support from different communities such as dependent projects or even companies. We also found that evaluation outcomes may be more complex than simply acceptance or rejection. In some cases, although a submitter's contribution was rejected, the core team fulfilled the submitter's technical goals by implementing an alternative solution. We found that the level of a submitter's prior interaction on a project changed how politely developers discussed the contribution and the nature of proposed alternative solutions. </p>",,,10.1145/2635868.2635882,,,GitHub;contribution;discussion;evaluation;open source;social computing;social media;transparency,,,,,9,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Verifying CTL-live properties of infinite state models using an SMT solver,A. Vakili; N. A. Day,"University of Waterloo, Canada",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,213,223,"<p> The ability to create and analyze abstract models is an important step in conquering software complexity. In this paper, we show that it is practical to verify dynamic properties of infinite state models expressed in a subset of CTL directly using an SMT solver without iteration, abstraction, or human intervention. We call this subset CTL-Live and it consists of the operators of CTL expressible using the least fixed point operator of the mu-calculus, which are commonly considered liveness properties (e.g., AF, AU). We show that using this method the verification of an infinite state model can sometimes complete more quickly than verifying a finite version of the model. We also examine modelling techniques to represent abstract models in first-order logic that facilitate this form of model checking. </p>",,,10.1145/2635868.2635911,,,CTL-Live;First-order logic;Infinite state model;Model checking;SMT solver,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
"Archie: a tool for detecting, monitoring, and preserving architecturally significant code",M. Mirakhorli; A. Fakhry; A. Grechko; M. Wieloch; J. Cleland-Huang,"Rochester Institute of Technology, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,739,742,"<p> The quality of a software architecture is largely dependent upon the underlying architectural decisions at the framework, tactic, and pattern levels. Decisions to adopt certain solutions determine the extent to which desired qualities such as security, availability, and performance are achieved in the delivered system. In this tool demo, we present our Eclipse plug-in named Archie as a solution for maintaining architectural qualities in the design and code despite long-term maintenance and evolution activities. Archie detects architectural tactics such as heartbeat, resource pooling, and role-based access control (RBAC) in the source code of a project; constructs traceability links between the tactics, design models, rationales and source code; and then uses these to monitor the environment for architecturally significant changes and to keep developers informed of underlying design decisions and their associated rationales. </p>",,,10.1145/2635868.2661671,,,Architecture;Degradation;Patterns;Tactics,,,,,7,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
A variability perspective of mutation analysis,X. Devroey; G. Perrouin; M. Cordy; M. Papadakis; A. Legay; P. Y. Schobbens,"University of Namur, Belgium",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,841,844,"<p> Mutation testing is an effective technique for either improving or generating fault-finding test suites. It creates defective or incorrect program artifacts of the program under test and evaluates the ability of test suites to reveal them. Despite being effective, mutation is costly since it requires assessing the test cases with a large number of defective artifacts. Even worse, some of these artifacts are behaviourally ``equivalent'' to the original one and hence, they unnecessarily increase the testing effort. We adopt a variability perspective on mutation analysis. We model a defective artifact as a transition system with a specific feature selected and consider it as a member of a mutant family. The mutant family is encoded as a Featured Transition System, a compact formalism initially dedicated to model-checking of software product lines. We show how to evaluate a test suite against the set of all candidate defects by using mutant families. We can evaluate all the considered defects at the same time and isolate some equivalent mutants. We can also assist the test generation process and efficiently consider higher-order mutants. </p>",,,10.1145/2635868.2666610,,,Featured Transition Systems;Mutation Testing,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
A path-sensitively sliced control flow graph,J. Jaffar; V. Murali,"National University of Singapore, Singapore",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,133,143,"<p> We present a new graph representation of programs with specified target variables. These programs are intended to be processed by third-party applications querying target variables such as testers and verifiers. The representation embodies two concepts. First, it is path-sensitive in the sense that multiple nodes representing one program point may exist so that infeasible paths can be excluded. Second, and more importantly, it is sliced with respect to the target variables. This key step is founded on a novel idea introduced in this paper, called ``Tree Slicing'', and on the fact that slicing is more effective when there is path sensitivity. Compared to the traditional Control Flow Graph (CFG), the new graph may be bigger (due to path-sensitivity) or smaller (due to slicing). We show that it is not much bigger in practice, if at all. The main result however concerns its quality: third-party testers and verifiers perform substantially better on the new graph compared to the original CFG. </p>",,,10.1145/2635868.2635884,,,Symbolic execution;program slicing;program transformation,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Powering the static driver verifier using corral,A. Lal; S. Qadeer,"Microsoft Research, India",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,202,212,"<p> The application of software-verification technology towards building realistic bug-finding tools requires working through several precision-scalability tradeoffs. For instance, a critical aspect while dealing with C programs is to formally define the treatment of pointers and the heap. A machine-level modeling is often intractable, whereas one that leverages high-level information (such as types) can be inaccurate. Another tradeoff is modeling integer arithmetic. Ideally, all arithmetic should be performed over bitvector representations whereas the current practice in most tools is to use mathematical integers for scalability. A third tradeoff, in the context of bounded program exploration, is to choose a bound that ensures high coverage without overwhelming the analysis. This paper works through these three tradeoffs when we applied Corral, an SMT-based verifier, inside Microsoft's Static Driver Verifier (SDV). Our decisions were guided by experimentation on a large set of drivers; the total verification time exceeded well over a month. We justify that each of our decisions were crucial in getting value out of Corral and led to Corral being accepted as the engine that powers SDV in the Windows 8.1 release, replacing the SLAM engine that had been used inside SDV for the past decade. </p>",,,10.1145/2635868.2635894,,,Bitvector Reasoning;Device Drivers;Language Semantics;Loop Coverage;SMT;Software Verification,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Omen+: a precise dynamic deadlock detector for multithreaded Java libraries,M. Samak; M. K. Ramanathan,"Indian Institute of Science, India",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,735,738,"<p> Designing thread-safe libraries without concurrency defects can be a challenging task. Detecting deadlocks while invoking methods in these libraries concurrently is hard due to the possible number of method invocation combinations, the object assignments to the parameters and the associated thread interleavings. In this paper, we describe the design and implementation of OMEN+ that takes a multithreaded library as the input and detects true deadlocks in a scalable manner. We achieve this by automatically synthesizing relevant multithreaded tests and analyze the associated execution traces using a precise deadlock detector. We validate the usefulness of OMEN+ by applying it on many multithreaded Java libraries and detect a number of deadlocks even in documented thread-safe libraries. The tool is available for free download at http://www.csa.iisc.ernet.in/~sss/tool/omenplus.html. </p>",,,10.1145/2635868.2661670,,,concurrency;deadlock detection;dynamic analysis,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
XMLMate: evolutionary XML test generation,N. Havrikov; M. H’_schele; J. P. Galeotti; A. Zeller,"Saarland University, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,719,722,"<p> Generating system inputs satisfying complex constraints is still a challenge for modern test generators. We present XMLMATE, a search-based test generator specially aimed at XML-based systems. XMLMATE leverages program structure, existing XML schemas, and XML inputs to generate, mutate, recombine, and evolve valid XML inputs. Over a set of seven XML-based systems, XMLMATE detected 31 new unique failures in production code, all triggered by system inputs and thus true alarms. </p>",,,10.1145/2635868.2661666,,,Test generator;XML;evolutionary algorithms;search-based testing,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Test case purification for improving fault localization,J. Xuan; M. Monperrus,"INRIA, France",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,52,63,"<p> Finding and fixing bugs are time-consuming activities in software development. Spectrum-based fault localization aims to identify the faulty position in source code based on the execution trace of test cases. Failing test cases and their assertions form test oracles for the failing behavior of the system under analysis. In this paper, we propose a novel concept of spectrum driven test case purification for improving fault localization. The goal of test case purification is to separate existing test cases into small fractions (called purified test cases) and to enhance the test oracles to further localize faults. Combining with an original fault localization technique (e.g., Tarantula), test case purification results in better ranking the program statements. Our experiments on 1800 faults in six open-source Java programs show that test case purification can effectively improve existing fault localization techniques. </p>",,,10.1145/2635868.2635906,,,Test case purification;dynamic program slicing;spectrum-based fault localization;test case atomization,,,,,16,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Speculative reprogramming,M. Palyart; G. C. Murphy; E. Murphy-Hill; X. Blanc,"University of British Columbia, Canada",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,837,840,"<p> Although software development involves making numerous decisions amongst alternatives, the design and implementation choices made typically become invisible; what a developer sees in the project's artifacts are the end result of all of the decisions. What if, instead, all of the choices made were tracked and it was easy for a developer to revisit a point where a decision was made and choose another alternative? What if the development environment could detect and suggest alternative choices? What if it was easy and low-cost to try another path? We explore the idea of speculative reprogramming that could support a what-if environment for the programming stages of software development. </p>",,,10.1145/2635868.2666609,,,Design alternatives;What-if,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
JSAI: a static analysis platform for JavaScript,V. Kashyap; K. Dewey; E. A. Kuefner; J. Wagner; K. Gibbons; J. Sarracino; B. Wiedermann; B. Hardekopf,"University of California at Santa Barbara, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,121,132,"<p> JavaScript is used everywhere from the browser to the server, including desktops and mobile devices. However, the current state of the art in JavaScript static analysis lags far behind that of other languages such as C and Java. Our goal is to help remedy this lack. We describe JSAI, a formally specified, robust abstract interpreter for JavaScript. JSAI uses novel abstract domains to compute a reduced product of type inference, pointer analysis, control-flow analysis, string analysis, and integer and boolean constant propagation. Part of JSAI's novelty is user-configurable analysis sensitivity, i.e., context-, path-, and heap-sensitivity. JSAI is designed to be provably sound with respect to a specific concrete semantics for JavaScript, which has been extensively tested against a commercial JavaScript implementation. We provide a comprehensive evaluation of JSAI's performance and precision using an extensive benchmark suite, including real-world JavaScript applications, machine generated JavaScript code via Emscripten, and browser addons. We use JSAI's configurability to evaluate a large number of analysis sensitivities (some well-known, some novel) and observe some surprising results that go against common wisdom. These results highlight the usefulness of a configurable analysis platform such as JSAI. </p>",,,10.1145/2635868.2635904,,,Abstract Interpretation;JavaScript Analysis,,,,,10,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Learning to rank relevant files for bug reports using domain knowledge,X. Ye; R. Bunescu; C. Liu,"Ohio University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,689,699,"<p> When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity. This paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code files into methods, API descriptions of library components used in the code, the bug-fixing history, and the code change history. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluated our system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the newly introduced learning-to-rank approach significantly outperforms two recent state-of-the-art methods in recommending relevant files for bug reports. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70% of the bug reports in the Eclipse Platform and Tomcat projects. </p>",,,10.1145/2635868.2635874,,,bug reports;learning to rank;software maintenance,,,,,18,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Counterexample guided abstraction refinement of product-line behavioural models,M. Cordy; P. Heymans; A. Legay; P. Y. Schobbens; B. Dawagne; M. Leucker,"University of Namur, Belgium",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,190,201,"<p> The model-checking problem for Software Products Lines (SPLs) is harder than for single systems: variability constitutes a new source of complexity that exacerbates the state-explosion problem. Abstraction techniques have successfully alleviated state explosion in single-system models. However, they need to be adapted to SPLs, to take into account the set of variants that produce a counterexample. In this paper, we apply CEGAR (Counterexample-Guided Abstraction Refinement) and we design new forms of abstraction specifically for SPLs. We carry out experiments to evaluate the efficiency of our new abstractions. The results show that our abstractions, combined with an appropriate refinement strategy, hold the potential to achieve large reductions in verification time, although they sometimes perform worse. We discuss in which cases a given abstraction should be used. </p>",,,10.1145/2635868.2635919,,,Abstraction;CEGAR;Model Checking;Software Product Lines,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Aalta: an LTL satisfiability checker over Infinite/Finite traces,J. Li; Y. Yao; G. Pu; L. Zhang; J. He,"East China Normal University, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,731,734,"<p> Linear Temporal Logic (LTL) is been widely used nowadays in verification and AI. Checking satisfiability of LTL formulas is a fundamental step in removing possible errors in LTL assertions. We present in this paper Aalta, a new LTL satisfiability checker, which supports satisfiability checking for LTL over both infinite and finite traces. Aalta leverages the power of modern SAT solvers. We have conducted a comprehensive comparison between Aalta and other LTL satisfiability checkers, and the experimental results show that Aalta is very competitive. The tool is available at www.lab205.org/aalta. </p>",,,10.1145/2635868.2661669,,,Model Checking;Satisfiability;Temporal Logic,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
A tool suite for the model-driven software engineering of cyber-physical systems,S. Dziwok; C. Gerking; S. Becker; S. Thiele; C. Heinzemann; U. Pohlmann,"University of Paderborn, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,715,718,"<p> Cyber-physical systems, e.g., autonomous cars or trains, interact with their physical environment. As a consequence, they commonly have to coordinate with other systems via complex message communication while realizing safety-critical and real-time tasks. As a result, those systems should be correct by construction. Software architects can achieve this by using the MechatronicUML process and language. This paper presents the MechatronicUML Tool Suite that offers unique features to support the MechatronicUML modeling and analyses tasks. </p>",,,10.1145/2635868.2661665,,,Cyber-physical systems;DSL;mechatronic systems;real-time coordination;system simulation;timed model checking,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Ten years with evidence-based software engineering. What is it? Has it had any impact? What's next?,M. J’Ùrgensen,"Simula Research Laboratory, Norway",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,3,3,"<p> An evidence-based software engineer is one who is able to: 1) Formulate a question, related to a decision or judgment, so that it can be answered by the use of evidence, 2) Collect, critically evaluate and summarise relevant evidence from research, practice and local studies, 3) Apply the evidence, integrated with knowledge about the local context, to guide decisions and judgments. The keynote reflects on what it in practise means to be evidence-based in software engineering contexts, where the number of different contexts is high and the research-based evidence sparse, and why there is a need for more evidence-based practises. We summarise our experience from ten years of Evidence-Based Software Engineering in the context of university courses, training of software engineers and systematic literature reviews of software engineering research. While there are challenges in training people in evidence-based practise, our experience suggest that it is feasible and that the training can make an important difference in terms of quality of software engineering judgment and decisions. Based on our experience we suggest changes in how evidence-based software engineering should be presented and taught, and how we should ease the transfer of research results into evidence-based practises. </p>",,,10.1145/2635868.2684428,,,Evidence-Based Software Engineering,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Search-based synthesis of equivalent method sequences,A. Goffi; A. Gorla; A. Mattavelli; M. Pezz’å; P. Tonella,"University of Lugano, Switzerland",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,366,376,"<p> Software components are usually redundant, since their interface offers different operations that are equivalent in their functional behavior. Several reliability techniques exploit this redundancy to either detect or tolerate faults in software. Metamorphic testing, for instance, executes pairs of sequences of operations that are expected to produce equivalent results, and identifies faults in case of mismatching outcomes. Some popular fault tolerance and self-healing techniques execute redundant operations in an attempt to avoid failures at runtime. The common assumption of these techniques, though, is that such redundancy is known a priori. This means that the set of operations that are supposed to be equivalent in a given component should be available in the specifications. Unfortunately, inferring this information manually can be expensive and error prone. This paper proposes a search-based technique to synthesize sequences of method invocations that are equivalent to a target method within a finite set of execution scenarios. The experimental results obtained on 47 methods from 7 classes show that the proposed approach correctly identifies equivalent method sequences in the majority of the cases where redundancy was known to exist, with very few false positives. </p>",,,10.1145/2635868.2635888,,,Redundancy;equivalent method sequences;search-based software engineering;specification mining,,,,,5,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Feedback generation for performance problems in introductory programming assignments,S. Gulwani; I. Radióek; F. Zuleger,"Microsoft Research, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,41,51,"<p> Providing feedback on programming assignments manually is a tedious, error prone, and time-consuming task. In this paper, we motivate and address the problem of generating feedback on performance aspects in introductory programming assignments. We studied a large number of functionally correct student solutions to introductory programming assignments and observed: (1) There are different algorithmic strategies, with varying levels of efficiency, for solving a given problem. These different strategies merit different feedback. (2) The same algorithmic strategy can be implemented in countless different ways, which are not relevant for reporting feedback on the student program. We propose a light-weight programming language extension that allows a teacher to define an algorithmic strategy by specifying certain key values that should occur during the execution of an implementation. We describe a dynamic analysis based approach to test whether a student's program matches a teacher's specification. Our experimental results illustrate the effectiveness of both our specification language and our dynamic analysis. On one of our benchmarks consisting of 2316 functionally correct implementations to 3 programming problems, we identified 16 strategies that we were able to describe using our specification language (in 95 minutes after inspecting 66, i.e., around 3%, implementations). Our dynamic analysis correctly matched each implementation with its corresponding specification, thereby automatically producing the intended feedback. </p>",,,10.1145/2635868.2635912,,,Education;MOOCs;dynamic analysis;performance analysis;trace specification,,,,,4,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
AI: a lightweight system for tolerating concurrency bugs,M. Zhang; Y. Wu; S. Lu; S. Qi; J. Ren; W. Zheng,"Tsinghua University, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,330,340,"<p> Concurrency bugs are notoriously difficult to eradicate during software testing because of their non-deterministic nature. Moreover, fixing concurrency bugs is time-consuming and error-prone. Thus, tolerating concurrency bugs during production runs is an attractive complementary approach to bug detection and testing. Unfortunately, existing bug-tolerating tools are usually either 1) constrained in types of bugs they can handle or 2) requiring roll-back mechanism, which can hitherto not be fully achieved efficiently without hardware supports. This paper presents a novel program invariant, called Anticipating Invariant (AI), which can help anticipate bugs before any irreversible changes are made. Benefiting from this ability of anticipating bugs beforehand, our software-only system is able to forestall the failures with a simple thread stalling technique, which does not rely on execution roll-back and hence has good performance Experiments with 35 real-world concurrency bugs demonstrate that AI is capable of detecting and tolerating most types of concurrency bugs, including both atomicity and order violations. Two new bugs have been detected and confirmed by the corresponding developers. Performance evaluation with 6 representative parallel programs shows that AI incurs negligible overhead (<1%) for many nontrivial desktop and server applications. </p>",,,10.1145/2635868.2635885,,,Bug Tolerating;Concurrency Bugs;Software Reliability,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Static analysis driven performance and energy testing,A. Banerjee,"National University of Singapore, Singapore",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,791,794,"<p> Software testing is the process of evaluating the properties of a software. Properties of a software can be divided into two categories: functional properties and non-functional properties. Properties that influence the input-output relationship of the software can be categorized as functional properties. On the other hand, properties that do not influence the input-output relationship of the software directly can be categorized as non-functional properties. In context of real-time system software, testing functional as well as non functional properties is equally important. Over the years considerable amount of research effort has been dedicated in developing tools and techniques that systematically test various functional properties of a software. However, the same cannot be said about testing non-functional properties. Systematic testing of non-functional properties is often much more challenging than testing functional properties. This is because non-functional properties not only depends on the inputs to the program but also on the underlying hardware. Additionally, unlike the functional properties, nonfunctional properties are seldom annotated in the software itself. Such challenges provide the objectives for this work. The primary objective of this work is to explore and address the major challenges in testing non-functional properties of a software. </p>",,,10.1145/2635868.2666602,,,Energy-aware Test Generation;Non-Functional Testing;Performance Stressing Test Input Generation,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Known unknowns: testing in the presence of uncertainty,S. Elbaum; D. S. Rosenblum,"University of Nebraska-Lincoln, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,833,836,"<p> Uncertainty is becoming more prevalent in the software systems we build, introducing challenges in the way we develop software, especially in software testing. In this work we explore how uncertainty affects software testing, how it is managed currently, and how it could be treated more effectively. </p>",,,10.1145/2635868.2666608,,,Fault Masking;Hidden Markov Models;Software Testing;Uncertainty Quantification,,,,,5,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Architecture challenges for internal software ecosystems: a large-scale industry case study,K. B. Schultis; C. Elsner; D. Lohmann,"Siemens, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,542,552,"<p> The idea of software ecosystems encourages organizations to open software projects for external businesses, governing the cross-organizational development by architectural and other measures. Even within a single organization, this paradigm can be of high value for large-scale decentralized software projects that involve various internal, yet self-contained organizational units. However, this intra-organizational decentralization causes architecture challenges that must be understood to reason about suitable architectural measures. We present an in-depth case study on collaboration and architecture challenges in two of these large-scale software projects at Siemens. We performed a total of 46 hours of semi-structured interviews with 17 leading software architects from all involved organizational units. Our major findings are: (1) three collaboration models on a continuum that ranges from high to low coupling, (2) a classification of architecture challenges, together with (3) a qualitative and quantitative exposure of the identified recurring issues along each collaboration model. Our study results provide valuable insights for both industry and academia: Practitioners that find themselves in one of the collaboration models can use empirical evidence on challenges to make informed decisions about counteractive measures. Researchers can focus their attention on challenges faced by practitioners to make software engineering more effective. </p>",,,10.1145/2635868.2635876,,,Software ecosystem;case study;collaboration;decentralized software engineering;software architecture;software product line,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Prioritizing the devices to test your app on: a case study of Android game apps,H. Khalid; M. Nagappan; E. Shihab; A. E. Hassan,"Queen&#039;s University, Canada",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,610,620,"<p> Star ratings that are given by the users of mobile apps directly impact the revenue of its developers. At the same time, for popular platforms like Android, these apps must run on hundreds of devices increasing the chance for device-specific problems. Device-specific problems could impact the rating assigned to an app, given the varying capabilities of devices (e.g., hardware and software). To fix device-specific problems developers must test their apps on a large number of Android devices, which is costly and inefficient. Therefore, to help developers pick which devices to test their apps on, we propose using the devices that are mentioned in user reviews. We mine the user reviews of 99 free game apps and find that, apps receive user reviews from a large number of devices: between 38 to 132 unique devices. However, most of the reviews (80%) originate from a small subset of devices (on average, 33%). Furthermore, we find that developers of new game apps with no reviews can use the review data of similar game apps to select the devices that they should focus on first. Finally, among the set of devices that generate the most reviews for an app, we find that some devices tend to generate worse ratings than others. Our findings indicate that focusing on the devices with the most reviews (in particular the ones with negative ratings), developers can effectively prioritize their limited Quality Assurance (QA) efforts, since these devices have the greatest impact on ratings. </p>",,,10.1145/2635868.2635909,,,Android fragmentation;Device prioritization;Mobile apps,,,,,18,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
ORBS: language-independent program slicing,D. Binkley; N. Gold; M. Harman; S. Islam; J. Krinke; S. Yoo,"Loyola University Maryland, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,109,120,"<p> Current slicing techniques cannot handle systems written in multiple programming languages. Observation-Based Slicing (ORBS) is a language-independent slicing technique capable of slicing multi-language systems, including systems which contain (third party) binary components. A potential slice obtained through repeated statement deletion is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted. The resulting slice is similar to a dynamic slice. We evaluate five variants of ORBS on ten programs of different sizes and languages showing that it is less expensive than similar existing techniques. We also evaluate it on bash and four other systems to demonstrate feasible large-scale operation in which a parallelised ORBS needs up to 82% less time when using four threads. The results show that an ORBS slicer is simple to construct, effective at slicing, and able to handle systems written in multiple languages without specialist analysis tools. </p>",,,10.1145/2635868.2635893,,,,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Panning requirement nuggets in stream of software maintenance tickets,S. Mani; K. Sankaranarayanan; V. S. Sinha; P. Devanbu,"IBM Research, India",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,678,688,"<p> There is an increasing trend to outsource maintenance of large applications and application portfolios of a business to third parties, specialising in application maintenance, who are incented to deliver the best possible maintenance at the lowest cost. To do so, they need to identify repeat problem areas, which cause more maintenance grief, and seek a unified remedy to avoid the costs spent on fixing these individually. These repeat areas, in a sense, represent major, evolving areas of need, or requirements, for the customer. The information about the repeating problem is typically embedded in the unstructured text of multiple tickets, waiting to be found and addressed. Currently, repeat problems are found by manual analysis; effective solutions depend on the collective experience of the team solving them. In this paper, we propose an approach to automatically analyze problem tickets to discover groups of problems being reported in them and provide meaningful, descriptive labels to help interpret these groups. Our approach incorporates a cleansing phase to handle the high level of noise observed in problem tickets and a method to incorporate multiple text clustering techniques and merge their results in a meaningful manner. We provide detailed experiments to quantitatively and qualitatively evaluate our approach </p>",,,10.1145/2635868.2635897,,,Mining Software Repositories;Requirements;Text Clustering,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Discovering refactoring opportunities in cascading style sheets,D. Mazinanian; N. Tsantalis; A. Mesbah,"Concordia University, Canada",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,496,506,"<p> Cascading Style Sheets (CSS) is a language used for describing the look and formatting of HTML documents. CSS has been widely adopted in web and mobile development practice, since it enables a clean separation of content from presentation. The language exhibits complex features, such as inheritance, cascading and specificity, which make CSS code hard to maintain. Therefore, it is important to find ways to improve the maintainability of CSS code. In this paper, we propose an automated approach to remove duplication in CSS code. More specifically, we have developed a technique that detects three types of CSS declaration duplication and recommends refactoring opportunities to eliminate those duplications. Our approach uses preconditions that ensure the application of a refactoring will preserve the original document styling. We evaluate our technique on 38 real-world web systems and 91 CSS files, in total. Our findings show that duplication in CSS code is widely prevalent. Additionally, there is a significant number of presentation-preserving refactoring opportunities that can reduce the size of the CSS files and increase the maintainability of the code. </p>",,,10.1145/2635868.2635879,,,Cascading style sheets;duplication;refactoring,,,,,8,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Automatic mining of specifications from invocation traces and method invariants,I. Krka; Y. Brun; N. Medvidovic,"Google, Switzerland",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,178,189,"<p> Software library documentation often describes individual methods' APIs, but not the intended protocols and method interactions. This can lead to library misuse, and restrict runtime detection of protocol violations and automated verification of software that uses the library. Specification mining, if accurate, can help mitigate these issues, which has led to significant research into new model-inference techniques that produce FSM-based models from program invariants and execution traces. However, there is currently a lack of empirical studies that, in a principled way, measure the impact of the inference strategies on model quality. To this end, we identify four such strategies and systematically study the quality of the models they produce for nine off-the-shelf libraries. We find that (1) using invariants to infer an initial model significantly improves model quality, increasing precision by 4% and recall by 41%, on average; (2) effective invariant filtering is crucial for quality and scalability of strategies that use invariants; and (3) using traces in combination with invariants greatly improves robustness to input noise. We present our empirical evaluation, implement new and extend existing model-inference techniques, and make public our implementations, ground-truth models, and experimental data. Our work can lead to higher-quality model inference, and directly improve the techniques and tools that rely on model inference. </p>",,,10.1145/2635868.2635890,,,Model inference;execution traces;log analysis,,,,,10,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Techniques for improving regression testing in continuous integration development environments,S. Elbaum; G. Rothermel; J. Penix,"University of Nebraska-Lincoln, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,235,245,"<p> In continuous integration development environments, software engineers frequently integrate new or changed code with the mainline codebase. This can reduce the amount of code rework that is needed as systems evolve and speed up development time. While continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase, it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable. In this work, we present algorithms that make continuous integration processes more cost-effective. In an initial pre-submit phase of testing, developers specify modules to be tested, and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more cost-effective. In a subsequent post-submit phase of testing, where dependent modules as well as changed modules are tested, we use test case prioritization techniques to ensure that failures are reported more quickly. In both cases, the techniques we utilize are novel, involving algorithms that are relatively inexpensive and do not rely on code coverage information -- two requirements for conducting testing cost-effectively in this context. To evaluate our approach, we conducted an empirical study on a large data set from Google that we make publicly available. The results of our study show that our selection and prioritization techniques can each lead to cost-effectiveness improvements in the continuous integration process. </p>",,,10.1145/2635868.2635910,,,Continuous Integration;Regression Test Selection;Regression Testing;Test Case Prioritization,,,,,19,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
RaPiD: a toolkit for reliability analysis of non-deterministic systems,L. Gui; J. Sun; Y. Liu; T. K. Nguyen; J. S. Dong,"National University of Singapore, Singapore",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,727,730,"<p> Non-determinism in concurrent or distributed software systems (i.e., various possible execution orders among different distributed components) presents new challenges to the existing reliability analysis methods based on Markov chains. In this work, we present a toolkit RaPiD for the reliability analysis of non-deterministic systems. Taking Markov decision process as reliability model, RaPiD can help in the analysis of three fundamental and rewarding aspects regarding software reliability. First, to have reliability assurance on a system, RaPiD can synthesize the overall system reliability given the reliability values of system components. Second, given a requirement on the overall system reliability, RaPiD can distribute the reliability requirement to each component. Lastly, RaPiD can identify the component that affects the system reliability most significantly. RaPiD has been applied to analyze several real-world systems including a financial stock trading system, a proton therapy control system and an ambient assisted living room system. </p>",,,10.1145/2635868.2661668,,,Markov Decision Process;non-determinism;reliability analysis,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
"Tsmart-GalsBlock: a toolkit for modeling, validation, and synthesis of multi-clocked embedded systems",Y. Jiang; H. Zhang; H. Zhang; X. Zhao; H. Liu; C. Sun; X. Song; M. Gu; J. Sun,"Tsinghua University, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,711,714,"<p> The key challenges of the model-driven approach to designing multi-clocked embedded systems are three-fold: (1) how to model local synchronous components and asynchronous communication between components in a single framework, (2) how to ensure the correctness of the model, and (3) how to maintain the consistency between the model and the implementation of the system. In this paper, we present Tsmart, a self-contained toolkit to address these three challenges. Tsmart seamlessly integrates (1) a graphical editor to facilitate the modeling of the complex behaviors and structures in an embedded system, (2) a simulator for interactive graphical simulation to understand and debug the system model, (3) a verication engine to verify the correctness of the system design, and (4) a synthesis engine to automatically generate ecient executable VHDL code from the model. The toolkit has been successfully applied to designing the main control system of a train communication controller, and the system has already been deployed and in operation. The evaluation of Tsmart on this real industrial application demonstrates the eectiveness and the potential of the toolkit. </p>",,,10.1145/2635868.2661664,,,graphical simulation;multi-clocked embedded system;program synthesis;veri- fication,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
From software engineering to software systems (invited talk),A. L. Wolf,"Imperial College London, UK",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,2,2,"<p> I began my career in software engineering research and now find myself working more in software systems research. Is there a difference? In this talk I reflect on this question by recalling the stream of ideas, students, and colleagues that have shaped my path. I present an overview of the current projects in which I am involved to understand at a technical level where the two research communities, software engineering and software systems, connect and diverge. </p>",,,10.1145/2635868.2684427,,,Software Systems,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Are you getting traction? tales from the tech transfer trenches (invited talk),S. Chandra,"Samsung Electronics, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,5,5,"<p> So you have developed a new software productivity tool, written an FSE or an ICSE paper about it, and are justifiably proud of your work. If you work for a company, your (curmudgeonly) manager now wants to see its Š—“impactŠ— on the business. This is the part where you have to convince someone else to use your shiny new tool in their day-to-day work, or ship it as a product. But you soon realize that getting traction with developers or product managers is significantly harder than the research itself. Sounds familiar? In the past several years, I have been involved in taking a variety of software productivity tools to various constituencies within a company: internal users, product teams, and service delivery teams. In this talk, I will share my experiences in interacting with these constituencies; sometimes successful experiences, but at other times not so successful ones. I will focus broadly on tools in two areas: bug finding and test automation. I will make some observations on when tech transfer works and when it stumbles. </p>",,,10.1145/2635868.2684430,,,Technology Transfer,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Semantics-based obfuscation-resilient binary code similarity comparison with applications to software plagiarism detection,L. Luo; J. Ming; D. Wu; P. Liu; S. Zhu,"Pennsylvania State University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,389,400,"<p> Existing code similarity comparison methods, whether source or binary code based, are mostly not resilient to obfuscations. In the case of software plagiarism, emerging obfuscation techniques have made automated detection increasingly difficult. In this paper, we propose a binary-oriented, obfuscation-resilient method based on a new concept, longest common subsequence of semantically equivalent basic blocks, which combines rigorous program semantics with longest common subsequence based fuzzy matching. We model the semantics of a basic block by a set of symbolic formulas representing the input-output relations of the block. This way, the semantics equivalence (and similarity) of two blocks can be checked by a theorem prover. We then model the semantics similarity of two paths using the longest common subsequence with basic blocks as elements. This novel combination has resulted in strong resiliency to code obfuscation. We have developed a prototype and our experimental results show that our method is effective and practical when applied to real-world software. </p>",,,10.1145/2635868.2635900,,,Software plagiarism detection;binary code similarity comparison;obfuscation;symbolic execution;theorem proving,,,,,14,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Sherlock: scalable deadlock detection for concurrent programs,M. Eslamimehr; J. Palsberg,"University of California at Los Angeles, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,353,365,"<p> We present a new technique to find real deadlocks in concurrent programs that use locks. For 4.5 million lines of Java, our technique found almost twice as many real deadlocks as four previous techniques combined. Among those, 33 deadlocks happened after more than one million computation steps, including 27 new deadlocks. We first use a known technique to find 1275 deadlock candidates and then we determine that 146 of them are real deadlocks. Our technique combines previous work on concolic execution with a new constraint-based approach that iteratively drives an execution towards a deadlock candidate. </p>",,,10.1145/2635868.2635918,,,Concurrency;deadlocks,,,,,5,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
"Detecting, isolating, and enforcing dependencies among and within test cases",J. Bell,"Columbia University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,799,802,"<p> Testing stateful applications is challenging, as it can be difficult to identify hidden dependencies on program state. These dependencies may manifest between several test cases, or simply within a single test case. When it's left to developers to document, understand, and respond to these dependencies, a mistake can result in unexpected and invalid test results. Although current testing infrastructure does not currently leverage state dependency information, we argue that it could, and that by doing so testing can be improved. Our results thus far show that by recovering dependencies between test cases and modifying the popular testing framework, JUnit, to utilize this information, we can optimize the testing process, reducing time needed to run tests by 62% on average. Our ongoing work is to apply similar analyses to improve existing state of the art test suite prioritization techniques and state of the art test case generation techniques. </p>",,,10.1145/2635868.2666597,,,Testing;program analysis;test dependencies,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
An empirical study on program comprehension with reactive programming,G. Salvaneschi; S. Amann; S. Proksch; M. Mezini,"TU Darmstadt, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,564,575,"<p> Starting from the first investigations with strictly functional languages, reactive programming has been proposed as THE programming paradigm for reactive applications. The advantages of designs based on this style over designs based on the Observer design pattern have been studied for a long time. Over the years, researchers have enriched reactive languages with more powerful abstractions, embedded these abstractions into mainstream languages Š—– including object-oriented languages Š—– and applied reactive programming to several domains, like GUIs, animations, Web applications, robotics, and sensor networks. However, an important assumption behind this line of research Š—– that, beside other advantages, reactive programming makes a wide class of otherwise cumbersome applications more comprehensible Š—– has never been evaluated. In this paper, we present the design and the results of the first empirical study that evaluates the effect of reactive programming on comprehensibility compared to the traditional object-oriented style with the Observer design pattern. Results confirm the conjecture that comprehensibility is enhanced by reactive programming. In the experiment, the reactive programming group significantly outperforms the other group. </p>",,,10.1145/2635868.2635895,,,Controlled Experiment;Empirical Study;Reactive Programming,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Developers' code context models for change tasks,T. Fritz; D. C. Shepherd; K. Kevic; W. Snipes; C. Br’_unlich,"University of Zurich, Switzerland",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,7,18,"<p> To complete a change task, software developers spend a substantial amount of time navigating code to understand the relevant parts. During this investigation phase, they implicitly build context models of the elements and relations that are relevant to the task. Through an exploratory study with twelve developers completing change tasks in three open source systems, we identified important characteristics of these context models and how they are created. In a second empirical analysis, we further examined our findings on data collected from eighty developers working on a variety of change tasks on open and closed source projects. Our studies uncovered, amongst other results, that code context models are highly connected, structurally and lexically, that developers start tasks using a combination of search and navigation and that code navigation varies substantially across developers. Based on these findings we identify and discuss design requirements to better support developers in the initial creation of code context models. We believe this work represents a substantial step in better understanding developers' code navigation and providing better tool support that will reduce time and effort needed for change tasks. </p>",,,10.1145/2635868.2635905,,,Context models;change task;navigation;search;user study,,,,,7,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
"Enablers, inhibitors, and perceptions of testing in novice software teams",R. Pham; S. Kiesling; O. Liskin; L. Singer; K. Schneider,"Leibniz Universit&#228;t Hannover, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,30,40,"<p> There are many different approaches to testing software, with different benefits for software quality and the development process. Yet, it is not well understood what developers struggle with when getting started with testing - and why some do not test at all or not as much as would be good for their project. This missing understanding keeps us from improving processes and tools to help novices adopt proper testing practices. We conducted a qualitative study with 97 computer science students. Through interviews, we explored their experiences and attitudes regarding testing in a collaborative software project. We found enabling and inhibiting factors for testing activities, the different testing strategies they used, and novicesŠ—È perceptions and attitudes of testing. Students push test automation to the end of the project, thus robbing themselves from the advantages of having a test suite during implementation. Students were not convinced of the return of investment in automated tests and opted for laborious manual tests - which they often regretted in the end. Understanding such challenges and opportunities novices face when confronted with adopting testing can help us improve testing processes, company policies, and tools. Our findings provide recommendations that can enable organizations to facilitate the adoption of testing practices by their members. </p>",,,10.1145/2635868.2635925,,,Adoption;Enablers;Inhibitors;Motivation;Testing,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
BugLocalizer: integrated tool support for bug localization,F. Thung; T. D. B. Le; P. S. Kochhar; D. Lo,"Singapore Management University, Singapore",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,767,770,"<p> To manage bugs that appear in a software, developers often make use of a bug tracking system such as Bugzilla. Users can report bugs that they encounter in such a system. Whenever a user reports a new bug report, developers need to read the summary and description of the bug report and manually locate the buggy files based on this information. This manual process is often time consuming and tedious. Thus, a number of past studies have proposed bug localization techniques to automatically recover potentially buggy files from bug reports. Unfortunately, none of these techniques are integrated to bug tracking systems and thus it hinders their adoption by practitioners. To help disseminate research in bug localization to practitioners, we develop a tool named BugLocalizer, which is implemented as a Bugzilla extension and builds upon a recently proposed bug localization technique. Our tool extracts texts from summary and description fields of a bug report and source code files. It then computes similarities of the bug report with source code files to find the buggy files. Developers can use our tool online from a Bugzilla web interface by providing a link to a git source code repository and specifying the version of the repository to be analyzed. We have released our tool publicly in GitHub, which is available at: https://github.com/smagsmu/buglocalizer. We have also provided a demo video, which can be accessed at: http://youtu.be/iWHaLNCUjBY. </p>",,,10.1145/2635868.2661678,,,Bug localization;Bugzilla;git,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Grail: context-aware fixing of concurrency bugs,P. Liu; O. Tripp; C. Zhang,"Wuhan University, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,318,329,"<p> Writing efficient synchronization for multithreaded programs is notoriously hard. The resulting code often contains subtle concurrency bugs. Even worse, many bug fixes introduce new bugs. A classic example, seen widely in practice, is deadlocks resulting from fixing of an atomicity violation. These complexities have motivated the development of automated fixing techniques. Current techniques generate fixes that are typically conservative, giving up on available parallelism. Moreover, some of the techniques cannot guarantee the correctness of a fix, and may introduce deadlocks similarly to manual fix, whereas techniques that ensure correctness do so at the expense of even greater performance loss. We present Grail, a novel fixing algorithm that departs from previous techniques by simultaneously providing both correctness and optimality guarantees. Grail synthesizes bug-free yet optimal lock-based synchronization. To achieve this, Grail builds an analysis model of the buggy code that is both contextual, distinguishing different aliasing contexts to ensure efficiency, and global, accounting for the entire synchronization behavior of the involved threads to ensure correctness. Evaluation of Grail on 12 bugs from popular codebases confirms its practical advantages, especially compared with existing techniques: Grail patches are, in general, >=40% more efficient than the patches produced by other techniques, and incur only 2% overhead. </p>",,,10.1145/2635868.2635881,,,Context-Aware fixing;concurrency bugs,,,,,4,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
ConceptCloud: a tagcloud browser for software archives,G. J. Greene; B. Fischer,"Stellenbosch University, South Africa",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,759,762,"<p> ConceptCloud is an interactive browser for SVN and Git repositories. Its main novelty is the combination of an intuitive tag cloud interface with an underlying concept lattice that provides a formal structure for navigation. This combination allows users to explore repositories serendipitously, without predefined search goals and along different navigation paths. ConceptCloud can derive different lattice types for a repository and supports concurrent navigation in multiple linked tag clouds that can each be individually customized, which allows multi-faceted repository explorations. </p>",,,10.1145/2635868.2661676,,,Browsing;Formal concept analysis;Software repositories;Tag clouds,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
How should we measure functional sameness from program source code? an exploratory study on Java methods,Y. Higo; S. Kusumoto,"Osaka University, Japan",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,294,305,"<p> Program source code is one of the main targets of software engineering research. A wide variety of research has been conducted on source code, and many studies have leveraged structural, vocabulary, and method signature similarities to measure the functional sameness of source code. In this research, we conducted an empirical study to ascertain how we should use three similarities to measure functional sameness. We used two large datasets and measured the three similarities between all the method pairs in the datasets, each of which included approximately 15 million Java method pairs. The relationships between the three similarities were analyzed to determine how we should use each to detect functionally similar code. The results of our study revealed the following. (1) Method names are not always useful for detecting functionally similar code. Only if there are a small number of methods having a given name, the methods are likely to include functionally similar code. (2) Existing file-level, method-level, and block-level clone detection techniques often miss functionally similar code generated by copy-and-paste operations between different projects. (3) In the cases we use structural similarity for detecting functionally similar code, we obtained many false positives. However, we can avoid detecting most false positives by using a vocabulary similarity in addition to a structural one. (4) Using a vocabulary similarity to detect functionally similar code is not suitable for method pairs in the same file because such method pairs use many of the same program elements such as private methods or private fields. </p>",,,10.1145/2635868.2635886,,,Clone Detection;Functionally similar code;Method name similarity;Structural similarity;Vocabulary similarity,,,,,4,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Dealing with uncertainty in verification of nondeterministic systems,Y. R. S. Llerena,"National University of Singapore, Singapore",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,787,790,"<p> Uncertainty complicates the formal verification of nondeterministic systems. Unpredictable changes and alterations in their environments can lead an invalid verification results and the decrease of confidence degree of these systems. However, current literature provides little account of addressing the uncertainty in formal verification. To address this problem, the goal of this research is to provide a method based on perturbation analysis for probabilistic model checking of nondeterministic systems which are modelled as Markov Decision Processes. And to apply our expected contributions to ubiquitous systems due to inherent presence of environment uncertainty and their resource limitations. </p>",,,10.1145/2635868.2666598,,,Markov Decision Processes;Perturbation Analysis;Probabilistic Model Checking;Uncertainty,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Methodology and culture: drivers of mediocrity in software engineering?,M. Petre; D. Damian,"Open University, UK",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,829,832,"<p> Methodology implementation failure is attributed to developer mediocrity (by management) Š—– not to organizational mediocrity (rigidity or control-driven, process-driven management), or to a lack of adaptation capability in the methodology. In supporting software construction as a creative process, however, we must promote excellence rather than conformity. We argue that we Š—– through principled research -- must pay attention to the interplay between methodology and culture Š—– the local adaptations needed to make things work, understand how the two co-evolve and how they may contribute together to software quality. </p>",,,10.1145/2635868.2666607,,,Methodology;culture;software engineering,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
RefDistiller: a refactoring aware code review tool for inspecting manual refactoring edits,E. L. G. Alves; M. Song; M. Kim,"University of Texas at Austin, USA / Federal University of Campina Grande, Brazil",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,751,754,"<p> Manual refactoring edits are error prone, as refactoring requires developers to coordinate related transformations and understand the complex inter-relationship between affected types, methods, and variables. We present RefDistiller, a refactoring-aware code review tool that can help developers detect potential behavioral changes in manual refactoring edits. It first detects the types and locations of refactoring edits by comparing two program versions. Based on the reconstructed refactoring information, it then detects potential anomalies in refactoring edits using two techniques: (1) a template-based checker for detecting missing edits and (2) a refactoring separator for detecting extra edits that may change a program's behavior. By helping developers be aware of deviations from pure refactoring edits, RefDistiller can help developers have high confidence about the correctness of manual refactoring edits. RefDistiller is available as an Eclipse plug-in at https://sites.google.com/site/refdistiller/ and its demonstration video is available at http://youtu.be/0Iseoc5HRpU. </p>",,,10.1145/2635868.2661674,,,Software evolution;refactoring,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Software maintenance like maintenance in other engineering disciplines,G. Villavicencio,"Universidad Cat&#243;lica de Santiago del Estero, Argentina",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,853,856,"<p> abstract Software maintenance exhibits many differences regarding how other engineering disciplines carry out maintenance on their artifacts. Such dissimilarity is caused due to the fact that it is easy to get a copy from the original artifact to be used in maintenance, and also because the flat dimension of the software text facilitates access to the components by simply using a text editor. Other engineering disciplines resort to different artifact versions (obtained by dissassembling) where the introduction of modifications (previous comprehension) is easier. After which the artifact is reassembled. In software engineering this approach can be simulated by combining program transformation techniques, search-based software engineering technology and design attributes. %%This easiness (absent in the other engineering sciences) as well as the intangible software nature can lead to the belief %%that a software maintenance model similar to those of the other engineering sciences is unnecessary or unfeasible. %%This paper states the opposite, and as a consequence, an entirely new and more robust software maintenance model emerges. abstract </p>",,,10.1145/2635868.2666613,,,Software maintenance;refactoring;software comprehension,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Sketches and diagrams in practice,S. Baltes; S. Diehl,"University of Trier, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,530,541,"<p> Sketches and diagrams play an important role in the daily work of software developers. In this paper, we investigate the use of sketches and diagrams in software engineering practice. To this end, we used both quantitative and qualitative methods. We present the results of an exploratory study in three companies and an online survey with 394 participants. Our participants included software developers, software architects, project managers, consultants, as well as researchers. They worked in different countries and on projects from a wide range of application areas. Most questions in the survey were related to the last sketch or diagram that the participants had created. Contrary to our expectations and previous work, the majority of sketches and diagrams contained at least some UML elements. However, most of them were informal. The most common purposes for creating sketches and diagrams were designing, explaining, and understanding, but analyzing requirements was also named often. More than half of the sketches and diagrams were created on analog media like paper or whiteboards and have been revised after creation. Most of them were used for more than a week and were archived. We found that the majority of participants related their sketches to methods, classes, or packages, but not to source code artifacts with a lower level of abstraction. </p>",,,10.1145/2635868.2635891,,,Diagrams;Empirical Study;Sketches;Source Code Artifacts,,,,,8,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
On the localness of software,Z. Tu; Z. Su; P. Devanbu,"University of California at Davis, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,269,280,"<p> The n-gram language model, which has its roots in statistical natural language processing, has been shown to successfully capture the repetitive and predictable regularities (Š—“naturalness"") of source code, and help with tasks such as code suggestion, porting, and designing assistive coding devices. However, we show in this paper that this natural-language-based model fails to exploit a special property of source code: localness. We find that human-written programs are localized: they have useful local regularities that can be captured and exploited. We introduce a novel cache language model that consists of both an n-gram and an added Š—“cache"" component to exploit localness. We show empirically that the additional cache component greatly improves the n-gram approach by capturing the localness of software, as measured by both cross-entropy and suggestion accuracy. Our modelŠ—Ès suggestion accuracy is actually comparable to a state-of-the-art, semantically augmented language model; but it is simpler and easier to implement. Our cache language model requires nothing beyond lexicalization, and thus is applicable to all programming languages. </p>",,,10.1145/2635868.2635875,,,Cache Language Model;Code Suggestion;Localness,,,,,17,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Solving complex path conditions through heuristic search on induced polytopes,P. Dinges; G. Agha,"University of Illinois at Urbana-Champaign, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,425,436,"<p> Test input generators using symbolic and concolic execution must solve path conditions to systematically explore a program and generate high coverage tests. However, path conditions may contain complicated arithmetic constraints that are infeasible to solve: a solver may be unavailable, solving may be computationally intractable, or the constraints may be undecidable. Existing test generators either simplify such constraints with concrete values to make them decidable, or rely on strong but incomplete constraint solvers. Unfortunately, simplification yields coarse approximations whose solutions rarely satisfy the original constraint. Moreover, constraint solvers cannot handle calls to native library methods. We show how a simple combination of linear constraint solving and heuristic search can overcome these limitations. We call this technique Concolic Walk. On a corpus of 11 programs, an instance of our Concolic Walk algorithm using tabu search generates tests with two- to three-times higher coverage than simplification-based tools while being up to five-times as efficient. Furthermore, our algorithm improves the coverage of two state-of-the-art test generators by 21% and 32%. Other concolic and symbolic testing tools could integrate our algorithm to solve complex path conditions without having to sacrifice any of their own capabilities, leading to higher overall coverage. </p>",,,10.1145/2635868.2635889,,,Concolic Testing;Local Search;Non-Linear Constraints,,,,,4,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Detecting and preventing the architectural roots of bugs,L. Xiao,"Drexel University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,811,813,"<p> Numerous techniques have been proposed to locate buggy files in a code base, but the problem of fixing one bug unexpectedly affecting other files is persistent and prevailing. Our recent study revealed that buggy files are usually architecturally connected by architecture issues such as unstable interfaces and modularity violations. We aim to detect and prevent these architecture issues that are the root causes of defects. Our contributions include (1) a new architecture model, Design Rule Space (DRSpace), that can express structural relations, quality, and evolutionary information simultaneously; (2) a method of automatically extracting defect-prone architecture roots by combining static architecture analysis with software revision history data mining. The preliminary application of our approach to dozens of open source and industry projects has demonstrated its significant potential to inform developers about how software defects should be discovered, examined, and handled. </p>",,,10.1145/2635868.2661679,,,Architecture Recovery;Software Architecture;Software Quality,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
EvoDroid: segmented evolutionary testing of Android apps,R. Mahmood; N. Mirzaei; S. Malek,"George Mason University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,599,609,"<p> Proliferation of Android devices and apps has created a demand for applicable automated software testing techniques. Prior research has primarily focused on either unit or GUI testing of Android apps, but not their end-to-end system testing in a systematic manner. We present EvoDroid, an evolutionary approach for system testing of Android apps. EvoDroid overcomes a key shortcoming of using evolutionary techniques for system testing, i.e., the inability to pass on genetic makeup of good individuals in the search. To that end, EvoDroid combines two novel techniques: (1) an Android-specific program analysis technique that identifies the segments of the code amenable to be searched independently, and (2) an evolutionary algorithm that given information of such segments performs a step-wise search for test cases reaching deep into the code. Our experiments have corroborated EvoDroidŠ—Ès ability to achieve significantly higher code coverage than existing Android testing tools. </p>",,,10.1145/2635868.2635896,,,Android;Evolutionary Testing;Program Analysis,,,,,26,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Numerical program analysis and testing,Z. Gao,"University College London, UK",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,779,782,"<p> Numerical software is playing an increasingly critical role in modern society, but composing correct numerical programs is difficult. This paper describes a doctoral research program that aims to alleviate this issue. It tackles real world problems and is guided by features learned from empirically studying these programs. By assisting developers in the production of numerical software, it improves the quality and productivity of software development. The research depends on numerical analysis and lies in the intersection of software engineering and program analysis. </p>",,,10.1145/2635868.2666603,,,Numerical program;real encoding;static analysis;testing,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
FlowTwist: efficient context-sensitive inside-out taint analysis for large codebases,J. Lerch; B. Hermann; E. Bodden; M. Mezini,"TU Darmstadt, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,98,108,"<p> Over the past years, widely used platforms such as the Java Class Library have been under constant attack through vulnerabilities that involve a combination of two taint-analysis problems: an integrity problem allowing attackers to trigger sensitive operations within the platform, and a confidentiality problem allowing the attacker to retrieve sensitive information or pointers from the results of those operations. While existing static taint analyses are good at solving either of those problems, we show that they scale prohibitively badly when being applied to situations that require the exploitation of both an integrity and confidentiality problem in combination. The main problem is the huge attack surface of libraries such as the Java Class Library, which exposes thousands of methods potentially controllable by an attacker. In this work we thus present FlowTwist, a novel taint-analysis approach that works inside-out, i.e., tracks data flows from potentially vulnerable calls to the outer level of the API which the attacker might control. This inside-out analysis requires a careful, context-sensitive coordination of both a backward and a forward taint analysis. In this work, we expose a design of the analysis approach based on the IFDS algorithm, and explain several extensions to IFDS that enable not only this coordination but also a helpful reporting of error situations to security analysts. Experiments with the Java Class Library show that, while a simple forward taint-analysis approach does not scale even with much machine power, FlowTwist's algorithm is able to fully analyze the library within 10 minutes. </p>",,,10.1145/2635868.2635878,,,IFDS;Taint analysis;confused deputy,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Towards a theory of architectural styles,D. Marmsoler,"TU M&#252;nchen, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,823,825,"<p> Architectural styles and patterns play an important role in software architectures. However, they are usually only stated informally, which may cause problems such as ambiguity and wrong conclusions. A rigorous theory of architectural styles --- consisting of (i) mathematical models for each style; (ii) axioms to identify different variants of a style; and (iii) rigorous analyses by means of mathematical proofs --- would address these problems. With this work we report on our progress towards such a rigorous theory of architectural styles. </p>",,,10.1145/2635868.2661683,,,Architecture Styles;Denotational Semantics;Distributed System Models;Software Engineering Theory,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
No issue left behind: reducing information overload in issue tracking,O. Baysal; R. Holmes; M. W. Godfrey,"Universit&#233; de Montr&#233;al, Canada",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,666,677,"<p> Modern software development tools such as issue trackers are often complex and multi-purpose tools that provide access to an immense amount of raw information. Unfortunately, developers sometimes feel frustrated when they cannot easily obtain the particular information they need for a given task; furthermore, the constant influx of new data Š—” the vast majority of which is irrelevant to their task at hand Š—” may result in issues being ""dropped on the floor"". In this paper, we present a developer-centric approach to issue tracking that aims to reduce information overload and improve developers' situational awareness. Our approach is motivated by a grounded theory study of developer comments, which suggests that customized views of a project's repositories that are tailored to developer-specific tasks can help developers better track their progress and understand the surrounding technical context. From the qualitative study, we uncovered a model of the kinds of information elements that are essential for developers in completing their daily tasks, and from this model we built a tool organized around customized issue-tracking dashboards. Further quantitative and qualitative evaluation demonstrated that this dashboard-like approach to issue tracking can reduce the volume of irrelevant emails by over 99% and also improve support for specific issue-tracking tasks. </p>",,,10.1145/2635868.2635887,,,Developer dashboards;information needs;issue tracking;personalization;situational awareness,,,,,5,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Automatic generation of release notes,L. Moreno; G. Bavota; M. Di Penta; R. Oliveto; A. Marcus; G. Canfora,"University of Texas at Dallas, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,484,495,"<p> This paper introduces ARENA (Automatic RElease Notes generAtor), an approach for the automatic generation of release notes. ARENA extracts changes from the source code, summarizes them, and integrates them with information from versioning systems and issue trackers. It was designed based on the manual analysis of 1,000 existing release notes. To evaluate the quality of the ARENA release notes, we performed three empirical studies involving a total of 53 participants (45 professional developers and 8 students). The results indicate that the ARENA release notes are very good approximations of those produced by developers and often include important information that is missing in the manually produced release notes. </p>",,,10.1145/2635868.2635870,,,Release notes;Software documentation;Software evolution,,,,,15,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Mining preconditions of APIs in large-scale code corpus,H. A. Nguyen; R. Dyer; T. N. Nguyen; H. Rajan,"Iowa State University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,166,177,"<p> Modern software relies on existing application programming interfaces (APIs) from libraries. Formal specifications for the APIs enable many software engineering tasks as well as help developers correctly use them. In this work, we mine large-scale repositories of existing open-source software to derive potential preconditions for API methods. Our key idea is that APIsŠ—È preconditions would appear frequently in an ultra-large code corpus with a large number of API usages, while project-specific conditions will occur less frequently. First, we find all client methods invoking APIs. We then compute a control dependence relation from each call site and mine the potential conditions used to reach those call sites. We use these guard conditions as a starting point to automatically infer the preconditions for each API. We analyzed almost 120 million lines of code from SourceForge and Apache projects to infer preconditions for the standard Java Development Kit (JDK) library. The results show that our technique can achieve high accuracy with recall from 75Š—–80% and precision from 82Š—–84%. We also found 5 preconditions missing from human written specifications. They were all confirmed by a specification expert. In a user study, participants found 82% of the mined preconditions as a good starting point for writing specifications. Using our mining result, we also built a benchmark of more than 4,000 precondition-related bugs. </p>",,,10.1145/2635868.2635924,,,Big Code Mining;JML;Preconditions;Specification Mining,,,,,5,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Efficient runtime-enforcement techniques for policy weaving,R. Joiner; T. Reps; S. Jha; M. Dhawan; V. Ganapathy,"University of Wisconsin-Madison, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,224,234,"<p> Policy weaving is a program-transformation technique that rewrites a program so that it is guaranteed to be safe with respect to a stateful security policy. It utilizes (i) static analysis to identify points in the program at which policy violations might occur, and (ii) runtime checks inserted at such points to monitor policy state and prevent violations from occurring. The promise of policy weaving stems from the possibility of blending the best aspects of static and dynamic analysis components. Therefore, a successful instantiation of policy weaving requires a careful balance and coordination between the two. In this paper, we examine the strategy of using a combination of transactional introspection and statement indirection to implement runtime enforcement in a policy-weaving system. Transactional introspection allows the state resulting from the execution of a statement to be examined and, if the policy would be violated, suppressed. Statement indirection serves as a light-weight runtime analysis that can recognize and instrument dynamically generated code that is not available to the static analysis. These techniques can be implemented via static rewriting so that all possible program executions are protected against policy violations. We describe our implementation of transactional introspection and statement indirection for policy weaving, and report experimental results that show the viability of the approach in the context of real-world JavaScript programs executing in a browser. </p>",,,10.1145/2635868.2635907,,,Security policy enforcement;dynamic runtime verification;speculative execution;statement indirection;transactional introspection,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Linking sketches and diagrams to source code artifacts,S. Baltes; P. Schmitz; S. Diehl,"University of Trier, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,743,746,"<p> Recent studies have shown that sketches and diagrams play an important role in the daily work of software developers. If these visual artifacts are archived, they are often detached from the source code they document, because there is no ad- equate tool support to assist developers in capturing, archiving, and retrieving sketches related to certain source code artifacts. This paper presents SketchLink, a tool that aims at increasing the value of sketches and diagrams created during software development by supporting developers in these tasks. Our prototype implementation provides a web application that employs the camera of smartphones and tablets to capture analog sketches, but can also be used on desktop computers to upload, for instance, computer-generated diagrams. We also implemented a plugin for a Java IDE that embeds the links in Javadoc comments and visualizes them in situ in the source code editor as graphical icons. </p>",,,10.1145/2635868.2661672,,,Diagrams;Documentation;Sketches;Source Code Artifacts,,,,,3,1,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Mining micro-practices from operational data,M. Zhou; A. Mockus,"Peking University, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,845,848,"<p> Micro-practices are actual (and usually undocumented or incorrectly documented) activity patterns used by individuals or projects to accomplish basic software development tasks, such as writing code, testing, triaging bugs, or mentoring newcomers. The operational data in software repositories presents the tantalizing possibility to discover such fine-scale behaviors and use them to understand and improve software development. We propose a large-scale evidence-based approach to accomplish this by first creating a mirror of the projects in the open source universe. The next step would involve the inductive generalization from in-depth studies of specific projects from one side and the categorization of micro-practices in the entire universe from the other side. </p>",,,10.1145/2635868.2666611,,,fine-scale activity pattern;micro-practice;operational data,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
CHOReOSynt: enforcing choreography realizability in the future internet,M. Autili; D. Di Ruscio; A. Di Salle; A. Perucci,"University of L&#039;Aquila, Italy",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,723,726,"<p> Choreographies are an emergent Service Engineering (SE) approach to compose together and coordinate services in a distributed way. A choreography formalizes the way business participants coordinate their interactions. The focus is not on orchestrations of the work performed within them, but rather on the exchange of messages between these participants. The problems usually addressed when considering a choreography-based specification of the system to be realized are realizability check, and conformance check. In this paper we describe the CHOReOSynt tool, which has been conceived to deal with an additional problem, namely, automated choreography enforcement. That is, when the goal is to actually realize a service choreography by reusing third-party services, their uncontrolled (or wrongly coordinated) composite behavior may show undesired interactions that preclude the choreography realization. CHOReOSynt solves this problem by automatically synthesizing additional software entities that, when interposed among the services, allow for preventing undesired interactions. Screencast: http://choreos.disim.univaq.it/downloads/ </p>",,,10.1145/2635868.2661667,,,Choreography Synthesis;Distributed Coordination,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Automatically generated patches as debugging aids: a human study,Y. Tao; J. Kim; S. Kim; C. Xu,"Hong Kong University of Science and Technology, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,64,74,"<p> Recent research has made significant progress in automatic patch generation, an approach to repair programs with less or no manual intervention. However, direct deployment of auto-generated patches remains difficult, for reasons such as patch quality variations and developers' intrinsic resistance. In this study, we take one step back and investigate a more feasible application scenario of automatic patch generation, that is, using generated patches as debugging aids. We recruited 95 participants for a controlled experiment, in which they performed debugging tasks with the aid of either buggy locations (i.e., the control group), or generated patches of varied qualities. We observe that: a) high-quality patches significantly improve debugging correctness; b) such improvements are more obvious for difficult bugs; c) when using low-quality patches, participants' debugging correctness drops to an even lower point than that of the control group; d) debugging time is significantly affected not by debugging aids, but by participant type and the specific bug to fix. These results highlight that the benefits of using generated patches as debugging aids are contingent upon the quality of the patches. Our qualitative analysis of participants' feedback further sheds light on how generated patches can be improved and better utilized as debugging aids. </p>",,,10.1145/2635868.2635873,,,Debugging;automatic patch generation;human study,,,,,4,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Querying sequential software engineering data,C. Sun; H. Zhang; J. G. Lou; H. Zhang; Q. Wang; D. Zhang; S. C. Khoo,"University of California at Davis, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,700,710,"<p> We propose a pattern-based approach to effectively and efficiently analyzing sequential software engineering (SE) data. Different from other types of SE data, sequential SE data preserves unique temporal properties, which cannot be easily analyzed without much programming effort. In order to facilitate the analysis of sequential SE data, we design a sequential pattern query language (SPQL), which specifies the temporal properties based on regular expressions, and is enhanced with variables and statements to store and manipulate matching states. We also propose a query engine to effectively process the SPQL queries. We have applied our approach to analyze two types of SE data, namely bug report history and source code change history. We experiment with 181,213 Eclipse bug reports and 323,989 code revisions of Android. SPQL enables us to explore interesting temporal properties underneath these sequential data with a few lines of query code and low matching overhead. The analysis results can help better under- stand a software process and identify process violations. </p>",,,10.1145/2635868.2635902,,,mining software repository;pattern matching;query;sequential data,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Experiences developing tools for developers (invited talk),J. Penix,"Google, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,4,4,"<p> Software Engineers are horrible customers for software tools. If they don't like your tools, they will just write their own. If your tool wastes a few minutes of a developer's day, good luck getting them to ever try your tool again. And if, after years of effort, you manage to develop tools they actually like, you are really in trouble. This is when they start building systems on top of your tools. No API? No problem! They will hack and scrape as needed to get their job done. In this talk I'll go through a number of examples of successes, non-successes and over-successes from the past 8 years of evolving the developer infrastructure at Google. I'll highlight the challenges we faced, our attempts to address the challenges and share our lessons learned. </p>",,,10.1145/2635868.2684429,,,Tool Development,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Beyond the rainbow: self-adaptive failure avoidance in configurable systems,J. Swanson; M. B. Cohen; M. B. Dwyer; B. J. Garvin; J. Firestone,"University of Nebraska-Lincoln, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,377,388,"<p> Self-adaptive software systems monitor their state and then adapt when certain conditions are met, guided by a global utility function. In prior work we developed algorithms and conducted a post-hoc analysis demonstrating the possibility of adapting to software failures by judiciously changing configurations. In this paper we present the REFRACT framework that realizes this idea in practice by building on the self-adaptive Rainbow architecture. REFRACT extends Rainbow with new components and algorithms targeting failure avoidance. We use REFRACT in a case study running four independently executing Firefox clients with 36 passing test cases and 7 seeded faults. The study show that workarounds for all but one of the seeded faults are found and the one that is not found never fails -- it is guarded from failing by a related workaround. Moreover, REFRACT finds workarounds for eight configuration-related unseeded failures from tests that were expected to pass (and did under the default configuration). Finally, the data show that when a failure and its workaround are found, configuration guards prevent the failure from appearing again. In a simulation lasting 24 hours we see over 150 guard activations and no failures with workarounds remaining beyond 16 hours. </p>",,,10.1145/2635868.2635915,,,Configurable Software;Self-Adaptive Software,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Retrofitting concurrency for Android applications through refactoring,Y. Lin; C. Radoi; D. Dig,"University of Illinois at Urbana-Champaign, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,341,352,"<p> Running compute-intensive or blocking I/O operations in the UI event thread of smartphone apps can severely degrade responsiveness. Despite the fact that Android supports writing concurrent code via AsyncTask, we know little about how developers use AsyncTask to improve responsiveness. To understand how AsyncTask is used/underused/misused in practice, we rst conduct a formative study using a corpus of top 104 most popular open-source Android apps comprising 1.34M SLOC. Our study shows that even though half of the apps use AsyncTask, there are hundreds of places where they missed opportunities to encapsulate long-running operations in AsyncTask. Second, 46% of the usages are manually refactored. However, the refactored code contains concurrency bugs (such as data races) and performance bugs (concurrent code still executes sequentially). Inspired by these ndings, we designed, developed, and evaluated Asynchronizer, an automated refactoring tool that enables developers to extract long-running operations into AsyncTask. Asynchronizer uses a points-to static analysis to determine the safety of the transformation. Our empirical evaluation shows that Asynchronizer is (i) highly applicable, (ii) accurate, (iii) safer than manual refactoring (iv) it saves development eort, (v) its results have been accepted by the open-source developers. This shows that Asynchronizer is useful. </p>",,,10.1145/2635868.2635903,,,Android;AsyncTask;Asynchrony,,,,,12,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Autonomous compliance monitoring of non-functional properties,M. Br’_nink,"National University of Singapore, Singapore",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,795,798,"<p> While there is a good understanding of functional requirements and the need to test them during development, non-functional requirements are more elusive. Defining non-functional requirements can end up in a major undertaking consuming significant resources. Even after defining non-functional requirements, chances are they do not reflect the real-world usage of a deployed system. Differences can occur due to workload, hardware, or utilised third-party libraries. To tackle these challenges we propose a fully automatic compliance monitoring solution for non-functional properties. The proposed system mines stable behavioural patterns of the system and automatically extracts assertions that can be used to detect deviations of expected non-functional behaviour. We especially focus on non-functional properties that require runtime observation, e.g. execution time, performance, throughput.The full automation of the process enables a deployment in the field, giving rise to a distributed non-functional behaviour extraction system. </p>",,,10.1145/2635868.2666599,,,Non-functional requirements;behavioural patterns;specification mining,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Variable-specific resolutions for feature interactions,C. Bocovich; J. M. Atlee,"University of Waterloo, Canada",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,553,563,"<p> Systems assembled from independently developed features suffer from feature interactions, in which features affect one another's behaviour in surprising ways. The feature-interaction problem states that the number of potential interactions is exponential in the number of features in a system. Resolution strategies offer general strategies that resolve entire classes of interactions, thereby reducing the work of the developer who is charged with the task of resolving interactions. In this paper, we focus on resolving interactions due to conflict. We present an approach, language, and implementation based on resolution modules in which the developer can specify an appropriate resolution for each variable under conflict. We performed a case study involving 24 automotive features, and found that the number of resolutions to be specified was much smaller than the number of possible feature interactions (6 resolutions for 24 features), that what constitutes an appropriate resolution strategy is different for different variables, and that the subset of situation calculus we used was sufficient to construct nontrivial resolution strategies for six distinct output variables. </p>",,,10.1145/2635868.2635927,,,conflict resolution;feature interaction,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Improving oracle quality by detecting brittle assertions and unused inputs in tests,C. Huo; J. Clause,"University of Delaware, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,621,631,"<p> Writing oracles is challenging. As a result, developers often create oracles that check too little, resulting in tests that are unable to detect failures, or check too much, resulting in tests that are brittle and difficult to maintain. In this paper we present a new technique for automatically analyzing test oracles. The technique is based on dynamic tainting and detects both brittle assertionsŠ—”assertions that depend on values that are derived from uncontrolled inputsŠ—”and unused inputsŠ—”inputs provided by the test that are not checked by an assertion. We also presented OraclePolish, an implementation of the technique that can analyze tests that are written in Java and use the JUnit testing framework. Using OraclePolish, we conducted an empirical evaluation of more than 4000 real test cases. The results of the evaluation show that OraclePolish is effective; it detected 164 tests that contain brittle assertions and 1618 tests that have unused inputs. In addition, the results also demonstrate that the costs associated with using the technique are reasonable. </p>",,,10.1145/2635868.2635917,,,Brittleness;Dynamic tainting;Improving oracles;Mutation;Unit testing;Unused inputs,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
SAFEWAPI: web API misuse detector for web applications,S. Bae; H. Cho; I. Lim; S. Ryu,"KAIST, South Korea",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,507,517,"<p> The evolution of Web 2.0 technologies makes web applications prevalent in various platforms including mobile devices and smart TVs. While one of the driving technologies of web applications is JavaScript, the extremely dynamic features of JavaScript make it very difficult to define and detect errors in JavaScript applications. The problem becomes more important and complicated for JavaScript web applications which may lead to severe security vulnerabilities. To help developers write safe JavaScript web applications using vendor-specific Web APIs, vendors specify their APIs often in Web IDL, which enables both API writers and users to communicate better by understanding the expected behaviors of the Web APIs. In this paper, we present SAFEWAPI, a tool to analyze Web APIs and JavaScript web applications that use the Web APIs and to detect possible misuses of Web APIs by the web applications. Even though the JavaScript language semantics allows to call a function defined with some parameters without any arguments, platform developers may require application writers to provide the exact number of arguments. Because the library functions in Web APIs expose their intended semantics clearly to web application developers unlike pure JavaScript functions, we can detect wrong uses of Web APIs precisely. For representative misuses of Web APIs defined by software quality assurance engineers, our SAFEWAPI detects such misuses in real-world JavaScript web applications. </p>",,,10.1145/2635868.2635916,,,JavaScript;bug detection;static analysis;web application,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Balancing trade-offs in test-suite reduction,A. Shi; A. Gyori; M. Gligoric; A. Zaytsev; D. Marinov,"University of Illinois at Urbana-Champaign, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,246,256,"<p> Regression testing is an important activity but can get expensive for large test suites. Test-suite reduction speeds up regression testing by identifying and removing redundant tests based on a given set of requirements. Traditional research on test-suite reduction is rather diverse but most commonly shares three properties: (1) requirements are defined by a coverage criterion such as statement coverage; (2) the reduced test suite has to satisfy all the requirements as the original test suite; and (3) the quality of the reduced test suites is measured on the software version on which the reduction is performed. These properties make it hard for test engineers to decide how to use reduced test suites. We address all three properties of traditional test-suite reduction: (1) we evaluate test-suite reduction with requirements defined by killed mutants; (2) we evaluate inadequate reduction that does not require reduced test suites to satisfy all the requirements; and (3) we propose evolution-aware metrics that evaluate the quality of the reduced test suites across multiple software versions. Our evaluations allow a more thorough exploration of trade-offs in test-suite reduction, and our evolution-aware metrics show how the quality of reduced test suites can change after the version where the reduction is performed. We compare the trade-offs among various reductions on 18 projects with a total of 261,235 tests over 3,590 commits and a cumulative history spanning 35 years of development. Our results help test engineers make a more informed decision about balancing size, coverage, and fault-detection loss of reduced test suites. </p>",,,10.1145/2635868.2635921,,,Test-suite reduction;software evolution,,,,,5,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Data hard with a vengeance (invited talk),T. Zimmermann,"Microsoft Research, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,6,6,"<p> Action flicks and the analysis of software data in industry have more in common than you think. Both action heroes and development teams are on tight deadlines to save the day. Getting wrong information can lead to disastrous outcomes. In this talk, I will share experiences from my six years of research in the Empirical Software Engineering Group working with engineers towards sound data-driven decision about software. </p>",,,10.1145/2635868.2684431,,,Software Data,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Focus-shifting patterns of OSS developers and their congruence with call graphs,Q. Xuan; A. Okano; P. Devanbu; V. Filkov,"University of California at Davis, USA / Zhejiang University of Technology, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,401,412,"<p> Developers in complex, self-organized open-source projects often work on many different files, and over time switch focus between them. Shifting focus can have impact on the software quality and productivity, and is thus an important topic of investigation. In this paper, we study focus shifting patterns (FSPs) of developers by comparing trace data from a dozen open source software (OSS) projects of their longitudinal commit activities and file dependencies from the projects call graphs. Using information theoretic measures of network structure, we find that fairly complex focus-shifting patterns emerge, and FSPs in the same project are more similar to each other. We show that developers tend to shift focus along with, rather than away from, software dependency links described by the call graphs. This tendency becomes weaker as either the interval between successive commits, or the organizational distance between committed files (i.e. directory distance), gets larger. Interestingly, this tendency appears stronger with more productive developers. We hope our study will initiate interest in further understanding of FSPs, which can ultimately help to (1) improve current recommender systems to predict the next focus of developers, and (2) provide insight into better call graph design, so as to facilitate developers' work. </p>",,,10.1145/2635868.2635914,,,Markov entropy;Time-series;layered network;sequence analysis;structural complexity,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Improving the software testing skills of novices during onboarding through social transparency,R. Pham,"Leibniz Universit&#228;t Hannover, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,803,806,"<p> Inexperienced software developers - for example, undergraduates entering the workforce - exhibit a lack of testing skills. They have trouble understanding and applying basic testing techniques. These inexperienced developers are hired by software companies, where this lack of testing skills has already been recognized. Companies allocate valuable resources and invest time and money in different onboarding strategies to introduce new hires to the organizationŠ—Ès testing practices. However, if the lack of testing skills is not addressed properly, the new hire is left to her own devices. This hinders her in becoming a high-quality engineer for the software company. This thesis proposes to improve the onboarding strategies with traits of social transparency in order to specifically address testing issues of inexperienced new hires. Social transparency has been shown to influence the testing behavior of development teams on a social coding site. An environment that is open for discussion helps newcomers to understand and adapt a teamŠ—Ès testing culture. Tailoring the onboarding process to better address testing skills of new hires makes it more effective and more efficient. This reduces the danger of carrying new hireŠ—Ès testing deficits into commercial software development. </p>",,,10.1145/2635868.2666604,,,Onboarding;Social Transparency;Testing,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Apposcopy: semantics-based detection of Android malware through static analysis,Y. Feng; S. Anand; I. Dillig; A. Aiken,"University of Texas at Austin, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,576,587,"<p> We present Apposcopy, a new semantics-based approach for identifying a prevalent class of Android malware that steals private user information. Apposcopy incorporates (i) a high-level language for specifying signatures that describe semantic characteristics of malware families and (ii) a static analysis for deciding if a given application matches a malware signature. The signature matching algorithm of Apposcopy uses a combination of static taint analysis and a new form of program representation called Inter-Component Call Graph to efficiently detect Android applications that have certain control- and data-flow properties. We have evaluated Apposcopy on a corpus of real-world Android applications and show that it can effectively and reliably pinpoint malicious applications that belong to certain malware families. </p>",,,10.1145/2635868.2635869,,,Android;Inter-component Call Graph;Taint Analysis,,,,,29,1,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Software developers' perceptions of productivity,A. N. Meyer; T. Fritz; G. C. Murphy; T. Zimmermann,"University of Zurich, Switzerland",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,19,29,"<p> The better the software development community becomes at creating software, the more software the world seems to demand. Although there is a large body of research about measuring and investigating productivity from an organizational point of view, there is a paucity of research about how software developers, those at the front-line of software construction, think about, assess and try to improve their productivity. To investigate software developers' perceptions of software development productivity, we conducted two studies: a survey with 379 professional software developers to help elicit themes and an observational study with 11 professional software developers to investigate emergent themes in more detail. In both studies, we found that developers perceive their days as productive when they complete many or big tasks without significant interruptions or context switches. Yet, the observational data we collected shows our participants performed significant task and activity switching while still feeling productive. We analyze such apparent contradictions in our findings and use the analysis to propose ways to better support software developers in a retrospection and improvement of their productivity through the development of new tools and the sharing of best practices. </p>",,,10.1145/2635868.2635892,,,goal setting;productivity;retrospection,,,,,12,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Diagnose crashing faults on production software,R. Wu,"Hong Kong University of Science and Technology, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,771,774,"<p> Software crashes are severe manifestations of software faults. Especially, software crashes in production software usually result in bad user experiences. Therefore, crashing faults mostly are required to be fixed with a high priority. Diagnosing crashing faults on production software is non-trivial, due to the characteristics of production environment. In general, it is required to address two major challenges. First, crash reports in production software are usually numerous, since production software is used by a large number of end users in various environments and configurations. Especially, a single fault may manifest as different crash reports, which makes the prioritizing debugging and understanding faults difficult. Second, deployed software is required to run with minimal overhead and cannot afford a heavyweight instrumentation approach to collect program execution information. Furthermore, end users require that the logged information should not reveal sensitive production data. This thesis contributes for developing crashing fault diagnosis tools that can be used in production environment. </p>",,,10.1145/2635868.2666601,,,Crash stack;software analytics;software crash;statistical debugging,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Titan: a toolset that connects software architecture with quality analysis,L. Xiao; Y. Cai; R. Kazman,"Drexel University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,763,766,"<p> In this tool demo, we will illustrate our tool---Titan---that supports a new architecture model: design rule spaces (DRSpaces). We will show how Titan can capture both architecture and evolutionary structure and help to bridge the gap between architecture and defect prediction. We will demo how to use our toolset to capture hundreds of buggy files into just a few architecturally related groups, and to reveal architecture issues that contribute to the error-proneness and change-proneness of these groups. Our tool has been used to analyze dozens of large-scale industrial projects, and has demonstrated its ability to provide valuable direction on which parts of the architecture are problematic, and on why, when, and how to refactor. The video demo of Titan can be found at https://art.cs.drexel.edu/~lx52/titan.mp4 </p>",,,10.1145/2635868.2661677,,,Software Architecture;Software Quality;Software maintenance,,,,,12,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
The plastic surgery hypothesis,E. T. Barr; Y. Brun; P. Devanbu; M. Harman; F. Sarro,"University College London, UK",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,306,317,"<p> Recent work on genetic-programming-based approaches to automatic program patching have relied on the insight that the content of new code can often be assembled out of fragments of code that already exist in the code base. This insight has been dubbed the plastic surgery hypothesis; successful, well-known automatic repair tools such as GenProg rest on this hypothesis, but it has never been validated. We formalize and validate the plastic surgery hypothesis and empirically measure the extent to which raw material for changes actually already exists in projects. In this paper, we mount a large-scale study of several large Java projects, and examine a history of 15,723 commits to determine the extent to which these commits are graftable, i.e., can be reconstituted from existing code, and find an encouraging degree of graftability, surprisingly independent of commit size and type of commit. For example, we find that changes are 43% graftable from the exact version of the software being changed. With a view to investigating the difficulty of finding these grafts, we study the abundance of such grafts in three possible sources: the immediately previous version, prior history, and other projects. We also examine the contiguity or chunking of these grafts, and the degree to which grafts can be found in the same file. Our results are quite promising and suggest an optimistic future for automatic program patching methods that search for raw material in already extant code in the project being patched. </p>",,,10.1145/2635868.2635898,,,Software graftability;automated program repair;code reuse;empirical software engineering;mining software repositories,,,,,16,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Critics: an interactive code review tool for searching and inspecting systematic changes,T. Zhang; M. Song; M. Kim,"University of California at Los Angeles, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,755,758,"<p> During peer code reviews, developers often examine program differences. When using existing program differencing tools, it is difficult for developers to inspect systematic changesŠ—”similar, related changes that are scattered across multiple files. Developers cannot easily answer questions such as ""what other code locations changed similar to this change?"" and ""are there any other locations that are similar to this code but are not updated?"" In this paper, we demonstrate Critics, an Eclipse plug-in that assists developers in inspecting systematic changes. It (1) allows developers to customize a context-aware change template, (2) searches for systematic changes using the template, and (3) detects missing or inconsistent edits. Developers can interactively refine the customized change template to see corresponding search results. Critics has potential to improve developer productivity in inspecting large, scattered edits during code reviews. The tool's demonstration video is available at https://www.youtube.com/watch?v=F2D7t_Z5rhk </p>",,,10.1145/2635868.2661675,,,Software Evolution;code reviews;program differencing,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Learning natural coding conventions,M. Allamanis; E. T. Barr; C. Bird; C. Sutton,"University of Edinburgh, UK",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,281,293,"<p> Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a projectŠ—Ès coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94 % accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted. </p>",,,10.1145/2635868.2635883,,,Coding conventions;naturalness of software,,,,,22,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Statistical symbolic execution with informed sampling,A. Filieri; C. S. PóÛsóÛreanu; W. Visser; J. Geldenhuys,"University of Stuttgart, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,437,448,"<p> Symbolic execution techniques have been proposed recently for the probabilistic analysis of programs. These techniques seek to quantify the likelihood of reaching program events of interest, e.g., assert violations. They have many promising applications but have scalability issues due to high computational demand. To address this challenge, we propose a statistical symbolic execution technique that performs Monte Carlo sampling of the symbolic program paths and uses the obtained information for Bayesian estimation and hypothesis testing with respect to the probability of reaching the target events. To speed up the convergence of the statistical analysis, we propose Informed Sampling, an iterative symbolic execution that first explores the paths that have high statistical significance, prunes them from the state space and guides the execution towards less likely paths. The technique combines Bayesian estimation with a partial exact analysis for the pruned paths leading to provably improved convergence of the statistical analysis. We have implemented statistical symbolic execution with informed sampling in the Symbolic PathFinder tool. We show experimentally that the informed sampling obtains more precise results and converges faster than a purely statistical analysis and may also be more efficient than an exact symbolic analysis. When the latter does not terminate symbolic execution with informed sampling can give meaningful results under the same time and memory limits. </p>",,,10.1145/2635868.2635899,,,Statistical Symbolic Execution,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Estimating the effectiveness of spectrum-based fault localization,S. Song,"Nanjing University, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,814,816,"<p> Spectrum-Based Fault Localization (SBFL) techniques calculate risk values to predict buggy units in a program,but they may cause heavy manual work when the calculated risk values are not reasonable on some application scenarios. In this paper, presents a preliminary study to estimate the effectiveness of SBFL before manual code walk through, so that we can decide whether to adopt SBFL for a given application. </p>",,,10.1145/2635868.2661680,,,Debugging;Estimator;Fault localization;Risk formula,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Traceability and model checking to support safety requirement verification,S. Kan,"Nanjing University of Aeronautics and Astronautics, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,783,786,"<p> Ensuring safety-critical software safety requires strict verification of the conformance between safety requirements and programs. Formal verification techniques, such as model checking and theorem proving, can be used to partially realize this objective. DO-178C, a standard for airborne systems, allows formal verification techniques to replace certain forms of testing. My research is concerned with applying model checking to verify the conformance between safety requirements and programs. First, a formal language for specifying software safety requirements which are relevant to event sequences is introduced. Second, the traceability information models between formalized safety requirements and programs are built. Third, the checking of a program against a safety requirement is decomposed into smaller model checking problems by utilizing traceability information model between them. </p>",,,10.1145/2635868.2666606,,,Safety-critical software;event automata;model checking;safety requirements;traceability,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Software programmer management: a machine learning and human computer interaction framework for optimal task assignment,H. R. Joseph,"TU M&#252;nchen, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,826,828,<p> This paper attempts optimal task assignment at the enterprise-level by assigning complexity metrics to the programming tasks and predicting task completion times for each of these tasks based on a machine learning framework that factors in programmer attributes. The framework also considers real-time programmer state by using a simple EEG device to detect programmer mood. A final task assignment is made using a PDTS solver. </p>,,,10.1145/2635868.2661684,,,Complexity metrics;completion time prediction;task assignment,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
BumbleBee: a refactoring environment for spreadsheet formulas,F. Hermans; D. Dig,"Delft University of Technology, Netherlands",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,747,750,"<p> Spreadsheets are widely used in industry. It is estimated that end-user programmers outnumber regular programmers by a factor of 5. However, spreadsheets are error-prone: several reports exist of companies that have lost big sums of money due to spreadsheet errors. In previous work, spreadsheet smells have proven to be the cause of some of these errors. To that end, we have developed a tool that can apply refactorings to spreadsheet formulas, implementing our previous work on spreadsheet refactoring, which showed that spreadsheet formula smells are very common and that refactorings for them are widely applicable and that refactoring them with a tool is both quicker and less error-prone. Our new tool Bumblebee is able to execute refactorings originating from both these papers, by means of an extensible syntax, and can furthermore apply refactorings on entire groups of formulas, thus improving upon the existing tool RefBook. Finally, BumbleBee can also execute transformations other than refactorings. </p>",,,10.1145/2635868.2661673,,,end-user programming;spreadsheets;transformation,,,,,6,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Achieving lightweight trustworthy traceability,J. Cleland-Huang; M. Rahimi; P. M’_der,"DePaul University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,849,852,"<p> Despite the fact that traceability is a required element of almost all safety-critical software development processes, the trace data is often incomplete, inaccurate, redundant, conflicting, and outdated. As a result, it is neither trusted nor trustworthy. In this vision paper we propose a philosophical change in the traceability landscape which transforms traceability from a heavy-weight process producing untrusted trace links, to a light-weight results-oriented trustworthy solution. Current traceability practices which retard agility are cast away and replaced with a disciplined, just-in-time approach. The novelty of our solution lies in a clear separation of trusted trace links from untrusted ones, the change in perspective from `living-with' inacurate traces toward rigorous and ongoing debridement of stale links from the trusted pool, and the notion of synthesizing available `project exhaust' as evidence to systematically construct or reconstruct purposed, highly-focused trace links. </p>",,,10.1145/2635868.2666612,,,Safety Critical;Traceability,,,,,3,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
A foundation for refactoring C with macros,J. L. Overbey; F. Behrang; M. Hafiz,"Auburn University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,75,85,"<p> This paper establishes the concept of ""preprocessor dependences"" as a foundation for building automated refactoring tools that transform source code containing lexical macros and conditional compilation directives, such as those provided by the C preprocessor. We define a preprocessor dependence graph (PPDG) that models the relationships among macro definitions, macro invocations, and conditional compilation directives in a file--the relationships that must be maintained for the semantics of the C preprocessor to be preserved. For many refactorings, a tool can construct a PPDG from the code before and after it is transformed, then perform a linear-time comparison of the two graphs to determine whether the refactoring will operate correctly in the presence of macros and conditional compilation directives. The proposed technique was implemented in OpenRefactory/C and tested by applying refactorings to GNU Coreutils version 8.21. Empirical results indicate that the technique is effective; it successfully handled refactoring scenarios in which Eclipse CDT, Visual Assist X, and XRefactory all refactored code incorrectly. </p>",,,10.1145/2635868.2635908,,,C;Preprocessor;Refactoring,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
On the efficiency of automated testing,M. B’_hme; S. Paul,"Saarland University, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,632,642,"<p> The aim of automated program testing is to gain confidence about a program's correctness by sampling its input space. The sampling process can be either systematic or random. For every systematic testing technique the sampling is informed by the analysis of some program artefacts, like the specification, the source code (e.g., to achieve coverage), or even faulty versions of the program (e.g., mutation testing). This analysis incurs some cost. In contrast, random testing is unsystematic and does not sustain any analysis cost. In this paper, we investigate the theoretical efficiency of systematic versus random testing. First, we mathematically model the most effective systematic testing technique S_0 in which every sampled test input strictly increases the ""degree of confidence"" and is subject to the analysis cost c. Note that the efficiency of S_0 depends on c. Specifically, if we increase c, we also increase the time it takes S_0 to establish the same degree of confidence. So, there exists a maximum analysis cost beyond which R is generally more efficient than S_0. Given that we require the confidence that the program works correctly for x% of its input, we prove an upper bound on c of S_0, beyond which R is more efficient on the average. We also show that this bound depends asymptotically only on x. For instance, let R take 10ms time to sample one test input; to establish that the program works correctly for 90% of its input, S_0 must take less than 41ms to sample one test input. Otherwise, R is expected to establish the 90%-degree of confidence earlier. We prove similar bounds on the cost if the software tester is interested in revealing as many errors as possible in a given time span. </p>",,,10.1145/2635868.2635923,,,Efficient Testing;Error-based Partitioning;Partition Testing;Random Testing;Testing Theory,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Building call graphs for embedded client-side code in dynamic web applications,H. V. Nguyen; C. K’_stner; T. N. Nguyen,"Iowa State University, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,518,529,"<p> When developing and maintaining a software system, programmers often rely on IDEs to provide editor services such as syntax highlighting, auto-completion, and ""jump to declaration"". In dynamic web applications, such tool support is currently limited to either the server-side code or to hand-written or generated client-side code. Our goal is to build a call graph for providing editor services on client-side code while it is still embedded as string literals within server-side code. First, we symbolically execute the server-side code to identify all possible client-side code variations. Subsequently, we parse the generated client-side code with all its variations into a VarDOM that compactly represents all DOM variations for further analysis. Based on the VarDOM, we build conditional call graphs for embedded HTML, CSS, and JS. Our empirical evaluation on real-world web applications show that our analysis achieves 100% precision in identifying call-graph edges. 62% of the edges cross PHP strings, and 17% of them cross files - in both situations, navigation without tool support is tedious and error prone. </p>",,,10.1145/2635868.2635928,,,Call Graphs;Embedded Code;Web Code Analysis,,,,,6,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Identifying the characteristics of vulnerable code changes: an empirical study,A. Bosu; J. C. Carver; M. Hafiz; P. Hilley; D. Janni,"University of Alabama, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,257,268,"<p> To focus the efforts of security experts, the goals of this empirical study are to analyze which security vulnerabilities can be discovered by code review, identify characteristics of vulnerable code changes, and identify characteristics of developers likely to introduce vulnerabilities. Using a three-stage manual and automated process, we analyzed 267,046 code review requests from 10 open source projects and identified 413 Vulnerable Code Changes (VCC). Some key results include: (1) code review can identify common types of vulnerabilities; (2) while more experienced contributors authored the majority of the VCCs, the less experienced contributors' changes were 1.8 to 24 times more likely to be vulnerable; (3) the likelihood of a vulnerability increases with the number of lines changed, and (4) modified files are more likely to contain vulnerabilities than new files. Knowing which code changes are more prone to contain vulnerabilities may allow a security expert to concentrate on a smaller subset of submitted code changes. Moreover, we recommend that projects should: (a) create or adapt secure coding guidelines, (b) create a dedicated security review team, (c) ensure detailed comments during review to help knowledge dissemination, and (d) encourage developers to make small, incremental changes rather than large changes. </p>",,,10.1145/2635868.2635880,,,code review;inspection;open source;security defects;vulnerability,,,,,7,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
How we get there: a context-guided search strategy in concolic testing,H. Seo; S. Kim,"Hong Kong University of Science and Technology, China",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,413,424,"<p> One of the biggest challenges in concolic testing, an automatic test generation technique, is its huge search space. Concolic testing generates next inputs by selecting branches from previous execution paths. However, a large number of candidate branches makes a simple exhaustive search infeasible, which often leads to poor test coverage. Several search strategies have been proposed to explore high-priority branches only. Each strategy applies different criteria to the branch selection process but most do not consider context, how we got to the branch, in the selection process. In this paper, we introduce a context-guided search (CGS) strategy. CGS looks at preceding branches in execution paths and selects a branch in a new context for the next input. We evaluate CGS with two publicly available concolic testing tools, CREST and CarFast, on six C subjects and six Java subjects. The experimental results show that CGS achieves the highest coverage of all twelve subjects and reaches a target coverage with a much smaller number of iterations on most subjects than other strategies. </p>",,,10.1145/2635868.2635872,,,Concolic testing;search strategies;symbolic execution,,,,,9,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Minimizing software conflicts through proactive detection of conflicts and task scheduling,B. K. Kasi,"University of Nebraska-Lincoln, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,807,810,"<p> Software conflicts arising because of conflicting changes are a regular occurrence and delay projects. Workspace awareness tools have been proposed to facilitate task coordination among developers, enabling them to identify potential conflicts early, while conflicts are still easy to resolve. However, these tools have limitations, as they identify conflicts after conflicts have already occurred and therefore, are unable to prevent developersŠ—È time and effort spent in resolving the conflicts. The goal of this Ph.D. research is to: (1) characterize the distribution of conflicts, their frequency and the factors within a project that affects the distribution and frequency of conflicts, (2) design and implement a conflict minimization technique that proactively identifies potential conflicts by analyzing developersŠ—È tasks and avoids them by scheduling tasks in a conflict minimal manner and (3) evaluate the proposed approach using historic data from OSS projects and through user evaluations. Thus far, we have implemented our approach and evaluated it with historic data from four OSS projects and through simulated data. </p>",,,10.1145/2635868.2666600,,,Collaborative software development;simulations;task scheduling,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Detecting energy bugs and hotspots in mobile apps,A. Banerjee; L. K. Chong; S. Chattopadhyay; A. Roychoudhury,"National University of Singapore, Singapore",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,588,598,"<p> Over the recent years, the popularity of smartphones has increased dramatically. This has lead to a widespread availability of smartphone applications. Since smartphones operate on a limited amount of battery power, it is important to develop tools and techniques that aid in energy-efficient application development. Energy inefficiencies in smartphone applications can broadly be categorized into energy hotspots and energy bugs. An energy hotspot can be described as a scenario where executing an application causes the smartphone to consume abnormally high amount of battery power, even though the utilization of its hardware resources is low. In contrast, an energy bug can be described as a scenario where a malfunctioning application prevents the smartphone from becoming idle, even after it has completed execution and there is no user activity. In this paper, we present an automated test generation framework that detects energy hotspots/bugs in Android applications. Our framework systematically generates test inputs that are likely to capture energy hotspots/bugs. Each test input captures a sequence of user interactions (e.g. touches or taps on the smartphone screen) that leads to an energy hotspot/bug in the application. Evaluation with 30 freely-available Android applications from Google Play/F-Droid shows the efficacy of our framework in finding hotspots/bugs. Manual validation of the experimental results shows that our framework reports reasonably low number of false positives. Finally, we show the usage of the generated results by improving the energy-efficiency of some Android applications. </p>",,,10.1145/2635868.2635871,,,Energy consumption;Mobile apps;Non-functional testing,,,,,26,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Integrating approaches for feature implementation,B. Behringer,"University of Luxembourg, Luxembourg / htw saar, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,775,778,"<p> Compositional and annotative approaches are two competing yet complementary candidates for implementing feature-oriented software product lines. While the former provides real modularity, the latter excels concerning expressiveness. To combine the respective advantages of compositional and annotative approaches, we aim at unifying their underlying representations by leveraging the snippet system instead of directories and files. In addition, to exploit this unification, we propose different editable views. </p>",,,10.1145/2635868.2666605,,,Software product lines;features;modularity;snippets;virtual separation of concerns,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
SymJS: automatic symbolic testing of JavaScript web applications,G. Li; E. Andreasen; I. Ghosh,"Fujitsu Labs, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,449,459,"<p> We present SymJS, a comprehensive framework for automatic testing of client-side JavaScript Web applications. The tool contains a symbolic execution engine for JavaScript, and an automatic event explorer for Web pages. Without any user intervention, SymJS can automatically discover and explore Web events, symbolically execute the associated JavaScript code, refine the execution based on dynamic feedbacks, and produce test cases with high coverage. The symbolic engine contains a symbolic virtual machine, a string-numeric solver, and a symbolic executable DOM model. SymJS's innovations include a novel symbolic virtual machine for JavaScript Web, symbolic+dynamic feedback directed event space exploration, and dynamic taint analysis for enhancing event sequence construction. We illustrate the effectiveness of SymJS on standard JavaScript benchmarks and various real-life Web applications. On average SymJS achieves over 90% line coverage for the benchmark programs, significantly outperforming existing methods. </p>",,,10.1145/2635868.2635913,,,Automatic Software Testing;Event Sequence;JavaScript;Symbolic Execution;Taint Analysis;Web,,,,,6,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Managing lots of models: the FaMine approach,D. Wille,"TU Braunschweig, Germany",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,817,819,<p> In this paper we present recent developments in reverse engineering variability for block-based data-flow models. </p>,,,10.1145/2635868.2661681,,,Analysis;Family Model Mining;SPL;Variability,,,,,1,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Vector abstraction and concretization for scalable detection of refactorings,N. A. Milea; L. Jiang; S. C. Khoo,"National University of Singapore, Singapore",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,86,97,"<p> Automated techniques have been proposed to either identify refactoring opportunities (i.e., code fragments that can be but have not yet been restructured in a program), or reconstruct historical refactorings (i.e., code restructuring operations that have happened between different versions of a program). In this paper, we propose a new technique that can detect both refactoring opportunities and historical refactorings in large code bases. The key of our technique is the design of vector abstraction and concretization operations that can encode code changes induced by certain refactorings as characteristic vectors. Thus, the problem of identifying refactorings can be reduced to the problem of identifying matching vectors, which can be solved efficiently. We have implemented our technique for Java. The prototype is applied to 200 bundle projects from the Eclipse ecosystem containing 4.5 million lines of code, and reports in total more than 32K instances of 17 types of refactoring opportunities, taking 25 minutes on average for each type. The prototype is also applied to 14 versions of 3 smaller programs (JMeter, Ant, XML-Security), and detects (1) more than 2.8K refactoring opportunities within individual versions with a precision of about 87%, and (2) more than 190 historical refactorings across consecutive versions of the programs with a precision of about 92%. </p>",,,10.1145/2635868.2635926,,,Refactoring Detection;Software Evolution;Vector-based Code Representation,,,,,2,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
An empirical analysis of flaky tests,Q. Luo; F. Hariri; L. Eloussi; D. Marinov,"University of Illinois at Urbana-Champaign, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,643,653,"<p> Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests often called flaky testsŠ—”have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most common root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of (avoiding) flaky tests. </p>",,,10.1145/2635868.2635920,,,Empirical study;flaky tests;non-determinism,,,,,18,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Selection and presentation practices for code example summarization,A. T. T. Ying; M. P. Robillard,"McGill University, Canada",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,460,471,"<p> Code examples are an important source for answering questions about software libraries and applications. Many usage contexts for code examples require them to be distilled to their essence: e.g., when serving as cues to longer documents, or for reminding developers of a previously known idiom. We conducted a study to discover how code can be summarized and why. As part of the study, we collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. Based on a qualitative analysis of this data we elicited a list of practices followed by the participants to summarize code examples and propose empirically-supported hypotheses justifying the use of specific practices. One main finding was that none of the participants exclusively extracted code verbatim for the summaries, motivating abstractive summarization. The results provide a grounded basis for the development of code example summarization and presentation technology. </p>",,,10.1145/2635868.2635877,,,,,,,,6,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Social network analysis in open source software peer review,X. Yang,"Nara Institute of Science and Technology, Japan",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,820,822,"<p> Software peer review (aka. code review) is regarded as one of the most important approaches to keep software quality and productivity. Due to the distributed collaborations and communication nature of Open Source Software (OSS), OSS review differs from traditional industry review. Unlike other related works, this study investigated OSS peer review pro- cesses from social perspective by using social network anal- ysis (SNA). We analyzed the review history from three typi- cal OSS projects. The results provide hints on relationships among the OSS reviewers which can help to understand how developers work and communicate with each other. </p>",,,10.1145/2635868.2661682,,,open source;peer review;social network,,,,,,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Are mutants a valid substitute for real faults in software testing?,R. Just; D. Jalali; L. Inozemtseva; M. D. Ernst; R. Holmes; G. Fraser,"University of Washington, USA",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,654,665,"<p> A good test suite is one that detects real faults. Because the set of faults in a program is usually unknowable, this definition is not useful to practitioners who are creating test suites, nor to researchers who are creating and evaluating tools that generate test suites. In place of real faults, testing research often uses mutants, which are artificial faults -- each one a simple syntactic variation -- that are systematically seeded throughout the program under test. Mutation analysis is appealing because large numbers of mutants can be automatically-generated and used to compensate for low quantities or the absence of known real faults. Unfortunately, there is little experimental evidence to support the use of mutants as a replacement for real faults. This paper investigates whether mutants are indeed a valid substitute for real faults, i.e., whether a test suiteŠ—Ès ability to detect mutants is correlated with its ability to detect real faults that developers have fixed. Unlike prior studies, these investigations also explicitly consider the conflating effects of code coverage on the mutant detection rate. Our experiments used 357 real faults in 5 open-source applications that comprise a total of 321,000 lines of code. Furthermore, our experiments used both developer-written and automatically-generated test suites. The results show a statistically significant correlation between mutant detection and real fault detection, independently of code coverage. The results also give concrete suggestions on how to improve mutation analysis and reveal some inherent limitations. </p>",,,10.1145/2635868.2635929,,,Test effectiveness;code coverage;mutation analysis;real faults,,,,,59,,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Mining idioms from source code,M. Allamanis; C. Sutton,"University of Edinburgh, UK",Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,20161111,2014,,,472,483,"<p> We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic purpose. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q&A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management. </p>",,,10.1145/2635868.2635901,,,code idioms;naturalness of source code;syntactic code patterns,,,,,9,1,,,,,16-21 Nov. 2014,,ACM,ACM Conferences
Guided differential testing of certificate validation in SSL/TLS implementations,Y. Chen; Z. Su,"Shanghai Jiao Tong University, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,793,804,"<p> Certificate validation in SSL/TLS implementations is critical for Internet security. There is recent strong effort, namely frankencert, in automatically synthesizing certificates for stress-testing certificate validation. Despite its early promise, it remains a significant challenge to generate effective test certificates as they are structurally complex with intricate syntactic and semantic constraints. This paper tackles this challenge by introducing mucert, a novel, guided technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates, and (2) diversify them by adapting Markov Chain Monte Carlo (MCMC) sampling. The diversified certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We have implemented mucert and extensively evaluated it against frankencert. Our experimental results show that mucert is significantly more cost-effective than frankencert. Indeed, 1K mucerts (i.e., mucert-mutated certificates) yield three times as many distinct discrepancies as 8M frankencerts (i.e., frankencert-synthesized certificates), and 200 mucerts can achieve higher code coverage than 100,000 frankencerts. This improvement is significant as it incurs much cost to test each generated certificate. We have analyzed and reported 20+ latent discrepancies (presumably missed by frankencert), and reported an additional 357 discrepancy-triggering certificates to SSL/TLS developers, who have already confirmed some of our reported issues and are investigating causes of all the reported discrepancies. In particular, our reports have led to bug fixes, active discussions in the community, and proposed changes to relevant IETFŠ—Ès RFCs. We believe that mucert is practical and effective for helping improve the robustness of SSL/TLS implementations. </p>",,,10.1145/2786805.2786835,,,Differential testing;certificate validation;mutation,,,,,4,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Automated attack surface approximation,C. Theisen,"North Carolina State University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1063,1065,"<p> While software systems are being developed and released to consumers more rapidly than ever, security remains an important issue for developers. Shorter development cycles means less time for these critical security testing and review efforts. The attack surface of a system is the sum of all paths for untrusted data into and out of a system. Code that lies on the attack surface therefore contains code with actual exploitable vulnerabilities. However, identifying code that lies on the attack surface requires the same contested security resources from the secure testing efforts themselves. My research proposes an automated technique to approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces user crashes represent activity that puts the system under stress, and is therefore indicative of potential security vulnerabilities. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Mozilla Firefox, the attack surface approximation selected 8.4% of files and contained 72.1% of known vulnerabilities. A similar trial was performed on the Windows 8 product. </p>",,,10.1145/2786805.2807563,,,Stack traces;attack surface;crash dumps,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Turning programs against each other: high coverage fuzz-testing using binary-code mutation and dynamic slicing,U. Karg’©n; N. Shahmehri,"Link&#246;ping University, Sweden",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,782,792,"<p> Mutation-based fuzzing is a popular and widely employed black-box testing technique for finding security and robustness bugs in software. It owes much of its success to its simplicity; a well-formed seed input is mutated, e.g. through random bit-flipping, to produce test inputs. While reducing the need for human effort, and enabling security testing even of closed-source programs with undocumented input formats, the simplicity of mutation-based fuzzing comes at the cost of poor code coverage. Often millions of iterations are needed, and the results are highly dependent on configuration parameters and the choice of seed inputs. In this paper we propose a novel method for automated generation of high-coverage test cases for robustness testing. Our method is based on the observation that, even for closed-source programs with proprietary input formats, an implementation that can generate well-formed inputs to the program is typically available. By systematically mutating the program code of such generating programs, we leverage information about the input format encoded in the generating program to produce high-coverage test inputs, capable of reaching deep states in the program under test. Our method works entirely at the machine-code level, enabling use-cases similar to traditional black-box fuzzing. We have implemented the method in our tool MutaGen, and evaluated it on 7 popular Linux programs. We found that, for most programs, our method improves code coverage by one order of magnitude or more, compared to two well-known mutation-based fuzzers. We also found a total of 8 unique bugs. </p>",,,10.1145/2786805.2786844,,,Fuzz testing;black-box;dynamic slicing;fuzzing;program mutation,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
A textual domain specific language for requirement modelling,O. Olajubu,"University of Northampton, UK",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1060,1062,"<p> Requirement specification is usually done with a combination of Natural Language (NL) and informal diagrams. Modeling approaches to support requirement engineering activities have involved a combination of text and graphical models. In this work, a textual domain specific modelling notation for requirement specification is presented. How certain requirement quality attributes are addressed using this notation is also demonstrated. </p>",,,10.1145/2786805.2807562,,,Domain specific languages;Requirement specification,,,,,,3,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Efficient dependency detection for safe Java test acceleration,J. Bell; G. Kaiser; E. Melski; M. Dattatreya,"Columbia University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,770,781,"<p> Slow builds remain a plague for software developers. The frequency with which code can be built (compiled, tested and packaged) directly impacts the productivity of developers: longer build times mean a longer wait before determining if a change to the application being built was successful. We have discovered that in the case of some languages, such as Java, the majority of build time is spent running tests, where dependencies between individual tests are complicated to discover, making many existing test acceleration techniques unsound to deploy in practice. Without knowledge of which tests are dependent on others, we cannot safely parallelize the execution of the tests, nor can we perform incremental testing (i.e., execute only a subset of an application's tests for each build). The previous techniques for detecting these dependencies did not scale to large test suites: given a test suite that normally ran in two hours, the best-case running scenario for the previous tool would have taken over 422 CPU days to find dependencies between all test methods (and would not soundly find all dependencies) Š—” on the same project the exhaustive technique (to find all dependencies) would have taken over 1e300 years. We present a novel approach to detecting all dependencies between test cases in large projects that can enable safe exploitation of parallelism and test selection with a modest analysis cost. </p>",,,10.1145/2786805.2786823,,,Test dependence;detection algorithms;empirical studies,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Combining eye tracking with navigation paths for identification of cross-language code dependencies,M. Konopka,"Slovak University of Technology in Bratislava, Slovakia",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1057,1059,"<p> In recent years, fine-grained monitoring of software developers during software development and maintenance activities has increased in popularity, together with use of devices for eye tracking and recording developerŠ—Ès biometric data. We look for everyday application of such data to support developers in their work. In this paper we discuss an approach to identify potential code dependencies in source code, even when written in different programming languages, by combining identification of areas-of-interest in source code using eye tracking with developerŠ—Ès navigation paths. Our plan is to evaluate it with data of developers working on real development tasks. </p>",,,10.1145/2786805.2807561,,,Eye Tracking;Interaction Data;Potential Code Dependencies,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
CLOTHO: saving programs from malformed strings and incorrect string-handling,A. Dhar; R. Purandare; M. Dhawan; S. Rangaswamy,"Xerox Research Center, India",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,555,566,"<p> Software is susceptible to malformed data originating from untrusted sources. Occasionally the programming logic or constructs used are inappropriate to handle the varied constraints imposed by legal and well-formed data. Consequently, softwares may produce unexpected results or even crash. In this paper, we present CLOTHO, a novel hybrid approach that saves such softwares from crashing when failures originate from malformed strings or inappropriate handling of strings. CLOTHO statically analyses a program to identify statements that are vulnerable to failures related to associated string data. CLOTHO then generates patches that are likely to satisfy constraints on the data, and in case of failures produces program behavior which would be close to the expected. The precision of the patches is improved with the help of a dynamic analysis. We have implemented CLOTHO for the JAVA String API, and our evaluation based on several popular open-source libraries shows that CLOTHO generates patches that are semantically similar to the patches generated by the programmers in the later versions. Additionally, these patches are activated only when a failure is detected, and thus CLOTHO incurs no runtime overhead during normal execution, and negligible overhead in case of failures. </p>",,,10.1145/2786805.2786877,,,Automatic Program Repair;Program Analysis;Strings,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Getting to know you: towards a capability model for Java,B. Hermann; M. Reif; M. Eichberg; M. Mezini,"TU Darmstadt, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,758,769,"<p> Developing software from reusable libraries lets developers face a security dilemma: Either be efficient and reuse libraries as they are or inspect them, know about their resource usage, but possibly miss deadlines as reviews are a time consuming process. In this paper, we propose a novel capability inference mechanism for libraries written in Java. It uses a coarse-grained capability model for system resources that can be presented to developers. We found that the capability inference agrees by 86.81% on expectations towards capabilities that can be derived from project documentation. Moreover, our approach can find capabilities that cannot be discovered using project documentation. It is thus a helpful tool for developers mitigating the aforementioned dilemma. </p>",,,10.1145/2786805.2786829,,,analysis;capability;library;reuse;security,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Automatically computing path complexity of programs,L. Bang; A. Aydin; T. Bultan,"University of California at Santa Barbara, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,61,72,"<p> Recent automated software testing techniques concentrate on achieving path coverage. We present a complexity measure that provides an upper bound for the number of paths in a program, and hence, can be used for assessing the difficulty of achieving path coverage for a given method. We define the path complexity of a program as a function that takes a depth bound as input and returns the number of paths in the control flow graph that are within that bound. We show how to automatically compute the path complexity function in closed form, and the asymptotic path complexity which identifies the dominant term in the path complexity function. Our results demonstrate that path complexity can be computed efficiently, and it is a better complexity measure for path coverage compared to cyclomatic complexity and NPATH complexity. </p>",,,10.1145/2786805.2786863,,,Path complexity;automated testing;path coverage,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Synthesizing tests for detecting atomicity violations,M. Samak; M. K. Ramanathan,"Indian Institute of Science, India",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,131,142,"<p> Using thread-safe libraries can help programmers avoid the complexities of multithreading. However, designing libraries that guarantee thread-safety can be challenging. Detecting and eliminating atomicity violations when methods in the libraries are invoked concurrently is vital in building reliable client applications that use the libraries. While there are dynamic analyses to detect atomicity violations, these techniques are critically dependent on effective multithreaded tests. Unfortunately, designing such tests is non-trivial. In this paper, we design a novel and scalable approach for synthesizing multithreaded tests that help detect atomicity violations. The input to the approach is the implementation of the library and a sequential seed testsuite that invokes every method in the library with random parameters. We analyze the execution of the sequential tests, generate variable lock dependencies and construct a set of three accesses which when interleaved suitably in a multithreaded execution can cause an atomicity violation. Subsequently, we identify pairs of method invocations that correspond to these accesses and invoke them concurrently from distinct threads with appropriate objects to help expose atomicity violations. We have incorporated these ideas in our tool, named Intruder, and applied it on multiple open-source Java multithreaded libraries. Intruder is able to synthesize 40 multithreaded tests across nine classes in less than two minutes to detect 79 harmful atomicity violations, including previously unknown violations in thread-safe classes. We also demonstrate the effectiveness of Intruder by comparing the results with other approaches designed for synthesizing multithreaded tests. </p>",,,10.1145/2786805.2786874,,,atomicity violation;concurrency;dynamic analysis,,,,,5,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
REMI: defect prediction for efficient API testing,M. Kim; J. Nam; J. Yeon; S. Choi; S. Kim,"Hong Kong University of Science and Technology, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,990,993,"<p> Quality assurance for common APIs is important since the the reliability of APIs affects the quality of other systems using the APIs. Testing is a common practice to ensure the quality of APIs, but it is a challenging and laborious task especially for industrial projects. Due to a large number of APIs with tight time constraints and limited resources, it is hard to write enough test cases for all APIs. To address these challenges, we present a novel technique, REMI that predicts high risk APIs in terms of producing potential bugs. REMI allows developers to write more test cases for the high risk APIs. We evaluate REMI on a real-world industrial project, Tizen-wearable, and apply REMI to the API development process at Samsung Electronics. Our evaluation results show that REMI predicts the bug-prone APIs with reasonable accuracy (0.681 f-measure on average). The results also show that applying REMI to the Tizen-wearable development process increases the number of bugs detected, and reduces the resources required for executing test cases. </p>",,,10.1145/2786805.2804429,,,API Testing;Defect Prediction;Quality Assurance,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Spotting familiar code snippet structures for program comprehension,V. Vinayakarao,"IIIT Delhi, India",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1054,1056,"<p> Developers deal with the persistent problem of understanding non-trivial code snippets. To understand the given implementation, its issues, and available choices, developers will benefit from reading relevant discussions and descriptions over the web. However, there is no easy way to know the relevant natural language terms so as to reach to such descriptions from a code snippet, especially if the documentation is inadequate and if the vocabulary used in the code is not helpful for web search. We propose an approach to solve this problem using a repository of topics and associated structurally variant snippets collected from a discussion forum. In this on-going work, we take Java methods from the code samples of three Java books, match them with the repository, and associate the topics with 76.9% precision and 66.7% recall. </p>",,,10.1145/2786805.2807560,,,Structure Matching;Variant Repository,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Responsive designs in a snap,N. Sinha; R. Karim,"IBM Research, India",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,544,554,"<p> With the massive adoption of mobile devices with different form- factors, UI designers face the challenge of designing responsive UIs which are visually appealing across a wide range of devices. De- signing responsive UIs requires a deep knowledge of HTML/CSS as well as responsive patterns - juggling through various design configurations and re-designing for multiple devices is laborious and time-consuming. We present DECOR, a recommendation tool for creating multi-device responsive UIs. Given an initial UI de- sign, user-specified design constraints and a list of devices, DECOR provides ranked, device-specific recommendations to the designer for approval. Design space exploration involves a combinatorial explosion: we formulate it as a design repair problem and devise several design space pruning techniques to enable efficient repair. An evaluation over real-life designs shows that DECOR is able to compute the desired recommendations, involving a variety of responsive design patterns, in less than a minute. </p>",,,10.1145/2786805.2786808,,,CSS;Constraint-based Design;HTML;Responsive Layout inference,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
FlexJava: language support for safe and modular approximate programming,J. Park; H. Esmaeilzadeh; X. Zhang; M. Naik; W. Harris,"Georgia Tech, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,745,757,"<p> Energy efficiency is a primary constraint in modern systems. Approximate computing is a promising approach that trades quality of result for gains in efficiency and performance. State- of-the-art approximate programming models require extensive manual annotations on program data and operations to guarantee safe execution of approximate programs. The need for extensive manual annotations hinders the practical use of approximation techniques. This paper describes FlexJava, a small set of language extensions, that significantly reduces the annotation effort, paving the way for practical approximate programming. These extensions enable programmers to annotate approximation-tolerant method outputs. The FlexJava compiler, which is equipped with an approximation safety analysis, automatically infers the operations and data that affect these outputs and selectively marks them approximable while giving safety guarantees. The automation and the languageŠ—–compiler codesign relieve programmers from manually and explicitly an- notating data declarations or operations as safe to approximate. FlexJava is designed to support safety, modularity, generality, and scalability in software development. We have implemented FlexJava annotations as a Java library and we demonstrate its practicality using a wide range of Java applications and by con- ducting a user study. Compared to EnerJ, a recent approximate programming system, FlexJava provides the same energy savings with significant reduction (from 2’ã to 17’ã) in the number of annotations. In our user study, programmers spend 6’ã to 12’ã less time annotating programs using FlexJava than when using EnerJ. </p>",,,10.1145/2786805.2786807,,,Language design;modular approximate programming,,,,,6,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Symbolic execution of programs with heap inputs,P. Braione; G. Denaro; M. Pezz’å,"University of Milano-Bicocca, Italy",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,602,613,"<p> Symbolic analysis is a core component of many automatic test generation and program verication approaches. To verify complex software systems, test and analysis techniques shall deal with the many aspects of the target systems at different granularity levels. In particular, testing software programs that make extensive use of heap data structures at unit and integration levels requires generating suitable input data structures in the heap. This is a main challenge for symbolic testing and analysis techniques that work well when dealing with numeric inputs, but do not satisfactorily cope with heap data structures yet. In this paper we propose a language HEX to specify invariants of partially initialized data structures, and a decision procedure that supports the incremental evaluation of structural properties in HEX. Used in combination with the symbolic execution of heap manipulating programs, HEX prevents the exploration of invalid states, thus improving the eefficiency of program testing and analysis, and avoiding false alarms that negatively impact on verication activities. The experimental data conrm that HEX is an effective and efficient solution to the problem of testing and analyzing heap manipulating programs, and outperforms the alternative approaches that have been proposed so far. </p>",,,10.1145/2786805.2786842,,,Symbolic execution;data structure invariants;lazy initialization,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Measure it? Manage it? Ignore it? software practitioners and technical debt,N. A. Ernst; S. Bellomo; I. Ozkaya; R. L. Nord; I. Gorton,"SEI, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,50,60,"<p> The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches. </p>",,,10.1145/2786805.2786848,,,Technical debt;architecture;survey,,,,,10,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Commit guru: analytics and risk prediction of software commits,C. Rosen; B. Grawi; E. Shihab,"Rochester Institute of Technology, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,966,969,"<p> Software quality is one of the most important research sub-areas of software engineering. Hence, a plethora of research has focused on the prediction of software quality. Much of the software analytics and prediction work has proposed metrics, models and novel approaches that can predict quality with high levels of accuracy. However, adoption of such techniques remain low; one of the reasons for this low adoption of the current analytics and prediction technique is the lack of actionable and publicly available tools. We present Commit Guru, a language agnostic analytics and prediction tool that identifies and predicts risky software commits. Commit Guru is publicly available and is able to mine any GIT SCM repository. Analytics are generated at both, the project and commit levels. In addition, Commit Guru automatically identifies risky (i.e., bug-inducing) commits and builds a prediction model that assess the likelihood of a recent commit introducing a bug in the future. Finally, to facilitate future research in the area, users of Commit Guru can download the data for any project that is processed by Commit Guru with a single click. Several large open source projects have been successfully processed using Commit Guru. Commit Guru is available online at commit.guru. Our source code is also released freely under the MIT license. </p>",,,10.1145/2786805.2803183,,,Risky Software Commits;Software Analytics;Software Metrics;Software Prediction,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
A user-guided approach to program analysis,R. Mangal; X. Zhang; A. V. Nori; M. Naik,"Georgia Tech, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,462,473,"<p> Program analysis tools often produce undesirable output due to various approximations. We present an approach and a system EUGENE that allows user feedback to guide such approximations towards producing the desired output. We formulate the problem of user-guided program analysis in terms of solving a combination of hard rules and soft rules: hard rules capture soundness while soft rules capture degrees of approximations and preferences of users. Our technique solves the rules using an off-the-shelf solver in a manner that is sound (satisfies all hard rules), optimal (maximally satisfies soft rules), and scales to real-world analyses and programs. We evaluate EUGENE on two different analyses with labeled output on a suite of seven Java programs of size 131Š—–198 KLOC. We also report upon a user study involving nine users who employ EUGENE to guide an information-flow analysis on three Java micro-benchmarks. In our experiments, EUGENE significantly reduces misclassified reports upon providing limited amounts of feedback. </p>",,,10.1145/2786805.2786851,,,User feedback;program analysis;report classification,,,,,4,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Improving model-based test generation by model decomposition,P. Arcaini; A. Gargantini; E. Riccobene,"Charles University in Prague, Czech Republic",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,119,130,"<p> One of the well-known techniques for model-based test generation exploits the capability of model checkers to return counterexamples upon property violations. However, this approach is not always optimal in practice due to the required time and memory, or even not feasible due to the state explosion problem of model checking. A way to mitigate these limitations consists in decomposing a system model into suitable subsystem models separately analyzable. In this paper, we show a technique to decompose a system model into subsystems by exploiting the model variables dependency, and then we propose a test generation approach which builds tests for the single subsystems and combines them later in order to obtain tests for the system as a whole. Such approach mitigates the exponential increase of the test generation time and memory consumption, and, compared with the same model-based test generation technique applied to the whole system, shows to be more efficient. We prove that, although not complete, the approach is sound. </p>",,,10.1145/2786805.2786837,,,Test case generation;abstraction;model-based testing;state explosion problem,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Semantic degrees for Industrie 4.0 engineering: deciding on the degree of semantic formalization to select appropriate technologies,C. H. Cheng; T. Guelfirat; C. Messinger; J. O. Schmitt; M. Schnelte; P. Weber,"ABB Research, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1010,1013,"<p> Under the context of Industrie 4.0 (I4.0), future production systems provide balanced operations between manufacturing flexibility and efficiency, realized in an autonomous, horizontal, and decentralized item-level production control framework. Structured interoperability via precise formulations on an appropriate degree is crucial to achieve software engineering efficiency in the system life cycle. However, selecting the degree of formalization can be challenging, as it crucially depends on the desired common understanding (semantic degree) between multiple parties. In this paper, we categorize different semantic degrees and map a set of technologies in industrial automation to their associated degrees. Furthermore, we created guidelines to assist engineers selecting appropriate semantic degrees in their design. We applied these guidelines on publicly available scenarios to examine the validity of the approach, and identified semantic elements over internally developed use cases concerning plug-and-produce. </p>",,,10.1145/2786805.2804434,,,Embedded software;Industry 4.0 engineering;design space exploration and evaluation guidelines;semantic degree,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Predicting field reliability,P. Rotella; S. Chulani; D. Goyal,"Cisco Systems, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,986,989,"<p> The objective of the work described is to accurately predict, as early as possible in the software lifecycle, how reliably a new software release will behave in the field. The initiative is based on a set of innovative mathematical models that have consistently shown a high correlation between key in-process metrics and our primary customer experience metric, SWDPMH (Software Defects per Million Hours [usage] per Month). We have focused on the three primary dimensions of testing Š—– incoming, fixed, and backlog bugs. All of the key predictive metrics described here are empirically-derived, and in specific quantitative terms have not previously been documented in the software engineering/quality literature. A key part of this work is the empirical determination of the precision of the measurements of the primary predictive variables, and the determination of the prediction (outcome) error. These error values enable teams to accurately gauge bug finding and fixing progress, week by week, during the primary test period. </p>",,,10.1145/2786805.2804428,,,Software release reliability;customer experience;error analysis;modeling;prediction;testing,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
I heart hacker news: expanding qualitative research findings by analyzing social news websites,T. Barik; B. Johnson; E. Murphy-Hill,"ABB Research, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,882,885,"<p> Grounded theory is an important research method in empirical software engineering, but it is also time consuming, tedious, and complex. This makes it difficult for researchers to assess if threats, such as missing themes or sample bias, have inadvertently materialized. To better assess such threats, our new idea is that we can automatically extract knowledge from social news websites, such as Hacker News, to easily replicate existing grounded theory research --- and then compare the results. We conduct a replication study on static analysis tool adoption using Hacker News. We confirm that even a basic replication and analysis using social news websites can offer additional insights to existing themes in studies, while also identifying new themes. For example, we identified that security was not a theme discovered in the original study on tool adoption. As a long-term vision, we consider techniques from the discipline of knowledge discovery to make this replication process more automatic. </p>",,,10.1145/2786805.2803200,,,Hacker News;computer-mediated discourse;grounded theory;representativeness;theoretical saturation,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Automated generation of programming language quizzes,S. Jain,"IIIT Delhi, India",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1051,1053,"<p> Formation of quizzes is a vital problem as they are an important part of learning. To create a quiz on a particular topic, its related terms need to be identified for further use in extraction of questions on the topic. These terms are referred to as entities for the topic and the task of distinguishing entities from general purpose terms is termed entity discovery. We know that discussion forums and question-answer sites on software contain questions using programming terms in their posts. In this work, we mine patterns in user queries from such a forum and then automatically discover entities for programming languages using these patterns. We use these entities to extract questions related to the programming language and form automated quizzes using them. </p>",,,10.1145/2786805.2807559,,,Entity Discovery;Programming Languages;Quiz Creation,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Is the cure worse than the disease? overfitting in automated program repair,E. K. Smith; E. T. Barr; C. Le Goues; Y. Brun,"University of Massachusetts at Amherst, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,532,543,"<p> Automated program repair has shown promise for reducing the significant manual effort debugging requires. This paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patches' correctness using the same set of tests. Since tests are an imperfect metric of program correctness, evaluations of this type do not discriminate between correct patches and patches that overfit the available tests and break untested but desired functionality. This paper evaluates two well-studied repair tools, GenProg and TrpAutoRepair, on a publicly available benchmark of bugs, each with a human-written patch. By evaluating patches using tests independent from those used during repair, we find that the tools are unlikely to improve the proportion of independent tests passed, and that the quality of the patches is proportional to the coverage of the test suite used during repair. For programs that pass most tests, the tools are as likely to break tests as to fix them. However, novice developers also overfit, and automated repair performs no worse than these developers. In addition to overfitting, we measure the effects of test suite coverage, test suite provenance, and starting program quality, as well as the difference in quality between novice-developer-written and tool-generated patches when quality is assessed with a test suite independent from the one used for patch generation. </p>",,,10.1145/2786805.2786825,,,GenProg;IntroClass;TrpAutoRepair;automated program repair;empirical evaluation;independent evaluation,,,,,15,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Efficient and reasonable object-oriented concurrency,S. West; S. Nanz; B. Meyer,"Google, Switzerland",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,734,744,"<p> Making threaded programs safe and easy to reason about is one of the chief difficulties in modern programming. This work provides an efficient execution model for SCOOP, a concurrency approach that provides not only data-race freedom but also pre/postcondition reasoning guarantees between threads. The extensions we propose influence both the underlying semantics to increase the amount of concurrent execution that is possible, exclude certain classes of deadlocks, and enable greater performance. These extensions are used as the basis of an efficient runtime and optimization pass that improve performance 15x over a baseline implementation. This new implementation of SCOOP is, on average, also 2x faster than other well-known safe concurrent languages. The measurements are based on both coordination-intensive and data-manipulation-intensive benchmarks designed to offer a mixture of workloads. </p>",,,10.1145/2786805.2786822,,,Concurrency;object-oriented;optimization;performance,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Impact of developer turnover on quality in open-source software,M. Foucault; M. Palyart; X. Blanc; G. C. Murphy; J. R. Falleri,"University of Bordeaux, France",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,829,841,"<p> Turnover is the phenomenon of continuous influx and retreat of human resources in a team. Despite being well-studied in many settings, turnover has not been characterized for open-source software projects. We study the source code repositories of five open-source projects to characterize patterns of turnover and to determine the effects of turnover on software quality. We define the base concepts of both external and internal turnover, which are the mobility of developers in and out of a project, and the mobility of developers inside a project, respectively. We provide a qualitative analysis of turnover patterns. We also found, in a quantitative analysis, that the activity of external newcomers negatively impact software quality. </p>",,,10.1145/2786805.2786870,,,mining software repositories;qualitative analysis;software metrics,,,,,5,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Rule-based extraction of goal-use case models from text,T. H. Nguyen; J. Grundy; M. Almorsy,"Swinburne University of Technology, Australia",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,591,601,"<p> Goal and use case modeling has been recognized as a key approach for understanding and analyzing requirements. However, in practice, goals and use cases are often buried among other content in requirements specifications documents and written in unstructured styles. It is thus a time-consuming and error-prone process to identify such goals and use cases. In addition, having them embedded in natural language documents greatly limits the possibility of formally analyzing the requirements for problems. To address these issues, we have developed a novel rule-based approach to automatically extract goal and use case models from natural language requirements documents. Our approach is able to automatically categorize goals and ensure they are properly specified. We also provide automated semantic parameterization of artifact textual specifications to promote further analysis on the extracted goal-use case models. Our approach achieves 85% precision and 82% recall rates on average for model extraction and 88% accuracy for the automated parameterization. </p>",,,10.1145/2786805.2786876,,,Goal-Use Case modeling;extraction;semantic parameterization,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Staged program repair with condition synthesis,F. Long; M. Rinard,"Massachusetts Institute of Technology, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,166,178,"<p> We present SPR, a new program repair system that combines staged program repair and condition synthesis. These techniques enable SPR to work productively with a set of parameterized transformation schemas to generate and efficiently search a rich space of program repairs. Together these techniques enable SPR to generate correct repairs for over five times as many defects as previous systems evaluated on the same benchmark set. </p>",,,10.1145/2786805.2786811,,,Condition synthesis;Program repair;Staged repair,,,,,17,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Suggesting accurate method and class names,M. Allamanis; E. T. Barr; C. Bird; C. Sutton,"University of Edinburgh, UK",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,38,49,"<p> Descriptive names are a vital part of readable, and hence maintainable, code. Recent progress on automatically suggesting names for local variables tantalizes with the prospect of replicating that success with method and class names. However, suggesting names for methods and classes is much more difficult. This is because good method and class names need to be functionally descriptive, but suggesting such names requires that the model goes beyond local context. We introduce a neural probabilistic language model for source code that is specifically designed for the method naming problem. Our model learns which names are semantically similar by assigning them to locations, called embeddings, in a high-dimensional continuous space, in such a way that names with similar embeddings tend to be used in similar contexts. These embeddings seem to contain semantic information about tokens, even though they are learned only from statistical co-occurrences of tokens. Furthermore, we introduce a variant of our model that is, to our knowledge, the first that can propose neologisms, names that have not appeared in the training corpus. We obtain state of the art results on the method, class, and even the simpler variable naming tasks. More broadly, the continuous embeddings that are learned by our model have the potential for wide application within software engineering. </p>",,,10.1145/2786805.2786849,,,Coding conventions;naturalness of software,,,,,6,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
NARCIA: an automated tool for change impact analysis in natural language requirements,C. Arora; M. Sabetzadeh; A. Goknil; L. C. Briand; F. Zimmer,"University of Luxembourg, Luxembourg",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,962,965,"<p> We present NARCIA, a tool for analyzing the impact of change in natural language requirements. For a given change in a requirements document, NARCIA calculates quantitative scores suggesting how likely each requirements statement in the document is to be impacted. These scores, computed using Natural Language Processing (NLP), are used for sorting the requirements statements, enabling the user to focus on statements that are most likely to be impacted. To increase the accuracy of change impact analysis, NARCIA provides a mechanism for making explicit the rationale behind changes. NARCIA has been empirically evaluated on two industrial case studies. The results of this evaluation are briefly highlighted. </p>",,,10.1145/2786805.2803185,,,Change Impact Analysis;Natural Language Processing (NLP);Natural Language Requirements,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
JITProf: pinpointing JIT-unfriendly JavaScript code,L. Gong; M. Pradel; K. Sen,"University of California at Berkeley, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,357,368,"<p> Most modern JavaScript engines use just-in-time (JIT) compilation to translate parts of JavaScript code into efficient machine code at runtime. Despite the overall success of JIT compilers, programmers may still write code that uses the dynamic features of JavaScript in a way that prohibits profitable optimizations. Unfortunately, there currently is no way to measure how prevalent such JIT-unfriendly code is and to help developers detect such code locations. This paper presents JITProf, a profiling framework to dynamically identify code locations that prohibit profitable JIT optimizations. The key idea is to associate meta-information with JavaScript objects and code locations, to update this information whenever particular runtime events occur, and to use the meta-information to identify JIT-unfriendly operations. We use JITProf to analyze widely used JavaScript web applications and show that JIT-unfriendly code is prevalent in practice. Furthermore, we show how to use the approach as a profiling technique that finds optimization opportunities in a program. Applying the profiler to popular benchmark programs shows that refactoring these programs to avoid performance problems identified by JITProf leads to statistically significant performance improvements of up to 26.3% in 15 benchmarks. </p>",,,10.1145/2786805.2786831,,,JITProf;Jalangi;JavaScript;dynamic analysis;just-in-time compilation;profiler,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Effective and precise dynamic detection of hidden races for Java programs,Y. Cai; L. Cao,"Institute of Software at Chinese Academy of Sciences, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,450,461,"<p> Happens-before relation is widely used to detect data races dynami-cally. However, it could easily hide many data races as it is inter-leaving sensitive. Existing techniques based on randomized sched-uling are ineffective on detecting these hidden races. In this paper, we propose DrFinder, an effective and precise dynamic technique to detect hidden races. Given an execution, DrFinder firstly analyz-es the lock acquisitions in it and collects a set of ""may-trigger"" relations. Each may-trigger relation consists of a method and a type of a Java object. It indicates that, during execution, the method may directly or indirectly acquire a lock of the type. In the subsequent executions of the same program, DrFinder actively schedules the execution according to the set of collected may-trigger relations. It aims to reverse the set of happens-before relation that may exist in the previous executions so as to expose those hidden races. To effectively detect hidden races in each execution, DrFinder also collects a new set of may-trigger relation during its scheduling, which is used in its next scheduling. Our experiment on a suite of real-world Java multithreaded programs shows that DrFinder is effective to detect 89 new data races in 10 runs. Many of these races could not be detected by existing techniques (i.e., FastTrack, ConTest, and PCT) even in 100 runs. </p>",,,10.1145/2786805.2786839,,,Data race;hidden race;synchronization order;thread scheduling,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Modeling readability to improve unit tests,E. Daka; J. Campos; G. Fraser; J. Dorn; W. Weimer,"University of Sheffield, UK",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,107,118,"<p> Writing good unit tests can be tedious and error prone, but even once they are written, the job is not done: Developers need to reason about unit tests throughout software development and evolution, in order to diagnose test failures, maintain the tests, and to understand code written by other developers. Unreadable tests are more difficult to maintain and lose some of their value to developers. To overcome this problem, we propose a domain-specific model of unit test readability based on human judgements, and use this model to augment automated unit test generation. The resulting approach can automatically generate test suites with both high coverage and also improved readability. In human studies users prefer our improved tests and are able to answer maintenance questions about them 14% more quickly at the same level of accuracy. </p>",,,10.1145/2786805.2786838,,,Readability;automated test generation;unit testing,,,,,10,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Barriers and enablers for shortening software development lead-time in mechatronics organizations: a case study,M. M. Mahally; M. Staron; J. Bosch,"Volvo, Sweden",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1006,1009,"<p> The automotive industry adopts various approaches to reduce the production lead time in order to be competitive on the market. Due to the increasing amount of in-house software development, this industry gets new opportunities to decrease the software development lead-time. This can have a significant impact on decreasing time to market and fewer resources spent in projects. In this paper we present a study of software development areas where we perceived barriers for fast development and where we have identified enablers to overcome these barriers. We conducted a case study at one of the vehicle manufacturers in Sweden using structured interviews. Our results show that there are 21 barriers and 21 corresponding enablers spread over almost all phases of software development. </p>",,,10.1145/2786805.2804433,,,Lead-time;V-model;automotive software,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
MatrixMiner: a red pill to architect informal product descriptions in the matrix,S. Ben Nasr; G. B’©can; M. Acher; J. B. Ferreira Filho; B. Baudry; N. Sannier; J. M. Davril,"University of Rennes 1, France / INRIA, France / IRISA, France",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,982,985,"<p> Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condensed view of a product line. This paper introduces MatrixMiner: a tool for automatically synthesizing product comparison matrices (PCMs) from a set of product descriptions written in natural language. MatrixMiner is capable of identifying and organizing features and values in a PCM Š—– despite the informality and absence of structure in the textual descriptions of products. Our empirical results of products mined from BestBuy show that the synthesized PCMs exhibit numerous quantitative, comparable information. Users can exploit MatrixMiner to visualize the matrix through a Web editor and review, refine, or complement the cell values thanks to the traceability with the original product descriptions and technical specifications. </p>",,,10.1145/2786805.2803180,,,Product Comparison Matrices;Software Product Lines;Variability Mining,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Bespoke tools: adapted to the concepts developers know,B. Johnson; R. Pandita; E. Murphy-Hill; S. Heckman,"North Carolina State University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,878,881,"<p> Even though different developers have varying levels of expertise, the tools in one developer's integrated development environment (IDE) behave the same as the tools in every other developers' IDE. In this paper, we propose the idea of automatically customizing development tools by modeling what a developer knows about software concepts. We then sketch three such ``bespoke'' tools and describe how development data can be used to infer what a developer knows about relevant concepts. Finally, we describe our ongoing efforts to make bespoke program analysis tools that customize their notifications to the developer using them. </p>",,,10.1145/2786805.2803197,,,IDE;adaptive tools;concept models,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Improving energy consumption in Android apps,C. Bernal-C’rdenas,"College of William and Mary, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1048,1050,"<p> Mobile applications sometimes exhibit behaviors that can be attributed to energy bugs depending on developer implementation decisions. In other words, certain design decisions that are technically Š—“correctŠ— might affect the energy performance of applications. Such choices include selection of color palettes, libraries used, API usage and task scheduling order. We study the energy consumption of Android apps using a power model based on a multi-objective approach that minimizes the energy consumption, maximizes the contrast, and minimizes the distance between the chosen colors by comparing the new options to the original palette. In addition, the usage of unnecessary resources can also be a cause of energy bugs depending on whether or not these are implemented correctly. We present an opportunity for continuous investigation of energy bugs by analyzing components in the background during execution on Android applications. This includes a potential new taxonomy type that is not covered by state-of-the-art approaches. </p>",,,10.1145/2786805.2807558,,,Energy consumption;empirical study;mobile applications,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Clone-based and interactive recommendation for modifying pasted code,Y. Lin; X. Peng; Z. Xing; D. Zheng; W. Zhao,"Fudan University, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,520,531,"<p> Developers often need to modify pasted code when programming with copy-and-paste practice. Some modifications on pasted code could involve lots of editing efforts, and any missing or wrong edit could incur bugs. In this paper, we propose a clone-based and interactive approach to recommending where and how to modify the pasted code. In our approach, we regard clones of the pasted code as the results of historical copy-and-paste operations and their differences as historical modifications on the same piece of code. Our approach first retrieves clones of the pasted code from a clone repository and detects syntactically complete differences among them. Then our approach transfers each clone difference into a modification slot on the pasted code, suggests options for each slot, and further mines modifying regulations from the clone differences. Based on the mined modifying regulations, our approach dynamically updates the suggested options and their ranking in each slot according to developer's modifications on the pasted code. We implement a proof-of-concept tool CCDemon based on our approach and evaluate its effectiveness based on code clones detected from five open source projects. The results show that our approach can identify 96.9% of the to-be-modified positions in pasted code and suggest 75.0% of the required modifications. Our human study further confirms that CCDemon can help developers to accomplish their modifications of pasted code more efficiently. </p>",,,10.1145/2786805.2786871,,,code clone;copy and paste;differencing;recommendation;reuse,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Proactive self-adaptation under uncertainty: a probabilistic model checking approach,G. A. Moreno; J. C’mara; D. Garlan; B. Schmerl,"SEI, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1,12,"<p> Self-adaptive systems tend to be reactive and myopic, adapting in response to changes without anticipating what the subsequent adaptation needs will be. Adapting reactively can result in inefficiencies due to the system performing a suboptimal sequence of adaptations. Furthermore, when adaptations have latency, and take some time to produce their effect, they have to be started with sufficient lead time so that they complete by the time their effect is needed. Proactive latency-aware adaptation addresses these issues by making adaptation decisions with a look-ahead horizon and taking adaptation latency into account. In this paper we present an approach for proactive latency-aware adaptation under uncertainty that uses probabilistic model checking for adaptation decisions. The key idea is to use a formal model of the adaptive system in which the adaptation decision is left underspecified through nondeterminism, and have the model checker resolve the nondeterministic choices so that the accumulated utility over the horizon is maximized. The adaptation decision is optimal over the horizon, and takes into account the inherent uncertainty of the environment predictions needed for looking ahead. Our results show that the decision based on a look-ahead horizon, and the factoring of both tactic latency and environment uncertainty, considerably improve the effectiveness of adaptation decisions. </p>",,,10.1145/2786805.2786853,,,Latency-aware;proactive;probabilistic model checking;self-adaptation,,,,,16,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
T3i: a tool for generating and querying test suites for Java,I. S. W. B. Prasetya,"Utrecht University, Netherlands",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,950,953,"<p> T3i is an automated unit-testing tool to test Java classes. To expose interactions T3i generates test-cases in the form of sequences of calls to the methods of the target class. What separates it from other testing tools is that it treats test suites as first class objects and allows users to e.g. combine, query, and filter them. With these operations, the user can construct a test suite with specific properties. Queries can be used to check correctness properties. Hoare triples, LTL formulas, and algebraic equations can be queried. T3i can be used interactively, thus facilitating more exploratory testing, as well as through a script. The familiar Java syntax can be used to control it, or alternatively one can use the much lighter Groovy syntax. </p>",,,10.1145/2786805.2803182,,,automated testing Java;automated testing Object Oriented programs;automated unit testing,,,,,5,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Witness validation and stepwise testification across software verifiers,D. Beyer; M. Dangl; D. Dietsch; M. Heizmann; A. Stahlbauer,"University of Passau, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,721,733,"<p> It is commonly understood that a verification tool should provide a counterexample to witness a specification violation. Until recently, software verifiers dumped error witnesses in proprietary formats, which are often neither human- nor machine-readable, and an exchange of witnesses between different verifiers was impossible. To close this gap in software-verification technology, we have defined an exchange format for error witnesses that is easy to write and read by verification tools (for further processing, e.g., witness validation) and that is easy to convert into visualizations that conveniently let developers inspect an error path. To eliminate manual inspection of false alarms, we develop the notion of stepwise testification: in a first step, a verifier finds a problematic program path and, in addition to the verification result FALSE, constructs a witness for this path; in the next step, another verifier re-verifies that the witness indeed violates the specification. This process can have more than two steps, each reducing the state space around the error path, making it easier to validate the witness in a later step. An obvious application for testification is the setting where we have two verifiers: one that is efficient but imprecise and another one that is precise but expensive. We have implemented the technique of error-witness-driven program analysis in two state-of-the-art verification tools, CPAchecker and Ultimate Automizer, and show by experimental evaluation that the approach is applicable to a large set of verification tasks. </p>",,,10.1145/2786805.2786867,,,Counterexample Validation;Error Witness;Model Checking;Program Analysis;Software Verification,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Crowd debugging,F. Chen; S. Kim,"Hong Kong University of Science and Technology, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,320,332,"<p> Research shows that, in general, many people turn to QA sites to solicit answers to their problems. We observe in Stack Overflow a huge number of recurring questions, 1,632,590, despite mechanisms having been put into place to prevent these recurring questions. Recurring questions imply developers are facing similar issues in their source code. However, limitations exist in the QA sites. Developers need to visit them frequently and/or should be familiar with all the content to take advantage of the crowd's knowledge. Due to the large and rapid growth of QA data, it is difficult, if not impossible for developers to catch up. To address these limitations, we propose mining the QA site, Stack Overflow, to leverage the huge mass of crowd knowledge to help developers debug their code. Our approach reveals 189 warnings and 171 (90.5%) of them are confirmed by developers from eight high-quality and well-maintained projects. Developers appreciate these findings because the crowd provides solutions and comprehensive explanations to the issues. We compared the confirmed bugs with three popular static analysis tools (FindBugs, JLint and PMD). Of the 171 bugs identified by our approach, only FindBugs detected six of them whereas JLint and PMD detected none. </p>",,,10.1145/2786805.2786819,,,Crowd Debugging;Crowd Sourcing;Debugging,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Developer onboarding in GitHub: the role of prior social links and language experience,C. Casalnuovo; B. Vasilescu; P. Devanbu; V. Filkov,"University of California at Davis, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,817,828,"<p> The team aspects of software engineering have been a subject of great interest since early work by Fred Brooks and others: how well do people work together in teams? why do people join teams? what happens if teams are distributed? Recently, the emergence of project ecosystems such as GitHub have created an entirely new, higher level of organization. GitHub supports numerous teams; they share a common technical platform (for work activities) and a common social platform (via following, commenting, etc). We explore the GitHub evidence for socialization as a precursor to joining a project, and how the technical factors of past experience and social factors of past connections to team members of a project affect productivity both initially and in the long run. We find developers preferentially join projects in GitHub where they have pre-existing relationships; furthermore, we find that the presence of past social connections combined with prior experience in languages dominant in the project leads to higher productivity both initially and cumulatively. Interestingly, we also find that stronger social connections are associated with slightly less productivity initially, but slightly more productivity in the long run. </p>",,,10.1145/2786805.2786854,,,GitHub;onboarding;productivity;social aspects,,,,,5,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Information retrieval and spectrum based bug localization: better together,T. D. B. Le; R. J. Oentaryo; D. Lo,"Singapore Management University, Singapore",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,579,590,"<p> Debugging often takes much effort and resources. To help developers debug, numerous information retrieval (IR)-based and spectrum-based bug localization techniques have been proposed. IR-based techniques process textual information in bug reports, while spectrum-based techniques process program spectra (i.e., a record of which program elements are executed for each test case). Both eventually generate a ranked list of program elements that are likely to contain the bug. However, these techniques only consider one source of information, either bug reports or program spectra, which is not optimal. To deal with the limitation of existing techniques, in this work, we propose a new multi-modal technique that considers both bug reports and program spectra to localize bugs. Our approach adaptively creates a bug-specific model to map a particular bug to its possible location, and introduces a novel idea of suspicious words that are highly associated to a bug. We evaluate our approach on 157 real bugs from four software systems, and compare it with a state-of-the-art IR-based bug localization method, a state-of-the-art spectrum-based bug localization method, and three state-of-the-art multi-modal feature location methods that are adapted for bug localization. Experiments show that our approach can outperform the baselines by at least 47.62%, 31.48%, 27.78%, and 28.80% in terms of number of bugs successfully localized when a developer inspects 1, 5, and 10 program elements (i.e., Top 1, Top 5, and Top 10), and Mean Average Precision (MAP) respectively. </p>",,,10.1145/2786805.2786880,,,Bug Localization;Information Retrieval;Program Spectra,,,,,10,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Generating TCP/UDP network data for automated unit test generation,A. Arcuri; G. Fraser; J. P. Galeotti,"Scienta, Norway / University of Luxembourg, Luxembourg",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,155,165,"<p> Although automated unit test generation techniques can in principle generate test suites that achieve high code coverage, in practice this is often inhibited by the dependence of the code under test on external resources. In particular, a common problem in modern programming languages is posed by code that involves networking (e.g., opening a TCP listening port). In order to generate tests for such code, we describe an approach where we mock (simulate) the networking interfaces of the Java standard library, such that a search-based test generator can treat the network as part of the test input space. This not only has the benefit that it overcomes many limitations of testing networking code (e.g., different tests binding to the same local ports, and deterministic resolution of hostnames and ephemeral ports), it also substantially increases code coverage. An evaluation on 23,886 classes from 110 open source projects, totalling more than 6.6 million lines of Java code, reveals that network access happens in 2,642 classes (11%). Our implementation of the proposed technique as part of the EVOSUITE testing tool addresses the networking code contained in 1,672 (63%) of these classes, and leads to an increase of the average line coverage from 29.1% to 50.8%. On a manual selection of 42 Java classes heavily depending on networking, line coverage with EVOSUITE more than doubled with the use of network mocking, increasing from 31.8% to 76.6%. </p>",,,10.1145/2786805.2786828,,,JUnit;Java;Unit testing;automated test generation,,,,,4,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Summarizing and measuring development activity,C. Treude; F. Figueira Filho; U. Kulesza,"Federal University of Rio Grande do Norte, Brazil",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,625,636,"<p> Software developers pursue a wide range of activities as part of their work, and making sense of what they did in a given time frame is far from trivial as evidenced by the large number of awareness and coordination tools that have been developed in recent years. To inform tool design for making sense of the information available about a developer's activity, we conducted an empirical study with 156 GitHub users to investigate what information they would expect in a summary of development activity, how they would measure development activity, and what factors influence how such activity can be condensed into textual summaries or numbers. We found that unexpected events are as important as expected events in summaries of what a developer did, and that many developers do not believe in measuring development activity. Among the factors that influence summarization and measurement of development activity, we identified development experience and programming languages. </p>",,,10.1145/2786805.2786827,,,Summarization;development activity;empirical study,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Detecting event anomalies in event-based systems,G. Safi; A. Shahbazian; W. G. J. Halfond; N. Medvidovic,"University of Southern California, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,25,37,"<p> Event-based interaction is an attractive paradigm because its use can lead to highly flexible and adaptable systems. One problem in this paradigm is that events are sent, received, and processed nondeterministically, due to the systemsŠ—È reliance on implicit invocation and implicit concurrency. This nondeterminism can lead to event anomalies, which occur when an event-based system receives multiple events that lead to the write of a shared field or memory location. Event anomalies can lead to unreliable, error-prone, and hard to debug behavior in an event-based system. To detect these anomalies, this paper presents a new static analysis technique, DEvA, for automatically detecting event anomalies. DEvA has been evaluated on a set of open-source event-based systems against a state-of-the-art technique for detecting data races in multithreaded systems, and a recent technique for solving a similar problem with event processing in Android applications. DEvA exhibited high precision with respect to manually constructed ground truths, and was able to locate event anomalies that had not been detected by the existing solutions. </p>",,,10.1145/2786805.2786836,,,Android application;Event anomaly;Event-based system;Race detection,,,,,5,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Nyx: a display energy optimizer for mobile web apps,D. Li; A. H. Tran; W. G. J. Halfond,"University of Southern California, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,958,961,"<p> Energy is a critical resource for current mobile devices. In a smartphone, display is one of the most energy consuming components. Modern smartphones often use OLED screens, which consume much more energy when displaying light colors than displaying dark colors. In our previous study, we proposed a technique to reduce display energy of mo- bile web apps by changing the color scheme automatically. With this approach, we achieved a 40% reduction in display power consumption and 97% user acceptance of the new color scheme. In this tool paper, we describe Nyx, which implements our approach. Nyx is implemented as a self- contained executable file with which users can optimize en- ergy consumption of their web apps with a simple command. </p>",,,10.1145/2786805.2803190,,,Energy;OLED;display;mobile,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
MemInsight: platform-independent memory debugging for JavaScript,S. H. Jensen; M. Sridharan; K. Sen; S. Chandra,"Snowflake Computing, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,345,356,"<p> JavaScript programs often suffer from memory issues that can either hurt performance or eventually cause memory exhaustion. While existing snapshot-based profiling tools can be helpful, the information provided is limited to the coarse granularity at which snapshots can be taken. We present MemInsight, a tool that provides detailed, time-varying analysis of the memory behavior of JavaScript applications, including web applications. MemInsight is platform independent and runs on unmodified JavaScript engines. It employs tuned source-code instrumentation to generate a trace of memory allocations and accesses, and it leverages modern browser features to track precise information for DOM (document object model) objects. It also computes exact object lifetimes without any garbage collector assistance, and exposes this information in an easily-consumable manner for further analysis. We describe several client analyses built into MemInsight, including detection of possible memory leaks and opportunities for stack allocation and object inlining. An experimental evaluation showed that with no modifications to the runtime, MemInsight was able to expose memory issues in several real-world applications. </p>",,,10.1145/2786805.2786860,,,Memory profiling;leak detection,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Finding schedule-sensitive branches,J. Huang; L. Rauchwerger,"Texas A&M University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,439,449,"<p> This paper presents an automated, precise technique, TAME, for identifying schedule-sensitive branches (SSBs) in concurrent programs, i.e., branches whose decision may vary depending on the actual scheduling of concurrent threads. The technique consists of 1) tracing events at fine-grained level; 2) deriving the constraints for each branch; and 3) invoking an SMT solver to find possible SSB, by trying to solve the negated branch condition. To handle the infeasibly huge number of computations that would be generated by the fine-grained tracing, TAME leverages concolic execution and implements several sound approximations to delimit the number of traces to analyse, yet without sacrificing precision. In addition, TAME implements a novel distributed trace partition approach distributing the analysis into smaller chunks. Evaluation on both popular benchmarks and real applications shows that TAME is effective in finding SSBs and has good scalability. TAME found a total of 34 SSBs, among which 17 are related to concurrency errors, and 9 are ad hoc synchronizations. </p>",,,10.1145/2786805.2786840,,,Schedule-Sensitive Branches;Symbolic Constraint Analysis,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
How practitioners perceive the relevance of software engineering research,D. Lo; N. Nagappan; T. Zimmermann,"Singapore Management University, Singapore",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,415,425,"<p> The number of software engineering research papers over the last few years has grown significantly. An important question here is: how relevant is software engineering research to practitioners in the field? To address this question, we conducted a survey at Microsoft where we invited 3,000 industry practitioners to rate the relevance of research ideas contained in 571 ICSE, ESEC/FSE and FSE papers that were published over a five year period. We received 17,913 ratings by 512 practitioners who labelled ideas as essential, worthwhile, unimportant, or unwise. The results from the survey suggest that practitioners are positive towards studies done by the software engineering research community: 71% of all ratings were essential or worthwhile. We found no correlation between the citation counts and the relevance scores of the papers. Through a qualitative analysis of free text responses, we identify several reasons why practitioners considered certain research ideas to be unwise. The survey approach described in this paper is lightweight: on average, a participant spent only 22.5 minutes to respond to the survey. At the same time, the results can provide useful insight to conference organizers, authors, and participating practitioners. </p>",,,10.1145/2786805.2786809,,,Industry;Software Engineering Research;Survey,,,,,9,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
GR(1) synthesis for LTL specification patterns,S. Maoz; J. O. Ringert,"Tel Aviv University, Israel",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,96,106,"<p> Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Two of the main challenges in bringing reactive synthesis to software engineering practice are its very high worst-case complexity -- for linear temporal logic (LTL) it is double exponential in the length of the formula, and the difficulty of writing declarative specifications using basic LTL operators. To address the first challenge, Piterman et al. have suggested the General Reactivity of Rank 1 (GR(1)) fragment of LTL, which has an efficient polynomial time symbolic synthesis algorithm. To address the second challenge, Dwyer et al. have identified 55 LTL specification patterns, which are common in industrial specifications and make writing specifications easier. In this work we show that almost all of the 55 LTL specification patterns identified by Dwyer et al. can be expressed as assumptions and guarantees in the GR(1) fragment of LTL. Specifically, we present an automated, sound and complete translation of the patterns to the GR(1) form, which effectively results in an efficient reactive synthesis procedure for any specification that is written using the patterns. We have validated the correctness of the catalog of GR(1) templates we have created. The work is implemented in our reactive synthesis environment. It provides positive, promising evidence, for the potential feasibility of using reactive synthesis in practice. </p>",,,10.1145/2786805.2786824,,,Linear temporal logic;specification patterns;synthesis,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Evaluating a formal scenario-based method for the requirements analysis in automotive software engineering,J. Greenyer; M. Haase; J. Marhenke; R. Bellmer,"Leibniz Universit&#228;t Hannover, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1002,1005,"<p> Automotive software systems often consist of multiple reactive components that must satisfy complex and safety-critical requirements. In automotive projects, the requirements are usually documented informally and are reviewed manually; this regularly causes inconsistencies to remain hidden until the integration phase, where their repair requires costly iterations. We therefore seek methods for the early automated requirement analysis and evaluated the scenario-based specification approach based on LSCs/MSDs; it promises to support an incremental and precise specification of requirements, and offers automated analysis through scenario execution and formal realizability checking. In a case study, we used ScenarioTools to model and analyze the requirements of a software to control a high-voltage coupling for electric vehicles. Our example contained 36 requirements and assumptions that we could successfully formalize, and we could successfully find specification defects by automated realizability checking. In this paper, we report on lessons learned, tool and method extensions we have introduced, and open challenges. </p>",,,10.1145/2786805.2804432,,,Automotive Software;Modal Sequence Diagrams;Reactive Systems;Realizability;Requirements Analysis,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
UEDashboard: awareness of unusual events in commit histories,L. Leite; C. Treude; F. Figueira Filho,"Federal University of Rio Grande do Norte, Brazil",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,978,981,"<p> To be able to respond to source code modifications with large impact or commits that necessitate further examination, developers and managers in a software development team need to be aware of anything unusual happening in their software projects. To address this need, we introduce UEDashboard, a tool which automatically detects unusual events in a commit history based on metrics and smells, and surfaces them in an event feed. Our preliminary evaluation with a team of professional software developers showed that our conceptualization of unusual correlates with developers' perceptions of task difficulty, and that UEDashboard could be useful in supporting development meetings and for pre-commit warnings. </p>",,,10.1145/2786805.2803184,,,Awareness;commit history;unusual events,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Iterative distribution-aware sampling for probabilistic symbolic execution,M. Borges; A. Filieri; M. D'Amorim; C. S. PóÛsóÛreanu,"University of Stuttgart, Germany / Federal University of Pernambuco, Brazil",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,866,877,"<p> Probabilistic symbolic execution aims at quantifying the probability of reaching program events of interest assuming that program inputs follow given probabilistic distributions. The technique collects constraints on the inputs that lead to the target events and analyzes them to quantify how likely it is for an input to satisfy the constraints. Current techniques either handle only linear constraints or only support continuous distributions using a Š—“discretizationŠ— of the input domain, leading to imprecise and costly results. We propose an iterative distribution-aware sampling approach to support probabilistic symbolic execution for arbitrarily complex mathematical constraints and continuous input distributions. We follow a compositional approach, where the symbolic constraints are decomposed into sub-problems whose solution can be solved independently. At each iteration the convergence rate of the com- putation is increased by automatically refocusing the analysis on estimating the sub-problems that mostly affect the accuracy of the results, as guided by three different ranking strategies. Experiments on publicly available benchmarks show that the proposed technique improves on previous approaches in terms of scalability and accuracy of the results. </p>",,,10.1145/2786805.2786832,,,Monte Carlo Sampling;Probabilistic Analysis;Symbolic Execution,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
The making of cloud applications: an empirical study on software development for the cloud,J. Cito; P. Leitner; T. Fritz; H. C. Gall,"University of Zurich, Switzerland",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,393,403,"<p> Cloud computing is gaining more and more traction as a deployment and provisioning model for software. While a large body of research already covers how to optimally operate a cloud system, we still lack insights into how professional software engineers actually use clouds, and how the cloud impacts development practices. This paper reports on the first systematic study on how software developers build applications for the cloud. We conducted a mixed-method study, consisting of qualitative interviews of 25 professional developers and a quantitative survey with 294 responses. Our results show that adopting the cloud has a profound impact throughout the software development process, as well as on how developers utilize tools and data in their daily work. Among other things, we found that (1) developers need better means to anticipate runtime problems and rigorously define metrics for improved fault localization and (2) the cloud offers an abundance of operational data, however, developers still often rely on their experience and intuition rather than utilizing metrics. From our findings, we extracted a set of guidelines for cloud development and identified challenges for researchers and tool vendors. </p>",,,10.1145/2786805.2786826,,,cloud computing;user study,,,,,6,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Navigating through the archipelago of refactorings,A. V. Zarras; T. Vartziotis; P. Vassiliadis,"University of Ioannina, Greece",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,922,925,"<p> The essence of refactoring is to improve software quality via the systematic combination of primitive refactorings. Yet, there are way too many refactorings. Choosing which refactorings to use, how to combine them and how to integrate them in more complex evolution tasks is really hard. Our vision is to provide the developer with a ""trip advisor"" for the archipelago of refactorings. The core idea of our approach is the map of the archipelago of refactorings, which identies the basic relations that guide the systematic and eective combination of refactorings. Based on the map, the trip advisor makes suggestions that allow the developer to decide how to start, assess the possible alternatives, have a clear picture of what has to be done before, during and after the refactorings and assess the possible implications. </p>",,,10.1145/2786805.2803203,,,Refactoring composition;Refactoring map;Refactoring recommendation,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Crash reproduction via test case mutation: let existing test cases help,J. Xuan; X. Xie; M. Monperrus,"Wuhan University, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,910,913,"<p> Developers reproduce crashes to understand root causes during software debugging. To reduce the manual effort by developers, automatic methods of crash reproduction generate new test cases for triggering crashes. However, due to the complex program structures, it is challenging to generate a test case to cover a specific program path. In this paper, we propose an approach to automatic crash reproduction via test case mutation, which updates existing test cases to trigger crashes rather than creating new test cases from scratch. This approach leverages major structures and objects in existing test cases and increases the chance of executing the specific path. Our preliminary result on 12 crashes in Apache Commons Collections shows that 7 crashes are reproduced by our approach of test case mutation. </p>",,,10.1145/2786805.2803206,,,Crash reproduction;stack trace;test case mutation,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Enhancing Android application bug reporting,K. Moran,"College of William and Mary, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1045,1047,"<p> The modern software development landscape has seen a shift in focus toward mobile applications as smartphones and tablets near ubiquitous adoption. Due to this trend, the complexity of these Š—“appsŠ— has been increasing, making development and maintenance challenging. Current bug tracking systems do not effectively facilitate the creation of bug reports with useful information that will directly lead to a bugŠ—Ès resolution. To address the need for an improved reporting system, we introduce a novel solution, called Fusion, that helps reporters auto-complete reproduction steps in bug reports for mobile apps by taking advantage of their GUI-centric nature. Fusion links information, that reporters provide, to program artifacts extracted through static and dynamic analysis performed beforehand. This allows our system to facilitate the reporting process for developers and testers, while generating more reproducible bug reports with immediately actionable information. </p>",,,10.1145/2786805.2807557,,,Bug reports;android;auto-completion;reproduction steps,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Heterogeneous defect prediction,J. Nam; S. Kim,"Hong Kong University of Science and Technology, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,508,519,"<p> Software defect prediction is one of the most active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. In recent studies, CPDP is proved to be feasible. However, CPDP requires projects that have the same metric set, meaning the metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or are comparable to WPDP with statistical significance. </p>",,,10.1145/2786805.2786814,,,Defect prediction;heterogeneous metrics;quality assurance,,,,,10,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Behavioral log analysis with statistical guarantees,N. Busany; S. Maoz,"Tel Aviv University, Israel",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,898,901,"<p> Scalability is a major challenge for existing behavioral log analysis algorithms, which extract finite-state automaton models or temporal properties from logs generated by running systems. In this work we propose to address scalability using statistical tools. The key to our approach is to consider behavioral log analysis as a statistical experiment. Rather than analyzing the entire log, we suggest to analyze only a sample of traces from the log and, most importantly, provide means to compute statistical guarantees for the correctness of the analysis result. We present two example applications of our approach as well as initial evidence for its effectiveness. </p>",,,10.1145/2786805.2803198,,,Log analysis;specification mining,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Assertions are strongly correlated with test suite effectiveness,Y. Zhang; A. Mesbah,"University of British Columbia, Canada",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,214,224,"<p> Code coverage is a popular test adequacy criterion in practice. Code coverage, however, remains controversial as there is a lack of coherent empirical evidence for its relation with test suite effectiveness. More recently, test suite size has been shown to be highly correlated with effectiveness. However, previous studies treat test methods as the smallest unit of interest, and ignore potential factors influencing this relationship. We propose to go beyond test suite size, by investigating test assertions inside test methods. We empirically evaluate the relationship between a test suiteŠ—Ès effectiveness and the (1) number of assertions, (2) assertion coverage, and (3) different types of assertions. We compose 6,700 test suites in total, using 24,000 assertions of five real-world Java projects. We find that the number of assertions in a test suite strongly correlates with its effectiveness, and this factor directly influences the relationship between test suite size and effectiveness. Our results also indicate that assertion coverage is strongly correlated with effectiveness and different types of assertions can influence the effectiveness of their containing test suites. </p>",,,10.1145/2786805.2786858,,,Test suite effectiveness;assertions;coverage,,,,,9,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
User-centric security: optimization of the security-usability trade-off,D. Feth,"Fraunhofer IESE, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1034,1037,"<p> Security and usability are highly important and interdependent quality attributes of modern IT systems. However, it is often hard to fully meet both in practice. Security measures are complex by nature and often complicate work flows. Vice versa, insecure systems are typically not usable in practice. To tackle this, we aim at finding the best balance between usability and security in software engineering and administration. Our methodology is based on active involvement of large user groups and analyzes user feedback in order to optimize security mechanisms with respect to their user experience, with a focus on security awareness. It is applied during requirements elicitation and prototyping, and to dynamically adapt unsuited security policies at runtime. </p>",,,10.1145/2786805.2803195,,,Measurement;Security;Security-Awareness;Usability,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
DexterJS: robust testing platform for DOM-based XSS vulnerabilities,I. Parameshwaran; E. Budianto; S. Shinde; H. Dang; A. Sadhu; P. Saxena,"National University of Singapore, Singapore",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,946,949,"<p> DOM-based cross-site scripting (XSS) is a client-side vulnerability that pervades JavaScript applications on the web, and has few known practical defenses. In this paper, we introduce DEXTERJS, a testing platform for detecting and validating DOM-based XSS vulnerabilities on web applications. DEXTERJS leverages source-to source rewriting to carry out character-precise taint tracking when executing in the browser contextŠ—”thus being able to identify vulnerable information flows in a web page. By scanning a web page, DEXTERJS produces working exploits that validate DOM-based XSS vulnerability on the page. DEXTERJS is robust, has been tested on AlexaŠ—Ès top 1000 sites, and has found a total of 820 distinct zero-day DOM-XSS confirmed exploits automatically. </p>",,,10.1145/2786805.2803191,,,DOM-based XSS;Taint Analysis;Web Security,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Mimic: computing models for opaque code,S. Heule; M. Sridharan; S. Chandra,"Stanford University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,710,720,"<p> Opaque code, which is executable but whose source is unavailable or hard to process, can be problematic in a number of scenarios, such as program analysis. Manual construction of models is often used to handle opaque code, but this process is tedious and error-prone. (In this paper, we use model to mean a representation of a piece of code suitable for program analysis.) We present a novel technique for automatic generation of models for opaque code, based on program synthesis. The technique intercepts memory accesses from the opaque code to client objects, and uses this information to construct partial execution traces. Then, it performs a heuristic search inspired by Markov Chain Monte Carlo techniques to discover an executable code model whose behavior matches the opaque code. Native execution, parallelization, and a carefully-designed fitness function are leveraged to increase the effectiveness of the search. We have implemented our technique in a tool Mimic for discovering models of opaque JavaScript functions, and used Mimic to synthesize correct models for a variety of array-manipulating routines. </p>",,,10.1145/2786805.2786875,,,JavaScript;MCMC;Opaque code;model generation;program synthesis,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
"Hey, you have given me too many knobs!: understanding and dealing with over-designed configuration in system software",T. Xu; L. Jin; X. Fan; Y. Zhou; S. Pasupathy; R. Talwadker,"University of California at San Diego, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,307,319,"<p> Configuration problems are not only prevalent, but also severely impair the reliability of today's system software. One fundamental reason is the ever-increasing complexity of configuration, reflected by the large number of configuration parameters (""knobs""). With hundreds of knobs, configuring system software to ensure high reliability and performance becomes a daunting, error-prone task. This paper makes a first step in understanding a fundamental question of configuration design: ""do users really need so many knobs?"" To provide the quantitatively answer, we study the configuration settings of real-world users, including thousands of customers of a commercial storage system (Storage-A), and hundreds of users of two widely-used open-source system software projects. Our study reveals a series of interesting findings to motivate software architects and developers to be more cautious and disciplined in configuration design. Motivated by these findings, we provide a few concrete, practical guidelines which can significantly reduce the configuration space. Take Storage-A as an example, the guidelines can remove 51.9% of its parameters and simplify 19.7% of the remaining ones with little impact on existing users. Also, we study the existing configuration navigation methods in the context of ""too many knobs"" to understand their effectiveness in dealing with the over-designed configuration, and to provide practices for building navigation support in system software. </p>",,,10.1145/2786805.2786852,,,Complexity;Configuration;Difficulty;Error;Navigation;Parameter;Simplification,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Quality and productivity outcomes relating to continuous integration in GitHub,B. Vasilescu; Y. Yu; H. Wang; P. Devanbu; V. Filkov,"University of California at Davis, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,805,816,"<p> Software processes comprise many steps; coding is followed by building, integration testing, system testing, deployment, operations, among others. Software process integration and automation have been areas of key concern in software engineering, ever since the pioneering work of Osterweil; market pressures for Agility, and open, decentralized, software development have provided additional pressures for progress in this area. But do these innovations actually help projects? Given the numerous confounding factors that can influence project performance, it can be a challenge to discern the effects of process integration and automation. Software project ecosystems such as GitHub provide a new opportunity in this regard: one can readily find large numbers of projects in various stages of process integration and automation, and gather data on various influencing factors as well as productivity and quality outcomes. In this paper we use large, historical data on process metrics and outcomes in GitHub projects to discern the effects of one specific innovation in process automation: continuous integration. Our main finding is that continuous integration improves the productivity of project teams, who can integrate more outside contributions, without an observable diminishment in code quality. </p>",,,10.1145/2786805.2786850,,,Continuous integration;GitHub;pull requests,,,,,12,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Pockets: a tool to support exploratory programming for novices and educators,E. Makihara,"NAIST, Japan",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1066,1068,"<p> Exploratory programming is one of the programming techniques, and it is considered to be an effective way to improve programming skills for novices. However, there is no existing system or programming environment educating exploratory programming for novices. Therefore, we have developed a tool, named as Pockets, to support novice's exploratory programming. Through Pockets, educators are able to identify where and when novices experience difficulties during exploratory programming. In addition, it is possible to assist educators' mentoring by referring collected logs through the proposed system. We have also conducted a case study and evaluated the usefulness of the tool. As a result, Pockets makes novices' exploratory programming more efficient, and also allows more accurate advice by educators. </p>",,,10.1145/2786805.2807564,,,coding process visualization;exploratory programming;programming education,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Query-based configuration of text retrieval solutions for software engineering tasks,L. Moreno; G. Bavota; S. Haiduc; M. Di Penta; R. Oliveto; B. Russo; A. Marcus,"University of Texas at Dallas, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,567,578,"<p> Text Retrieval (TR) approaches have been used to leverage the textual information contained in software artifacts to address a multitude of software engineering (SE) tasks. However, TR approaches need to be configured properly in order to lead to good results. Current approaches for automatic TR configuration in SE configure a single TR approach and then use it for all possible queries. In this paper, we show that such a configuration strategy leads to suboptimal results, and propose QUEST, the first approach bringing TR configuration selection to the query level. QUEST recommends the best TR configuration for a given query, based on a supervised learning approach that determines the TR configuration that performs the best for each query according to its properties. We evaluated QUEST in the context of feature and bug localization, using a data set with more than 1,000 queries. We found that QUEST is able to recommend one of the top three TR configurations for a query with a 69% accuracy, on average. We compared the results obtained with the configurations recommended by QUEST for every query with those obtained using a single TR configuration for all queries in a system and in the entire data set. We found that using QUEST we obtain better results than with any of the considered TR configurations. </p>",,,10.1145/2786805.2786859,,,Configuration;Feature and Bug Localization;Text-Retrieval in Software Engineering,,,,,6,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Systematic testing of asynchronous reactive systems,A. Desai; S. Qadeer; S. A. Seshia,"University of California at Berkeley, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,73,83,"<p> We introduce the concept of a delaying explorer with the goal of performing prioritized exploration of the behaviors of an asynchronous reactive program. A delaying explorer stratifies the search space using a custom strategy, and a delay operation that allows deviation from that strategy. We show that prioritized search with a delaying explorer performs significantly better than existing prioritization techniques. We also demonstrate empirically the need for writing different delaying explorers for scalable systematic testing and hence, present a flexible delaying explorer interface. We introduce two new techniques to improve the scalability of search based on delaying explorers. First, we present an algorithm for stratified exhaustive search and use efficient state caching to avoid redundant exploration of schedules. We provide soundness and termination guarantees for our algorithm. Second, for the cases where the state of the system cannot be captured or there are resource constraints, we present an algorithm to randomly sample any execution from the stratified search space. This algorithm guarantees that any such execution that requires $d$ delay operations is sampled with probability at least $1/L^d$, where $L$ is the maximum number of program steps. We have implemented our algorithms and evaluated them on a collection of real-world fault-tolerant distributed protocols. </p>",,,10.1145/2786805.2786861,,,Systematic testing;asynchronous programs;distributed systems;model checking;random sampling,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Optimizing energy consumption of GUIs in Android apps: a multi-objective approach,M. Linares-V’squez; G. Bavota; C. E. B. C’rdenas; R. Oliveto; M. Di Penta; D. Poshyvanyk,"College of William and Mary, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,143,154,"<p> The wide diffusion of mobile devices has motivated research towards optimizing energy consumption of software systemsŠ—” including appsŠ—”targeting such devices. Besides efforts aimed at dealing with various kinds of energy bugs, the adoption of Organic Light-Emitting Diode (OLED) screens has motivated research towards reducing energy consumption by choosing an appropriate color palette. Whilst past research in this area aimed at optimizing energy while keeping an acceptable level of contrast, this paper proposes an approach, named GEMMA (Gui Energy Multi-objective optiMization for Android apps), for generating color palettes using a multi- objective optimization technique, which produces color solutions optimizing energy consumption and contrast while using consistent colors with respect to the original color palette. An empirical evaluation that we performed on 25 Android apps demonstrates not only significant improvements in terms of the three different objectives, but also confirmed that in most cases users still perceived the choices of colors as attractive. Finally, for several apps we interviewed the original developers, who in some cases expressed the intent to adopt the proposed choice of color palette, whereas in other cases pointed out directions for future improvements </p>",,,10.1145/2786805.2786847,,,Empirical Study;Energy consumption;Mobile applications,,,,,12,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
OnSpot system: test impact visibility during code edits in real software,M. U. Janjua,"Microsoft, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,994,997,"<p> For maintaining the quality of software updates to complex software products (e.g. Windows 7 OS), an extensive, broad level regression testing is conducted whenever releasing new code fixes or updates. Despite the huge cost and investment in the test infrastructure to execute these massive tests, the developer of the code fix has to wait for the regression test failures to be reported after checkin. These regression tests typically run way later from the code editing stage and consequently the developer has no test impact visibility while introducing the code changes at compile time or before checkin. We argue that it is valuable and practically feasible to tailor the entire development/testing process to provide valuable and actionable test feedback at the development/compilation stage as well. With this goal, this paper explores a system model that provides a near real-time test feedback based on regression tests while the code change is under development or as soon as it becomes compilable. OnSpot system dynamically overlays the results of tests on relevant source code lines in the development environment; thereby highlighting test failures akin to syntax failures enabling quicker correction and re-run at compile time rather than late when the damage is already done. We evaluate OnSpot system with the security fixes in Windows 7 while considering various factors like test feedback time, coverage ratio. We found out that on average nearly 40% of the automated Windows 7 regression test collateral could run under 30 seconds providing same level of coverage; thereby making OnSpot approach practically feasible and manageable during compile time </p>",,,10.1145/2786805.2804430,,,analysis;code writing;development;early regression;real product testing;software;testing,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Automatically deriving pointer reference expressions from binary code for memory dump analysis,Y. Fu; Z. Lin; D. Brumley,"University of Texas at Dallas, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,614,624,"<p> Given a crash dump or a kernel memory snapshot, it is often desirable to have a capability that can traverse its pointers to locate the root cause of the crash, or check their integrity to detect the control flow hijacks. To achieve this, one key challenge lies in how to locate where the pointers are. While locating a pointer usually requires the data structure knowledge of the corresponding program, an important advance made by this work is that we show a technique of extracting address-independent data reference expressions for pointers through dynamic binary analysis. This novel pointer reference expression encodes how a pointer is accessed through the combination of a base address (usually a global variable) with certain offset and further pointer dereferences. We have applied our techniques to OS kernels, and our experimental results with a number of real world kernel malware show that we can correctly identify the hijacked kernel function pointers by locating them using the extracted pointer reference expressions when only given a memory snapshot. </p>",,,10.1145/2786805.2786810,,,Kernel Integrity;memory forensics;taint analysis,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
OSSMETER: a software measurement platform for automatically analysing open source software projects,D. Di Ruscio; D. S. Kolovos; I. Korkontzelos; N. Matragkas; J. J. Vinju,"University of L&#039;Aquila, Italy",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,970,973,"<p> Deciding whether an open source software (OSS) project meets the required standards for adoption in terms of quality, maturity, activity of development and user support is not a straightforward process as it involves exploring various sources of information. Such sources include OSS source code repositories, communication channels such as newsgroups, forums, and mailing lists, as well as issue tracking systems. OSSMETER is an extensible and scalable platform that can monitor and incrementally analyse a large number of OSS projects. The results of this analysis can be used to assess various aspects of OSS projects, and to directly compare different OSS projects with each other. </p>",,,10.1145/2786805.2803186,,,Open source software;Source code analysis;Text mining techniques,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Hidden truths in dead software paths,M. Eichberg; B. Hermann; M. Mezini; L. Glanz,"TU Darmstadt, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,474,484,"<p> Approaches and techniques for statically finding a multitude of issues in source code have been developed in the past. A core property of these approaches is that they are usually targeted towards finding only a very specific kind of issue and that the effort to develop such an analysis is significant. This strictly limits the number of kinds of issues that can be detected. In this paper, we discuss a generic approach based on the detection of infeasible paths in code that can discover a wide range of code smells ranging from useless code that hinders comprehension to real bugs. Code issues are identified by calculating the difference between the control-flow graph that contains all technically possible edges and the corresponding graph recorded while performing a more precise analysis using abstract interpretation. We have evaluated the approach using the Java Development Kit as well as the Qualitas Corpus (a curated collection of over 100 Java Applications) and were able to find thousands of issues across a wide range of categories. </p>",,,10.1145/2786805.2786865,,,Abstract Interpretation;Finding Bugs;Infeasible Paths;Java;Scalable Analysis,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Towards automating the security compliance value chain,S. Ghaisas; M. Motwani; B. Balasubramaniam; A. Gajendragadkar; R. Kelkar; H. Vin,"Tata Consultancy Services, India",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1014,1017,"<p> Information security is of paramount importance in this digital era. While businesses strive to adopt industry-accepted system-hardening standards such as benchmarks recommended by the Center for Internet Security (CIS) to combat threats, they are confronted with an additional challenge of ever-evolving regulations that address security concerns. These create additional requirements, which must be incorporated into software systems. In this paper, we present a generic approach towards automating different activities of the Security Compliance Value Chain (SCVC) in organizations. We discuss the approach in the context of the Payment Card Industry Data Security Standard (PCI-DSS) regulations. Specifically, we present automation of (1) interpretation of PCI-DSS regulations to infer system requirements, (2) traceability of the inferred system requirements to CIS security controls (3) implementation of appropriate security controls, and finally, (4) verification and reporting of compliance. </p>",,,10.1145/2786805.2804435,,,Rule Model;Rule act;Rule intent,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
GitSonifier: using sound to portray developer conflict history,K. J. North; S. Bolan; A. Sarma; M. B. Cohen,"University of Nebraska-Lincoln, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,886,889,"<p> There are many tools that help software engineers analyze data about their software, projects, and teams. These tools primarily use visualizations to portray data in a concise and understandable way. However, software engineering tasks are often multi-dimensional and temporal, making some visualizations difficult to understand. An alternative for representing data, which can easily incorporate higher dimensionality and temporal information, is the use of sound. In this paper we propose the use of sonification to help portray collaborative development history. Our approach, GitSonifier, combines sound primitives to represent developers, days, and conflicts over the history of a program's development. In a formative user study on an open source project's data, we find that users can easily extract meaningful information from sound clips and differentiate users, passage of time, and development conflicts, suggesting that sonification has the potential to provide benefit in this context. </p>",,,10.1145/2786805.2803199,,,conflicts;sonification;version control history,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
MultiSE: multi-path symbolic execution using value summaries,K. Sen; G. Necula; L. Gong; W. Choi,"University of California at Berkeley, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,842,853,"<p> Dynamic symbolic execution (DSE) has been proposed to effectively generate test inputs for real-world programs. Unfortunately, DSE techniques do not scale well for large realistic programs, because often the number of feasible execution paths of a program increases exponentially with the increase in the length of an execution path. In this paper, we propose MultiSE, a new technique for merging states incrementally during symbolic execution, without using auxiliary variables. The key idea of MultiSE is based on an alternative representation of the state, where we map each variable, including the program counter, to a set of guarded symbolic expressions called a value summary. MultiSE has several advantages over conventional DSE and conventional state merging techniques: value summaries enable sharing of symbolic expressions and path constraints along multiple paths and thus avoid redundant execution. MultiSE does not introduce auxiliary symbolic variables, which enables it to 1) make progress even when merging values not supported by the constraint solver, 2) avoid expensive constraint solver calls when resolving function calls and jumps, and 3) carry out most operations concretely. Moreover, MultiSE updates value summaries incrementally at every assignment instruction, which makes it unnecessary to identify the join points and to keep track of variables to merge at join points. We have implemented MultiSE for JavaScript programs in a publicly available open-source tool. Our evaluation of MultiSE on several programs shows that 1) value summaries are an eective technique to take advantage of the sharing of value along multiple execution path, that 2) MultiSE can run significantly faster than traditional dynamic symbolic execution and, 3) MultiSE saves a substantial number of state merges compared to conventional state-merging techniques. </p>",,,10.1145/2786805.2786830,,,JavaScript;MultiSE;concolic testing;symbolic execution;test generation;value summary,,,,,7,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
"When, how, and why developers (do not) test in their IDEs",M. Beller; G. Gousios; A. Panichella; A. Zaidman,"Delft University of Technology, Netherlands",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,179,190,"<p> The research community in Software Engineering and Software Testing in particular builds many of its contributions on a set of mutually shared expectations. Despite the fact that they form the basis of many publications as well as open-source and commercial testing applications, these common expectations and beliefs are rarely ever questioned. For example, Frederic BrooksŠ—È statement that testing takes half of the development time seems to have manifested itself within the community since he first made it in the Š—“Mythical Man MonthŠ— in 1975. With this paper, we report on the surprising results of a large-scale field study with 416 software engineers whose development activity we closely monitored over the course of five months, resulting in over 13 years of recorded work time in their integrated development environments (IDEs). Our findings question several commonly shared assumptions and beliefs about testing and might be contributing factors to the observed bug proneness of software in practice: the majority of developers in our study does not test; developers rarely run their tests in the IDE; Test-Driven Development (TDD) is not widely practiced; and, last but not least, software developers only spend a quarter of their work time engineering tests, whereas they think they test half of their time. </p>",,,10.1145/2786805.2786843,,,Developer Testing;Field Study;Test-Driven Development (TDD);Testing Effort;Unit Tests,,,,,18,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Cross-language program slicing for dynamic web applications,H. V. Nguyen; C. K’_stner; T. N. Nguyen,"Iowa State University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,369,380,"<p> During software maintenance, program slicing is a useful technique to assist developers in understanding the impact of their changes. While different program-slicing techniques have been proposed for traditional software systems, program slicing for dynamic web applications is challenging since the client-side code is generated from the server-side code and data entities are referenced across different languages and are often embedded in string literals in the server-side program. To address those challenges, we introduce WebSlice, an approach to compute program slices across different languages for web applications. We first identify data-flow dependencies among data entities for PHP code based on symbolic execution. We also compute SQL queries and a conditional DOM that represents client-code variations and construct the data flows for embedded languages: SQL, HTML, and JavaScript. Next, we connect the data flows across different languages and across PHP pages. Finally, we compute a program slice for a given entity based on the established data flows. Running WebSlice on five real-world, open-source PHP systems, we found that, out of 40,670 program slices, 10% cross languages, 38% cross files, and 13% cross string fragments, demonstrating the potential benefit of tool support for cross-language program slicing in dynamic web applications. </p>",,,10.1145/2786805.2786872,,,Program slicing;cross-language analysis;dynamic web applications,,,,,7,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Automated multi-objective control for self-adaptive software design,A. Filieri; H. Hoffmann; M. Maggio,"University of Stuttgart, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,13,24,"<p> While software is becoming more complex everyday, the requirements on its behavior are not getting any easier to satisfy. An application should offer a certain quality of service, adapt to the current environmental conditions and withstand runtime variations that were simply unpredictable during the design phase. To tackle this complexity, control theory has been proposed as a technique for managing software's dynamic behavior, obviating the need for human intervention. Control-theoretical solutions, however, are either tailored for the specific application or do not handle the complexity of multiple interacting components and multiple goals. In this paper, we develop an automated control synthesis methodology that takes, as input, the configurable software components (or knobs) and the goals to be achieved. Our approach automatically constructs a control system that manages the specified knobs and guarantees the goals are met. These claims are backed up by experimental studies on three different software applications, where we show how the proposed automated approach handles the complexity of multiple knobs and objectives. </p>",,,10.1145/2786805.2786833,,,Adaptive software;control theory;dynamic systems;non-functional requirements;run-time verification,,,,,8,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
iTrace: enabling eye tracking on software artifacts within the IDE to support software engineering tasks,T. R. Shaffer; J. L. Wise; B. M. Walters; S. C. M’_ller; M. Falcone; B. Sharif,"Youngstown State University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,954,957,"<p> The paper presents iTrace, an Eclipse plugin that implicitly records developers' eye movements while they work on change tasks. iTrace is the first eye tracking environment that makes it possible for researchers to conduct eye tracking studies on large software systems. An overview of the design and architecture is presented along with features and usage scenarios. iTrace is designed to support a variety of eye trackers. The design is flexible enough to record eye movements on various types of software artifacts (Java code, text/html/xml documents, diagrams), as well as IDE user interface elements. The plugin has been successfully used for software traceability tasks and program comprehension tasks. iTrace is also applicable to other tasks such as code summarization and code recommendations based on developer eye movements. A short video demonstration is available at https://youtu.be/3OUnLCX4dXo. </p>",,,10.1145/2786805.2803188,,,comprehension;eye-tracking;plugin;traceability,,,,,6,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
On the use of delta debugging to reduce recordings and facilitate debugging of web applications,M. Hammoudi; B. Burg; G. Bae; G. Rothermel,"University of Nebraska-Lincoln, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,333,344,"<p> Recording the sequence of events that lead to a failure of a web application can be an effective aid for debugging. Nevertheless, a recording of an event sequence may include many events that are not related to a failure, and this may render debugging more difficult. To address this problem, we have adapted Delta Debugging to function on recordings of web applications, in a manner that lets it identify and discard portions of those recordings that do not influence the occurrence of a failure. We present the results of three empirical studies that show that (1) recording reduction can achieve significant reductions in recording size and replay time on actual web applications obtained from developer forums, (2) reduced recordings do in fact help programmers locate faults significantly more efficiently as, and no less effectively than non-reduced recordings, and (3) recording reduction produces even greater reductions on larger, more complex applications. </p>",,,10.1145/2786805.2786846,,,Delta Debugging;Record/Replay Techniques;Recording Reduction;Web Applications,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
A method to identify and correct problematic software activity data: exploiting capacity constraints and data redundancies,Q. Zheng; A. Mockus; M. Zhou,"Peking University, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,637,648,"<p> Mining software repositories to understand and improve software development is a common approach in research and practice. The operational data obtained from these repositories often do not faithfully represent the intended aspects of software development and, therefore, may jeopardize the conclusions derived from it. We propose an approach to identify problematic values based on the constraints of software development and to correct such values using data redundancies. We investigate the approach using issue and commit data of Mozilla project. In particular, we identified problematic data in four types of events and found the fraction of problematic values to exceed 10% and rapidly rising. We found the corrected values to be 50% closer to the most accurate estimate of task completion time. Finally, we found that the models of time until fix changed substantially when data were corrected, with the corrected data providing a 20% better fit. We discuss how the approach may be generalized to other types of operational data to increase fidelity of software measurement in practice and in research. </p>",,,10.1145/2786805.2786866,,,capacity constraint;data quality;data redundancy;mining software repositories,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
What change history tells us about thread synchronization,R. Gu; G. Jin; L. Song; L. Zhu; S. Lu,"Columbia University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,426,438,"<p> Multi-threaded programs are pervasive, yet difficult to write. Missing proper synchronization leads to correctness bugs and over synchronization leads to performance problems. To improve the correctness and efficiency of multi-threaded software, we need a better understanding of synchronization challenges faced by real-world developers. This paper studies the code repositories of open-source multi-threaded software projects to obtain a broad and in- depth view of how developers handle synchronizations. We first examine how critical sections are changed when software evolves by checking over 250,000 revisions of four representative open-source software projects. The findings help us answer questions like how often synchronization is an afterthought for developers; whether it is difficult for devel- opers to decide critical section boundaries and lock variables; and what are real-world over-synchronization problems. We then conduct case studies to better understand (1) how critical sections are changed to solve performance prob- lems (i.e. over-synchronization issues) and (2) how soft- ware changes lead to synchronization-related correctness problems (i.e. concurrency bugs). This in-depth study shows that tool support is needed to help developers tackle over-synchronization problems; it also shows that concur- rency bug avoidance, detection, and testing can be improved through better awareness of code revision history. </p>",,,10.1145/2786805.2786815,,,Concurrency Bugs;Empirical Study;Locks;Multi-Threaded Software;Performance Bugs;Repository Mining,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
An empirical study of goto in C code from GitHub repositories,M. Nagappan; R. Robbes; Y. Kamei; ’_. Tanter; S. McIntosh; A. Mockus; A. E. Hassan,"Rochester Institute of Technology, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,404,414,"<p> It is nearly 50 years since Dijkstra argued that goto obscures the flow of control in program execution and urged programmers to abandon the goto statement. While past research has shown that goto is still in use, little is known about whether goto is used in the unrestricted manner that Dijkstra feared, and if it is Š—…harmfulŠ—È enough to be a part of a post-release bug. We, therefore, conduct a two part empirical study - (1) qualitatively analyze a statistically rep- resentative sample of 384 files from a population of almost 250K C programming language files collected from over 11K GitHub repositories and find that developers use goto in C files for error handling (80.21Î±5%) and cleaning up resources at the end of a procedure (40.36 Î± 5%); and (2) quantitatively analyze the commit history from the release branches of six OSS projects and find that no goto statement was re- moved/modified in the post-release phase of four of the six projects. We conclude that developers limit themselves to using goto appropriately in most cases, and not in an unrestricted manner like Dijkstra feared, thus suggesting that goto does not appear to be harmful in practice. </p>",,,10.1145/2786805.2786834,,,Dijkstra;Empirical SE;Github;Use of goto statements,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Detecting semantic merge conflicts with variability-aware execution,H. V. Nguyen; M. H. Nguyen; S. C. Dang; C. K’_stner; T. N. Nguyen,"Iowa State University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,926,929,"<p> In collaborative software development, changes made in parallel by multiple developers may conflict. Previous research has shown that conflicts are common and occur as textual conflicts or semantic conflicts, which manifest as build or test failures. With many parallel changes, it is desirable to identify conflicts early and pinpoint the (minimum) set of changes involved. However, the costs of identifying semantic conflicts can be high because tests need to be executed on many merge scenarios. We propose Semex, a novel approach to detect semantic conflicts using variability-aware execution. We encode all parallel changes into a single program, in which ""if"" statements guard the alternative code fragments. Then, we run the test cases using variability-aware execution, exploring all possible concrete executions of the combined program with regard to all possible merge scenarios, while exploiting similarities among the executions to speed up the process. Variability-aware execution returns a formula describing all failing merge scenarios. In our preliminary experimental study on seven PHP programs with a total of 50 test cases and 19 semantic conflicts, Semex correctly detected all 19 conflicts. </p>",,,10.1145/2786805.2803208,,,Variability-aware execution;n-way merge;semantic merge conflicts,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
RDIT: race detection from incomplete traces,A. K. Rajagopalan; J. Huang,"Texas A&M University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,914,917,"<p> We present RDIT, a novel dynamic algorithm to precisely detect data races in multi-threaded programs with incomplete trace information -- the presence of missing events. RDIT enhances the classical Happens-Before algorithm by relaxing the need to collect the full execution trace, while still guaranteeing full precision. The key idea behind RDIT is to abstract away the missing events by capturing the invocation data of the missing methods. This provides valuable information to approximate the possible synchronization behavior introduced by the missing events. By making the least conservative approximation that two missing methods introduce synchronization only when they access common data, RDIT guarantees to detect a maximal set of true races from the information available. We have conducted a preliminary study of RDIT on a real system and our results show that RDIT is promising; it detects no false positive when events are missed, whereas Happens-Before reports many. </p>",,,10.1145/2786805.2803209,,,Data Race;Happens-Before;Missing Trace;Precise;Reachable Addresses,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Inner oracles: input-specific assertions on internal states,Y. Xiong; D. Hao; L. Zhang; T. Zhu; M. Zhu; T. Lan,"Peking University, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,902,905,"<p> Traditional test oracles are defined on the outputs of test executions, and cannot assert internal states of executions. Traditional assertions are common to all test execution, and are usually more difficult to construct than on oracle for one test input. In this paper we propose the concept of inner oracles, which are assertions on internal states that are specific to one test input. We first motivate the necessity of inner oracles, and then show that it can be implemented easily using the available programming mechanisms. Next, we report two initial empirical studies on inner oracles, showing that inner oracles have a significant impact on both the fault-detection capability of tests and the performance of test suite reduction. Finally, we highlight the implications of inner oracles on several research and practical problems. </p>",,,10.1145/2786805.2803204,,,Assertions;Test Oracles;Testing,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Test report prioritization to assist crowdsourced testing,Y. Feng; Z. Chen; J. A. Jones; C. Fang; B. Xu,"Nanjing University, China / University of California at Irvine, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,225,236,"<p> In crowdsourced testing, users can be incentivized to perform testing tasks and report their results, and because crowdsourced workers are often paid per task, there is a financial incentive to complete tasks quickly rather than well. These reports of the crowdsourced testing tasks are called ""test reports"" and are composed of simple natural language and screenshots. Back at the software-development organization, developers must manually inspect the test reports to judge their value for revealing faults. Due to the nature of crowdsourced work, the number of test reports are often difficult to comprehensively inspect and process. In order to help with this daunting task, we created the first technique of its kind, to the best of our knowledge, to prioritize test reports for manual inspection. Our technique utilizes two key strategies: (1) a diversity strategy to help developers inspect a wide variety of test reports and to avoid duplicates and wasted effort on falsely classified faulty behavior, and (2) a risk strategy to help developers identify test reports that may be more likely to be fault-revealing based on past observations. Together, these strategies form our DivRisk strategy to prioritize test reports in crowd- sourced testing. Three industrial projects have been used to evaluate the effectiveness of test report prioritization methods. The results of the empirical study show that: (1) DivRisk can significantly outperform random prioritization; (2) DivRisk can approximate the best theoretical result for a real-world industrial mobile application. In addition, we provide some practical guidelines of test report prioritization for crowdsourced testing based on the empirical study and our experiences. </p>",,,10.1145/2786805.2786862,,,Crowdsourcing testing;natural language processing;test diversity;test report prioritization,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Automated unit test generation for evolving software,S. Shamshiri,"University of Sheffield, UK",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1038,1041,"<p> As developers make changes to software programs, they want to ensure that the originally intended functionality of the software has not been affected. As a result, developers write tests and execute them after making changes. However, high quality tests are needed that can reveal unintended bugs, and not all developers have access to such tests. Moreover, since tests are written without the knowledge of future changes, sometimes new tests are needed to exercise such changes. While this problem has been well studied in the literature, the current approaches for automatically generating such tests either only attempt to reach the change and do not aim to propagate the infected state to the output, or may suffer from scalability issues, especially when a large sequence of calls is required for propagation. We propose a search-based approach that aims to automatically generate tests which can reveal functionality changes, given two versions of a program (e.g., pre-change and post-change). Developers can then use these tests to identify unintended functionality changes (i.e., bugs). Initial evaluation results show that our approach can be effective on detecting such changes, but there remain challenges in scaling up test generation and making the tests useful to developers, both of which we aim to overcome. </p>",,,10.1145/2786805.2803196,,,Automated Unit Test Generation;Genetic Algorithms;Regression Testing;Search-Based Testing,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Effective test suites for mixed discrete-continuous stateflow controllers,R. Matinnejad; S. Nejati; L. C. Briand; T. Bruckmann,"University of Luxembourg, Luxembourg",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,84,95,"<p> Modeling mixed discrete-continuous controllers using Stateflow is common practice and has a long tradition in the embedded software system industry. Testing Stateflow models is complicated by expensive and manual test oracles that are not amenable to full automation due to the complex continuous behaviors of such models. In this paper, we reduce the cost of manual test oracles by providing test case selection algorithms that help engineers develop small test suites with high fault revealing power for Stateflow models. We present six test selection algorithms for discrete-continuous Stateflows: An adaptive random test selection algorithm that diversifies test inputs, two white-box coverage-based algorithms, a black-box algorithm that diversifies test outputs, and two search-based black-box algorithms that aim to maximize the likelihood of presence of continuous output failure patterns. We evaluate and compare our test selection algorithms, and find that our three output-based algorithms consistently outperform the coverage- and input-based algorithms in revealing faults in discrete-continuous Stateflow models. Further, we show that our output-based algorithms are complementary as the two search-based algorithms perform best in revealing specific failures with small test suites, while the output diversity algorithm is able to identify different failure types better than other algorithms when test suites are above a certain size. </p>",,,10.1145/2786805.2786818,,,Stateflow testing;failure-based testing;mixed discrete-continuous behaviors;output diversity;structural coverage,,,,,4,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
"Products, developers, and milestones: how should I build my N-Gram language model",J. Saraiva; C. Bird; T. Zimmermann,"Federal University of Pernambuco, Brazil",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,998,1001,"<p> Recent work has shown that although programming languages enable source code to be rich and complex, most code tends to be repetitive and predictable. The use of natural language processing (NLP) techniques applied to source code such as n-gram language models show great promise in areas such as code completion, aiding impaired developers, and code search. In this paper, we address three questions related to different methods of constructing language models in an industrial context. Specifically, we ask: (1) Do application specific, but smaller language models perform better than language models across applications? (2) Are developer specific language models effective and do they differ depending on what parts of the codebase a developer is working in? (3) Finally, do language models change over time, i.e., does a language model from early development model change later on in development? The answers to these questions enable techniques that make use of programming language models in development to choose the model training corpus more effectively. We evaluate these questions by building 28 language models across developers, time periods, and applications within Microsoft Office and present the results in this paper. We find that developer and application specific language models perform better than models from the entire codebase, but that temporality has little to no effect on language model performance. </p>",,,10.1145/2786805.2804431,,,N-gram Models;Natural Language Processing,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Comprehensive service matching with MatchBox,P. B’_rding; M. Bruns; M. C. Platenius,"University of Paderborn, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,974,977,"<p> Nowadays, many service providers offer software components in the form of Software as a Service. Requesters that want to discover those services in order to use or to integrate them, need to find out which service satisfies their requirements best. For this purpose, service matching approaches determine how well the specifications of provided services satisfy their requirements (including structural, behavioral, and non-functional requirements). In this paper, we describe the tool-suite MatchBox that allows the integration of existing service matchers and their combination as part of flexibly configurable matching processes. Taking requirements and service specifications as an input, MatchBox is able to execute such matching processes and deliver rich matching results. In contrast to related tools, MatchBox allows users to take into account many different kinds of requirements, while it also provides the flexibility to control the matching process in many different ways. </p>",,,10.1145/2786805.2803181,,,Framework;Fuzzy Matching;Matching Processes;Service Discovery;Service Matching;Software Requirements,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
P3: partitioned path profiling,M. Afraz; D. Saha; A. Kanade,"Indian Institute of Science, India",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,485,495,"<p> Acyclic path profile is an abstraction of dynamic control flow paths of procedures and has been found to be useful in a wide spectrum of activities. Unfortunately, the runtime overhead of obtaining such a profile can be high, limiting its use in practice. In this paper, we present partitioned path profiling (P3) which runs K copies of the program in parallel, each with the same input but on a separate core, and collects the profile only for a subset of intra-procedural paths in each copy, thereby, distributing the overhead of profiling. P3 identifies Š—“profitableŠ— procedures and assigns disjoint subsets of paths of a profitable procedure to different copies for profiling. To obtain exact execution frequencies of a subset of paths, we design a new algorithm, called PSPP. All paths of an unprofitable procedure are assigned to the same copy. P3 uses the classic Ball-Larus algorithm for profiling unprofitable procedures. Further, P3 attempts to evenly distribute the profiling overhead across the copies. To the best of our knowledge, P3 is the first algorithm for parallel path profiling. We have applied P3 to profile several programs in the SPEC 2006 benchmark. Compared to sequential profiling, P3 substantially reduced the runtime overhead on these programs averaged across all benchmarks. The reduction was 23%, 43% and 56% on average for 2, 4 and 8 cores respectively. P3 also performed better than a coarse-grained approach that treats all procedures as unprofitable and distributes them across available cores. For 2 cores, the profiling overhead of P3 was on average 5% less compared to the coarse-grained approach across these programs. For 4 and 8 cores, it was respectively 18% and 25% less. </p>",,,10.1145/2786805.2786868,,,Distributed;Divide and Conquer;Parallel;Path Profiling,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
"Requirements, architecture, and quality in a mission critical system: 12 lessons learned",A. Koski; T. Mikkonen,"Insta DefSec, Finland",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1018,1021,"<p> Public tender processes typically start with a comprehensive specification phase, where representatives of the eventual owner of the system, usually together with a hired group of consultants, spend a considerable amount of time to determine the needs of the owner. For the company that implements the system, this setup introduces two major challenges: (1) the written down requirements can never truly describe to a person, at least to one external to the specification process, the true intent behind the requirement; (2) the vision of the future system, stemming from the original idea, will change during the specification process Š—– over time simultaneously invalidating at least some of the requirements. This paper reflects the experiences encountered in a large-scale mission critical information system Š—– ERICA, an information system for the emergency services in Finland Š—– regarding design, implementation, and deployment. Based on the experiences we propose more dynamic ways of system specification, leading to simpler design, implementation, and deployment phases and finally to a better perceived quality. </p>",,,10.1145/2786805.2804436,,,Vision;architecture;external quality;internal quality;process quality;requirements;user stories,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Automatically recommending test code examples to inexperienced developers,R. Pham; Y. Stoliar; K. Schneider,"Leibniz Universit&#228;t Hannover, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,890,893,"<p> New graduates joining the software engineering workforce sometimes have trouble writing test code. Coming from university, they lack a hands-on approach to testing and have little experience with writing tests in a real-world setting. Software companies resort to costly training camps or mentoring initiatives. Not overcoming this lack of testing skills early on can hinder the newcomerŠ—Ès professional progress in becoming a high-quality engineer. Studying open source developers, we found that they rely on a projectŠ—Ès pre-existing test code to learn how to write tests and adapt test code for their own use. We propose to strategically present useful and contextual test code examples from a projectŠ—Ès test suite to newcomers in order to facilitate learning and test writing. With an automatic suggestion mechanism for valuable test code, the newcomer is enabled to learn how senior developers write tests and copy it. Having access to suitable tests lowers the barrier for writing new tests. </p>",,,10.1145/2786805.2803202,,,Examples;Newcomers;Recommendation;Testing,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Assertion guided symbolic execution of multithreaded programs,S. Guo; M. Kusano; C. Wang; Z. Yang; A. Gupta,"Virginia Tech, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,854,865,"<p> Symbolic execution is a powerful technique for systematic testing of sequential and multithreaded programs. However, its application is limited by the high cost of covering all feasible intra-thread paths and inter-thread interleavings. We propose a new assertion guided pruning framework that identifies executions guaranteed not to lead to an error and removes them during symbolic execution. By summarizing the reasons why previously explored executions cannot reach an error and using the information to prune redundant executions in the future, we can soundly reduce the search space. We also use static concurrent program slicing and heuristic minimization of symbolic constraints to further reduce the computational overhead. We have implemented our method in the Cloud9 symbolic execution tool and evaluated it on a large set of multithreaded C/C++ programs. Our experiments show that the new method can reduce the overall computational cost significantly. </p>",,,10.1145/2786805.2786841,,,Symbolic execution;concurrency;partial order reduction;test generation;weakest precondition,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
How developers search for code: a case study,C. Sadowski; K. T. Stolee; S. Elbaum,"Google, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,191,201,"<p> With the advent of large code repositories and sophisticated search capabilities, code search is increasingly becoming a key software development activity. In this work we shed some light into how developers search for code through a case study performed at Google, using a combination of survey and log-analysis methodologies. Our study provides insights into what developers are doing and trying to learn when per- forming a search, search scope, query properties, and what a search session under different contexts usually entails. Our results indicate that programmers search for code very frequently, conducting an average of five search sessions with 12 total queries each workday. The search queries are often targeted at a particular code location and programmers are typically looking for code with which they are somewhat familiar. Further, programmers are generally seeking answers to questions about how to use an API, what code does, why something is failing, or where code is located. </p>",,,10.1145/2786805.2786855,,,code search;developer tools;user evaluation,,,,,5,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Detecting JavaScript races that matter,E. Mutlu; S. Tasiran; B. Livshits,"Ko&#231; University, Turkey",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,381,392,"<p> As JavaScript has become virtually omnipresent as the language for programming large and complex web applications in the last several years, we have seen an increase in interest in finding data races in client-side JavaScript. While JavaScript execution is single-threaded, there is still enough potential for data races, created largely by the non-determinism of the scheduler. Recently, several academic efforts have explored both static and run-time analysis approaches in an effort to find data races. However, despite this, we have not seen these analysis techniques deployed in practice and we have only seen scarce evidence that developers find and fix bugs related to data races in JavaScript. In this paper we argue for a different formulation of what it means to have a data race in a JavaScript application and distinguish between benign and harmful races, affecting persistent browser or server state. We further argue that while benign races Š—” the subject of the majority of prior work Š—” do exist, harmful races are exceedingly rare in practice (19 harmful vs. 621 benign). Our results shed a new light on the issues of data race prevalence and importance. To find races, we also propose a novel lightweight run-time symbolic exploration algorithm for finding races in traces of run-time execution. Our algorithm eschews schedule exploration in favor of smaller run-time overheads and thus can be used by beta testers or in crowd-sourced testing. In our experiments on 26 sites, we demonstrate that benign races are considerably more common than harmful ones. </p>",,,10.1145/2786805.2786820,,,JavaScript;asynchrony;non-determinism;race detection,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Gamification for enforcing coding conventions,C. R. Prause; M. Jarke,"DLR, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,649,660,"<p> Software is a knowledge intensive product, which can only evolve if there is effective and efficient information exchange between developers. Complying to coding conventions improves information exchange by improving the readability of source code. However, without some form of enforcement, compliance to coding conventions is limited. We look at the problem of information exchange in code and propose gamification as a way to motivate developers to invest in compliance. Our concept consists of a technical prototype and its integration into a Scrum environment. By means of two experiments with agile software teams and subsequent surveys, we show that gamification can effectively improve adherence to coding conventions. </p>",,,10.1145/2786805.2786806,,,code style;experiment;gamification;software quality,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Product lines can jeopardize their trade secrets,M. Acher; G. B’©can; B. Combemale; B. Baudry; J. M. J’©z’©quel,"University of Rennes 1, France / INRIA, France / IRISA, France",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,930,933,"<p> What do you give for free to your competitor when you exhibit a product line? This paper addresses this question through several cases in which the discovery of trade secrets of a product line is possible and can lead to severe consequences. That is, we show that an outsider can understand the variability realization and gain either confidential business information or even some economical direct advantage. For instance, an attacker can identify hidden constraints and bypass the product line to get access to features or copyrighted data. This paper warns against possible naive modeling, implementation, and testing of variability leading to the existence of product lines that jeopardize their trade secrets. Our vision is that defensive methods and techniques should be developed to protect specifically variability Š—– or at least further complicate the task of reverse engineering it. </p>",,,10.1145/2786805.2803210,,,Product lines;obfuscation;reverse engineering;security,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
TACO: test suite augmentation for concurrent programs,T. Yu,"University of Kentucky, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,918,921,"<p> The advent of multicore processors has greatly increased the prevalence of concurrent programs to achieve higher performance. As programs evolve, test suite augmentation techniques are used in regression testing to identify where new test cases are needed and then generate them. Prior work on test suite augmentation has focused on sequential software, but to date, no work has considered concurrent software systems for which regression testing is expensive due to large number of possible thread interleavings. In this paper, we present TACO, an automated test suite augmentation framework for concurrent programs in which our goal is not only to generate new inputs to exercise uncovered changed code but also to explore new thread interleavings induced by the changes. Our technique utilizes results from reuse of existing test inputs following random schedules, together with a predicative scheduling strategy and an incremental concolic testing algorithm to automatically generate new inputs that drive program through affected interleaving space so that it can effectively and efficiently validate changes that have not been exercised by existing test cases. Toward the end, we discuss several main challenges and opportunities of our approach. </p>",,,10.1145/2786805.2803201,,,Concurrency;Regression Testing,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Targeted program transformations for symbolic execution,C. Cadar,"Imperial College London, UK",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,906,909,"<p> Semantics-preserving program transformations, such as refactorings and optimisations, can have a significant impact on the effectiveness of symbolic execution testing and analysis. Furthermore, semantics-preserving transformations that increase the performance of native execution can in fact decrease the scalability of symbolic execution. Similarly, semantics-altering transformations, such as type changes and object size modifications, can often lead to substantial improvements in the testing effectiveness achieved by symbolic execution in the original program. As a result, we argue that one should treat program transformations as first-class ingredients of scalable symbolic execution, alongside widely-accepted aspects such as search heuristics and constraint solving optimisations. First, we propose to understand the impact of existing program transformations on symbolic execution, to increase scalability and improve experimental design and reproducibility. Second, we argue for the design of testability transformations specifically targeted toward more scalable symbolic execution. </p>",,,10.1145/2786805.2803205,,,Testability transformations;dynamic symbolic execution,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Comparing and combining test-suite reduction and regression test selection,A. Shi; T. Yung; A. Gyori; D. Marinov,"University of Illinois at Urbana-Champaign, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,237,247,"<p> Regression testing is widely used to check that changes made to software do not break existing functionality, but regression test suites grow, and running them fully can become costly. Researchers have proposed test-suite reduction and regression test selection as two approaches to reduce this cost by not running some of the tests from the test suite. However, previous research has not empirically evaluated how the two approaches compare to each other, and how well a combination of these approaches performs. We present the first extensive study that compares test-suite reduction and regression test selection approaches individually, and also evaluates a combination of the two approaches. We also propose a new criterion to measure the quality of tests with respect to software changes. Our experiments on 4,793 commits from 17 open-source projects show that regression test selection runs on average fewer tests (by 40.15pp) than test-suite reduction. However, test-suite reduction can have a high loss in fault-detection capability with respect to the changes, whereas a (safe) regression test selection has no loss. The experiments also show that a combination of the two approaches runs even fewer tests (on average 5.34pp) than regression test selection, but these tests still have a loss in fault-detection capability with respect to the changes. </p>",,,10.1145/2786805.2786878,,,Regression test selection;Regression testing;Test-suite reduction,,,,,5,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Increasing the efficiency of search-based unit test generation using parameter control,T. White,"University of Sheffield, UK",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1042,1044,"<p> Automatically generating test suites with high coverage is of great importance to software engineers, but this process is hindered by the vast amount of parameters the tools use to generate tests. Developers usually lack knowledge about the workings of the tools that generate test suites to set the parameters to optimal values, and the optimal values usually change during runtime. Parameter Control automatically adapts parameters during test generation, and has shown to help solve this problem in other areas. To investigate any improvements parameter control could have in search-based generation of test suites, we adapted multiple methods of controlling mutation and crossover rate in EvoSuite, a tool that automatically generates unit test suites. Upon evaluation, clear benefits to controlling parameters were found, but surprisingly, controlling some parameters can sometimes be more harmful to the search than beneficial through increased computation costs. </p>",,,10.1145/2786805.2807556,,,Parameter control;genetic algorithm;search based software engineering;test suite generation;testing,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning,X. Jing; F. Wu; X. Dong; F. Qi; B. Xu,"Wuhan University, China / Nanjing University of Posts and Telecommunications, China",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,496,507,"<p> Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. In this paper, we aim to provide an effective solution for HCCDP. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. The proposed approach is effective for HCCDP. </p>",,,10.1145/2786805.2786813,,,Heterogeneous cross-company defect prediction (HCCDP);canonical correlation analysis (CCA);common metrics;company-specific metrics;unified metric representation,,,,,12,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Decentralized self-adaptation in large-scale distributed systems,L. Florio,"Politecnico di Milano, Italy",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1022,1025,"<p> The evolution of technology is leading to a world where computational systems are made of a huge number of components spread over a logical network: these components operate in a highly dynamic and unpredictable environment, joining or leaving the system and creating connections between them at runtime. This scenario poses new challenges to software engineers that have to design and implement such complex systems. We want to address this problem, designing and developing an infrastructure, GRU, that uses self-adaptive decentralized techniques to manage large-scale distributed systems. GRU will help developers to focus on the functional part of their application instead of the needed self-adaptive infrastructure. We aim to evaluate our project with concrete case studies, providing evidence on the validity of our approach, and with the feedback provided by developers that will test our system. We believe this approach can contribute to fill the gap between the theoretical study of self-adaptive systems and their application in a production context. </p>",,,10.1145/2786805.2803192,,,Decentralized;Docker;Microservices;Self-Adaptive,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Using software theater for the demonstration of innovative ubiquitous applications,H. Xu; S. Krusche; B. Bruegge,"TU M&#252;nchen, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,894,897,"<p> Software development has to cope with uncertainties and changing requirements that constantly arise in the development process. Agile methods address this challenge by adopting an incremental development process and delivering working software frequently. However, current validation techniques used in sprint reviews are not sufficient for emerging applications based on ubiquitous technologies. To fill this gap, we propose a new way of demonstration called Software Theater. Based on ideas from theater plays, it aims at presenting scenario-based demonstration in a theatrical way to highlight new features, new user experience and new technical architecture in an integrated performance. We have used Software Theater in more than twenty projects and the result is overall positive. </p>",,,10.1145/2786805.2803207,,,Demonstration;Design evaluation;Informal models;Prototypes;Rapid iteration;Scenarios,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Tracing software developers' eyes and interactions for change tasks,K. Kevic; B. M. Walters; T. R. Shaffer; B. Sharif; D. C. Shepherd; T. Fritz,"University of Zurich, Switzerland",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,202,213,"<p> What are software developers doing during a change task? While an answer to this question opens countless opportunities to support developers in their work, only little is known about developers' detailed navigation behavior for realistic change tasks. Most empirical studies on developers performing change tasks are limited to very small code snippets or are limited by the granularity or the detail of the data collected for the study. In our research, we try to overcome these limitations by combining user interaction monitoring with very fine granular eye-tracking data that is automatically linked to the underlying source code entities in the IDE. In a study with 12 professional and 10 student developers working on three change tasks from an open source system, we used our approach to investigate the detailed navigation of developers for realistic change tasks. The results of our study show, amongst others, that the eye tracking data does indeed capture different aspects than user interaction data and that developers focus on only small parts of methods that are often related by data flow. We discuss our findings and their implications for better developer tool support. </p>",,,10.1145/2786805.2786864,,,change task;eye-tracking;gaze;user study,,,,,9,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
String analysis for Java and Android applications,D. Li; Y. Lyu; M. Wan; W. G. J. Halfond,"University of Southern California, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,661,672,"<p> String analysis is critical for many verification techniques. However, accurately modeling string variables is a challeng- ing problem. Current approaches are generally customized for certain problem domains or have critical limitations in handling loops, providing context-sensitive inter-procedural analysis, and performing efficient analysis on complicated apps. To address these limitations, we propose a general framework, Violist, for string analysis that allows researchers to more flexibly choose how they will address each of these challenges by separating the representation and interpreta- tion of string operations. In our evaluation, we show that our approach can achieve high accuracy on both Java and Android apps in a reasonable amount of time. We also com- pared our approach with a popular and widely used string analyzer and found that our approach has higher precision and shorter execution time while maintaining the same level of recall. </p>",,,10.1145/2786805.2786879,,,String analysis;mobile apps,,,,,5,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
JSketch: sketching for Java,J. Jeon; X. Qiu; J. S. Foster; A. Solar-Lezama,"University of Maryland, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,934,937,"<p> Sketch-based synthesis, epitomized by the Sketch tool, lets developers synthesize software starting from a partial program, also called a sketch or template. This paper presents JSketch, a tool that brings sketch-based synthesis to Java. JSketch's input is a partial Java program that may include holes, which are unknown constants, expression generators, which range over sets of expressions, and class generators, which are partial classes. JSketch then translates the synthesis problem into a Sketch problem; this translation is complex because Sketch is not object-oriented. Finally, JSketch synthesizes an executable Java program by interpreting the output of Sketch. </p>",,,10.1145/2786805.2803189,,,Java;Program Synthesis;Programming by Example;Sketch,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Questions developers ask while diagnosing potential security vulnerabilities with static analysis,J. Smith; B. Johnson; E. Murphy-Hill; B. Chu; H. R. Lipford,"North Carolina State University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,248,259,"<p> Security tools can help developers answer questions about potential vulnerabilities in their code. A better understanding of the types of questions asked by developers may help toolsmiths design more effective tools. In this paper, we describe how we collected and categorized these questions by conducting an exploratory study with novice and experienced software developers. We equipped them with Find Security Bugs, a security-oriented static analysis tool, and observed their interactions with security vulnerabilities in an open-source system that they had previously contributed to. We found that they asked questions not only about security vulnerabilities, associated attacks, and fixes, but also questions about the software itself, the social ecosystem that built the software, and related resources and tools. For example, when participants asked questions about the source of tainted data, their tools forced them to make imperfect tradeoffs between systematic and ad hoc program navigation strategies. </p>",,,10.1145/2786805.2786812,,,Developer questions;human factors;security;static analysis,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Vehicle level continuous integration in the automotive industry,S. V’_st,"University of Stuttgart, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1026,1029,"<p> Embedded systems are omnipresent in the modern world. This naturally includes the automobile industry, where electronic functions are becoming prevalent. In the automotive domain, embedded systems today are highly distributed systems and manufactured in great numbers and variance. To ensure correct functionality, systematic integration and testing on the system level is key. In software engineering, continuous integration has been used with great success. In the automotive industry though, system tests are still performed in a big-bang integration style, which makes tracing and fixing errors very expensive and time-consuming. Thus, I want to investigate whether and how continuous integration can be applied to the automotive industry on the system level. Doing so, I present an adapted process of Continuous Integration including methods for test case specification and selection. I will apply this process as a pilot project in a production environment at BMW and evaluate the effectiveness by gathering both qualitative and quantitative data. From the gained experience, I will derive possible improvements to the process for future implementations and requirements on test hardware used for Continuous Integration. </p>",,,10.1145/2786805.2803193,,,Automotive;continuous integration;embedded;testing,,,,,3,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Auto-completing bug reports for Android applications,K. Moran; M. Linares-V’squez; C. Bernal-C’rdenas; D. Poshyvanyk,"College of William and Mary, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,673,686,"<p> The modern software development landscape has seen a shift in focus toward mobile applications as tablets and smartphones near ubiquitous adoption. Due to this trend, the complexity of these Š—“appsŠ— has been increasing, making development and maintenance challenging. Additionally, current bug tracking systems are not able to effectively support construction of reports with actionable information that directly lead to a bugŠ—Ès resolution. To address the need for an improved reporting system, we introduce a novel solution, called FUSION, that helps users auto-complete reproduction steps in bug reports for mobile apps. FUSION links user-provided information to program artifacts extracted through static and dynamic analysis performed before testing or release. The approach that FUSION employs is generalizable to other current mobile software platforms, and constitutes a new method by which off-device bug reporting can be conducted for mobile software projects. In a study involving 28 participants we applied FUSION to support the maintenance tasks of reporting and reproducing defects from 15 real-world bugs found in 14 open source Android apps while qualitatively and qualitatively measuring the user experience of the system. Our results demonstrate that FUSION both effectively facilitates reporting and allows for more reliable reproduction of bugs from reports compared to traditional issue tracking systems by presenting more detailed contextual app information. </p>",,,10.1145/2786805.2786857,,,Bug reports;android;auto-completion;reproduction steps,,,,,8,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Don't panic: reverse debugging of kernel drivers,P. Dovgalyuk; D. Dmitriev; V. Makarov,"Russian Academy of Sciences, Russia",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,938,941,"<p> Debugging of device drivers' failures is a very tough task because of kernel panics, blue screens of death, hardware volatility, long periods of time required to expose the bug, perturbation of the drivers by the debugger, and non-determinism of multi-threaded environment. This paper shows how reverse debugging reduces the influence of these factors to the process of drivers debugging. We present reverse debugger as a practical tool, which was tested for i386, x86-64, and ARM platforms, for Windows and Linux guest operating systems. We show that our tool incurs very low overhead (about 10%), which allows using it for debugging of the time sensitive applications. The paper also presents the case study which demonstrates reverse debugging of the USB kernel drivers for Linux. </p>",,,10.1145/2786805.2803179,,,Debugging;USB;deterministic replay;kernel debugging;reverse debugging,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Quantifying developers' adoption of security tools,J. Witschey; O. Zielinska; A. Welk; E. Murphy-Hill; C. Mayhorn; T. Zimmermann,"North Carolina State University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,260,271,"<p> Security tools could help developers find critical vulnerabilities, yet such tools remain underused. We surveyed developers from 14 companies and 5 mailing lists about their reasons for using and not using security tools. The resulting thirty-nine predictors of security tool use provide both expected and unexpected insights. As we expected, developers who perceive security to be important are more likely to use security tools than those who do not. But that was not the strongest predictor of security tool use, it was instead developers' ability to observe their peers using security tools. </p>",,,10.1145/2786805.2786816,,,adoption;developers;security;tools,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Quantifying architectural debts,L. Xiao,"Drexel University, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,1030,1033,"<p> In our prior research, we found that problematic architectural connections can propagate errors. We also found that among multiple files, the architectural connections that violate common design principles strongly correlate with the error-proneness of files. The flawed architectural connections, if not fixed properly and timely, can become debts that accumulate high interest in terms of maintenance costs over time. In this paper, we define architectural debts as clusters of files with problematic architectural connections among them, and their connections incur high maintenance costs over time. Our goal is to 1) precisely identify which and how many files are involved in architectural debts; 2) quantify the penalties of architectural debts in terms of maintenance costs; and 3) model the growth trend of penalties---maintenance costs---that accumulate due to architectural debts. We plan to provide a quantitative model for project managers and stakeholders as a reference in making decisions of whether, when and where to invest in refactoring. </p>",,,10.1145/2786805.2803194,,,architectural debt;maintenance costs;refactoring;software architecture;software quality,,,,,,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
CLAPP: characterizing loops in Android applications,Y. Fratantonio; A. Machiry; A. Bianchi; C. Kruegel; G. Vigna,"University of California at Santa Barbara, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,687,697,"<p> When performing program analysis, loops are one of the most important aspects that needs to be taken into account. In the past, many approaches have been proposed to analyze loops to perform different tasks, ranging from compiler optimizations to Worst-Case Execution Time (WCET) analysis. While these approaches are powerful, they focus on tackling very specific categories of loops and known loop patterns, such as the ones for which the number of iterations can be statically determined. In this work, we developed a static analysis framework to characterize and analyze generic loops, without relying on techniques based on pattern matching. For this work, we focus on the Android platform, and we implemented a prototype, called CLAPP, that we used to perform the first large-scale empirical study of the usage of loops in Android applications. In particular, we used our tool to analyze a total of 4,110,510 loops found in 11,823 Android applications. As part of our evaluation, we provide the detailed results of our empirical study, we show how our analysis was able to determine that the execution of 63.28% of the loops is bounded, and we discuss several interesting insights related to the performance issues and security aspects associated with loops. </p>",,,10.1145/2786805.2786873,,,Android;Loop Analysis;Static Analysis,,,,,4,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
UMTG: a toolset to automatically generate system test cases from use case specifications,C. Wang; F. Pastore; A. Goknil; L. C. Briand; Z. Iqbal,"University of Luxembourg, Luxembourg",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,942,945,"<p> We present UMTG, a toolset for automatically generating executable and traceable system test cases from use case specifications. UMTG employs Natural Language Processing (NLP), a restricted form of use case specifications, and constraint solving. Use cases are expected to follow a template with restriction rules that reduce imprecision and enable NLP. NLP is used to capture the control flow implicitly described in use case specifications. Finally, to generate test input, constraint solving is applied to OCL constraints referring to the domain model of the system. UMTG is integrated with two tools that are widely adopted in industry, IBM Doors and Rhapsody. UMTG has been successfully evaluated on an industrial case study. </p>",,,10.1145/2786805.2803187,,,NLP;Test Cases Generation;Use Case Specifications,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Auto-patching DOM-based XSS at scale,I. Parameshwaran; E. Budianto; S. Shinde; H. Dang; A. Sadhu; P. Saxena,"National University of Singapore, Singapore",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,272,283,"<p> DOM-based cross-site scripting (XSS) is a client-side code injection vulnerability that results from unsafe dynamic code generation in JavaScript applications, and has few known practical defenses. We study dynamic code evaluation practices on nearly a quarter million URLs crawled starting from the the Alexa Top 1000 websites. Of 777,082 cases of dynamic HTML/JS code generation we observe, 13.3% use unsafe string interpolation for dynamic code generation Š—” a well-known dangerous coding practice. To remedy this, we propose a technique to generate secure patches that replace unsafe string interpolation with safer code that utilizes programmatic DOM construction techniques. Our system transparently auto-patches the vulnerable site while incurring only 5.2 Š_ê 8.07% overhead. The patching mechanism requires no access to server-side code or modification to browsers, and thus is practical as a turnkey defense. </p>",,,10.1145/2786805.2786821,,,Auto-patching;DOM-based XSS;Taint Analysis;Web Security,,,,,2,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
"TLV: abstraction through testing, learning, and validation",J. Sun; H. Xiao; Y. Liu; S. W. Lin; S. Qin,"Singapore University of Technology and Design, Singapore",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,698,709,"<p> A (Java) class provides a service to its clients (i.e., programs which use the class). The service must satisfy certain specifications. Different specifications might be expected at different levels of abstraction depending on the client's objective. In order to effectively contrast the class against its specifications, whether manually or automatically, one essential step is to automatically construct an abstraction of the given class at a proper level of abstraction. The abstraction should be correct (i.e., over-approximating) and accurate (i.e., with few spurious traces). We present an automatic approach, which combines testing, learning, and validation, to constructing an abstraction. Our approach is designed such that a large part of the abstraction is generated based on testing and learning so as to minimize the use of heavy-weight techniques like symbolic execution. The abstraction is generated through a process of abstraction/refinement, with no user input, and converges to a specific level of abstraction depending on the usage context. The generated abstraction is guaranteed to be correct and accurate. We have implemented the proposed approach in a toolkit named TLV and evaluated TLV with a number of benchmark programs as well as three real-world ones. The results show that TLV generates abstraction for program analysis and verification more efficiently. </p>",,,10.1145/2786805.2786817,,,Automata Learning;Behavior Models;Program Abstraction;Symbolic Execution,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Performance-influence models for highly configurable systems,N. Siegmund; A. Grebhahn; S. Apel; C. K’_stner,"University of Passau, Germany",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,284,294,"<p> Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them. </p>",,,10.1145/2786805.2786845,,,Performance-influence models;machine learning;sampling,,,,,9,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
Users beware: preference inconsistencies ahead,F. Behrang; M. B. Cohen; A. Orso,"Georgia Tech, USA",Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,20161111,2015,,,295,306,"<p> The structure of preferences for modern highly-configurable software systems has become extremely complex, usually consisting of multiple layers of access that go from the user interface down to the lowest levels of the source code. This complexity can lead to inconsistencies between layers, especially during software evolution. For example, there may be preferences that users can change through the GUI, but that have no effect on the actual behavior of the system because the related source code is not present or has been removed going from one version to the next. These inconsistencies may result in unexpected program behaviors, which range in severity from mild annoyances to more critical security or performance problems. To address this problem, we present SCIC (Software Configuration Inconsistency Checker), a static analysis technique that can automatically detect these kinds of inconsistencies. Unlike other configuration analysis tools, SCIC can handle software that (1) is written in multiple programming languages and (2) has a complex preference structure. In an empirical evaluation that we performed on 10 years worth of versions of both the widely used Mozilla Core and Firefox, SCIC was able to find 40 real inconsistencies (some determined as severe), whose lifetime spanned multiple versions, and whose detection required the analysis of code written in multiple languages. </p>",,,10.1145/2786805.2786869,,,Configurable systems;software evolution,,,,,1,,,,,,Aug. 30 2015-Sept. 4 2015,,ACM,ACM Conferences
T2API: synthesizing API code usage templates from English texts with statistical translation,T. Nguyen; P. C. Rigby; A. T. Nguyen; M. Karanfil; T. N. Nguyen,"Iowa State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1013,1017,"<p> In this work, we develop T2API, a statistical machine translation-based tool that takes a given English description of a programming task as a query, and synthesizes the API usage template for the task by learning from training data. T2API works in two steps. First, it derives the API elements relevant to the task described in the input by statistically learning from a StackOverflow corpus of text descriptions and corresponding code. To infer those API elements, it also considers the context of the words in the textual input and the context of API elements that often go together in the corpus. The inferred API elements with their relevance scores are ensembled into an API usage by our novel API usage synthesis algorithm that learns the API usages from a large code corpus via a graph-based language model. Importantly, T2API is capable of generating new API usages from smaller, previously-seen usages. </p>",,,10.1145/2950290.2983931,,,API Usage Synthesis;Graph-based Statistical Machine Translation;Text-to-Code Translation,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
PUMConf: a tool to configure product specific use case and domain models in a product line,I. Hajri; A. Goknil; L. C. Briand; T. Stephany,"University of Luxembourg, Luxembourg",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1008,1012,"<p> We present PUMConf, a tool for supporting configuration that currently focuses on requirements and enables effective product line management in the context of use case-driven development. By design, it relies exclusively on variability modeling for artifacts that are commonly used in such contexts (i.e., use case diagram, specifications and domain model). For given Product Line (PL) use case and domain models, PUMConf checks the consistency of the models, interactively receives configuration decisions from analysts, automatically checks decision consistency, and generates Product Specific (PS) use case and domain models from the PL models and decisions. It has been evaluated on an industrial case study in the automotive domain. </p>",,,10.1145/2950290.2983935,,,Product Line Engineering;Use Case-Driven Development,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
A discrete-time feedback controller for containerized cloud applications,L. Baresi; S. Guinea; A. Leva; G. Quattrocchi,"Politecnico di Milano, Italy",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,217,228,"<p> Modern Web applications exploit Cloud infrastructures to scale their resources and cope with sudden changes in the workload. While the state of practice is to focus on dynamically adding and removing virtual machines, we advocate that there are strong benefits in containerizing the applications and in scaling the containers. </p> <p> In this paper we present an autoscaling technique that allows containerized applications to scale their resources both at the VM level and at the container level. Furthermore, applications can combine this infrastructural adaptation with platform-level adaptation. The autoscaling is made possible by our planner, which consists of a grey-box discrete-time feedback controller. </p> <p> The work has been validated using two application benchmarks deployed to Amazon EC2. Our experiments show that our planner outperforms Amazon's AutoScaling by 78% on average without containers; and that the introduction of containers allows us to improve by yet another 46% on average. </p>",,,10.1145/2950290.2950328,,,Adaptive systems;cloud computing;containers;control theory;software adaptation,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Time-travel debugging for JavaScript/Node.js,E. T. Barr; M. Marron; E. Maurer; D. Moseley; G. Seth,"University College London, UK",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1003,1007,"<p> Time-traveling in the execution history of a program during debugging enables a developer to precisely track and understand the sequence of statements and program values leading to an error. To provide this functionality to real world developers, we embarked on a two year journey to create a production quality time-traveling debugger in Microsoft's open-source ChakraCore JavaScript engine and the popular Node.js application framework. </p>",,,10.1145/2950290.2983933,,,JavaScript;Node.js;Time-Travel Debugging,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
WebRanz: web page randomization for better advertisement delivery and web-bot prevention,W. Wang; Y. Zheng; X. Xing; Y. Kwon; X. Zhang; P. Eugster,"Purdue University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,205,216,"<p> <p>Nowadays, a rapidly increasing number of web users are using Ad-blockers to block online advertisements. Ad-blockers are browser-based software that can block most Ads on the websites, speeding up web browsers and saving bandwidth. Despite these benefits to end users, Ad-blockers could be catastrophic for the economic structure underlying the web, especially considering the rise of Ad-blocking as well as the number of technologies and services that rely exclusively on Ads to compensate their cost. In this paper, we introduce WebRanz that utilizes a randomization mechanism to circumvent Ad-blocking. Using WebRanz, content publishers can constantly mutate the internal HTML elements and element attributes of their web pages, without affecting their visual appearances and functionalities. Randomization invalidates the pre-defined patterns that Ad-blockers use to filter out Ads. Though the design of WebRanz is motivated by evading Ad-blockers, WebRanz also benefits the defense against bot scripts. We evaluate the effectiveness of WebRanz and its overhead using 221 randomly sampled top Alexa web pages and 8 representative bot scripts.</p> </p>",,,10.1145/2950290.2950352,,,Ad-blockers;randomization;web bots,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
TIPMerge: recommending developers for merging branches,C. Costa; J. Figueiredo; A. Sarma; L. Murta,"Federal University of Acre, Brazil / Federal Fluminense University, Brazil",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,998,1002,"<p> Development in large projects often involves branches, where changes are performed in parallel and merged periodically. This merge process often combines two independent and long sequences of commits that may have been performed by multiple, different developers. It is nontrivial to identify the right developer to perform the merge, as the developer must have enough understanding of changes in both branches to ensure that the merged changes comply with the objective of both lines of work (branches), which may have been active for several months. We designed and developed TIPMerge, a novel tool that recommends developers who are best suited to perform the merge between two given branches. TIPMerge does so by taking into consideration developersŠ—È past experience in the project, their changes in the branches, and the dependencies among modified files in the branches. In this paper we demonstrate TIPMerge over a real merge case from the Voldemort project. </p>",,,10.1145/2950290.2983936,,,Branch Merge;Expertise Recommendation;Version Control,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
String analysis for side channels with segmented oracles,L. Bang; A. Aydin; Q. S. Phan; C. S. PóÛsóÛreanu; T. Bultan,"University of California at Santa Barbara, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,193,204,"<p> We present an automated approach for detecting and quantifying side channels in Java programs, which uses symbolic execution, string analysis and model counting to compute information leakage for a single run of a program. We further extend this approach to compute information leakage for multiple runs for a type of side channels called segmented oracles, where the attacker is able to explore each segment of a secret (for example each character of a password) independently. We present an efficient technique for segmented oracles that computes information leakage for multiple runs using only the path constraints generated from a single run symbolic execution. Our implementation uses the symbolic execution tool Symbolic PathFinder (SPF), SMT solver Z3, and two model counting constraint solvers LattE and ABC. Although LattE has been used before for analyzing numeric constraints, in this paper, we present an approach for using LattE for analyzing string constraints. We also extend the string constraint solver ABC for analysis of both numeric and string constraints, and we integrate ABC in SPF, enabling quantitative symbolic string analysis. </p>",,,10.1145/2950290.2950362,,,Side-channel analysis;String constraints;Symbolic execution,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Field-exhaustive testing,P. Ponzio; N. Aguirre; M. F. Frias; W. Visser,"National University of R&#237;o Cuarto, Argentina / CONICET, Argentina",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,908,919,"<p> <p>We present a testing approach for object oriented programs, which encompasses a testing criterion and an automated test generation technique. The criterion, that we call <em>field-exhaustive testing</em>, requires a user-provided limit <i>n</i> on the size of data domains, and is based on the idea of considering enough inputs so as to exhaustively cover the <em>extension</em> of class fields, within the limit <i>n</i>. Intuitively, the extension of a field <i>f</i> is the binary relation established between objects and their corresponding values for field <i>f</i>, in valid instances. Thus, a suite <i>S</i> is field-exhaustive if whenever a field <i>f</i> relates an object <i>o</i> with a value <i>v</i> (i.e., <i>o</i>.<i>f</i> = <i>v</i>) within a valid instance <i>I</i> of size bounded by <i>n</i>, then <i>S</i> contains at least one input <i>I</i>’¢ ´è_ covering such relationship, i.e., <i>o</i> must also be part of <i>I</i>’¢ ´è_, and <i>o</i>.<i>f</i> = <i>v</i> must hold in <i>I</i>’¢ ´è_. Our test generation technique uses incremental SAT solving to produce <em>small</em> field-exhaustive suites: field-exhaustiveness can be achieved with a suite containing at most # <i>F</i> ´è_ <i>n</i><sup>2</sup> inputs, where # <i>F</i> is the number of fields in the class under test. </p><p>We perform an experimental evaluation on two different testing domains drawn from the literature: implementations of data structures, and of a refactoring engine. The experiments show that field-exhaustive suites can be computed efficiently, and retain similar levels of code coverage and mutation killing as significantly larger bounded exhaustive and random suites, thus consuming a fraction of the cost of test execution compared to these automated testing approaches.</p> </p>",,,10.1145/2950290.2950336,,,Automated Test Generation;SAT Solving;Software Testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Budgeted testing through an algorithmic lens,M. B. Cohen; A. Pavan; N. V. Vinodchandran,"University of Nebraska-Lincoln, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,948,951,"<p> Automated testing has been a focus of research for a long time. As such, we tend to think about this in a coverage centric manner. Testing budgets have also driven research such as prioritization and test selection, but as a secondary concern. As our systems get larger, are more dynamic, and impact more people with each change, we argue that we should switch from a coverage centric view to a budgeted testing centric view. Researchers in other fields have designed approximation algorithms for such budgeted scenarios and these are often simple to implement and run. In this paper we present an exemplar study on combinatorial interaction testing (CIT) to show that a budgeted greedy algorithm, when adapted to our problem for various budgets, does almost as well coverage-wise as a state of the art greedy CIT algorithm, better in some cases than a state of the art simulated annealing, and always improves over random. This suggests that we might benefit from switching our focus in large systems, from coverage to budgets. </p>",,,10.1145/2950290.2983987,,,Algorithms;Combinatorial Testing;Software Testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
NonDex: a tool for detecting and debugging wrong assumptions on Java API specifications,A. Gyori; B. Lambeth; A. Shi; O. Legunsen; D. Marinov,"University of Illinois at Urbana-Champaign, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,993,997,"<p> We present NonDex, a tool for detecting and debugging wrong assumptions on Java APIs. Some APIs have underdetermined specifications to allow implementations to achieve different goals, e.g., to optimize performance. When clients of such APIs assume stronger-than-specified guarantees, the resulting client code can fail. For example, HashSetŠ—Ès iteration order is underdetermined, and code assuming some implementation-specific iteration order can fail. NonDex helps to proactively detect and debug such wrong assumptions. NonDex performs detection by randomly exploring different behaviors of underdetermined APIs during test execution. When a test fails during exploration, NonDex searches for the invocation instance of the API that caused the failure. NonDex is open source, well-integrated with Maven, and also runs from the command line. During our experiments with the NonDex Maven plugin, we detected 21 new bugs in eight Java projects from GitHub, and, using the debugging feature of NonDex, we identified the underlying wrong assumptions for these 21 new bugs and 54 previously detected bugs. We opened 13 pull requests; developers already accepted 12, and one project changed the continuous-integration configuration to run NonDex on every push. The demo video is at: https://youtu.be/h3a9ONkC59c </p>",,,10.1145/2950290.2983932,,,NonDex;flaky tests;underdetermined API,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Multi-representational security analysis,E. Kang; A. Milicevic; D. Jackson,"University of California at Berkeley, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,181,192,"<p> <p>Security attacks often exploit flaws that are not anticipated in an abstract design, but are introduced inadvertently when high-level interactions in the design are mapped to low-level behaviors in the supporting platform. This paper proposes a <i>multi-representational</i> approach to security analysis, where models capturing distinct (but possibly overlapping) views of a system are automatically composed in order to enable an end-to-end analysis. This approach allows the designer to incrementally explore the impact of design decisions on security, and discover attacks that span multiple layers of the system. This paper describes Poirot, a prototype implementation of the approach, and reports on our experience on applying Poirot to detect previously unknown security flaws in publicly deployed systems.</p> </p>",,,10.1145/2950290.2950356,,,Security;composition;modeling;representation;verification,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Continuous deployment of mobile software at facebook (showcase),C. Rossi; E. Shibley; S. Su; K. Beck; T. Savor; M. Stumm,"Facebook, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,12,23,"<p> Continuous deployment is the practice of releasing software updates to production as soon as it is ready, which is receiving increased adoption in industry. The frequency of updates of mobile software has traditionally lagged the state of practice for cloud-based services for a number of reasons. Mobile versions can only be released periodically. Users can choose when and if to upgrade, which means that several different releases coexist in production. There are hundreds of Android hardware variants, which increases the risk of having errors in the software being deployed. </p> <p> Facebook has made significant progress in increasing the frequency of its mobile deployments. Over a period of 4 years, the Android release has gone from a deployment every 8 weeks to a deployment every week. In this paper, we describe in detail the mobile deployment process at FB. We present our findings from an extensive analysis of software engineering metrics based on data collected over a period of 7 years. A key finding is that the frequency of deployment does not directly affect developer productivity or software quality. We argue that this finding is due to the fact that increasing the frequency of continuous deployment forces improved release and deployment automation, which in turn reduces developer workload. Additionally, the data we present shows that dog-fooding and obtaining feedback from alpha and beta customers is critical to maintaining release quality. </p>",,,10.1145/2950290.2994157,,,Agile development;Continuous delivery;Continuous deployment;Mobile code testing;Software release,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Directed test generation to detect loop inefficiencies,M. Dhok; M. K. Ramanathan,"Indian Institute of Science, India",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,895,907,"<p> Redundant traversal of loops in the context of other loops has been recently identified as a source of performance bugs in many Java libraries. This has resulted in the design of static and dynamic analysis techniques to detect these performance bugs automatically. However, while the effectiveness of dynamic analyses is dependent on the analyzed input tests, static analyses are less effective in automatically validating the presence of these problems, validating the fixes and avoiding regressions in future versions. This necessitates the design of an approach to automatically generate tests for exposing redundant traversal of loops. </p> <p> In this paper, we design a novel, scalable and automatic approach that addresses this goal. Our approach takes a library and an initial set of coverage-driven randomly generated tests as input and generates tests which enable detection of redundant traversal of loops. Our approach is broadly composed of three phases Š—– analysis of the execution of random tests to generate method summaries, identification of methods with potential nested loops along with the appropriate context to expose the problem, and test generation to invoke the identified methods with the appropriate parameters. The generated tests can be analyzed by existing dynamic tools to detect possible performance issues. </p> <p> We have implemented our approach on top of the SOOT bytecode analysis framework and validated it on many open-source Java libraries. Our experiments reveal the effectiveness of our approach in generating 224 tests that reveal 46 bugs across seven libraries, including 34 previously unknown bugs. The tests generated using our approach significantly outperform the randomly generated tests in their ability to expose the inefficiencies, demonstrating the usefulness of our design. The implementation of our tool, named Glider, is available at http://drona.csa.iisc.ac.in/~sss/tools/glider. </p>",,,10.1145/2950290.2950360,,,performance;redundant traversal bugs;testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
DeepSoft: a vision for a deep model of software,H. K. Dam; T. Tran; J. Grundy; A. Ghose,"University of Wollongong, Australia",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,944,947,"<p> Although software analytics has experienced rapid growth as a research area, it has not yet reached its full potential for wide industrial adoption. Most of the existing work in software analytics still relies heavily on costly manual feature engineering processes, and they mainly address the traditional classification problems, as opposed to predicting future events. We present a vision for DeepSoft, an end-to-end generic framework for modeling software and its development process to predict future risks and recommend interventions. DeepSoft, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term temporal dependencies that occur in software evolution. Such deep learned patterns of software can be used to address a range of challenging problems such as code and task recommendation and prediction. DeepSoft provides a new approach for research into modeling of source code, risk prediction and mitigation, developer modeling, and automatically generating code patches from bug reports. </p>",,,10.1145/2950290.2983985,,,Mining Software Repositories;Software Analytics,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Automated test input generation for Android: are we really there yet in an industrial case?,X. Zeng; D. Li; W. Zheng; F. Xia; Y. Deng; W. Lam; W. Yang; T. Xie,"Tencent, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,987,992,"<p> Given the ever increasing number of research tools to automatically generate inputs to test Android applications (or simply apps), researchers recently asked the question ""Are we there yet?"" (in terms of the practicality of the tools). By conducting an empirical study of the various tools, the researchers found that Monkey (the most widely used tool of this category in industrial practices) outperformed all of the research tools that they studied. In this paper, we present two significant extensions of that study. First, we conduct the first industrial case study of applying Monkey against WeChat, a popular messenger app with over 762 million monthly active users, and report the empirical findings on Monkey's limitations in an industrial setting. Second, we develop a new approach to address major limitations of Monkey and accomplish substantial code-coverage improvements over Monkey, along with empirical insights for future enhancements to both Monkey and our approach. </p>",,,10.1145/2950290.2983958,,,Android;Code coverage;GUI testing;Test case generating,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Detecting sensitive data disclosure via bi-directional text correlation analysis,J. Huang; X. Zhang; L. Tan,"Purdue University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,169,180,"<p> Traditional sensitive data disclosure analysis faces two challenges: to identify sensitive data that is not generated by specific API calls, and to report the potential disclosures when the disclosed data is recognized as sensitive only after the sink operations. We address these issues by developing BidText, a novel static technique to detect sensitive data disclosures. BidText formulates the problem as a type system, in which variables are typed with the text labels that they encounter (e.g., during key-value pair operations). The type system features a novel bi-directional propagation technique that propagates the variable label sets through forward and backward data-flow. A data disclosure is reported if a parameter at a sink point is typed with a sensitive text label. BidText is evaluated on 10,000 Android apps. It reports 4,406 apps that have sensitive data disclosures, with 4,263 apps having log based disclosures and 1,688 having disclosures due to other sinks such as HTTP requests. Existing techniques can only report 64.0% of what BidText reports. And manual inspection shows that the false positive rate for BidText is 10%. </p>",,,10.1145/2950290.2950348,,,Android apps;Bi-directional Text Correlation;Sensitive Data Disclosure,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Correct or usable? the limits of traditional verification (impact paper award),D. Jackson; M. Vaziri,"Massachusetts Institute of Technology, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,11,11,"<p> Since our work on verification sixteen years ago, our views of the role of verification, and the centrality of correctness, have evolved. In our presentation, weŠ—Èll talk about some of our concerns about the limitations of this kind of technology, including: usability as a key factor; the unknowable properties of the environment; and the inadequacy of specifications as a means of capturing usersŠ—È desires. WeŠ—Èll describe two approaches weŠ—Ère currently working on to mitigate these concerns Š—” (1) moving to higher level abstractions with correctness by construction and (2) focusing on the conceptual structure of applications Š—” and will argue that, combined with traditional verification tools, these offer the possibility of applications that are both usable and correct. </p>",,,10.1145/2950290.2994161,,,Software Design;Software Verification,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Isomorphic regression testing: executing uncovered branches without test augmentation,J. Zhang; Y. Lou; L. Zhang; D. Hao; L. Zhang; H. Mei,"Peking University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,883,894,"<p> In software testing, it is very hard to achieve high coverage with the program under test, leaving many behaviors unexplored. To alleviate this problem, various automated test generation and augmentation approaches have been proposed, among which symbolic execution and search-based techniques are the most competitive, while each has key challenges to be solved. Different from prior work, we present a new methodology for regression testing --Isomorphic Regression Testing,which explores the behaviors of the program under test by creating its variants (i.e., modified programs) instead of generating tests. In this paper, we make the first implementation of isomorphic regression testing through an approach named ISON, which creates program variants by negating branch conditions. The results show that ISON is able to additionally execute 5.3% to 80.0% branches that are originally uncovered. Furthermore, ISON also detects a number of faults not detected by a popular automated test generation tool (i.e., EvoSuite) under the scenario of regression testing. </p>",,,10.1145/2950290.2950313,,,branch negation;regression testing;software testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Studying developer gaze to empower software engineering research and practice,B. Sharif; B. Clark; J. I. Maletic,"Youngstown State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,940,943,"<p> A new research paradigm is proposed that leverages developer eye gaze to improve the state of the art in software engineering research and practice. The vision of this new paradigm for use on software engineering tasks such as code summarization, code recommendations, prediction, and continuous traceability is described. Based on this new paradigm, it is foreseen that new benchmarks will emerge based on developer gaze. The research borrows from cognitive psychology, artificial intelligence, information retrieval, and data mining. It is hypothesized that new algorithms will be discovered that work with eye gaze data to help improve current IDEs, thus improving developer productivity. Conducting empirical studies using an eye tracker will lead to inventing, evaluating, and applying innovative methods and tools that use eye gaze to support the developer. The implications and challenges of this paradigm for future software engineering research is discussed. </p>",,,10.1145/2950290.2983988,,,Eye tracking;benchmarks;continuous traceability;mining gaze data;predictions;recommendations;summarizations,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Combinatorial generation of structurally complex test inputs for commercial software applications,H. Zhong; L. Zhang; S. Khurshid,"Google, USA / University of Texas at Austin, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,981,986,"<p> Despite recent progress in automated test generation research, significant challenges remain for applying these techniques on large-scale software systems. These systems under test often require structurally complex test inputs within a large input domain. It is challenging to automatically generate a reasonable number of tests that are both legal and behaviorally-diverse to exercise these systems. Constraint-based test generation is an effective approach for generating structurally complex inputs for systematic testing. While this approach can typically generate large numbers of tests, it has limited scalability Š—– tests generated are usually only up to a small bound on input size. Combinatorial test generation, e.g., pair-wise testing, is a more scalable approach but is challenging to apply on commercial software systems that require complex input structures that cannot be formed by using arbitrary combinations. This paper introduces comKorat, which unifies constraint-based generation of structurally complex tests with combinatorial testing. Specifically, comKorat integrates Korat and ACTS test generators to generate test suites for large scale software systems with structurally complex test inputs. We have successfully applied comKorat on four software applications developed at eBay and Yahoo!. The experimental results show that comKorat outperforms existing solutions in execution time and test coverage. Furthermore, comKorat found a total of 59 previously unknown bugs in the four applications. </p>",,,10.1145/2950290.2983959,,,ACTS;Combinatorial test generation;Constraint-based test generation;Korat,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
CacheOptimizer: helping developers configure caching frameworks for hibernate-based database-centric web applications,T. H. Chen; W. Shang; A. E. Hassan; M. Nasser; P. Flora,"Queen's University, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,666,677,"<p> To help improve the performance of database-centric cloud-based web applications, developers usually use caching frameworks to speed up database accesses. Such caching frameworks require extensive knowledge of the application to operate effectively. However, all too often developers have limited knowledge about the intricate details of their own application. Hence, most developers find configuring caching frameworks a challenging and time-consuming task that requires extensive and scattered code changes. Furthermore, developers may also need to frequently change such configurations to accommodate the ever changing workload. </p> <p> In this paper, we propose CacheOptimizer, a lightweight approach that helps developers optimize the configuration of caching frameworks for web applications that are implemented using Hibernate. CacheOptimizer leverages readily-available web logs to create mappings between a workload and database accesses. Given the mappings, CacheOptimizer discovers the optimal cache configuration using coloured Petri nets, and automatically adds the appropriate cache configurations to the application. We evaluate CacheOptimizer on three open-source web applications. We find that i) CacheOptimizer improves the throughput by 27--138%; and ii) after considering both the memory cost and throughput improvement, CacheOptimizer still brings statistically significant gains (with mostly large effect sizes) in comparison to the application's default cache configuration and to blindly enabling all possible caches. </p>",,,10.1145/2950290.2950303,,,ORM;Performance;application-level cache;web application,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Effort-aware just-in-time defect prediction: simple unsupervised models could be better than supervised models,Y. Yang; Y. Zhou; J. Liu; Y. Zhao; H. Lu; L. Xu; B. Xu; H. Leung,"Nanjing University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,157,168,"<p> Unsupervised models do not require the defect data to build the prediction models and hence incur a low building cost and gain a wide application range. Consequently, it would be more desirable for practitioners to apply unsupervised models in effort-aware just-in-time (JIT) defect prediction if they can predict defect-inducing changes well. However, little is currently known on their prediction effectiveness in this context. We aim to investigate the predictive power of simple unsupervised models in effort-aware JIT defect prediction, especially compared with the state-of-the-art supervised models in the recent literature. We first use the most commonly used change metrics to build simple unsupervised models. Then, we compare these unsupervised models with the state-of-the-art supervised models under cross-validation, time-wise-cross-validation, and across-project prediction settings to determine whether they are of practical value. The experimental results, from open-source software systems, show that many simple unsupervised models perform better than the state-of-the-art supervised models in effort-aware JIT defect prediction. </p>",,,10.1145/2950290.2950353,,,Defect;changes;effort-aware;just-in-time;prediction,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Building a socio-technical theory of coordination: why and how (outstanding research award),J. Herbsleb,"Carnegie Mellon University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,2,10,"<p> Research aimed at understanding and addressing coordination breakdowns experienced in global software development (GSD) projects at Lucent Technologies took a path from open-ended qualitative exploratory studies to quantitative studies with a tight focus on a key problem Š—– delay Š—– and its causes. Rather than being directly associated with delay, multi-site work items involved more people than comparable same-site work items, and the number of people was a powerful predictor of delay. To counteract this, we developed and deployed tools and practices to support more effective communication and expertise location. After conducting two case studies of open source development, an extreme form of GSD, we realized that many tools and practices could be effective for multi-site work, but none seemed to work under all conditions. To achieve deeper insight, we developed and tested our Socio-Technical Theory of Coordination (STTC) in which the dependencies among engineering decisions are seen as defining a constraint satisfaction problem that the organization can solve in a variety of ways. I conclude by explaining how we applied these ideas to transparent development environments, then sketch important open research questions. </p>",,,10.1145/2950290.2994160,,,Coordination;collaboration;empirical studies;global software development;open source;socio-technical theory of coordination;transparent environments,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Effectiveness of code contribution: from patch-based to pull-request-based tools,J. Zhu; M. Zhou; A. Mockus,"Peking University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,871,882,"<p> Code contributions in Free/Libre and Open Source Software projects are controlled to maintain high-quality of software. Alternatives to patch-based code contribution tools such as mailing lists and issue trackers have been developed with the pull request systems being the most visible and widely available on GitHub. Is the code contribution process more effective with pull request systems? To answer that, we quantify the effectiveness via the rates contributions are accepted and ignored, via the time until the first response and final resolution and via the numbers of contributions. To control for the latent variables, our study includes a project that migrated from an issue tracker to the GitHub pull request system and a comparison between projects using mailing lists and pull request systems. Our results show pull request systems to be associated with reduced review times and larger numbers of contributions. However, not all the comparisons indicate substantially better accept or ignore rates in pull request systems. These variations may be most simply explained by the differences in contribution practices the projects employ and may be less affected by the type of tool. Our results clarify the importance of understanding the role of tools in effective management of the broad network of potential contributors and may lead to strategies and practices making the code contribution more satisfying and efficient from both contributors' and maintainers' perspectives. </p>",,,10.1145/2950290.2950364,,,Code contribution;FLOSS;effectiveness;issue tracker;mailing list;pull request,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
What would users change in my app? summarizing app reviews for recommending software changes,A. Di Sorbo; S. Panichella; C. V. Alexandru; J. Shimagaki; C. A. Visaggio; G. Canfora; H. C. Gall,"University of Sannio, Italy",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,499,510,"<p> Mobile app developers constantly monitor feedback in user reviews with the goal of improving their mobile apps and better meeting user expectations. Thus, automated approaches have been proposed in literature with the aim of reducing the effort required for analyzing feedback contained in user reviews via automatic classification/prioritization according to specific topics. In this paper, we introduce SURF (Summarizer of User Reviews Feedback), a novel approach to condense the enormous amount of information that developers of popular apps have to manage due to user feedback received on a daily basis. SURF relies on a conceptual model for capturing user needs useful for developers performing maintenance and evolution tasks. Then it uses sophisticated summarisation techniques for summarizing thousands of reviews and generating an interactive, structured and condensed agenda of recommended software changes. We performed an end-to-end evaluation of SURF on user reviews of 17 mobile apps (5 of them developed by Sony Mobile), involving 23 developers and researchers in total. Results demonstrate high accuracy of SURF in summarizing reviews and the usefulness of the recommended changes. In evaluating our approach we found that SURF helps developers in better understanding user needs, substantially reducing the time required by developers compared to manually analyzing user (change) requests and planning future software changes. </p>",,,10.1145/2950290.2950299,,,Mobile Application;Text Summarization;User Feedback,,,,,8,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
PerfGuard: binary-centric application performance monitoring in production environments,C. H. Kim; J. Rhee; K. H. Lee; X. Zhang; D. Xu,"Purdue University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,595,606,<p> Diagnosis of performance problems is an essential part of software development and maintenance. This is in particular a challenging problem to be solved in the production environment where only program binaries are available with limited or zero knowledge of the source code. This problem is compounded by the integration with a significant number of third-party software in most large-scale applications. Existing approaches either require source code to embed manually constructed logic to identify performance problems or support a limited scope of applications with prior manual analysis. This paper proposes an automated approach to analyze application binaries and instrument the binary code transparently to inject and apply performance assertions on application transactions. Our evaluation with a set of large-scale application binaries without access to source code discovered 10 publicly known real world performance bugs automatically and shows that PerfGuard introduces very low overhead (less than 3% on Apache and MySQL server) to production systems. </p>,,,10.1145/2950290.2950347,,,Performance diagnosis;post-development testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Automatic performance testing using input-sensitive profiling,Q. Luo,"College of William and Mary, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1139,1141,"<p> During performance testing, software engineers commonly perform application profiling to analyze an application's execution traces with different inputs to understand the performance behaviors, such as the time and space consumption. However, a non-trivial application commonly has a large number of input data, and it is mostly manual to identify the specific inputs leading to performance bottlenecks. Thus, it is challenge is to automate application profiling and find these specific inputs. To solve these problems, we propose novel approaches, namely FOREPOST, GA-Prof and PerfImpact, which automatically profile applications for finding the specific combinations of inputs triggering performance bottlenecks, and further analyze the corresponding execution traces to identify problematic methods. Specially, our approaches work in two different types of real-world scenarios of performance testing: i) a single-version scenario, in which performance bottlenecks are detected in a single software release, and ii) a two-version scenario, in which code changes responsible for performance regressions are detected by considering two consecutive software releases. </p>",,,10.1145/2950290.2983975,,,Input-sensitive profiling;change impact analysis;genetic algorithms;machine learning algorithms;performance testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Repairing test dependence,W. Lam,"University of Illinois at Urbana-Champaign, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1121,1123,"<p> <p>In a test suite, all the tests should be independent: no test should affect another test’¢ s result, and running the tests in any order should yield the same test results. The assumption of such test independence is important so that tests behave consistently as designed. However, this critical assumption often does not hold in practice due to test dependence. </p><p>Test dependence causes two serious problems: a dependent test may spuriously fail even when the software is correct (a false positive alarm), or it may spuriously pass even when a bug exists in the software (a false negative). Existing approaches to cope with test dependence require tests to be executed in a given order or for each test to be executed in a separate virtual machine. This paper presents an approach that can automatically repair test dependence so that each test in a suite yields the same result regardless of their execution order. At compile time, the approach refactors code under test and test code to eliminate test dependence and prevent spurious test successes or failures. </p><p>We develop a prototype of our approach to handle one of the most common causes of test dependence and evaluate the prototype on five subject programs. In our experimental evaluation, our prototype is capable of eliminating up to 12.5% of the test dependence in the subject programs.</p> </p>",,,10.1145/2950290.2983969,,,False alarm;Refactoring;Repairing;Test dependence;Testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Reaching the masses: a new subdiscipline of app programmer education,C. Weir; A. Rashid; J. Noble,"Security Lancaster, UK",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,936,939,"<p> ProgrammersŠ—È lack of knowledge and interest in secure development threatens everyone who uses mobile apps. The rise of apps has engaged millions of independent app developers, who rarely encounter any but low level security techniques. But what if software security were presented as a game, or a story, or a discussion? What if learning app security techniques could be fun as well as empowering? Only by introducing the powerful motivating techniques developed for other disciplines can we hope to upskill independent app developers, and achieve the security that weŠ—Èll need in 2025 to safeguard our identities and our data. </p>",,,10.1145/2950290.2983981,,,app developer;app development;app programmer;app security;application security;continued learning;mobile app;programmer education;secure app;secure app development;security technique;software development;software security;whole system security,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Learning for test prioritization: an industrial case study,B. Busjaeger; T. Xie,"Salesforce.com, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,975,980,"<p> Modern cloud-software providers, such as Salesforce.com, increasingly adopt large-scale continuous integration environments. In such environments, assuring high developer productivity is strongly dependent on conducting testing efficiently and effectively. Specifically, to shorten feedback cycles, test prioritization is popularly used as an optimization mechanism for ranking tests to run by their likelihood of revealing failures. To apply test prioritization in industrial environments, we present a novel approach (tailored for practical applicability) that integrates multiple existing techniques via a systematic framework of machine learning to rank. Our initial empirical evaluation on a large real-world dataset from Salesforce.com shows that our approach significantly outperforms existing individual techniques. </p>",,,10.1145/2950290.2983954,,,Regression testing;learning to rank;test prioritization,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Mining performance specifications,M. Br’_nink; D. S. Rosenblum,"National University of Singapore, Singapore",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,39,49,"<p> Functional testing is widespread and supported by a multitude of tools, including tools to mine functional specifications. In contrast, non-functional attributes like performance are often less well understood and tested. While many profiling tools are available to gather raw performance data, interpreting this raw data requires expert knowledge and a thorough understanding of the underlying software and hardware infrastructure. In this work we present an approach that mines performance specifications from running systems autonomously. The tool creates performance models during runtime. The mined models are analyzed further to create compact and comprehensive performance assertions. The resulting assertions can be used as an evidence-based performance specification for performance regression testing, performance monitoring, or as a foundation for more formal performance specifications. </p>",,,10.1145/2950290.2950314,,,Specification mining;performance modeling;regression testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
iGen: dynamic interaction inference for configurable software,T. Nguyen; U. Koc; J. Cheng; J. S. Foster; A. A. Porter,"University of Maryland at College Park, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,655,665,"<p> To develop, analyze, and evolve today's highly configurable software systems, developers need deep knowledge of a system's configuration options, e.g., how options need to be set to reach certain locations, what configurations to use for testing, etc. Today, acquiring this detailed information requires manual effort that is difficult, expensive, and error prone. In this paper, we propose iGen, a novel, lightweight dynamic analysis technique that automatically discovers a program's interactions---expressive logical formulae that give developers rich and detailed information about how a system's configuration option settings map to particular code coverage. iGen employs an iterative algorithm that runs a system under a small set of configurations, capturing coverage data; processes the coverage data to infer potential interactions; and then generates new configurations to further refine interactions in the next iteration. We evaluated iGen on 29 programs spanning five languages; the breadth of this study would be unachievable using prior interaction inference tools. Our results show that iGen finds precise interactions based on a very small fraction of the number of possible configurations. Moreover, iGen's results confirm several earlier hypotheses about typical interaction distributions and structures. </p>",,,10.1145/2950290.2950311,,,Program analysis;configurable systems;dynamic analysis;software testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Discovering bug patterns in JavaScript,Q. Hanam; F. S. d. M. Brito; A. Mesbah,"University of British Columbia, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,144,156,"<p> JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug finding tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using language-construct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript. </p>",,,10.1145/2950290.2950308,,,Bug patterns;JavaScript;Node.js;data mining;static analysis,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
"""Womenomics"" and gender-inclusive software: what software engineers need to know (invited talk)",M. Burnett,"Oregon State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1,1,"<p> This short paper is a summary of my keynote at FSEŠ—È16, with accompanying references for follow-up. </p>",,,10.1145/2950290.2994159,,,GenderMag;gender-inclusive software;software inspection process,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Efficiency of projectional editing: a controlled experiment,T. Berger; M. V’_lter; H. P. Jensen; T. Dangprasert; J. Siegmund,"Chalmers University of Technology, Sweden / University of Gothenburg, Sweden",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,763,774,"<p> Projectional editors are editors where a user's editing actions directly change the abstract syntax tree without using a parser. They promise essentially unrestricted language com position as well as flexible notations, which supports aligning languages with their respective domain and constitutes an essential ingredient of model-driven development. Such editors have existed since the 1980s and gained widespread attention with the Intentional Programming paradigm, which used projectional editing at its core. However, despite the benefits, programming still mainly relies on editing textual code, where projectional editors imply a very different -- typically perceived as worse -- editing experience, often seen as the main challenge prohibiting their widespread adoption. We present an experiment of code-editing activities in a projectional editor, conducted with 19 graduate computer-science students and industrial developers. We investigate the effects of projectional editing on editing efficiency, editing strategies, and error rates -- each of which we also compare to conventional, parser-based editing. We observe that editing is efficient for basic-editing tasks, but that editing strategies and typical errors differ. More complex tasks require substantial experience and a better understanding of the abstract-syntax-tree structure -- then, projectional editing is also efficient. We also witness a tradeoff between fewer typing mistakes and an increased complexity of code editing. </p>",,,10.1145/2950290.2950315,,,experiment;language workbench;projectional editing,,,,,3,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Why we refactor? confessions of GitHub contributors,D. Silva; N. Tsantalis; M. T. Valente,"Federal University of Minas Gerais, Brazil",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,858,870,"<p> Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code. However, there is a limited number of studies empirically investigating the actual motivations behind specific refactoring operations applied by developers. To fill this gap, we monitored Java projects hosted on GitHub to detect recently applied refactorings, and asked the developers to explain the reasons behind their decision to refactor the code. By applying thematic analysis on the collected responses, we compiled a catalogue of 44 distinct motivations for 12 well-known refactoring types. We found that refactoring activity is mainly driven by changes in the requirements and much less by code smells. Extract Method is the most versatile refactoring operation serving 11 different purposes. Finally, we found evidence that the IDE used by the developers affects the adoption of automated refactoring tools. </p>",,,10.1145/2950290.2950305,,,GitHub;Refactoring;code smells;software evolution,,,,,6,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Revamping JavaScript static analysis via localization and remediation of root causes of imprecision,S. Wei; O. Tripp; B. G. Ryder; J. Dolby,"University of Maryland, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,487,498,"<p> Static analysis is challenged by the dynamic language constructs of JavaScript which often lead to unacceptable performance and/or precision results. We describe an approach that focuses on improving the practicality and accuracy of points-to analysis and call graph construction for JavaScript programs. The approach first identifies program constructs which are sources of imprecision (i.e., root causes) through monitoring the static analysis process. We then examine and suggest specific context-sensitive analyses to apply. Our technique is able to to find that the root causes comprise less than 2% of the functions in JavaScript library applications. Moreover, the specialized analysis derived by our approach finishes within a few seconds, even on programs which can not complete within 10 minutes with the original analysis. </p>",,,10.1145/2950290.2950338,,,JavaScript;program analysis,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
An extensive study of static regression test selection in modern software evolution,O. Legunsen; F. Hariri; A. Shi; Y. Lu; L. Zhang; D. Marinov,"University of Illinois at Urbana-Champaign, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,583,594,"<p> Regression test selection (RTS) aims to reduce regression testing time by only re-running the tests affected by code changes. Prior research on RTS can be broadly split into dy namic and static techniques. A recently developed dynamic RTS technique called Ekstazi is gaining some adoption in practice, and its evaluation shows that selecting tests at a coarser, class-level granularity provides better results than selecting tests at a finer, method-level granularity. As dynamic RTS is gaining adoption, it is timely to also evaluate static RTS techniques, some of which were proposed over three decades ago but not extensively evaluated on modern software projects. </p> <p> This paper presents the first extensive study that evaluates the performance benefits of static RTS techniques and their safety; a technique is safe if it selects to run all tests that may be affected by code changes. We implemented two static RTS techniques, one class-level and one method-level, and compare several variants of these techniques. We also compare these static RTS techniques against Ekstazi, a state-of-the-art, class-level, dynamic RTS technique. The experimental results on 985 revisions of 22 open-source projects show that the class-level static RTS technique is comparable to Ekstazi, with similar performance benefits, but at the risk of being unsafe sometimes. In contrast, the method-level static RTS technique performs rather poorly. </p>",,,10.1145/2950290.2950361,,,class firewall;regression test selection;static analysis,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Finding and breaking test dependencies to speed up test execution,S. Kappler,"Saarland University, Germany",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1136,1138,"<p> Software testing takes up the major part of the build time, which hinders developers' ability to promptly identify and fix problems. </p> <p> Test parallelization is an effective means to speed up test executions, hence improving software development. Effective and sound test parallelization requires that tests are independent or that test dependencies are known in advance. However, current techniques to detect test dependencies are either precise but slow, or fast but inaccurate. Further, available algorithms for test parallelization either over-constraint test executions, which reduces their level of parallelism, or re-execute the same tests multiple times, which increases the execution effort. </p> <p> This research addresses both sides of the problem of speeding up test execution: it aims to devise a practical test detection technique that can suitably balance efficiency and accuracy, and develop a novel technique to break test dependencies which allows both sound and efficient test executions. </p>",,,10.1145/2950290.2983974,,,Software testing;Test dependence;detection algorithms,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
How should static analysis tools explain anomalies to developers?,T. Barik,"North Carolina State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1118,1120,"<p> Despite the advanced static analysis tools available within modern integrated development environments (IDEs), the error messages these tools produce remain perplexing for developers to comprehend. This research postulates that tools can computationally expose their internal reasoning processes to generate assistive error explanations that more closely align with how developers explain errors to themselves. </p>",,,10.1145/2950290.2983968,,,error messages;explanations;program comprehension;static analysis,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Training the future workforce through task curation in an OSS ecosystem,A. Sarma; M. A. Gerosa; I. Steinmacher; R. Leano,"Oregon State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,932,935,"<p> Volunteers to Open Source Software (OSS) projects contribute not only to help creating software that they use, but also to gain skills and enrich their expertise and resumes. However, newcomers to OSS face several challenges when joining a project. Particularly, they do not know where to start, or choose tasks that they can be successful at. Here, we describe our vision towards BugExchange, a system that curates tasks from OSS projects and helps train newcomers. While evaluating and executing these tasks, newcomers can gain an understanding about the project, its technology, and concepts. There are many challenges in designing such a system. For example, identifying the information needs of newcomers, creating task recommendations that match newcomersŠ—È skills and career goals, and providing mentoring and networking support. We plan to leverage our previous work to conceive and prototype our system, which will include multiple research lines. BugExchange has the potential to improve newcomer learning experiences, reduce dropouts, and foster community building. </p>",,,10.1145/2950290.2983984,,,Newcomers;onboarding;open source projects;task labeling,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
A portable interface for runtime energy monitoring,C. Imes; L. Bergstrom; H. Hoffmann,"University of Chicago, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,968,974,"<p> As energy consumption becomes a first class concern for computing systems, there is an increasing need for application-level access to runtime power/energy measurements. To support this need, a growing number of power and energy monitors are being developed, each with their own interfaces. In fact, the approaches are extremely diverse, and porting energy-aware code to new platforms with new hardware can involve significant rewriting effort. To reduce this effort and support portable, application-level energy monitoring, a common interface is needed. In this paper, we propose EnergyMon, a portable application interface that is independent of underlying power/energy data sources. We demonstrate EnergyMon's flexibility with two case studies -- energy-aware profiling and self-adaptive systems, each of which requires monitoring energy across a range of hardware from different manufacturers. We release the EnergyMon interface, implementations, utilities, and Java and Rust bindings and abstractions as open source. </p>",,,10.1145/2950290.2983956,,,portable energy measurement,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Titanium: efficient analysis of evolving alloy specifications,H. Bagheri; S. Malek,"University of Nebraska-Lincoln, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,27,38,"<p> The Alloy specification language, and the corresponding Alloy Analyzer, have received much attention in the last two decades with applications in many areas of software engineering. Increasingly, formal analyses enabled by Alloy are desired for use in an on-line mode, where the specifications are automatically kept in sync with the running, possibly changing, software system. However, given Alloy Analyzer's reliance on computationally expensive SAT solvers, an important challenge is the time it takes for such analyses to execute at runtime. The fact that in an on-line mode, the analyses are often repeated on slightly revised versions of a given specification, presents us with an opportunity to tackle this challenge. We present Titanium, an extension of Alloy for formal analysis of evolving specifications. By leveraging the results from previous analyses, Titanium narrows the state space of the revised specification, thereby greatly reducing the required computational effort. We describe the semantic basis of Titanium in terms of models specified in relational logic. We show how the approach can be realized atop an existing relational logic model finder. Our experimental results show Titanium achieves a significant speed-up over Alloy Analyzer when applied to the analysis of evolving specifications. </p>",,,10.1145/2950290.2950337,,,Evolving Software;Formal Verification;Partial Models;Relational Logic,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Build system with lazy retrieval for Java projects,A. Celik; A. Knaust; A. Milicevic; M. Gligoric,"University of Texas at Austin, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,643,654,"<p> In the modern-day development, projects use Continuous Integration Services (CISs) to execute the build for every change in the source code. To ensure that the project remains correct and deployable, a CIS performs a clean build each time. In a clean environment, a build system needs to retrieve the project's dependencies (e.g., guava.jar). The retrieval, however, can be costly due to dependency bloat: despite a project using only a few files from each library, the existing build systems still eagerly retrieve all the libraries at the beginning of the build. </p> <p> This paper presents a novel build system, Molly, which lazily retrieves parts of libraries (i.e., files) that are needed during the execution of a build target. For example, the compilation target needs only public interfaces of classes within the libraries and the test target needs only implementation of the classes that are being invoked by the tests. Additionally, Molly generates a transfer script that retrieves parts of libraries based on prior builds. Molly's design requires that we ignore the boundaries set by the library developers and look at the files within the libraries. We implemented Molly for Java and evaluated it on 17 popular open-source projects. We show that test targets (on average) depend on only 9.97% of files in libraries. A variant of Molly speeds up retrieval by 44.28%. Furthermore, the scripts generated by Molly retrieve dependencies, on average, 93.81% faster than the Maven build system. </p>",,,10.1145/2950290.2950358,,,Build system;continuous integration service,,,,,,1,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Factors influencing code review processes in industry,T. Baum; O. Liskin; K. Niklas; K. Schneider,"Leibniz Universit&#228;t Hannover, Germany",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,85,96,"<p> Code review is known to be an efficient quality assurance technique. Many software companies today use it, usually with a process similar to the patch review process in open source software development. However, there is still a large fraction of companies performing almost no code reviews at all. And the companies that do code reviews have a lot of variation in the details of their processes. For researchers trying to improve the use of code reviews in industry, it is important to know the reasons for these process variations. We have performed a grounded theory study to clarify process variations and their rationales. The study is based on interviews with software development professionals from 19 companies. These interviews provided insights into the reasons and influencing factors behind the adoption or non-adoption of code reviews as a whole as well as for different process variations. We have condensed these findings into seven hypotheses and a classification of the influencing factors. Our results show the importance of cultural and social issues for review adoption. They trace many process variations to differences in development context and in desired review effects. </p>",,,10.1145/2950290.2950323,,,Code reviews;Empirical software engineering,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Crash consistency validation made easy,Y. Jiang; H. Chen; F. Qin; C. Xu; X. Ma; J. Lu,"Nanjing University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,133,143,"<p> Software should behave correctly even in adverse conditions. Particularly, we study the problem of automated validation of crash consistency, i.e., file system data safety when systems crash. Existing work requires non-trivial manual efforts of specifying checking scripts and workloads, which is an obstacle for software developers. Therefore, we propose C3, a novel approach that makes crash consistency validation as easy as pressing a single button. With a program and an input, C3 automatically reports inconsistent crash sites. C3 not only exempts developers from the need of writing crash site checking scripts (by an algorithm that computes editing distance between file system snapshots) but also reduces the reliance on dedicated workloads (by test amplification). We implemented C3 as an open-source tool. With C3, we found 14 bugs in open-source software that have severe consequences at crash and 11 of them were previously unknown to the developers, including in highly mature software (e.g., GNU zip and GNU coreutils sort) and popular ones being actively developed (e.g., Adobe Brackets and TeXstudio). </p>",,,10.1145/2950290.2950327,,,File system;crash consistency;software reliability,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
WATERFALL: an incremental approach for repairing record-replay tests of web applications,M. Hammoudi; G. Rothermel; A. Stocco,"University of Nebraska-Lincoln, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,751,762,"<p> Software engineers use record/replay tools to capture use case scenarios that can serve as regression tests for web applications. Such tests, however, can be brittle in the face of code changes. Thus, researchers have sought automated approaches for repairing broken record/replay tests. To date, such approaches have operated by directly analyzing differences between the releases of web applications. Often, however, intermediate versions or commits exist between releases, and these represent finer-grained sequences of changes by which new releases evolve. In this paper, we present WATERFALL, an incremental test repair approach that applies test repair techniques iteratively across a sequence of fine-grained versions of a web application. The results of an empirical study on seven web applications show that our approach is substantially more effective than a coarse-grained approach (209% overall), while maintaining an acceptable level of overhead. </p>",,,10.1145/2950290.2950294,,,record/replay tests;test case repair;web applications,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Paradise unplugged: identifying barriers for female participation on stack overflow,D. Ford; J. Smith; P. J. Guo; C. Parnin,"North Carolina State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,846,857,"<p> It is no secret that females engage less in programming fields than males. However, in online communities, such as Stack Overflow, this gender gap is even more extreme: only 5.8% of contributors are female. In this paper, we use a mixed-methods approach to identify contribution barriers females face in online communities. Through 22 semi-structured interviews with a spectrum of female users ranging from non-contributors to a top 100 ranked user of all time, we identified 14 barriers preventing them from contributing to Stack Overflow. We then conducted a survey with 1470 female and male developers to confirm which barriers are gender related or general problems for everyone. Females ranked five barriers significantly higher than males. A few of these include doubts in the level of expertise needed to contribute, feeling overwhelmed when competing with a large number of users, and limited awareness of site features. Still, there were other barriers that equally impacted all Stack Overflow users or affected particular groups, such as industry programmers. Finally, we describe several implications that may encourage increased participation in the Stack Overflow community across genders and other demographics. </p>",,,10.1145/2950290.2950331,,,Barriers;Females in Computing;Online Communities;Social Q&A,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Call graph construction for Java libraries,M. Reif; M. Eichberg; B. Hermann; J. Lerch; M. Mezini,"TU Darmstadt, Germany",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,474,486,"<p> Today, every application uses software libraries. Yet, while a lot of research exists w.r.t. analyzing applications, research that targets the analysis of libraries independent of any application is scarce. This is unfortunate, because, for developers of libraries, such as the Java Development Kit (JDK), it is crucial to ensure that the library behaves as intended regardless of how it is used. To fill this gap, we discuss the construction of call graphs for libraries that abstract over all potential library usages. Call graphs are particularly relevant as they are a precursor of many advanced analyses, such as inter-procedural data-flow analyses. </p> <p> We show that the current practice of using call graph algorithms designed for applications to analyze libraries leads to call graphs that, at the same time, lack relevant call edges and contain unnecessary edges. This motivates the need for call graph construction algorithms dedicated to libraries. Unlike algorithms for applications, call graph construction algorithms for libraries must take into consideration the goals of subsequent analyses. Specifically, we show that it is essential to distinguish between the scenario of an analysis for potential exploitable vulnerabilities from the scenario of an analysis for general software quality attributes, e.g., dead methods or unused fields. This distinction affects the decision about what constitutes the library-private implementation, which therefore, needs special treatment. Thus, building one call graph that satisfies all needs is not sensical. Overall, we observed that the proposed call graph algorithms reduce the number of call edges up to 30% when compared to existing approaches. </p>",,,10.1145/2950290.2950312,,,Call Graph Construction;Java;Libraries,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Analyzing the validity of selective mutation with dominator mutants,B. Kurtz; P. Ammann; J. Offutt; M. E. Delamaro; M. Kurtz; N. G’_k’_e,"George Mason University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,571,582,"<p> <p>Various forms of selective mutation testing have long been accepted as valid approximations to full mutation testing. This paper presents counterevidence to traditional selective mutation. The recent development of dominator mutants and minimal mutation analysis lets us analyze selective mutation without the noise introduced by the redundancy inherent in traditional mutation. We then exhaustively evaluate all small sets of mutation operators for the Proteum mutation system and determine dominator mutation scores and required work for each of these sets on an empirical test bed. The results show that <i>all</i> possible selective mutation approaches have poor dominator mutation scores on at least some of these programs. This suggests that to achieve high performance with respect to full mutation analysis, selective approaches will have to become more sophisticated, possibly by choosing mutants based on the specifics of the artifact under test, that is, specialized selective mutation.</p> </p>",,,10.1145/2950290.2950322,,,Mutation analysis;dominator mutants;subsumption,,,,,3,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Discovering additional violations of Java API invariants,W. Huang,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1145,1147,"<p> In the absence of formal specifications or test oracles, automating testing is made possible by the fact that a program must satisfy certain requirements set down by the programming language. This work describes Randoop, an automatic unit test generator which checks for invariants specified by the Java API. Randoop is able to detect violations to invariants as specified by the Java API and create error tests that reveal related bugs. Randoop is also able to produce regression tests, meant to be added to regression test suites, that capture expected behavior. We discuss additional extensions that we have made to Randoop which expands its capability for the detection of violation of specified invariants. We also examine an optimization and a heuristic for making the invariant checking process more efficient. </p>",,,10.1145/2950290.2983977,,,Automated unit test generation;bug detection;regression tests,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
SmartDebug: an interactive debug assistant for Java,X. Guo,"Tsinghua University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1127,1129,"<p> Debugging has long been recognized as one of the most labour- and time- consuming activities in software development. Recent research on automated debugging tries to facilitate this process by automatically generating patches for buggy programs so that they pass a predefined test suite. Despite the promising experimental results, several major obstacles emerge when we apply these techniques in active coding process. Inadequate test cases, multiple errors in one program and possible bug categories overlooked by existing fix generation search spaces impede these techniques to perform at their best. </p> <p> To overcome these obstacles, we designed an interactive usage paradigm that allows a developer to characterize his or her judgments of program running state and utilize such information to guide the fix generation process. We implemented a prototype of this design, an Eclipse plugin called SmartDebug as a debug assistant for Java programs. Experiment results show that SmartDebug helped to debug 15 out of 25 buggy programs successfully. All programs contain less than 3 test cases. In 14 programs it accelerated the debugging process compared to pure human debugging, while one of which contains 2 buggy statements. This indicates that the proposed usage paradigm is capable of facilitating the debugging process in active coding process. </p>",,,10.1145/2950290.2983971,,,automated debugging;generate and validate systems;user-interaction,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Social health cues developers use when choosing open source packages,A. Head,"University of California at Berkeley, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1133,1135,"<p> Developers choose open source packages from many alternatives. One increasingly important factor when choosing a package is its ""social health"", or a developerŠ—Ès ability to get help on communication channels. We conduct a study to understand how developers learn about the social health of open source packages before using them. We offer preliminary results of the cues developers find. </p>",,,10.1145/2950290.2983973,,,Social health;community;open source;packages,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Evaluation of fault localization techniques,S. Pearson,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1115,1117,"<p> Fault localization (FL) takes as input a faulty program and produces as output a list of code locations ranked by probability of being defective. A programmer doing debugging, or a program repair tool, could save time by focusing on the most suspicious locations. </p> <p> Researchers evaluate new FL techniques on programs with known faults, and score a technique based on where in its list the actual defect appears. This enables comparison of multiple FL techniques to determine which one is best. </p> <p> Previous research has primarily evaluated FL techniques using artificial faults, generated either by hand or automatically. Other prior work has shown that artificial faults have both similarities to and differences from real faults; given this, it is not obvious that the techniques that perform best on artificial faults will also perform best on real faults. </p> <p> This work compares 7 previously-studied FL techniques, both on artificial faults (as a replication study) and on real faults (to validate the assumption that artificial faults are useful proxies for real faults for comparisons of FL techniques). Our replication largely agreed with prior work, but artificial faults were not useful for predicting which FL techniques perform best on real faults. </p> <p> We also studied which characteristics make FL techniques perform well on real faults. We identified a design space that includes those 7 previously-studied FL techniques as well as 149 new ones, and determined which decisions were most important in designing a new technique. </p>",,,10.1145/2950290.2983967,,,debugging;fault localization;software testing,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Effective assignment and assistance to software developers and reviewers,M. B. Zanjani,"Wichita State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1091,1093,"<p> Human reliance and dominance are ubiquitous in sustaining a high-quality large software system. Automatically assigning the right solution providers to the maintenance task at hand is arguably as important as providing the right tool support for it, especially in the far too commonly found state of inadequate or obsolete documentation of large-scale software systems. Two maintenance tasks related to assignment and assistance to software developers and reviewers are addressed, and solutions are proposed. The key insight behind these proposed solutions is the analysis and use of micro-levels of human-to-code and human-to-human interactions (eg., code review). We analyzed code reviews that are managed by Gerrit and found different markers of developer expertise associated with the source code changes and their acceptance, time line, and human roles and feedback involved in the reviews. We formed a developer-expertise model from these markers and showed its application in bug triaging. Specifically, we derived a developer recommendation approach for an incoming change request, named rDevX , from this expertise model. Additionally, we present an approach, namely cHRev, to automatically recommend reviewers who are best suited to participate in a given review, based on their historical contributions as demonstrated in their prior reviews. Furthermore, a comparative study on other previous approaches for developer recommendation and reviewer recommendation was performed. The metrics recall and MRR were used to measure their quantitative effectiveness. Results show that the proposed approaches outperform the subjected competitors with statistical significance. </p>",,,10.1145/2950290.2983960,,,Developer Recommendation;Reviewer recommendation,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Automatic trigger generation for end user written rules for home automation,C. Nandi,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1109,1111,"<p> To customize the behavior of a smart home, an end user writes rules. When an external event satisfies a rule's trigger, the rule's action executes; for example, when the temperature is above a certain threshold, then window awnings might be extended. End users often write incorrect rules. This paper's technique prevents a certain category of errors in the rules: errors due to too few triggers. It statically analyzes a rule's actions to automatically determine a set of necessary and sufficient triggers. </p> <p> We implemented the technique in a tool called TrigGen and tested it on 96 end-user written rules for openHAB, an open-source home automation platform. It identified that 80% of the rules had fewer triggers than required for correct behavior. The missing triggers could lead to unexpected behavior and security vulnerabilities in a smart home. </p>",,,10.1145/2950290.2983965,,,Security;Smart homes;Static analysis;Trigger action programs,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Hunter: next-generation code reuse for Java,Y. Wang; Y. Feng; R. Martins; A. Kaushik; I. Dillig; S. P. Reiss,"University of Texas at Austin, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1028,1032,"<p> In many common scenarios, programmers need to implement functionality that is already provided by some third party library. This paper presents a tool called Hunter that facilitates code reuse by finding relevant methods in large code bases and automatically synthesizing any necessary wrapper code. Since Hunter internally uses advanced program synthesis technology, it can automatically reuse existing methods even when code adaptation is necessary. We have implemented Hunter as an Eclipse plug-in and evaluate it by (a) comparing it against S6, a state-of-the-art code reuse tool, and (b) performing a user study. Our evaluation shows that Hunter compares favorably with S6 and increases programmer productivity. </p>",,,10.1145/2950290.2983934,,,code adaptation;code reuse;program synthesis,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Parameter-free probabilistic API mining across GitHub,J. Fowkes; C. Sutton,"University of Edinburgh, UK",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,254,265,"<p> <p>Existing API mining algorithms can be difficult to use as they require expensive parameter tuning and the returned set of API calls can be large, highly redundant and difficult to understand. To address this, we present PAM (Probabilistic API Miner), a near parameter-free probabilistic algorithm for mining the most interesting API call patterns. We show that PAM significantly outperforms both MAPO and UPMiner, achieving 69% test-set precision, at retrieving relevant API call sequences from GitHub. Moreover, we focus on libraries for which the developers have explicitly provided code examples, yielding over 300,000 LOC of hand-written API example code from the 967 client projects in the data set. This evaluation suggests that the hand-written examples actually have limited coverage of real API usages.</p> </p>",,,10.1145/2950290.2950319,,,API mining;sequential pattern mining,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Disrupting developer productivity one bot at a time,M. A. Storey; A. Zagalsky,"University of Victoria, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,928,931,"<p> Bots are used to support different software development activities, from automating repetitive tasks to bridging knowledge and communication gaps in software teams. We anticipate the use of Bots will increase and lead to improvements in software quality and developer and team productivity, but what if the disruptive effect is not what we expect? </p> <p> Our goal in this paper is to provoke and inspire researchers to study the impact (positive and negative) of Bots on software development. We outline the modern Bot landscape and use examples to describe the common roles Bots occupy in software teams. We propose a preliminary cognitive support framework that can be used to understand these roles and to reflect on the impact of Bots in software development on productivity. Finally, we consider challenges that Bots may bring and propose some directions for future research. </p>",,,10.1145/2950290.2983989,,,Human computer interaction;computer supported collaborative work;productivity;software engineering,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Cluster-based test suite functional analysis,M. Zalmanovici; O. Raz; R. Tzoref-Brill,"IBM Research, Israel",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,962,967,"<p> <p>A common industrial challenge is that of analyzing large legacy free text test suites in order to comprehend their functional content. The analysis results are used for different purposes, such as dividing the test suite into disjoint functional parts for automation and management purposes, identifying redundant test cases, and extracting models for combinatorial test generation while reusing the legacy test suite. Currently the analysis is performed manually, which hinders the ability to analyze many such large test suites due to time and resource constraints. </p><p>We report on our practical experience in automated analysis of real-world free text test suites from six different industrial companies. Our novel, cluster-based approach provides significant time savings for the analysis of the test suites, varying from a reduction of 35% to 97% compared to the human time required, thus enabling functional analysis in many cases where manual analysis is infeasible in practice.</p> </p>",,,10.1145/2950290.2983957,,,Cluster-based testing;Combinatorial testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Developer workflow at google (showcase),C. Sadowski,"Google, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,26,26,"<p> This talk describes the developer workflow at Google, and our use of program analysis, testing, metrics, and tooling to reduce errors when creating and committing changes to source code. Software development at Google has several unique characteristics such as our monolithic codebase and distributed hermetic build system. Changes are vetted both manually, via our internal code review tool, and automatically, via sources such as the Tricorder program analysis platform and our automated testing infrastructure. </p>",,,10.1145/2950290.2994156,,,Developer tools;developer workflow,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Code relatives: detecting similarly behaving software,F. H. Su; J. Bell; K. Harvey; S. Sethumadhavan; G. Kaiser; T. Jebara,"Columbia University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,702,714,"<p> Detecting Š—“similar codeŠ— is useful for many software engineering tasks. Current tools can help detect code with statically similar syntactic andŠ—–or semantic features (code clones) and with dynamically similar functional input/output (simions). Unfortunately, some code fragments that behave similarly at the finer granularity of their execution traces may be ignored. In this paper, we propose the term Š—“code relativesŠ— to refer to code with similar execution behavior. We define code relatives and then present DyCLINK, our approach to detecting code relatives within and across codebases. DyCLINK records instruction-level traces from sample executions, organizes the traces into instruction-level dynamic dependence graphs, and employs our specialized subgraph matching algorithm to efficiently compare the executions of candidate code relatives. In our experiments, DyCLINK analyzed 422+ million prospective subgraph matches in only 43 minutes. We compared DyCLINK to one static code clone detector from the community and to our implementation of a dynamic simion detector. The results show that DyCLINK effectively detects code relatives with a reasonable analysis time. </p>",,,10.1145/2950290.2950321,,,Code relatives;code clones;link analysis;runtime behavior;subgraph matching,,,,,3,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Deep API learning,X. Gu; H. Zhang; D. Zhang; S. Kim,"Hong Kong University of Science and Technology, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,631,642,"<p> Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bags-of-words and lack a deep understanding of the semantics of the query. We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bag-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches. </p>",,,10.1145/2950290.2950334,,,API;API usage;RNN;code search;deep learning,,,,,7,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
A cross-tool communication study on program analysis tool notifications,B. Johnson; R. Pandita; J. Smith; D. Ford; S. Elder; E. Murphy-Hill; S. Heckman; C. Sadowski,"North Carolina State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,73,84,"<p> Program analysis tools use notifications to communicate with developers, but previous research suggests that developers encounter challenges that impede this communication. This paper describes a qualitative study that identifies 10 kinds of challenges that cause notifications to miscommunicate with developers. Our resulting notification communication theory reveals that many challenges span multiple tools and multiple levels of developer experience. Our results suggest that, for example, future tools that model developer experience could improve communication and help developers build more accurate mental models. </p>",,,10.1145/2950290.2950304,,,communication;human factors;program analysis tools,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Flow-sensitive composition of thread-modular abstract interpretation,M. Kusano; C. Wang,"Virginia Tech, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,799,809,"<p> We propose a constraint-based flow-sensitive static analysis for concurrent programs by iteratively composing thread-modular abstract interpreters via the use of a system of lightweight constraints. Our method is compositional in that it first applies sequential abstract interpreters to individual threads and then composes their results. It is flow-sensitive in that the causality ordering of interferences (flow of data from global writes to reads) is modeled by a system of constraints. These interference constraints are lightweight since they only refer to the execution order of program statements as opposed to their numerical properties: they can be decided efficiently using an off-the-shelf Datalog engine. Our new method has the advantage of being more accurate than existing, flow-insensitive, static analyzers while remaining scalable and providing the expected soundness and termination guarantees even for programs with unbounded data. We implemented our method and evaluated it on a large number of benchmarks, demonstrating its effectiveness at increasing the accuracy of thread-modular abstract interpretation. </p>",,,10.1145/2950290.2950291,,,Abstract interpretation;Concurrency;Datalog;Interference;Invariant generation;Threadmodular reasoning,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Python predictive analysis for bug detection,Z. Xu; P. Liu; X. Zhang; B. Xu,"Nanjing University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,121,132,"<p> Python is a popular dynamic language that allows quick software development. However, Python program analysis engines are largely lacking. In this paper, we present a Python predictive analysis. It first collects the trace of an execution, and then encodes the trace and unexecuted branches to symbolic constraints. Symbolic variables are introduced to denote input values, their dynamic types, and attribute sets, to reason about their variations. Solving the constraints identifies bugs and their triggering inputs. Our evaluation shows that the technique is highly effective in analyzing real-world complex programs with a lot of dynamic features and external library calls, due to its sophisticated encoding design based on traces. It identifies 46 bugs from 11 real-world projects, with 16 new bugs. All reported bugs are true positives. </p>",,,10.1145/2950290.2950357,,,Debugging;Dynamic Language;Predictive Analysis;Python,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
End-to-end memory behavior profiling with DINAMITE,S. Miucin; C. Brady; A. Fedorova,"University of British Columbia, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1042,1046,"<p> Performance bottlenecks related to a program's memory behavior are common, yet very hard to debug. Tools that attempt to aid software engineers in diagnosing these bugs are typically designed to handle specific use cases; they do not provide information to comprehensively explore memory problems and to find solutions. Detailed traces of memory accesses would enable developers to ask various questions about the program's memory behaviour, but these traces quickly become very large even for short executions. We present DINAMITE: a toolkit for Dynamic INstrumentation and Analysis for MassIve Trace Exploration. DINAMITE instruments every memory access with highly debug information and provides a suite of extensible analysis tools to aid programmers in pinpointing memory bottlenecks. </p>",,,10.1145/2950290.2983941,,,LLVM;Spark Streaming;instrumentation;memory optimization,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Semi-supervised verified feedback generation,S. Kaleeswaran; A. Santhiar; A. Kanade; S. Gulwani,"Indian Institute of Science, India",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,739,750,"<p> <p>Students have enthusiastically taken to online programming lessons and contests. Unfortunately, they tend to struggle due to lack of personalized feedback. There is an urgent need of program analysis and repair techniques capable of handling both the scale and variations in student submissions, while ensuring quality of feedback. </p><p>Towards this goal, we present a novel methodology called <em>semi-supervised verified feedback generation</em>. We cluster submissions by solution strategy and ask the instructor to identify or add a correct submission in each cluster. We then verify every submission in a cluster against the instructor-validated submission in the same cluster. If faults are detected in the submission then feedback suggesting fixes to them is generated. Clustering reduces the burden on the instructor and also the variations that have to be handled during feedback generation. The verified feedback generation ensures that only correct feedback is generated. </p><p>We implemented a tool, named CoderAssist, based on this approach and evaluated it on dynamic programming assignments. We have designed a novel <em>counter-example guided feedback generation</em> algorithm capable of suggesting fixes to all faults in a submission. In an evaluation on 2226 submissions to 4 problems, CoderAssist could generate <em>verified</em> feedback for 1911 (85%) submissions in 1.6s each on an average. It does a good job of reducing the burden on the instructor. Only one submission had to be manually validated or added for every 16 submissions.</p> </p>",,,10.1145/2950290.2950363,,,Clustering;Feedback generation;MOOCs;Verification,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Parallel data race detection for task parallel programs with locks,A. Yoga; S. Nagarakatte; A. Gupta,"Rutgers University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,833,845,"<p> Programming with tasks is a promising approach to write performance portable parallel code. In this model, the programmer explicitly specifies tasks and the task parallel runtime employs work stealing to distribute tasks among threads. Similar to multithreaded programs, task parallel programs can also exhibit data races. Unfortunately, prior data race detectors for task parallel programs either run the program serially or do not handle locks, and/or detect races only in the schedule observed by the analysis. This paper proposes PTRacer, a parallel on-the-fly data race detector for task parallel programs that use locks. PTRacer detects data races not only in the observed schedule but also those that can happen in other schedules (which are permutations of the memory operations in the observed schedule) for a given input. It accomplishes the above goal by leveraging the dynamic execution graph of a task parallel execution to determine whether two accesses can happen in parallel and by maintaining constant amount of access history metadata with each distinct set of locks held for each shared memory location. To detect data races (beyond the observed schedule) in programs with branches sensitive to scheduling decisions, we propose static compiler instrumentation that records memory accesses that will be executed in the other path with simple branches. PTRacer has performance overheads similar to the state-of-the-art race detector for task parallel programs, SPD3, while detecting more races in programs with locks. </p>",,,10.1145/2950290.2950329,,,Data Races;Fork Join Programs;Intel TBB,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Input-sensitive performance testing,Q. Luo,"College of William and Mary, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1085,1087,"<p> One goal of performance testing is to find specific test input data for exposing performance bottlenecks and identify the methods responsible for these performance bottlenecks. A big and important challenges of performance testing is how to deeply understand the performance behaviors of a nontrivial software system in terms of test input data to properly select the specific test input values for finding the problematic methods. Thus, we propose this research program to automatically analyze performance behaviors in software and link these behaviors with test input data for selecting the specific ones that can expose performance bottlenecks. In addition, this research further examines the corresponding execution traces of selected inputs for targeting the problematic methods. </p>",,,10.1145/2950290.2983953,,,Performance testing;change impact analysis;genetic algorithms;machine learning algorithms,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
On-demand strong update analysis via value-flow refinement,Y. Sui; J. Xue,"UNSW, Australia",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,460,473,"<p> We present a new Strong UPdate Analysis for C programs, called Supa, that enables computing points-to information on-demand via value-flow refinement, in environments with small time and memory budgets such as IDEs. We formulate Supa by solving a graph-reachability problem on a value- flow graph representation of the program, so that strong updates are performed where needed, as long as the total analysis budget is not exhausted. Supa facilitates efficiency and precision tradeoffs by allowing different pointer analyses to be applied in a hybrid multi-stage analysis framework. </p> <p> We have implemented Supa in LLVM with its artifact available at [1]. We evaluate Supa by choosing uninitialized pointer detection as a major client on 12 open-source C programs. As the analysis budget increases, Supa achieves improved precision, with its single-stage flow-sensitive analysis reaching 97% of that achieved by whole-program flow- sensitive analysis by consuming about 0.19 seconds and 36KB of memory per query, on average (with a budget of at most 10000 value-flow edges per query). </p>",,,10.1145/2950290.2950296,,,flow sensitivity;pointer analysis;strong updates;value flow,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Understanding behavioural patterns in JavaScript,S. Alimadadi,"University of British Columbia, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1076,1078,"<p> JavaScript is one of the most popular programming languages. How- ever, understanding the dynamic behaviour of JavaScript apps is challenging in practice. There are many factors that hinder JavaScript comprehension, such as its dynamic, asynchronous, and event- driven nature, the dynamic interplay between JavaScript and the Document Object Model, and the asynchronous communication between client and server. In this research work, we have already proposed methods for understanding event-based and asynchronous JavaScript behaviour. To enhance the scalability of our methods, we propose a new technique that adopts bio-informatics algorithms to extract sequences of actions from execution traces that form higher-level patterns. </p>",,,10.1145/2950290.2983947,,,JavaScript;behavioural patterns;comprehension,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Minimizing GUI event traces,L. Clapp; O. Bastani; S. Anand; A. Aiken,"Stanford University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,422,434,"<p> GUI input generation tools for Android apps, such as Android's Monkey, are useful for automatically producing test inputs, but these tests are generally orders of magnitude larger than necessary, making them difficult for humans to understand. We present a technique for minimizing the output of such tools. Our technique accounts for the non-deterministic behavior of mobile apps, producing small event traces that reach a desired activity with high probability. </p> <p> We propose a variant of delta debugging, augmented to handle non-determinism, to solve the problem of trace minimization. We evaluate our algorithm on two sets of commercial and open-source Android applications, showing that we can minimize large event traces reaching a particular application activity, producing traces that are, on average, less than 2% the size of the original traces. </p>",,,10.1145/2950290.2950342,,,Android;delta debugging;testing;trace minimization,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Guided code synthesis using deep neural networks,C. V. Alexandru,"University of Zurich, Switzerland",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1068,1070,"<p> Can we teach computers how to program? Recent advances in neural network research reveal that certain neural networks are able not only to learn the syntax, grammar and semantics of arbitrary character sequences, but also synthesize new samples `in the style of' the original training data. We explore the adaptation of these techniques to code classification, comprehension and completion. </p>",,,10.1145/2950290.2983951,,,Code Classification;Code Synthesis;Deep Learning,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Gray links in the use of requirements traceability,N. Niu; W. Wang; A. Gupta,"University of Cincinnati, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,384,395,"<p> The value of traceability is in its use. How do different software engineering tasks affect the tracing of the same requirement? In this paper, we answer the question via an empirical study where we explicitly assign the participants into 3 trace-usage groups of one requirement: finding its implementation for verification and validation purpose, changing it within the original software system, and reusing it toward another application. The results uncover what we call ""gray links""--around 20% of the total traces are voted to be true links with respect to only one task but not the others. We provide a mechanism to identify such gray links and discuss how they can be leveraged to advance the research and practice of value-based requirements traceability. </p>",,,10.1145/2950290.2950354,,,Traceability;gray links;requirements change;requirements reuse;software engineering task;using traceability,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Cozy: synthesizing collection data structures,C. Loncaric,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1103,1105,"<p> Many applications require specialized data structures not found in standard libraries. Implementing new data structures by hand is tedious and error-prone. To alleviate this difficulty, we built a tool called Cozy that synthesizes data structures using counter-example guided inductive synthesis. We evaluate Cozy by showing how its synthesized implementations compare to handwritten implementations in terms of correctness and performance across four real-world programs. Cozy's data structures match the performance of the handwritten implementations while avoiding human error. </p>",,,10.1145/2950290.2986032,,,Data structure synthesis,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Extracting instruction semantics via symbolic execution of code generators,N. Hasabnis; R. Sekar,"Intel, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,301,313,"<p> <p>Binary analysis and instrumentation form the basis of many tools and frameworks for software debugging, security hardening, and monitoring. Accurate modeling of instruction semantics is paramount in this regard, as errors can lead to program crashes, or worse, bypassing of security checks. Semantic modeling is a daunting task for modern processors such as x86 and ARM that support over a thousand instructions, many of them with complex semantics. This paper describes a new approach to automate this semantic modeling task. Our approach leverages instruction semantics knowledge that is already encoded into today’¢ s production compilers such as GCC and LLVM. Such an approach can greatly reduce manual effort, and more importantly, avoid errors introduced by manual modeling. Furthermore, it is applicable to any of the numerous architectures already supported by the compiler. In this paper, we develop a new symbolic execution technique to extract instruction semantics from a compiler’¢ s source code. Unlike previous applications of symbolic execution that were focused on identifying a single program path that violates a property, our approach addresses the all paths problem, extracting the entire input/output behavior of the code generator. We have applied it successfully to the 120K lines of C-code used in GCC’¢ s code generator to extract x86 instruction semantics. To demonstrate architecture-neutrality, we have also applied it to AVR, a processor used in the popular Arduino platform.</p> </p>",,,10.1145/2950290.2950335,,,Code generators;Instruction-set semantics extraction;Symbolic execution,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Developing a reusable control-based approach to build self-adaptive software systems with formal guarantees,S. Shevtsov,"Linnaeus University, Sweden",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1060,1062,"<p> An increasingly important concern of software engineers is handling uncertainty at runtime. Over the last decade researchers have applied architecture-based self-adaptation approaches to address this concern. However, providing guarantees required by current software systems has shown to be challenging with these approaches. To tackle this challenge, we study the application of control theory to realize self-adaptation and develop novel control-based adaptation mechanisms that guarantee desired system properties. Results are validated on systems with strict requirements. </p>",,,10.1145/2950290.2983949,,,control theory;guarantees;self-adaptive;software,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
On-the-fly decomposition of specifications in software model checking,S. Apel; D. Beyer; V. Mordan; V. Mutilin; A. Stahlbauer,"University of Passau, Germany",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,349,361,"<p> Major breakthroughs have increased the efficiency and effectiveness of software model checking considerably, such that this technology is now applicable to industrial-scale software. However, verifying the full formal specification of a software system is still considered too complex, and in practice, sets of properties are verified one by one in isolation. We propose an approach that takes the full formal specification as input and first tries to verify all properties simultaneously in one verification run. Our verification algorithm monitors itself and detects situations for which the full set of properties is too complex. In such cases, we perform an automatic decomposition of the full set of properties into smaller sets, and continue the verification seamlessly. To avoid state-space explosion for large sets of properties, we introduce on-the-fly property weaving: properties get weaved into the program's transition system on the fly, during the analysis; which properties to weave and verify is determined dynamically during the verification process. We perform an extensive evaluation based on verification tasks that were derived from 4336 Linux kernel modules, and a set of properties that define the correct usage of the Linux API. Checking several properties simultaneously can lead to a significant performance gain, due to the fact that abstract models share many parts among different properties. </p>",,,10.1145/2950290.2950349,,,Decomposition;Formal Methods;Software Model Checking;Verification,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
A large-scale empirical comparison of static and dynamic test case prioritization techniques,Q. Luo; K. Moran; D. Poshyvanyk,"College of William and Mary, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,559,570,"<p> <p>The large body of existing research in Test Case Prioritization (TCP) techniques, can be broadly classified into two categories: <i>dynamic</i> techniques (that rely on run-time execution information) and <i>static</i> techniques (that operate directly on source and test code). Absent from this current body of work is a comprehensive study aimed at understanding and evaluating the static approaches and comparing them to dynamic approaches on a large set of projects. </p><p>In this work, we perform the first extensive study aimed at empirically evaluating four static TCP techniques comparing them with state-of-research dynamic TCP techniques at different test-case granularities (e.g., method and class-level) in terms of effectiveness, efficiency and similarity of faults detected. This study was performed on 30 real-word Java programs encompassing 431 KLoC. In terms of effectiveness, we find that the static call-graph-based technique outperforms the other static techniques at test-class level, but the topic-model-based technique performs better at test-method level. In terms of efficiency, the static call-graph-based technique is also the most efficient when compared to other static techniques. When examining the similarity of faults detected for the four static techniques compared to the four dynamic ones, we find that on average, the faults uncovered by these two groups of techniques are quite dissimilar, with the top 10% of test cases agreeing on only ’¢ 25% - 30% of detected faults. This prompts further research into the severity/importance of faults uncovered by these techniques, and into the potential for combining static and dynamic information for more effective approaches.</p> </p>",,,10.1145/2950290.2950344,,,Regression testing;dynamic;static;test case prioritization,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
JBSE: a symbolic executor for Java programs with complex heap inputs,P. Braione; G. Denaro; M. Pezz’å,"University of Milano-Bicocca, Italy",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1018,1022,"<p> <p>We present the Java Bytecode Symbolic Executor (<i>JBSE</i>), a symbolic executor for Java programs that operates on complex heap inputs. <i>JBSE</i> implements both the novel Heap EXploration Logic (HEX), a symbolic execution approach to deal with heap inputs, and the main state-of-the-art approaches that handle data structure constraints expressed as either executable programs (<i>repOk</i> methods) or declarative specifications. <i>JBSE</i> is the first symbolic executor specifically designed to deal with programs that operate on complex heap inputs, to experiment with the main state-of-the-art approaches, and to combine different decision procedures to explore possible synergies among approaches for handling symbolic data structures.</p> </p>",,,10.1145/2950290.2983940,,,Alloy;Heap Exploration Logic;Heap data structures;Pointer Assertion Logic;RepOk;Symbolic Execution,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Keep it SIMPLEX: satisfying multiple goals with guarantees in control-based self-adaptive systems,S. Shevtsov; D. Weyns,"Linnaeus University, Sweden",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,229,241,"<p> An increasingly important concern of software engineers is handling uncertainties at design time, such as environment dynamics that may be difficult to predict or requirements that may change during operation. The idea of self-adaptation is to handle such uncertainties at runtime, when the knowledge becomes available. As more systems with strict requirements require self-adaptation, providing guarantees for adaptation has become a high-priority. Providing such guarantees with traditional architecture-based approaches has shown to be challenging. In response, researchers have studied the application of control theory to realize self-adaptation. However, existing control-theoretic approaches applied to adapt software systems have primarily focused on satisfying only a single adaptation goal at a time, which is often too restrictive for real applications. In this paper, we present Simplex Control Adaptation, SimCA, a new approach to self-adaptation that satisfies multiple goals, while being optimal with respect to an additional goal. SimCA offers robustness to measurement inaccuracy and environmental disturbances, and provides guarantees. We evaluate SimCA for two systems with strict requirements that have to deal with uncertainties: an underwater vehicle system used for oceanic surveillance, and a tele-assistance system for health care support. </p>",,,10.1145/2950290.2950301,,,Self-adaptive system;control theory;multiple goals;simplex,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Sustainable software design,M. P. Robillard,"McGill University, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,920,923,"<p> Although design plays a central role in software development, the information produced in this activity is often left to progressively evaporate as the result of software evolution, loss of artifacts, or the fading of related knowledge held by the development team. This paper introduces the concept of sustainability for software design, and calls for its integration into the existing catalog of design quality attributes. Applied to software design, sustainability conveys the idea that a particular set of design decisions and their rationale can be succinctly reflected in the host technology and/or described in documentation in a way that is checkable for conformance with the code and generally resistant to evaporation. The paper discusses the relation between sustainability and existing research areas in software engineering, and highlights future research challenges related to sustainable software design. </p>",,,10.1145/2950290.2983983,,,Software Design;Software Evolution,,,,,,1,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Reasoning with imprecise privacy preferences,I. Omoronyia,"University of Glasgow, UK",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,952,955,"<p> User reluctance and context-dependent factors during information disclosure imply that people cannot always be counted on to indicate their appropriate privacy preference. This phenomenon is the well-known 'privacy paradox', which shows that users of modern technologies are constantly concerned about their privacy, but do not apply these concerns to their usage behaviour accordingly. The problem is that this mismatch between privacy concerns and the indicated privacy preference in software, is not considered when reasoning about the satisfaction of privacy requirements. </p> <p> This paper is a research vision that draws connections between the imprecisions in user privacy preferences, and reasoning about the satisfaction of privacy requirements. We outline the close relationship between privacy and user beliefs and uncertainties. We then propose a multi-agent framework that leverage on this relationship when reasoning about the satisfaction of privacy requirements. We anticipate that this vision will help reduce the gap between an increasingly complex information age and the software techniques needed to protect user privacy. </p>",,,10.1145/2950290.2983982,,,belief;privacy;requirements verification;uncertainty,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
"Model, execute, and deploy: answering the hard questions in end-user programming (showcase)",S. S. Huang,"LogicBlox, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,24,24,"<p> End-user programming, a frequently recurring dream, has thus far eluded large-scale, complex applications. Very real, hard questions stand in the way of its realization. How can its languages and tools support: (1) The development of applications with large data sets and sophisticated computation? (2) The co-development by end-users and professional developers when the complexity of an application demands it? (3) Beyond development, the maintenance, distribution, monitoring, and integration with other applications and services? We discuss our approach to these questions, as implemented in the LogicBlox Modeler. We discuss its use in developing applications for governments, major financial institutions, and large global retailers. We highlight the essential synergies between Programming Languages, Software Engineering, and Database research to achieve self-service at scale, and present open questions to which we look to the FSE community for inspirations and solutions. </p>",,,10.1145/2950290.2994158,,,Declarative programming;End-user programming;Hybrid transactional analytical data processing;Live programming,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
BinGo: cross-architecture cross-OS binary search,M. Chandramohan; Y. Xue; Z. Xu; Y. Liu; C. Y. Cho; H. B. K. Tan,"Nanyang Technological University, Singapore",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,678,689,"<p> Binary code search has received much attention recently due to its impactful applications, e.g., plagiarism detection, malware detection and software vulnerability auditing. However, developing an effective binary code search tool is challenging due to the gigantic syntax and structural differences in binaries resulted from different compilers, architectures and OSs. In this paper, we propose BINGO Š—” a scalable and robust binary search engine supporting various architectures and OSs. The key contribution is a selective inlining technique to capture the complete function semantics by inlining relevant library and user-defined functions. In addition, architecture and OS neutral function filtering is proposed to dramatically reduce the irrelevant target functions. Besides, we introduce length variant partial traces to model binary functions in a program structure agnostic fashion. The experimental results show that BINGO can find semantic similar functions across architecture and OS boundaries, even with the presence of program structure distortion, in a scalable manner. Using BINGO, we also discovered a zero-day vulnerability in Adobe PDF Reader, a COTS binary. </p>",,,10.1145/2950290.2950350,,,Binary Code Searching;Vulnerability Matching,,,,,4,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
API code recommendation using statistical learning from fine-grained changes,A. T. Nguyen; M. Hilton; M. Codoban; H. A. Nguyen; L. Mast; E. Rademacher; T. N. Nguyen; D. Dig,"Iowa State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,511,522,"<p> Learning and remembering how to use APIs is difficult. While code-completion tools can recommend API methods, browsing a long list of API method names and their documentation is tedious. Moreover, users can easily be overwhelmed with too much information. We present a novel API recommendation approach that taps into the predictive power of repetitive code changes to provide relevant API recommendations for developers. Our approach and tool, APIREC, is based on statistical learning from fine-grained code changes and from the context in which those changes were made. Our empirical evaluation shows that APIREC correctly recommends an API call in the first position 59% of the time, and it recommends the correct API call in the top five positions 77% of the time. This is a significant improvement over the state-of-the-art approaches by 30-160% for top-1 accuracy, and 10-30% for top-5 accuracy, respectively. Our result shows that APIREC performs well even with a one-time, minimal training dataset of 50 publicly available projects. </p>",,,10.1145/2950290.2950333,,,API Recommendation;Fine-grained Code Changes;Statistical Learning,,,,,4,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Python probabilistic type inference with natural language support,Z. Xu; X. Zhang; L. Chen; K. Pei; B. Xu,"Nanjing University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,607,618,"<p> We propose a novel type inference technique for Python programs. Type inference is difficult for Python programs due to their heavy dependence on external APIs and the dynamic language features. We observe that Python source code often contains a lot of type hints such as attribute accesses and variable names. However, such type hints are not reliable. We hence propose to use probabilistic inference to allow the beliefs of individual type hints to be propagated, aggregated, and eventually converge on probabilities of variable types. Our results show that our technique substantially outperforms a state-of-the-art Python type inference engine based on abstract interpretation. </p>",,,10.1145/2950290.2950343,,,Dynamic Languages;Probabilistic Inference;Python;Type Inference,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Enforcing correct array indexes with a type system,J. Santino,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1142,1144,"<p> We have built the Index Checker, a type checker that issues warnings about array, list, and string accesses that are potentially unsafe. An example is shown in Figure 1. As with any sound tool, some of its warnings may be false positives. If the Index Checker issues no warning, then the programmer is guaranteed that no array access will cause an IndexOutOfBoundsException at run time (modulo suppressed warnings and unchecked code). The Index Checker ships with knowledge of Java APIs. The developer can optionally write a few type annotations in the program to make the Index Checker more precise. Our system includes five new type qualifiers, defined in Figure 2, that can be applied to integral types such as Java int. These are dependent types that indicate the relationship between the int and given arrays. Figures 3 and 4 show the relationship among these type qualifiers. The type system also contains a type qualifier for arrays, @MinLen, which is a lower bound on its length and permits use of literal integers to access the array or to construct a new array. The Index Checker is built upon the Checker Framework (http://CheckerFramework.org/). </p>",,,10.1145/2950290.2983976,,,Pluggable type checking;array bounds error;verification,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Combining bug detection and test case generation,M. Kellogg,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1124,1126,"<p> Detecting bugs in software is an important software engineering activity. Static bug finding tools can assist in detecting bugs automatically, but they suffer from high false positive rates. Automatic test generation tools can generate test cases which can find bugs, but they suffer from an oracle problem. We present N-Prog, a hybrid of the two approaches. N-Prog iteratively presents the developer an interesting, real input/output pair. The developer either classifies it as a bug (when the output is incorrect) or adds it to the regression test suite (when the output is correct). N-Prog selects input/output pairs whose input produces different output on a mutated version of the program which passes the test suite of the original. In initial experiments, N-Prog detected bugs and rediscovered test cases that had been removed from a test suite. </p>",,,10.1145/2950290.2983970,,,N-Prog;N-variant systems;mutation;mutational robustness,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Designing minimal effective normative systems with the help of lightweight formal methods,J. Hao; E. Kang; J. Sun; D. Jackson,"Tianjin University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,50,60,"<p> Normative systems (i.e., a set of rules) are an important approach to achieving effective coordination among (often an arbitrary number of) agents in multiagent systems. A normative system should be effective in ensuring the satisfaction of a desirable system property, and minimal (i.e., not containing norms that unnecessarily over-constrain the behaviors of agents). Designing or even automatically synthesizing minimal effective normative systems is highly non-trivial. Previous attempts on synthesizing such systems through simulations often fail to generate normative systems which are both minimal and effective. In this work, we propose a framework that facilitates designing of minimal effective normative systems using lightweight formal methods. Given a minimal effective normative system which coordinates many agents must be minimal and effective for a small number of agents, we start with automatically synthesizing one such system with a few agents. We then increase the number of agents so as to check whether the same design remains minimal and effective. If it is, we manually establish an induction proof so as to lift the design to an arbitrary number of agents. </p>",,,10.1145/2950290.2950307,,,Lightweight formal methods;Minimal effective;Norm Synthesis,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
ECHO: instantaneous in situ race detection in the IDE,S. Zhan; J. Huang,"Texas A&#38;#38;M University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,775,786,"<p> We present ECHO, a new technique that detects data races instantaneously in the IDE while developers code. ECHO is the first technique of its kind for incremental race detection supporting both code addition and deletion in the IDE. Unlike conventional static race detectors, ECHO warns developers of potential data races immediately as they are introduced into the program. The core underpinning ECHO is a set of new change-aware static analyses based on a novel static happens-before graph that, given a program change, efficiently compute the change-relevant information without re-analyzing the whole program. Our evaluation within a Java environment on both popular benchmarks and real- world applications shows promising results: for each code addition, or deletion, ECHO can instantly pinpoint all the races in a few milliseconds on average, three to four orders of magnitude faster than a conventional whole-program race detector with the same precision. </p>",,,10.1145/2950290.2950332,,,Change-aware;Data Races;IDE;Instantaneous Detection,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
"Foraging and navigations, fundamentally: developers' predictions of value and cost",D. Piorkowski; A. Z. Henley; T. Nabi; S. D. Fleming; C. Scaffidi; M. Burnett,"Oregon State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,97,108,"<p> Empirical studies have revealed that software developers spend 35%Š—–50% of their time navigating through source code during development activities, yet fundamental questions remain: Are these percentages too high, or simply inherent in the nature of software development? Are there factors that somehow determine a lower bound on how effectively developers can navigate a given information space? Answering questions like these requires a theory that captures the core of developers' navigation decisions. Therefore, we use the central proposition of Information Foraging Theory to investigate developers' ability to predict the value and cost of their navigation decisions. Our results showed that over 50% of developers' navigation choices produced less value than they had predicted and nearly 40% cost more than they had predicted. We used those results to guide a literature analysis, to investigate the extent to which these challenges are met by current research efforts, revealing a new area of inquiry with a rich and crosscutting set of research challenges and open problems. </p>",,,10.1145/2950290.2950302,,,Information foraging theory;navigation value and costs,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Preventing signedness errors in numerical computations in Java,C. A. Mackie,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1148,1150,"<p> We have developed and implemented a type system, the Signedness Type System, that captures usage of signed and unsigned integers in Java programs. This type system enables developers to detect errors regarding unsigned integers at compile time, and guarantees that such errors cannot occur at run time. In a case study. our type system proved easy to use and detected a previously unknown bug. Our type system is implemented as the Signedness Checker and will be available with the Checker Framework (http://CheckerFramework.org/). </p>",,,10.1145/2950290.2983978,,,java;pluggable type checker;type system;unsigned numbers,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Static loop analysis and its applications,X. Xiaofei,"Tianjin University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1130,1132,"<p> Loops are challenging structures in program analysis, and an effective loop analysis is crucial in the applications, such as symbolic execution and program verification. In the research, we will first perform a deep analysis and propose a classification according to the complexity of the loops. Then try to propose techniques for analyzing and summarizing different loops. At last, we apply the techniques in multiple applications. </p>",,,10.1145/2950290.2983972,,,Loop Summarization;Program Verification;Termination,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
RABIEF: range analysis based integer error fixing,X. Cheng,"Tsinghua University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1094,1096,"<p> <p>C language has complicated semantics for integers. Integer errors lead to serious software failures or exploitable vulnerabilities while they are harbored in various real-world programs. It is labor-intensive and error-prone to manually address integer errors. The usability of existing automated techniques is generally poor, as they heavily rely on external specifications or simply transform bugs into crash. We propose RABIEF, a novel and fully automatic approach to fix C integer errors based on range analysis. RABIEF is inspired by the following insights: (1) fixes for various integer errors have typical patterns including sanitization, explicit cast and declared type alteration; (2) range analysis provides sound basis for error detection and guides fix generation. We implemented RABIEF into a tool Argyi. Its effectiveness and efficiency have been substantiated by the facts that: (1) Argyi succeeds in fixing 93.9% of 5414 integer bugs from Juliet test suite, scaling to 600 KLOC within 5500 seconds; (2) Argyi is confirmed to correctly fix 20 errors from 4 real-world programs within only 240 seconds.</p> </p>",,,10.1145/2950290.2983961,,,fixing pattern;integer error;range analysis,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Hotspot symbolic execution of floating-point programs,M. Quan,"National University of Defense Technology, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1112,1114,"<p> This paper presents hotspot symbolic execution (HSE) to scale the symbolic execution of floating-point programs. The essential idea of HSE is to (1) explore the paths of some functions (called hotspot functions) in priority, and (2) divide the paths of a hotspot function into different equivalence classes, and explore as fewer path as possible inside the function while ensuring the coverage of all the classes. We have implemented HSE on KLEE and carried out extensive experiments on all 5528 functions in GNU Scientific Library (GSL). The experimental results demonstrate the effectiveness and efficiency of HSE. Compared with the baseline, HSE detects >12 times of exceptions in 30 minutes. </p>",,,10.1145/2950290.2983966,,,Floating point;Hotspot;Symbolic Execution,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
BigDebug: interactive debugger for big data analytics in Apache Spark,M. A. Gulzar; M. Interlandi; T. Condie; M. Kim,"University of California at Los Angeles, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1033,1037,"<p> To process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. In terms of debugging, DISC systems support post-mortem log analysis but do not provide interactive debugging features in realtime. This tool demonstration paper showcases a set of concrete usecases on how BigDebug can help debug Big Data Applications by providing interactive, realtime debug primitives. To emulate interactive step-wise debugging without reducing throughput, BigDebug provides simulated breakpoints to enable a user to inspect a program without actually pausing the entire computation. To minimize unnecessary communication and data transfer, BigDebug provides on-demand watchpoints that enable a user to retrieve intermediate data using a guard and transfer the selected data on demand. To support systematic and efficient trial-and-error debugging, BigDebug also enables users to change program logic in response to an error at runtime and replay the execution from that step. BigDebug is available for download at http://web.cs.ucla.edu/~miryung/software.html </p>",,,10.1145/2950290.2983930,,,Debugging;big data analytics;data-intensive scalable computing (DISC);fault localization and recovery;interactive tools,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
API deprecation: a retrospective analysis and detection method for code examples on the web,J. Zhou; R. J. Walker,"University of Calgary, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,266,277,"<p> Deprecation allows the developers of application programming interfaces (APIs) to signal to other developers that a given API item ought to be avoided. But little is known about deprecation practices beyond anecdotes. We examine how API deprecation has been used in 26 open source Java frameworks and libraries, finding that the classic deprecateŠ—–replaceŠ—–remove cycle is often not followed, as many APIs were removed without prior deprecation, many deprecated APIs were subsequently un-deprecated, and removed APIs are even resurrected with surprising frequency. Furthermore, we identify several problems in the information commonly (not) provided to help API consumers transition their dependent code. </p> <p> As a consequence of deprecation, coding examples on the web --- an increasingly important source of information for developers --- can easily become outdated. Code examples that demonstrate how to use deprecated APIs can be difficult to disregard and a waste of time for developers. We propose a lightweight, version-sensitive framework to detect deprecated API usages in source code examples on the web so developers can be informed of such usages before they invest time and energy into studying them. We reify the framework as a prototype tool (Deprecation Watcher). Our evaluation on detecting deprecated Android API usages in code examples on Stack Overflow shows our tool obtains a precision of 100% and a recall of 86% in a random sample of 200 questions. </p>",,,10.1145/2950290.2950298,,,API deprecation;Deprecation Watcher;deprecation practices;web-based documentation,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Understanding and generating high quality patches for concurrency bugs,H. Liu; Y. Chen; S. Lu,"University of Chicago, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,715,726,"<p> Concurrency bugs are time-consuming to fix correctly by developers and a severe threat to software reliability. Although many auto-fixing techniques have been proposed recently for concurrency bugs, there is still a big gap between the quality of automatically generated patches and manually designed ones. This paper first conducts an in-depth study of manual patches for 77 real-world concurrency bugs, which provides both assessments for existing techniques and actionable suggestions for future research. Guided by this study, a new tool HFix is designed. It can automatically generate patches, which have matching quality as manual patches, for many concurrency bugs. </p>",,,10.1145/2950290.2950309,,,automated bug fixing;concurrency bugs;empirical study;multi-threaded software,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
A deployable sampling strategy for data race detection,Y. Cai; J. Zhang; L. Cao; J. Liu,"Institute of Software at Chinese Academy of Sciences, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,810,821,"<p> Dynamic data race detection incurs heavy runtime overheads. Recently, many sampling techniques have been proposed to detect data races. However, some sampling techniques (e.g., Pacer) are based on traditional happens-before relation and incur a large basic overhead. Others utilize hardware to reduce their sampling overhead (e.g., DataCollider) and they, however, detect a race only when the race really occurs by delaying program executions. In this paper, we study the limitations of existing techniques and propose a new data race definition, named as Clock Races, for low overhead sampling purpose. The innovation of clock races is that the detection of them does not rely on concrete locks and also avoids heavy basic overhead from tracking happens-before relation. We further propose CRSampler (Clock Race Sampler) to detect clock races via hardware based sampling without directly delaying program executions, to further reduce runtime overhead. We evaluated CRSampler on Dacapo benchmarks. The results show that CRSampler incurred less than 5% overhead on average at 1% sampling rate. Whereas, Pacer and DataCollider incurred larger than 25% and 96% overhead, respectively. Besides, at the same sampling rate, CRSampler detected significantly more data races than that by Pacer and DataCollider. </p>",,,10.1145/2950290.2950310,,,Data race;concurrency bugs;data breakpoints;sampling,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Validate your SPDX files for open source license violations,D. Paschalides; G. M. Kapitsaki,"University of Cyprus, Cyprus",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1047,1051,"<p> Licensing decisions for new Open Source Software are not always straightforward. However, the license that accompanies the software is important as it largely affects its subsequent distribution and reuse. License information for software products is captured - among other data - in the Software Package Data Exchange (SPDX) files. The SPDX specification is gaining popularity in the software industry and has been adopted by many organizations internally. In this demonstration paper, we present our tool for the validation of SPDX files regarding proper license use. Software packages described in SPDX format are examined in order to detect license violations that may occur when a product combines different software sources that carry different and potentially contradicting licenses. The SPDX License Validation Tool (SLVT) gives the opportunity to check the compatibility of one or more SPDX files. The evaluation performed on a number of software packages demonstrates its usefulness for drawing conclusions on license use, revealing violations in some of the test projects. </p>",,,10.1145/2950290.2983939,,,Software Package Data Exchange;licensing;open source software,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
On the utility of dominator mutants for mutation testing,B. Kurtz,"George Mason University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1088,1090,"<p> Mutation testing has been shown to support the generation of test sets that are highly effective at detecting faults. However, practitioner adoption of mutation testing has been minimal in part because of problems that arise from the huge numbers of redundant and equivalent mutants that are generated. The research described here examines the relationship between mutants and attempts to reduce the number of redundant and equivalent mutants in order to make mutation testing more practical for the software tester. </p>",,,10.1145/2950290.2983950,,,Mutation analysis;dominator mutants;subsumption,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Regression testing of web applications using Record/Replay tools,M. Hammoudi,"University of Nebraska-Lincoln, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1079,1081,"<p> Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this man- ner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle; they can easily break as applications change. For this reason, researchers have be- gun to seek approaches for automatically repairing record/replay tests. This research investigates different aspects in relation to test- ing web applications using record/replay tools. The areas that we are interested in include taxonomizing the causes behind breakages and developing automated techniques to repair breakages, creating prevention techniques to stop the occurrence of breakages and de- veloping automated frameworks for root cause analysis. Finally, we intend to evaluate all of these activities via controlled studies involving software engineers and real web application tests. </p>",,,10.1145/2950290.2983942,,,Record/Replay Tools;Regression Testing;Web Applications,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Causal impact analysis for app releases in google play,W. Martin; F. Sarro; M. Harman,"University College London, UK",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,435,446,"<p> App developers would like to understand the impact of their own and their competitorsŠ—È software releases. To address this we introduce Causal Impact Release Analysis for app stores, and our tool, CIRA, that implements this analysis. We mined 38,858 popular Google Play apps, over a period of 12 months. For these apps, we identified 26,339 releases for which there was adequate prior and posterior time series data to facilitate causal impact analysis. We found that 33% of these releases caused a statistically significant change in user ratings. We use our approach to reveal important characteristics that distinguish causal significance in Google Play. To explore the actionability of causal impact analysis, we elicited the opinions of app developers: 56 companies responded, 78% concurred with the causal assessment, of which 33% claimed that their company would consider changing its app release strategy as a result of our findings. </p>",,,10.1145/2950290.2950320,,,App Store Mining and Analysis;Causal Impact,,,,,3,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Generating interactive web pages from storyboards,P. Panchekha,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1071,1072,"<p> Web design is a visual art, but web designs are code. Designers work visually and must then manually translate their designs to code. We propose using synthesis to automatically translate the storyboards designers already produce into CSS stylesheets and JavaScript code. To build a synthesis tool for this complex domain, we will use a novel composition mechanism that allows splitting the synthesis task among domain-specific solvers. </p> <p> We have built a domain-specific solver for CSS stylesheets; solvers for DOM actions and JavaScript code can be built with similar techniques. To compose the three domain-specific solvers, we propose using partial counterexamples to exchange information between different domains. Early results suggest that this composition mechanism is fast and allows specializing each solver to its domain. </p>",,,10.1145/2950290.2983948,,,JavaScript;Synthesis;cascading style sheets;document object model;web design,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Understanding and detecting wake lock misuses for Android applications,Y. Liu; C. Xu; S. C. Cheung; V. Terragni,"Hong Kong University of Science and Technology, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,396,409,"<p> Wake locks are widely used in Android apps to protect critical computations from being disrupted by device sleeping. Inappropriate use of wake locks often seriously impacts user experience. However, little is known on how wake locks are used in real-world Android apps and the impact of their misuses. To bridge the gap, we conducted a large-scale empirical study on 44,736 commercial and 31 open-source Android apps. By automated program analysis and manual investigation, we observed (1) common program points where wake locks are acquired and released, (2) 13 types of critical computational tasks that are often protected by wake locks, and (3) eight patterns of wake lock misuses that commonly cause functional and non-functional issues, only three of which had been studied by existing work. Based on our findings, we designed a static analysis technique, Elite, to detect two most common patterns of wake lock misuses. Our experiments on real-world subjects showed that Elite is effective and can outperform two state-of-the-art techniques. </p>",,,10.1145/2950290.2950297,,,Wake lock;critical computation;lock necessity analysis,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Constraint-based event trace reduction,J. Wang,"University of Chinese Academy of Sciences, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1106,1108,"<p> Various record-replay techniques are developed to facilitate web application debugging. However, it is time-consuming to inspect all recorded events that reveal a failure. To reduce the cost of debugging, delta-debugging and program slicing are used to remove failure-irrelevant events. However, delta-debugging does not scale well for long traces, and program slicing fails to remove irrelevant events that the failure has program dependence on. In this paper, we propose an effective and efficient approach to remove failure-irrelevant events from the event trace. Our approach builds constraints among events and the failure (e.g., a variable can read any of its earlier type-compatible values), to search for a minimal event trace that satisfies these constraints. Our evaluation on 10 real-world web applications shows that our approach can further remove 70% of events in the reduced trace of dynamic slicing, and needs 80% less iterations and 86% less time than delta-debugging. </p>",,,10.1145/2950290.2983964,,,JavaScript;event trace reduction;failure,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Efficient generation of inductive validity cores for safety properties,E. Ghassabani; A. Gacek; M. W. Whalen,"University of Minnesota, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,314,325,"<p> Symbolic model checkers can construct proofs of properties over very complex models. However, the results reported by the tool when a proof succeeds do not generally provide much insight to the user. It is often useful for users to have traceability information related to the proof: which portions of the model were necessary to construct it. This traceability information can be used to diagnose a variety of modeling problems such as overconstrained axioms and underconstrained properties, and can also be used to measure completeness of a set of requirements over a model. In this paper, we present a new algorithm to efficiently compute the em inductive validity core (IVC) within a model necessary for inductive proofs of safety properties for sequential systems. The algorithm is based on the UNSAT core support built into current SMT solvers and a novel encoding of the inductive problem to try to generate a minimal inductive validity core. We prove our algorithm is correct, and describe its implementation in the JKind model checker for Lustre models. We then present an experiment in which we benchmark the algorithm in terms of speed, diversity of produced cores, and minimality, with promising results. </p>",,,10.1145/2950290.2950346,,,IC3/PDR;Requirements Completeness;Traceability;k-Induction,,,,,3,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Automating repetitive code changes using examples,R. Rolim,"Federal University of Campina Grande, Brazil",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1063,1065,"<p> While adding features, fixing bugs, or refactoring the code, developers may perform repetitive code edits. Although Integrated Development Environments (IDEs) automate some transformations such as renaming, many repetitive edits are performed manually, which is error-prone and time-consuming. To help developers to apply these edits, we propose a technique to perform repetitive edits using examples. The technique receives as input the source code before and after the developer edits some target locations of the change and produces as output the top-ranked program transformation that can be applied to edit the remaining target locations in the codebase. The technique uses a state-of-the-art program synthesis methodology and has three main components: a) a DSL for describing program transformations; b) synthesis algorithms to learn program transformations in this DSL; c) ranking algorithms to select the program transformation with the higher probability of performing the desired repetitive edit. In our preliminary evaluation, in a dataset of 59 repetitive edit cases taken from real C# source code repositories, the technique performed, in 83% of the cases, the intended transformation using only 2.8 examples. </p>",,,10.1145/2950290.2983944,,,Program synthesis;Programming by examples;Software evolution,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
On well-separation of GR(1) specifications,S. Maoz; J. O. Ringert,"Tel Aviv University, Israel",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,362,372,"<p> Specifications for reactive synthesis, an automated procedure to obtain a correct-by-construction reactive system, consist of assumptions and guarantees. One way a controller may satisfy the specification is by preventing the environment from satisfying the assumptions, without satisfying the guarantees. Although valid this solution is usually undesired and specifications that allow it are called non-well-separated. </p> <p> In this work we investigate non-well-separation in the context of GR(1), an expressive fragment of LTL that enables efficient synthesis. We distinguish different cases of non-well-separation, and compute strategies showing how the environment can be forced to violate its assumptions. Moreover, we show how to find a core, a minimal set of assumptions that lead to non-well-separation, and further extend our work to support past-time LTL and patterns. </p> <p> We implemented our work and evaluated it on 79 specifications. The evaluation shows that non-well-separation is a common problem in specifications and that our tools can be efficiently applied to identify it and its causes. </p>",,,10.1145/2950290.2950300,,,GR(1);assumptions;reactive synthesis;well-separation,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
ARdoc: app reviews development oriented classifier,S. Panichella; A. Di Sorbo; E. Guzman; C. A. Visaggio; G. Canfora; H. C. Gall,"University of Zurich, Switzerland",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1023,1027,"<p> Google Play, Apple App Store and Windows Phone Store are well known distribution platforms where users can download mobile apps, rate them and write review comments about the apps they are using. Previous research studies demonstrated that these reviews contain important information to help developers improve their apps. However, analyzing reviews is challenging due to the large amount of reviews posted every day, the unstructured nature of reviews and its varying quality. </p> <p> In this demo we present ARdoc, a tool which combines three techniques: (1) Natural Language Parsing, (2) Text Analysis and (3) Sentiment Analysis to automatically classify useful feedback contained in app reviews important for performing software maintenance and evolution tasks. Our quantitative and qualitative analysis (involving mobile professional developers) demonstrates that ARdoc correctly classifies feedback useful for maintenance perspectives in user reviews with high precision (ranging between 84% and 89%), recall (ranging between 84% and 89%), and F-Measure (ranging between 84% and 89%). While evaluating our tool developers of our study confirmed the usefulness of ARdoc in extracting important maintenance tasks for their mobile applications. </p> <p> Demo URL: https://youtu.be/Baf18V6sN8E </p> <p> Demo Web Page: http://www.ifi.uzh.ch/seal/people/panichella/tools/ARdoc.html </p>",,,10.1145/2950290.2983938,,,Mobile Applications;Natural Language Processing;Sentiment Analysis;Text classification;User Reviews,,,,,5,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Automated change impact analysis between SysML models of requirements and design,S. Nejati; M. Sabetzadeh; C. Arora; L. C. Briand; F. Mandoux,"University of Luxembourg, Luxembourg",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,242,253,"<p> <p>An important activity in systems engineering is analyzing how a change in requirements will impact the design of a system. Performing this analysis manually is expensive, particularly for complex systems. In this paper, we propose an approach to automatically identify the impact of requirements changes on system design, when the requirements and design elements are expressed using models. We ground our approach on the Systems Modeling Language (SysML) due to SysML’¢ s increasing use in industrial applications. </p><p>Our approach has two steps: For a given change, we first apply a static slicing algorithm to extract an estimated set of impacted model elements. Next, we rank the elements of the resulting set according to a quantitative measure designed to predict how likely it is for each element to be impacted. The measure is computed using Natural Language Processing (NLP) applied to the textual content of the elements. Engineers can then inspect the ranked list of elements and identify those that are actually impacted. We evaluate our approach on an industrial case study with 16 real-world requirements changes. Our results suggest that, using our approach, engineers need to inspect on average only 4.8% of the entire design in order to identify the actually-impacted elements. We further show that our results consistently improve when our analysis takes into account both structural and behavioral diagrams rather than only structural ones, and the natural-language content of the diagrams in addition to only their structural and behavioral content.</p> </p>",,,10.1145/2950290.2950293,,,Change Impact Analysis;Model Slicing;Natural Language Processing;SysML;Traceability Information Model,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Designing for dystopia: software engineering research for the post-apocalypse,T. Barik; R. Pandita; J. Middleton; E. Murphy-Hill,"North Carolina State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,924,927,"<p> Software engineering researchers have a tendency to be optimistic about the future. Though useful, optimism bias bolsters unrealistic expectations towards desirable outcomes. We argue that explicitly framing software engineering research through pessimistic futures, or dystopias, will mitigate optimism bias and engender more diverse and thought-provoking research directions. We demonstrate through three pop culture dystopias, Battlestar Galactica, Fallout 3, and Children of Men, how reflecting on dystopian scenarios provides research opportunities as well as implications, such as making research accessible to non-experts, that are relevant to our present. </p>",,,10.1145/2950290.2983986,,,culture;design fiction;dystopia;ideation;post-apocalypse;software engineering,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Bing developer assistant: improving developer productivity by recommending sample code,H. Zhang; A. Jain; G. Khandelwal; C. Kaushik; S. Ge; W. Hu,"Microsoft Research, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,956,961,"<p> In programming practice, developers often need sample code in order to learn how to solve a programming-related problem. For example, how to reuse an Application Programming Interface (API) of a large-scale software library and how to implement a certain functionality. We believe that previously written code can help developers understand how others addressed the similar problems and can help them write new programs. We develop a tool called Bing Developer Assistant (BDA), which improves developer productivity by recommending sample code mined from public software repositories (such as GitHub) and web pages (such as Stack Overflow). BDA can automatically mine code snippets that implement an API or answer a code search query. It has been implemented as a free-downloadable extension of Microsoft Visual Studio and has received more than 670K downloads since its initial release in December 2014. BDA is publicly available at: http://aka.ms/devassistant. </p>",,,10.1145/2950290.2983955,,,API;API Usage Extraction;Code Search;GitHub;Software Reuse,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Making invisible things visible: tracking down known vulnerabilities at 3000 companies (showcase),G. Mahmud,"Sonatype, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,25,25,"<p> This year, software development teams around the world are consuming BILLIONS of open source and third-party components. The good news: they are accelerating time to market. The bad news: 1 in 17 components they are using include known security vulnerabilities. In this talk, I will describe what Sonatype, the company behind The Central Repository that supports Apache Maven, has learned from analyzing how thousands of applications use open source components. I will also discuss how organizations like Mayo Clinic, Exxon, Capital One, the U.S. FDA and Intuit are utilizing the principles of software supply chain automation to improve application security and how organizations can balance the need for speed withÎ¾quality and security early in theÎ¾development cycle. </p>",,,10.1145/2950290.2994155,,,Continuous Integration;DevOps;Nexus Firewall;Nexus Repository OSS;OSS Repository Hosting;OSSRH;Open Source Software;Software Component Lifecycle Management;Software Variability Management;Sonatype;The Central Repository,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Relationship-aware code search for JavaScript frameworks,X. Li; Z. Wang; Q. Wang; S. Yan; T. Xie; H. Mei,"Peking University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,690,701,"<p> JavaScript frameworks, such as jQuery, are widely used for developing web applications. To facilitate using these JavaScript frameworks to implement a feature (e.g., functionality), a large number of programmers often search for code snippets that implement the same or similar feature. However, existing code search approaches tend to be ineffective, without taking into account the fact that JavaScript code snippets often implement a feature based on various relationships (e.g., sequencing, condition, and callback relationships) among the invoked framework API methods. To address this issue, we present a novel Relationship-Aware Code Search (RACS) approach for finding code snippets that use JavaScript frameworks to implement a specific feature. In advance, RACS collects a large number of code snippets that use some JavaScript frameworks, mines API usage patterns from the collected code snippets, and represents the mined patterns with method call relationship (MCR) graphs, which capture framework API methodsŠ—È signatures and their relationships. Given a natural language (NL) search query issued by a programmer, RACS conducts NL processing to automatically extract an action relationship (AR) graph, which consists of actions and their relationships inferred from the query. In this way, RACS reduces code search to the problem of graph search: finding similar MCR graphs for a given AR graph. We conduct evaluations against representative real-world jQuery questions posted on Stack Overflow, based on 308,294 code snippets collected from over 81,540 files on the Internet. The evaluation results show the effectiveness of RACS: the top 1 snippet produced by RACS matches the target code snippet for 46% questions, compared to only 4% achieved by a relationship-oblivious approach. </p>",,,10.1145/2950290.2950341,,,Code search;JavaScript code mining;natural language processing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
TIPMerge: recommending experts for integrating changes across branches,C. Costa; J. Figueiredo; L. Murta; A. Sarma,"Federal University of Acre, Brazil / Federal Fluminense University, Brazil",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,523,534,"<p> Parallel development in branches is a common software practice. However, past work has found that integration of changes across branches is not easy, and often leads to failures. Thus far, there has been little work to recommend developers who have the right expertise to perform a branch integration. We propose TIPMerge, a novel tool that recommends developers who are best suited to perform merges, by taking into consideration developersŠ—È past experience in the project, their changes in the branches, and de-pendencies among modified files in the branches. We evaluated TIPMerge on 28 projects, which included up to 15,584 merges with at least two developers, and potentially conflicting changes. On average, 85% of the top-3 recommendations by TIPMerge correctly included the developer who performed the merge. Best (accuracy) results of recommendations were at 98%. Our inter-views with developers of two projects reveal that in cases where the TIPMerge recommendation did not match the actual merge developer, the recommended developer had the expertise to per-form the merge, or was involved in a collaborative merge session. </p>",,,10.1145/2950290.2950339,,,Branch Merge;Expertise Recommendation;Version Control,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Detecting and fixing precision-specific operations for measuring floating-point errors,R. Wang; D. Zou; X. He; Y. Xiong; L. Zhang; G. Huang,"Peking University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,619,630,"<p> The accuracy of the floating-point calculation is critical to many applications and different methods have been proposed around floating-point accuracies, such as detecting the errors in the program, verifying the accuracy of the program, and optimizing the program to produce more accurate results. These approaches need a specification of the program to understand the ideal calculation performed by the program, which is usually approached by interpreting the program in a precision-unspecific way. </p> <p> However, many operations programmed in existing code are inherently precision-specific, which cannot be easily interpreted in a precision-unspecific way. In fact, the semantics used in existing approaches usually fail to interpret precision-specific operations correctly. </p> <p> In this paper, we present a systematic study on precision-specific operations. First, we propose a detection approach to detect precision-specific operations. Second, we propose a fixing approach to enable the tuning of precisions under the presence of precision-specific operations. Third, we studied the precision-specific operations in the GNU C standard math library based on our detection and fixing approaches. Our results show that (1) a significant number of code fragments in the standard C math library are precision-specific operations, and some large inaccuracies reported in existing studies are false positives or potential false positives due to precision-specific operations; (2) our detection approach has high precision and recall; (3) our fixing approach can lead to overall more accurate result. </p>",,,10.1145/2950290.2950355,,,Floating-point accuracy;precision-specific operations,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Proteus: computing disjunctive loop summary via path dependency analysis,X. Xie; B. Chen; Y. Liu; W. Le; X. Li,"Tianjin University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,61,72,"<p> Loops are challenging structures for program analysis, especially when loops contain multiple paths with complex interleaving executions among these paths. In this paper, we first propose a classification of multi-path loops to understand the complexity of the loop execution, which is based on the variable updates on the loop conditions and the execution order of the loop paths. Secondly, we propose a loop analysis framework, named Proteus, which takes a loop program and a set of variables of interest as inputs and summarizes path-sensitive loop effects on the variables. The key contribution is to use a path dependency automaton (PDA) to capture the execution dependency between the paths. A DFS-based algorithm is proposed to traverse the PDA to summarize the effect for all feasible executions in the loop. The experimental results show that Proteus is effective in three applications: Proteus can 1) compute a more precise bound than the existing loop bound analysis techniques; 2) significantly outperform state-of-the-art tools for loop verification; and 3) generate test cases for deep loops within one second, while KLEE and Pex either need much more time or fail. </p>",,,10.1145/2950290.2950340,,,Disjunctive Summary;Loop Summarization,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Detecting table clones and smells in spreadsheets,W. Dou; S. C. Cheung; C. Gao; C. Xu; L. Xu; J. Wei,"Institute of Software at Chinese Academy of Sciences, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,787,798,"<p> Spreadsheets are widely used by end users for various business tasks, such as data analysis and financial reporting. End users may perform similar tasks by cloning a block of cells (table) in their spreadsheets. The corresponding cells in these cloned tables are supposed to keep the same or similar computational semantics. However, when spreadsheets evolve, thus cloned tables can become inconsistent due to ad-hoc modifications, and as a result suffer from smells. In this paper, we propose TableCheck to detect table clones and related smells due to inconsistency among them. We observe that two tables with the same header information at their corresponding cells are likely to be table clones. Inspired by existing fingerprint-based code clone detection techniques, we developed a detection algorithm to detect this kind of table clones. We further detected outliers among corresponding cells as smells in the detected table clones. We implemented our idea into TableCheck, and applied it to real-world spreadsheets from the EUSES corpus. Experimental results show that table clones commonly exist (21.8%), and 25.6% of the spreadsheets with table clones suffer from smells due to inconsistency among these clones. TableCheck detected table clones and their smells with a precision of 92.2% and 85.5%, respectively, while existing techniques detected no more than 35.6% true smells that TableCheck could detect. </p>",,,10.1145/2950290.2950359,,,Spreadsheet;copy and paste;end-user programming;smell detection;table clone,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
How to break an API: cost negotiation and community values in three software ecosystems,C. Bogart; C. K’_stner; J. Herbsleb; F. Thung,"Carnegie Mellon University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,109,120,"<p> Change introduces conflict into software ecosystems: breaking changes may ripple through the ecosystem and trigger rework for users of a package, but often developers can invest additional effort or accept opportunity costs to alleviate or delay downstream costs. We performed a multiple case study of three software ecosystems with different tooling and philosophies toward change, Eclipse, R/CRAN, and Node.js/npm, to understand how developers make decisions about change and change-related costs and what practices, tooling, and policies are used. We found that all three ecosystems differ substantially in their practices and expectations toward change and that those differences can be explained largely by different community values in each ecosystem. Our results illustrate that there is a large design space in how to build an ecosystem, its policies and its supporting infrastructure; and there is value in making community values and accepted tradeoffs explicit and transparent in order to resolve conflicts and negotiate change-related costs. </p>",,,10.1145/2950290.2950325,,,Collaboration;Dependency management;Qualitative research;Software ecosystems;semantic versioning,,,,,5,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Bounded model checking of state-space digital systems: the impact of finite word-length effects on the implementation of fixed-point digital controllers based on state-space modeling,F. R. Monteiro,"Federal University of Amazonas, Brazil",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1151,1153,"<p> The extensive use of digital controllers demands a growing effort to prevent design errors that appear due to finite-word length (FWL) effects. However, there is still a gap, regarding verification tools and methodologies to check implementation aspects of control systems. Thus, the present paper describes an approach, which employs bounded model checking (BMC) techniques, to verify fixed-point digital controllers represented by state-space equations. The experimental results demonstrate the sensitivity of such systems to FWL effects and the effectiveness of the proposed approach to detect them. To the best of my knowledge, this is the first contribution tackling formal verification through BMC of fixed-point state-space digital controllers. </p>",,,10.1145/2950290.2983979,,,Digital Controllers;Formal Verification;Model Checking;Real-time Systems;State-Space,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Fine-grained binary code authorship identification,X. Meng,"University of Wisconsin-Madison, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1097,1099,"<p> Binary code authorship identification is the task of determining the authors of a piece of binary code from a set of known authors. Modern software often contains code from multiple authors. However, existing techniques assume that each program binary is written by a single author. We present a new finer-grained technique to the tougher problem of determining the author of each basic block. Our evaluation shows that our new technique can discriminate the author of a basic block with 52% accuracy among 282 authors, as opposed to 0.4% accuracy by random guess, and it provides a practical solution for identifying multiple authors in software. </p>",,,10.1145/2950290.2983962,,,Basic block level;Code features;Software forensics,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Visualizing code and coverage changes for code review,S. Oosterwaal; A. v. Deursen; R. Coelho; A. A. Sawant; A. Bacchelli,"Delft University of Technology, Netherlands",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1038,1041,"<p> One of the tasks of reviewers is to verify that code modifications are well tested. However, current tools offer little support in understanding precisely how changes to the code relate to changes to the tests. In particular, it is hard to see whether (modified) test code covers the changed code. To mitigate this problem, we developed Operias, a tool that provides a combined visualization of fine-grained source code differences and coverage impact. Operias works both as a stand-alone tool on specific project versions and as a service hooked to GitHub. In the latter case, it provides automated reports for each new pull request, which reviewers can use to assess the code contribution. Operias works for any Java project that works with maven and its standard Cobertura coverage plugin. We present how Operias could be used to identify test-related problems in real-world pull requests. Operias is open source and available on GitHub with a demo video: https://github.com/SERG-Delft/operias </p>",,,10.1145/2950290.2983929,,,code review;software evolution;software testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
When should internal interfaces be promoted to public?,A. Hora; M. T. Valente; R. Robbes; N. Anquetil,"Federal University of Minas Gerais, Brazil / Federal University of Mato Grosso do Sul, Brazil",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,278,289,"<p> Commonly, software systems have public (and stable) interfaces, and internal (and possibly unstable) interfaces. Despite being discouraged, client developers often use internal interfaces, which may cause their systems to fail when they evolve. To overcome this problem, API producers may promote internal interfaces to public. In practice, however, API producers have no assistance to identify public interface candidates. In this paper, we study the transition from internal to public interfaces. We aim to help API producers to deliver a better product and API clients to benefit sooner from public interfaces. Our empirical investigation on five widely adopted Java systems present the following observations. First, we identified 195 promotions from 2,722 internal interfaces. Second, we found that promoted internal interfaces have more clients. Third, we predicted internal interface promotion with precision between 50%-80%, recall 26%-82%, and AUC 74%-85%. Finally, by applying our predictor on the last version of the analyzed systems, we automatically detected 382 public interface candidates. </p>",,,10.1145/2950290.2950306,,,API Usage;Internal Interface Analysis;Software Evolution,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Anti-patterns in search-based program repair,S. H. Tan; H. Yoshida; M. R. Prasad; A. Roychoudhury,"National University of Singapore, Singapore",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,727,738,"<p> Search-based program repair automatically searches for a program fix within a given repair space. This may be accomplished by retrofitting a generic search algorithm for program repair as evidenced by the GenProg tool, or by building a customized search algorithm for program repair as in SPR. Unfortunately, automated program repair approaches may produce patches that may be rejected by programmers, because of which past works have suggested using human-written patches to produce templates to guide program repair. In this work, we take the position that we will not provide templates to guide the repair search because that may unduly restrict the repair space and attempt to overfit the repairs into one of the provided templates. Instead, we suggest the use of a set of anti-patterns --- a set of generic forbidden transformations that can be enforced on top of any search-based repair tool. We show that by enforcing our anti-patterns, we obtain repairs that localize the correct lines or functions, involve less deletion of program functionality, and are mostly obtained more efficiently. Since our set of anti-patterns are generic, we have integrated them into existing search based repair tools, including GenProg and SPR, thereby allowing us to obtain higher quality program patches with minimal effort. </p>",,,10.1145/2950290.2950295,,,Debugging;and repair;fault localization,,,,,2,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Online shared memory dependence reduction via bisectional coordination,Y. Jiang; C. Xu; D. Li; X. Ma; J. Lu,"Nanjing University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,822,832,"<p> Order of shared memory accesses, known as the shared memory dependence, is the cornerstone of dynamic analyses of concurrent programs. In this paper, we study the problem of reducing shared memory dependences. We present the first online software-only algorithm to reduce shared memory dependences without vector clock maintenance, opening a new direction to a broad range of applications (e.g., deterministic replay and data race detection). Our algorithm exploits a simple yet effective observation, that adaptive variable grouping can recognize and match spatial locality in shared memory accesses, to reduce shared memory dependences. We designed and implemented the bisectional coordination protocol, which dynamically maintains a partition of the program's address space without its prior knowledge, such that shared variables in each partitioned interval have consistent thread and spatial locality properties. Evaluation on a set of real-world programs showed that by paying a 0--54.7% (median 21%) slowdown, bisectional coordination reduced 0.95--97% (median 55%) and 16--99.99% (median 99%) shared memory dependences compared with RWTrace and LEAP, respectively. </p>",,,10.1145/2950290.2950326,,,Concurrency;dynamic analysis;shared memory dependence reduction,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
FSX: a tool for fine-grained incremental unit test generation for C/C++ programs,H. Yoshida; S. Tokumoto; M. R. Prasad; I. Ghosh; T. Uehara,"Fujitsu Labs, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1052,1056,"<p> Automated unit test generation bears the promise of significantly reducing test cost and hence improving software quality. However, the maintenance cost of the automatically generated tests presents a significant barrier to adoption of this technology. To address this challenge, in previous work, we proposed a novel technique for automated and fine-grained incremental generation of unit tests through minimal augmentation of an existing test suite. In this paper we describe a tool FSX, implementing this technique. We describe the architecture, user-interface, and salient features of FSX, and specific practical use-cases of its technology. We also report on a real, large-scale deployment of FSX, as a practical validation of the underlying research contribution and of automated test generation research in general. </p>",,,10.1145/2950290.2983937,,,Automatic test generation;symbolic execution;unit testing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Supporting change in product lines within the context of use case-driven development and testing,I. Hajri,"University of Luxembourg, Luxembourg",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1082,1084,"<p> Product Line Engineering (PLE) is becoming a common practice in industry to enhance product quality, to reduce development costs, and to improve time-to-market. At the same time, many development contexts are use case-driven and this strongly influences their requirements engineering and system testing practices. In this PhD project, we aim to achieve automated and effective change management in a product family within the context of use case-driven development and system testing. To this end, we first provide a modeling method for capturing variability information explicitly in Product Line (PL) use case and domain models. Then, we propose an automated configuration approach to automatically generate Product Specific (PS) use case and domain models from PL models. In addition, we plan to provide a change impact analysis approach for PL use case and domain models and automated regression test selection for system test cases derived from PL use case models. </p>",,,10.1145/2950290.2983945,,,Change Impact Analysis;Product Line Engineering;Regression Test Selection;Use Case-Driven Development,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Static DOM event dependency analysis for testing web applications,C. Sung; M. Kusano; N. Sinha; C. Wang,"Virginia Tech, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,447,459,"<p> The number and complexity of JavaScript-based web applications are rapidly increasing, but methods and tools for automatically testing them are lagging behind, primarily due to the difficulty in analyzing the subtle interactions between the applications and the event-driven execution environment. Although static analysis techniques have been routinely used on software written in traditional programming languages, such as Java and C++, adapting them to handle JavaScript code and the HTML DOM is difficult. In this work, we propose the first constraint-based declarative program analysis procedure for computing dependencies over program variables as well as event-handler functions of the various DOM elements, which is crucial for analyzing the behavior of a client-side web application. We implemented the method in a software tool named JSDEP and evaluated it in ARTEMIS, a platform for automated web application testing. Our experiments on a large set of web applications show the new method can significantly reduce the number of redundant test sequences and significantly increase test coverage with minimal overhead. </p>",,,10.1145/2950290.2950292,,,Automated testing;Event dependency;JavaScript;Partial order reduction;Static analysis,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Data structure synthesis,C. Loncaric,"University of Washington, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1073,1075,"<p> All mainstream languages ship with libraries implementing lists, maps, sets, trees, and other common data structures. These libraries are sufficient for some use cases, but other applications need specialized data structures with different operations. For such applications, the standard libraries are not enough. </p> <p> I propose to develop techniques to automatically synthesize data structure implementations from high-level specifications. My initial results on a large class of collection data structures demonstrate that this is possible and lend hope to the prospect of general data structure synthesis. Synthesized implementations can save programmer time and improve correctness while matching the performance of handwritten code. </p>",,,10.1145/2950290.2983946,,,Automatic Programming;Data Structures;Program Synthesis,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
DiagDroid: Android performance diagnosis via anatomizing asynchronous executions,Y. Kang; Y. Zhou; H. Xu; M. R. Lyu,"Chinese University of Hong Kong, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,410,421,"<p> Rapid UI responsiveness is a key consideration to Android app developers. However, the complicated concurrency model of Android makes it hard for developers to understand and further diagnose the UI performance. This paper presents DiagDroid, a tool specifically designed for Android UI performance diagnosis. The key notion of DiagDroid is that UI-triggered asynchronous executions contribute to the UI performance, and hence their performance and their runtime dependency should be properly captured to facilitate performance diagnosis. However, there are tremendous ways to start asynchronous executions, posing a great challenge to profiling such executions and their runtime dependency. To this end, we properly abstract five categories of asynchronous executions as the building basis. As a result, they can be tracked and profiled based on the specifics of each category with a dynamic instrumentation approach carefully tailored for Android. DiagDroid can then accordingly profile the asynchronous executions in a task granularity, equipping it with low-overhead and high compatibility merits. The tool is successfully applied in diagnosing 33 real-world open-source apps, and we find 14 of them contain 27 performance issues. It shows the effectiveness of our tool in Android UI performance diagnosis. The tool is open-source released online. </p>",,,10.1145/2950290.2950316,,,Android;Performance Diagnosis;UI Responsiveness,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Correctness witnesses: exchanging verification results between verifiers,D. Beyer; M. Dangl; D. Dietsch; M. Heizmann,"LMU Munich, Germany",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,326,337,"<p> Standard verification tools provide a counterexample to witness a specification violation, and, since a few years, such a witness can be validated by an independent validator using an exchangeable witness format. This way, information about the violation can be shared across verification tools and the user can use standard tools to visualize and explore witnesses. This technique is not yet established for the correctness case, where a program fulfills a specification. Even for simple programs, it is often difficult for users to comprehend why a given program is correct, and there is no way to independently check the verification result. We close this gap by complementing our earlier work on violation witnesses with correctness witnesses. While we use an extension of the established common exchange format for violation witnesses to represent correctness witnesses, the techniques for producing and validating correctness witnesses are different. The overall goal to make proofs available to engineers is probably as old as programming itself, and proof-carrying code was proposed two decades ago --- our goal is to make it practical: We consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the important principle of separation of concerns. At any time, the invariants from the correctness witness can be used to reconstruct a correctness proof to establish trust. We extended two state-of-the-art verifiers, CPAchecker and Ultimate Automizer, to produce and validate witnesses, and report that the approach is promising on a large set of verification tasks. </p>",,,10.1145/2950290.2950351,,,Correctness Witness;Model Checking;Program Analysis;Software Verification;Witness Validation,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Understanding and improving continuous integration,M. Hilton,"Oregon State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1066,1067,"<p> Continuous Integration (CI) has been widely adopted in the software development industry. However, the usage of CI in practice has been ignored for far too long by the research community. We propose to fill this blind spot by doing in- depth research into CI usage in practice. We will answer how questions by using using quantitative methods, such as investigating open source data that is publicly available. We will answer why questions using qualitative methods, such as semi-structured interviews and large scale surveys. In the course of our research, we plan on identifying barriers that developers face when using CI. We will develop techniques to overcome those barriers via automation. This work is advised by Professor Danny Dig. </p>",,,10.1145/2950290.2983952,,,Continuous Integration;Software Engineering,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Lightweight specification and analysis of dynamic systems with rich configurations,N. Macedo; J. Brunel; D. Chemouil; A. Cunha; D. Kuperberg,"INESC TEC, Portugal / University of Minho, Portugal",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,373,383,"<p> <p>Model-checking is increasingly popular in the early phases of the software development process. To establish the correctness of a software design one must usually verify both <em>structural</em> and <em>behavioral</em> (or temporal) properties. Unfortunately, most specification languages, and accompanying model-checkers, excel only in analyzing either one or the other kind. This limits their ability to verify dynamic systems with rich <em>configurations</em>: systems whose state space is characterized by rich structural properties, but whose evolution is also expected to satisfy certain temporal properties. </p><p>To address this problem, we first propose Electrum, an extension of the Alloy specification language with temporal logic operators, where both rich configurations and expressive temporal properties can easily be defined. Two alternative model-checking techniques are then proposed, one bounded and the other unbounded, to verify systems expressed in this language, namely to verify that every desirable temporal property holds for every possible configuration.</p> </p>",,,10.1145/2950290.2950318,,,Model-checking;formal specification language,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Interactive and guided architectural refactoring with search-based recommendation,Y. Lin; X. Peng; Y. Cai; D. Dig; D. Zheng; W. Zhao,"Fudan University, China",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,535,546,"<p> Architectural refactorings can contain hundreds of steps and experienced developers could carry them out over several weeks. Moreover, developers need to explore a correct sequence of refactorings steps among many more incorrect alternatives. Thus, carrying out architectural refactorings is costly, risky, and challenging. In this paper, we present Refactoring Navigator: a tool-supported and interactive recommendation approach for aiding architectural refactoring. Our approach takes a given implementation as the starting point, a desired high-level design as the target, and iteratively recommends a series of refactoring steps. Moreover, our approach allows the user to accept, reject, or ignore a recommended refactoring step, and uses the user's feedback in further refactoring recommendations. We evaluated the effectiveness of our approach and tool using a controlled experiment and an industrial case study. The controlled experiment shows that the participants who used Refactoring Navigator accomplished their tasks in 77.4% less time and manually edited 98.3% fewer lines than the control group. The industrial case study suggests that Refactoring Navigator has the potential to help with architectural refactorings in practice. </p>",,,10.1145/2950290.2950317,,,automatic refactoring;high-level design;interactive;reflexion model;user feedback,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
"Atlas: an intelligent, performant framework for web-based grid computing",S. Gullapalli,"Yale University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1154,1156,"<p> This paper presents Atlas, a distributed computing system that allows for collaboration over Internet browsers. It allows users to donate the wasted processing power from their Internet-connected machines to help researchers and companies compute difficult tasks. The platform aims at maintaining similar speeds to available cloud computing services while running these tasks, while doing so at a lower cost. In order to do so, Atlas minimizes the amount of time needed per computation and intelligently distributes jobs by utilizing user browsing patterns. Benchmarks demonstrate that Atlas may be a viable alternative to traditional parallel platforms. </p>",,,10.1145/2950290.2983980,,,OpenMP;grid computing;volunteer computing,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Identifying participants for collaborative merge,C. Costa,"Federal Fluminense University, Brazil",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1100,1102,"<p> Software development is typically a collaborative activity. Development in large projects often involves branches, where changes are performed in parallel and merged periodically. While, there is no consensus on who should perform the merge, team members typically try to find someone with enough knowledge about the changes in the branches. This task can be difficult in cases where many different developers have made significant changes. My research proposes an approach, TIPMerge, to help select the most appropriate developers to participate in a collaborative merge session, such that we maximize the knowledge spread across changes. The goal of this research is to select a specified number of developers with the highest joint coverage. We use an optimization algorithm to find which developers form the best team together to deal with a specific merge case. We have implemented all the steps of our approach and evaluate parts of them. </p>",,,10.1145/2950290.2983963,,,Branch Merging;Participants Recommendation;Version Control,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
POLLUX: safely upgrading dependent application libraries,S. Kalra; A. Goel; D. Khanna; M. Dhawan; S. Sharma; R. Purandare,"IIIT Delhi, India",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,290,300,"<p> Software evolution in third-party libraries across version upgrades can result in addition of new functionalities or change in existing APIs. As a result, there is a real danger of impairment of backward compatibility. Application developers, therefore, must keep constant vigil over library enhancements to ensure application consistency, i.e., application retains its semantic behavior across library upgrades. In this paper, we present the design and implementation of POLLUX, a framework to detect application-affecting changes across two versions of the same dependent non-adversarial library binary, and provide feedback on whether the application developer should link to the newer version or not. POLLUX leverages relevant application test cases to drive execution through both versions of the concerned library binary, records all concrete effects on the environment, and compares them to determine semantic similarity across the same API invocation for the two library versions. Our evaluation with 16 popular, open-source library binaries shows that POLLUX is accurate with no false positives and works across compiler optimizations. </p>",,,10.1145/2950290.2950345,,,Dynamic binary analysis;Library upgrade;Software maintenance,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Refactoring and migration of cascading style sheets: towards optimization and improved maintainability,D. Mazinanian,"Concordia University, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,1057,1059,"<p> Cascading Style Sheets is the standard styling language, and is extensively used for defining the presentation of web, mobile and desktop applications. Despite its popularity, the language's design shortcomings have made CSS development and maintenance challenging. This thesis aims at developing techniques for safely transforming CSS code (through refactoring, or migration to a preprocessor language), with the goal of optimization and improved maintainability. </p>",,,10.1145/2950290.2983943,,,Cascading style sheets;duplication;migration;refactoring,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
SMT-based verification of parameterized systems,A. Gurfinkel; S. Shoham; Y. Meshman,"Software Engineering Institute, USA / University of Waterloo, Canada",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,338,348,"<p> It is well known that verification of safety properties of sequential programs is reducible to satisfiability modulo theory of a first-order logic formula, called a verification condition (VC). The reduction is used both in deductive and automated verification, the difference is only in whether the user or the solver provides candidates for inductive invariants. In this paper, we extend the reduction to parameterized systems consisting of arbitrary many copies of a user-specified process, and whose transition relation is definable in first-order logic modulo theory of linear arithmetic and arrays. We show that deciding whether a parameterized system has a universally quantified inductive invariant is reducible to satisfiability of (non-linear) Constraint Horn Clauses (CHC). As a consequence of our reduction, we obtain a new automated procedure for verifying parameterized systems using existing PDR and CHC engines. While the new procedure is applicable to a wide variety of systems, we show that it is a decision procedure for several decidable fragments. </p>",,,10.1145/2950290.2950330,,,invariant inference;model checking;parameterized systems;safety verification,,,,,,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences
Can testedness be effectively measured?,I. Ahmed; R. Gopinath; C. Brindescu; A. Groce; C. Jensen,"Oregon State University, USA",Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,20170413,2016,,,547,558,"<p> Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and deciding when to stop testing. Test the least-tested code, and stop when all code is well-tested, is a reasonable answer. Many measures of ""testedness"" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite quality. The first measure is statement coverage, the simplest and best-known code coverage measure. The second measure is mutation score, a supposedly more powerful, though expensive, measure. </p> <p> We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer future bug-fixes than a ""poorly tested"" element. If not, then it seems likely that we are not effectively measuring testedness. Using a large number of open source Java programs from Github and Apache, we show that both statement coverage and mutation score have only a weak negative correlation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Program elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation. </p>",,,10.1145/2950290.2950324,,,coverage criteria;mutation testing;statistical analysis;test suite evaluation,,,,,1,,,,,,13-18 Nov. 2016,,ACM,ACM Conferences