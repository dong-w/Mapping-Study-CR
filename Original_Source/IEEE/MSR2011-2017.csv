Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication_Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier,,page_num
A qualitative study on performance bugs,S. Zaman; B. Adams; A. E. Hassan,"SAIL, Queen's University, Canada",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,199,208,"Software performance is one of the important qualities that makes software stand out in a competitive market. However, in earlier work we found that performance bugs take more time to fix, need to be fixed by more experienced developers and require changes to more code than non-performance bugs. In order to be able to improve the resolution of performance bugs, a better understanding is needed of the current practice and shortcomings of reporting, reproducing, tracking and fixing performance bugs. This paper qualitatively studies a random sample of 400 performance and non-performance bug reports of Mozilla Firefox and Google Chrome across four dimensions (Impact, Context, Fix and Fix validation). We found that developers and users face problems in reproducing performance bugs and have to spend more time discussing performance bugs than other kinds of bugs. Sometimes performance regressions are tolerated as a tradeoff to improve something else.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224281,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224281,Chromium;Mozilla Firefox;Performance bugs;qualitative study,Browsers;Computer bugs;Context;Fires;Google;Software;Switches,program debugging,Google Chrome;Mozilla Firefox;context;fix validation;impact;non-performance bugs;performance bugs;performance regressions;qualitative study;software performance,,23,,24,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
An empirical investigation of changes in some software properties over time,J. Gil; M. Goldstein; D. Moshkovich,The Technion - Israel Institute of Technology,2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,227,236,"Software metrics are easy to define, but not so easy to justify. It is hard to prove that a metric is valid, i.e., that measured numerical values imply anything on the vaguely defined, yet crucial software properties such as complexity and maintainability. This paper employs statistical analysis and tests to check some plausible assumptions on the behavior of software and metrics measured for this software in retrospective on its versions evolution history. Among those are the reliability assumption implicit in the application of any code metric, and the assumption that the magnitude of change, i.e., increase or decrease of its size, in a software artifact is correlated with changes to its version number. Putting a suite of 36 metrics to the trial, we confirm most of the assumptions on a large repository of software artifacts. Surprisingly, we show that a substantial portion of the reliability of some metrics can be observed even in random changes to architecture. Another surprising result is that Boolean-valued metrics tend to flip their values more often in minor software version increments than in major increments.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224285,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224285,,Java;Measurement;Software reliability;Software systems;Taxonomy,Boolean functions;configuration management;software architecture;software metrics;statistical analysis,Boolean-valued metrics;numerical values;software architecture;software artifact;software metrics;software properties over time;software version increments;statistical analysis;statistical tests;version number;versions evolution history,,1,,16,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
An empirical study of supplementary bug fixes,J. Park; M. Kim; B. Ray; D. H. Bae,Korea Advanced Institute of Science and Technology,2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,40,49,"A recent study finds that errors of omission are harder for programmers to detect than errors of commission. While several change recommendation systems already exist to prevent or reduce omission errors during software development, there have been very few studies on why errors of omission occur in practice and how such errors could be prevented. In order to understand the characteristics of omission errors, this paper investigates a group of bugs that were fixed more than once in open source projects - those bugs whose initial patches were later considered incomplete and to which programmers applied supplementary patches. Our study on Eclipse JDT core, Eclipse SWT, and Mozilla shows that a significant portion of resolved bugs (22% to 33%) involves more than one fix attempt. Our manual inspection shows that the causes of omission errors are diverse, including missed porting changes, incorrect handling of conditional statements, or incomplete refactorings, etc. While many consider that missed updates to code clones often lead to omission errors, only a very small portion of supplementary patches (12% in JDT, 25% in SWT, and 9% in Mozilla) have a content similar to their initial patches. This implies that supplementary change locations cannot be predicted by code clone analysis alone. Furthermore, 14% to 15% of files in supplementary patches are beyond the scope of immediate neighbors of their initial patch locations - they did not overlap with the initial patch locations nor had direct structural dependencies on them (e.g. calls, accesses, subtyping relations, etc.). These results call for new types of omission error prevention approaches that complement existing change recommendation systems.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224298,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224298,bug fixes;empirical study;patches;software evolution,Cloning;Computer bugs;Databases;Dispersion;Entropy;History;Manuals,inspection;program debugging;project management;public domain software;recommender systems;software maintenance,Eclipse JDT core;Eclipse SWT;Mozilla;change recommendation system;code clone analysis;conditional statement handling;manual inspection;omission error detection;omission error prevention approach;open source project;software development;supplementary bug fixes;supplementary patch,,7,2,26,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Analysis of customer satisfaction survey data,P. Rotella; S. Chulani,"Cisco Systems, Inc. - 7200 Kit Creek Road, Research Triangle Park, NC, USA 27709",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,88,97,"Cisco Systems, Inc., conducts a customer satisfaction survey (CSAT) each year to gauge customer sentiment regarding Cisco products, technical support, partner- and Cisco-provided technical services, order fulfillment, and a number of other aspects of the companys business. The results of the analysis of this data are used for several purposes, including ascertaining the viability of new products, determining if customer support objectives are being met, setting engineering in-process and customer experience yearly metrics goals, and assessing, indirectly, the success of engineering initiatives. Analyzing this data, which includes 110,000 yearly sets of survey responses that address over 100 product and services categories, is in many respects complicated. For example, skip logic is an integral part of the survey mechanics, and forming aggregate views of customer sentiment is statistically challenging in this data environment. In this paper, we describe several of the various analysis approaches currently used, pointing out some situations where a high level of precision is not easily achieved, and some situations in which it is possible to easily end up with erroneous results. The analysis and statistical territory covered in this paper is in parts well-known and straightforward, but other parts, which we address, are susceptible to large inaccuracies and errors. We address several of these difficulties and develop reasonable solutions for two known issues, high missing value levels and high colinearity of independent variables.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224304,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224304,CSAT survey (customer satisfaction survey);colinearity;customer satisfaction;data imputation;dominance analysis;listwise deletion;mean substitution;missing values,Analytical models;Couplings;Customer satisfaction;Mathematical model;Measurement;Software,consumer behaviour;customer satisfaction;data analysis;statistical analysis;surveying;technical support services,CSAT survey;Cisco Systems Inc;Cisco products;Cisco-provided technical services;company business;customer experience yearly metrics goals;customer satisfaction survey data analysis;customer sentiment;customer support objectives;engineering initiatives;skip logic;statistical territory;survey mechanics;technical support,,2,,26,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Can we predict types of code changes? An empirical analysis,E. Giger; M. Pinzger; H. C. Gall,University of Zurich,2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,217,226,"There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes (SCC) capture such detailed code changes and their semantics on the statement level. These SCC can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of SCC. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of SCC types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224284,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224284,Machine Learning;Software maintenance;Software quality,Artificial neural networks;Computational modeling;Correlation;Measurement;Object oriented modeling;Predictive models;Semantics,learning (artificial intelligence);neural nets;object-oriented methods;software maintenance;software metrics;software quality,Azureus 3 project;Eclipse platform;SCC;change-prone file;change-proneness;condition change;fine-grained source code change;interface modification;machine learning;neural network model;object-oriented metrics;prediction model;semantics;social network centrality measures;software maintenance;software quality;software system;source file;statement change;statement level;static source code dependency graph,,19,,41,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Discovering complete API rules with mutation testing,A. C. Nguyen; S. C. Khoo,"Department of Computer Science, National University of Singapore, Singapore, Singapore",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,151,160,"Specifications are important for many activities during software construction and maintenance process such as testing, verification, debugging and repairing. Despite their importance, specifications are often missing, informal or incomplete because they are difficult to write manually. Many techniques have been proposed to automatically mine specifications describing method call sequence from execution traces or source code using frequent pattern mining. Unfortunately, a sizeable number of such _ÑÒinteresting_Ñù specifications discovered by frequent pattern mining may not capture the correct use patterns of method calls. Consequently, when used in software testing or verification, these mined specifications lead to many false positive defects, which in turn consume much effort for manual investigation. We present a novel framework for automatically discovering legitimate specifications from execution traces using a mutation testing based approach. Such an approach gives a semantics bearing to the legitimacy of the discovered specifications. We introduce the notion of maximal precision and completeness as the desired forms of discovered specifications, and describe in detail suppression techniques that aid efficient discovery. Preliminary evaluation of this approach on several open source software projects shows that specifications discovered through our approach, compared with those discovered through frequent pattern mining, are much more precise and complete. When used in finding bugs, our specifications also locate defects with significantly fewer false positives and more true positives.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224275,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224275,formal specifications;mutation testing;specification mining,Computer bugs;Data mining;Java;Open source software;Semantics;Testing,application program interfaces;data mining;formal specification;formal verification;legislation;program testing;public domain software;software maintenance;source coding,API rule;automatic mine specification;call sequence;execution trace;false positive defect;frequent pattern mining;legitimate specification;mutation testing;open source software;semantics;software construction;software maintenance;software testing;software verification;source code;suppression technique,,0,,23,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Do faster releases improve software quality? An empirical case study of Mozilla Firefox,F. Khomh; T. Dhaliwal; Y. Zou; B. Adams,"Dept. of Elec. and Comp. Engineering, Queen's University, Kingston, Ontario, Canada",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,179,188,"Nowadays, many software companies are shifting from the traditional 18-month release cycle to shorter release cycles. For example, Google Chrome and Mozilla Firefox release new versions every 6 weeks. These shorter release cycles reduce the users' waiting time for a new release and offer better marketing opportunities to companies, but it is unclear if the quality of the software product improves as well, since shorter release cycles result in shorter testing periods. In this paper, we empirically study the development process of Mozilla Firefox in 2010 and 2011, a period during which the project transitioned to a shorter release cycle. We compare crash rates, median uptime, and the proportion of post-release bugs of the versions that had a shorter release cycle with those having a traditional release cycle, to assess the relation between release cycle length and the software quality observed by the end user. We found that (1) with shorter release cycles, users do not experience significantly more post-release bugs and (2) bugs are fixed faster, yet (3) users experience these bugs earlier during software execution (the program crashes earlier).",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224279,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224279,Software release;bugs;release cycle;software quality;testing,Companies;Computer bugs;Data mining;Fires;Software quality,configuration management;online front-ends;program debugging;software houses;software process improvement;software quality,Mozilla Firefox;post-release bugs;release cycle length;software companies;software execution;software product quality improvement;software versions;user waiting time reduction,,34,,35,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Explaining software defects using topic models,T. H. Chen; S. W. Thomas; M. Nagappan; A. E. Hassan,"Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Canada",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,189,198,"Researchers have proposed various metrics based on measurable aspects of the source code entities (e.g., methods, classes, files, or modules) and the social structure of a software project in an effort to explain the relationships between software development and software defects. However, these metrics largely ignore the actual functionality, i.e., the conceptual concerns, of a software system, which are the main technical concepts that reflect the business logic or domain of the system. For instance, while lines of code may be a good general measure for defects, a large entity responsible for simple I/O tasks is likely to have fewer defects than a small entity responsible for complicated compiler implementation details. In this paper, we study the effect of conceptual concerns on code quality. We use a statistical topic modeling technique to approximate software concerns as topics; we then propose various metrics on these topics to help explain the defect-proneness (i.e., quality) of the entities. Paramount to our proposed metrics is that they take into account the defect history of each topic. Case studies on multiple versions of Mozilla Firefox, Eclipse, and Mylyn show that (i) some topics are much more defect-prone than others, (ii) defect-prone topics tend to remain so over time, and (iii) defect-prone topics provide additional explanatory power for code quality over existing structural and historical metrics.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224280,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224280,code quality;software concerns;topic modeling,Correlation;Fires;History;Java;Measurement;Software systems,software fault tolerance;software metrics;software quality;statistical analysis,Eclipse;I/O tasks;Mozilla Firefox;Mylyn;business logic;code lines;code quality;defect-proneness;historical metrics;software defects;software development;software project;source code entities;statistical topic modeling technique;structural metrics,,10,,38,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
GHTorrent: Github's data from a firehose,G. Gousios; D. Spinellis,"Department of Management Science and Technology, Athens University of Economics and Business, Athens, Greece",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,12,21,"A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224294,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224294,GitHub;commits;dataset;events;repository,Communities;Distributed databases;Electronic mail;Organizations;Peer to peer computing;Protocols,application program interfaces;data acquisition;public domain software;software engineering;storage management,GHTorrent;Github data;REST API;collaboration platform;data acquisition;data curation;mirroring platform;open source software;project hosting platform;project resources;software engineering studies;software repositories;user actions,,21,,24,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Green mining: A methodology of relating software change to power consumption,A. Hindle,"Department of Computing Science, University of Alberta, Edmonton, Canada",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,78,87,"Power consumption is becoming more and more important with the increased popularity of smart-phones, tablets and laptops. The threat of reducing a customer's battery-life now hangs over the software developer who asks, _ÑÒwill this next change be the one that causes my software to drain a customer's battery?_Ñù One solution is to detect power consumption regressions by measuring the power usage of tests, but this is time-consuming and often noisy. An alternative is to rely on software metrics that allow us to estimate the impact that a change might have on power consumption thus relieving the developer from expensive testing. This paper presents a general methodology for investigating the impact of software change on power consumption, we relate power consumption to software changes, and then investigate the impact of static OO software metrics on power consumption. We demonstrated that software change can effect power consumption using the Firefox web-browser and the Azureus/Vuze BitTorrent client. We found evidence of a potential relationship between some software metrics and power consumption. In conclusion, we explored the effect of software change on power consumption on two projects; and we provide an initial investigation on the impact of software metrics on power consumption.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224303,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224303,dynamic analysis;mining software repositories;power;power consumption;software metrics;sustainable-software,Data mining;Fires;Mobile communication;Power demand;Power measurement;Software;Software metrics,mobile computing;power aware computing;power consumption;software metrics,Azureus/Vuze BitTorrent client;Firefox web-browser;green mining;power consumption regressions;software change;software metrics,,25,,19,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Incorporating version histories in Information Retrieval based bug localization,B. Sisman; A. C. Kak,"Purdue University, West Lafayette, IN",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,50,59,"Fast and accurate localization of software defects continues to be a difficult problem since defects can emanate from a large variety of sources and can often be intricate in nature. In this paper, we show how version histories of a software project can be used to estimate a prior probability distribution for defect proneness associated with the files in a given version of the project. Subsequently, these priors are used in an IR (Information Retrieval) framework to determine the posterior probability of a file being the cause of a bug. We first present two models to estimate the priors, one from the defect histories and the other from the modification histories, with both types of histories as stored in the versioning tools. Referring to these as the base models, we then extend them by incorporating a temporal decay into the estimation of the priors. We show that by just including the base models, the mean average precision (MAP) for bug localization improves by as much as 30%. And when we also factor in the time decay in the estimates of the priors, the improvements in MAP can be as large as 80%.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224299,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224299,Bug Localization;Document Priors;Information Retrieval;Software Maintenance,Computational modeling;Computer bugs;Frequency estimation;History;Maximum likelihood estimation;Software,information retrieval;probability;program debugging;software development management,bug localization;information retrieval;mean average precision;probability distribution;software defects;software project;temporal decay;version histories,,18,1,25,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Inferring semantically related words from software context,J. Yang; L. Tan,"University of Waterloo, Waterloo, ON, Canada",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,161,170,"Code search is an integral part of software development and program comprehension. The difficulty of code search lies in the inability to guess the exact words used in the code. Therefore, it is crucial for keyword-based code search to expand queries with semantically related words, e.g., synonyms and abbreviations, to increase the search effectiveness. However, it is limited to rely on resources such as English dictionaries and WordNet to obtain semantically related words in software, because many words that are semantically related in software are not semantically related in English. This paper proposes a simple and general technique to automatically infer semantically related words in software by leveraging the context of words in comments and code. We achieve a reasonable accuracy in seven large and popular code bases written in C and Java. Our further evaluation against the state of art shows that our technique can achieve a higher precision and recall.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224276,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224276,Semantically related words;code search;program comprehension,Context;Dictionaries;Gold;Java;Kernel;Linux,C language;Java;query processing;software engineering;word processing,C language;English dictionaries;Java;WordNet;abbreviations;code search;keyword-based code search;program comprehension;semantically related word inference;software context;software development;synonyms,,15,2,48,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
MIC check: A correlation tactic for ESE data,D. Posnett; P. Devanbu; V. Filkov,"Department of Computer Science, University of California, Davis, Davis, CA",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,22,31,"Empirical software engineering researchers are concerned with understanding the relationships between outcomes of interest, e.g. defects, and process and product measures. The use of correlations to uncover strong relationships is a natural precursor to multivariate modeling. Unfortunately, correlation coefficients can be difficult and/or misleading to interpret. For example, a strong correlation occurs between variables that stand in a polynomial relationship; this may lead one mistakenly, and eventually misleadingly, to model a polynomially related variable in a linear regression. Likewise, a non-monotonic functional, or even non-functional relationship might be entirely missed by a correlation coefficient. Outliers can influence standard correlation measures, tied values can unduly influence even robust non-parametric rank correlation, measures, and smaller sample sizes can cause instability in correlation measures. A new bivariate measure of association, Maximal Information Coefficient (MIC) [1], promises to simultaneously discover if two variables have: a) any association, b) a functional relationship, and c) a nonlinear relationship. The MIC is a very useful complement to standard and rank correlation measures. It separately characterizes the existence of a relationship and its precise nature; thus, it enables more informed choices in modeling non-functional and nonlinear relationships, and a more nuanced indicator of potential problems with the values reported by standard and rank correlation measures. We illustrate the use of MIC using a variety of software engineering metrics. We study and explain the distributional properties of MIC and related measures in software engineering data, and illustrate the value of these measures for the empirical software engineering researcher.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224295,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224295,,Atmospheric measurements;Correlation;Microwave integrated circuits;Size measurement;Software engineering;Software measurement;Standards,software metrics,ESE data;MIC check;bivariate measure;correlation coefficients;correlation measures;correlation tactic;even nonfunctional relationship;functional relationship;linear regression;maximal information coefficient;multivariate modeling;nonlinear relationship;nonmonotonic functional relationship;polynomial relationship;rank correlation measures;software engineering data;software engineering metrics;software engineering researcher,,1,,18,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Mining usage data and development artifacts,O. Baysal; R. Holmes; M. W. Godfrey,"Software Architecture Group (SWAG), David R. Cheriton School of Computer Science, University of Waterloo",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,98,107,"Software repository mining techniques generally focus on analyzing, unifying, and querying different kinds of development artifacts, such as source code, version control meta-data, defect tracking data, and electronic communication. In this work, we demonstrate how adding real-world usage data enables addressing broader questions of how software systems are actually used in practice, and by inference how development characteristics ultimately affect deployment, adoption, and usage. In particular, we explore how usage data that has been extracted from web server logs can be unified with product release history to study questions that concern both users' detailed dynamic behaviour as well as broad adoption trends across different deployment environments. To validate our approach, we performed a study of two open source web browsers: Firefox and Chrome. We found that while Chrome is being adopted at a consistent rate across platforms, Linux users have an order of magnitude higher rate of Firefox adoption. Also, Firefox adoption has been concentrated mainly in North America, while Chrome users appear to be more evenly distributed across the globe. Finally, we detected no evidence in age-specific differences in navigation behaviour among Chrome and Firefox users; however, we hypothesize that younger users are more likely to have more up-to-date versions than more mature users.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224305,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224305,dynamic behaviour;release history;usage mining;user adoption,Browsers;Data mining;Fires;History;Linux;Operating systems;Web servers,data mining;meta data;online front-ends;public domain software;software development management,Chrome;Firefox;Linux users;North America;Web server logs;defect tracking data;development artifacts;electronic communication;open source Web browsers;product release history;software repository mining techniques;software systems;source code;usage data mining;version control meta-data,,8,,19,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
"Think locally, act globally: Improving defect and effort prediction models",N. Bettenburg; M. Nagappan; A. E. Hassan,"Software Analysis and Intelligence Lab (SAIL), Queen's University, School of Computing, Kingston, Ontario K1N 3L6 Canada",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,60,69,"Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224300,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224300,models;software metrics;techniques,Adaptation models;Biological system modeling;Data models;Measurement;Predictive models;Software;Software engineering,data handling;learning (artificial intelligence);regression analysis;software engineering,effort prediction models;fine-grained subsets;machine learning models;prediction model defect;software engineering;splitting datasets;statistical regression models,,28,,30,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Towards improving bug tracking systems with game mechanisms,R. Lotufo; L. Passos; K. Czarnecki,University of Waterloo,2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,2,11,"Low bug report quality and human conflicts pose challenges to keep bug tracking systems productive. This work proposes to address these issues by applying game mechanisms to bug tracking systems. We investigate the use of game mechanisms in Stack Overflow, an online community organized to resolve computer programming related problems, for which the improvements we seek for bug tracking systems also turn out to be relevant. The results of our Stack Overflow investigation show that its game mechanisms could be used to address these issues by motivating contributors to increase contribution frequency and quality, by filtering useful contributions, and by creating an agile and dependable moderation system. We proceed by mapping these mechanisms to open-source bug tracking systems, and find that most benefits are applicable. Additionally, our results motivate tailoring a reward and reputation system and summarizing bug reports as future directions for increasing the benefits of game mechanisms in bug tracking systems.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224293,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224293,,Collaboration;Communities;Ecosystems;Games;Open source software;Programming;Uncertainty,program debugging;public domain software;software quality,bug report quality;computer programming related problem;game mechanism;human conflict;moderation system;online community;open-source bug tracking system;reward and reputation system;stack overflow,,5,,27,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Who? Where? What? Examining distributed development in two large open source projects,C. Bird; N. Nagappan,"Microsoft Research, Redmond, Washington",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,237,246,"To date, a large body of knowledge has been built up around understanding open source software development. However, there is limited research on examining levels of geographic and organizational distribution within open source software projects, despite many studies examining these same aspects in commercial contexts. We set out to fill this gap in OSS knowledge by manually collecting data for two large, mature, successful projects in an effort to assess how distributed they are, both geographically and organizationally. Both Firefox and Eclipse have been the subject of many studies and are ubiquitous in the areas of software development and internet usage respectively. We identified the top contributors that made 95% of the changes over multiple major releases of Firefox and Eclipse and determined their geographic locations and organizational affiliations. We examine the distribution in each project's constituent subsystems and report the relationship of pre- and post-release defects with distribution levels.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224286,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224286,,Companies;Electronic mail;Fires;Open source software;Standards organizations,Internet;organisational aspects;project management;public domain software,ECLIPSE;FIREFOX;Internet;OSS project;distributed development;geographic location distribution;open source software development;organizational distribution;postrelease defect;prerelease defect;project constituent subsystem,,7,,32,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
Why do software packages conflict?,C. Artho; K. Suzaki; R. Di Cosmo; R. Treinen; S. Zacchiroli,"Research Center for Information Security, AIST, Umezono 1-1-1, Tsukuba, Ibaraki 305-8568, Japan",2012 9th IEEE Working Conference on Mining Software Repositories (MSR),20120625,2012,,,141,150,"Determining whether two or more packages cannot be installed together is an important issue in the quality assurance process of package-based distributions. Unfortunately, the sheer number of different configurations to test makes this task particularly challenging, and hundreds of such incompatibilities go undetected by the normal testing and distribution process until they are later reported by a user as bugs that we call _ÑÒconflict defects_Ñù. We performed an extensive case study of conflict defects extracted from the bug tracking systems of Debian and Red Hat. According to our results, conflict defects can be grouped into five main categories. We show that with more detailed package meta-data, about 30 % of all conflict defects could be prevented relatively easily, while another 30 % could be found by targeted testing of packages that share common resources or characteristics. These results allow us to make precise suggestions on how to prevent and detect conflict defects in the future.",2160-1852;21601852,Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3,10.1109/MSR.2012.6224274,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224274,,Computer bugs;Databases;Libraries;Manuals;Software packages;Testing,program debugging;program testing;quality assurance;software packages;software quality,Debian;Red Hat;bug tracking system;conflict defect;distribution process;normal testing;package meta-data;package-based distribution;quality assurance process;software package,,7,2,21,,,,2-3 June 2012,,IEEE,IEEE Conferences,,9
A contextual approach towards more accurate duplicate bug report detection,A. Alipour; A. Hindle; E. Stroulia,"Department of Computing Science, University of Alberta, Edmonton, Canada",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,183,192,"Bug-tracking and issue-tracking systems tend to be populated with bugs, issues, or tickets written by a wide variety of bug reporters, with different levels of training and knowledge about the system being discussed. Many bug reporters lack the skills, vocabulary, knowledge, or time to efficiently search the issue tracker for similar issues. As a result, issue trackers are often full of duplicate issues and bugs, and bug triaging is time consuming and error prone. Many researchers have approached the bug-deduplication problem using off-the-shelf information-retrieval tools, such as BM25F used by Sun et al. In our work, we extend the state of the art by investigating how contextual information, relying on our prior knowledge of software quality, software architecture, and system-development (LDA) topics, can be exploited to improve bug-deduplication. We demonstrate the effectiveness of our contextual bug-deduplication method on the bug repository of the Android ecosystem. Based on this experience, we conclude that researchers should not ignore the context of software engineering when using IR tools for deduplication.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624026,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624026,contextual information;deduplication;duplicate bug reports;information retrieval;machine learning;textual similarity;triaging,Accuracy;Androids;Computer bugs;Context;Humanoid robots;Software;Sun,Linux;information retrieval;program debugging;software architecture;software quality,Android ecosystem;IR tools;LDA topics;bug repository;contextual bug-deduplication method;contextual information;duplicate bug report detection;information-retrieval tools;software architecture;software engineering;software quality;system-development,,20,,26,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
An empirical study of the fault-proneness of clone mutation and clone migration,S. Xie; F. Khomh; Y. Zou,"Department of Electrical and Computer Engineering, Queen's University, Canada",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,149,158,"When implementing new features into a software system, developers may duplicate several lines of code to reuse some existing code segments. This action creates code clones in the software system. The literature has documented different types of code clone (e.g., Type-1, Type-2, and Type-3). Once created, code clones evolve as they are modified during both the development and maintenance phases of the software system. The evolution of code clones across the revisions of a software system is known as a clone genealogy. Existing work has investigated the fault-proneness of Type-1 and Type-2 clone genealogies. In this study, we investigate clone genealogies containing Type-3 clones. We analyze three long-lived software systems Apache-Ant, ArgoUML, and JBoss, which are all written in Java. Using the NiCad clone detection tool, we build clone genealogies and examine two evolutionary phenomena on clones: the mutation of the type of a clone during the evolution of a system, and the migration of clone segments across the repositories of a software system. Results show that 1) mutation and migration occur frequently in software systems; 2) the mutation of a clone group to Type-2 or Type-3 clones increases the risk for faults; 3) increasing the distance between code segments in a clone group also increases the risk for faults.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624022,Types of clones;clone genealogy;clone migration;fault-proneness,Cloning;History;Java;Layout;Maintenance engineering;Software systems,software fault tolerance;software maintenance,APACHE-ANT;ARGOUML;JAVA;JBOSS;NICAD clone detection tool;Type-1 clone genealogy;Type-2 clone genealogy;Type-3 clone genealogy;clone migration;clone mutation;clone segment reuse;evolutionary phenomena;fault-proneness;long-lived software systems;software system development phase;software system maintenance phase;software system repositories,,11,,19,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Asking for (and about) permissions used by Android apps,R. Stevens; J. Ganz; V. Filkov; P. Devanbu; H. Chen,"University of California, Davis Davis, CA, USA",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,31,40,"Security policies, which specify what applications are allowed to do, are notoriously difficult to specify correctly. Many applications were found to request over-liberal permissions. On mobile platforms, this might prevent a cautious user from installing an otherwise harmless application or, even worse, increase the attack surface in vulnerable applications. As a result of such difficulties, programmers frequently ask about them in on-line fora. Our goal is to gain some insight into both the misuse of permissions and the discussions of permissions in on-line fora. We analyze about 10,000 free apps from popular Android markets and found a significant sub-linear relationship between the popularity of a permission and the number of times when it is misused. We also study the relationship of permission use and the number of questions about the permission on StackOverflow. Finally, we study the effect of the influence of a permission (the functionality that it controls) and the interference of a permission (the number of other permissions that influence the same classes) on the occurrence of both permission misuse and permission discussions in StackOverflow.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624000,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624000,,Androids;Documentation;Humanoid robots;Interference;Java;Security;Software,Linux;Web sites;authorisation;mobile computing,Android markets;StackOverflow;free-Android applications;mobile platforms;online fora;over-liberal permission request;permission discussions;permission interference;permission misuse;permission popularity;security policies;sublinear relationship,,12,,22,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Assisting code search with automatic Query Reformulation for bug localization,B. Sisman; A. C. Kak,"Purdue University, West Lafayette, IN",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,309,318,"Source code retrieval plays an important role in many software engineering tasks. However, designing a query that can accurately retrieve the relevant software artifacts can be challenging for developers as it requires a certain level of knowledge and experience regarding the code base. This paper demonstrates how the difficulty of designing a proper query can be alleviated through automatic Query Reformulation (QR) - an under-the-hood operation for reformulating a user's query with no additional input from the user. The proposed QR framework works by enriching a user's search query with certain specific additional terms drawn from the highest-ranked artifacts retrieved in response to the initial query. The important point here is that these additional terms injected into a query are those that are deemed to be _ÑÒclose_Ñù to the original query terms in the source code on the basis of positional proximity. This similarity metric is based on the notion that terms that deal with the same concepts in source code are usually proximal to one another in the same files. We demonstrate the superiority of our QR framework in relation to the QR frameworks well-known in the natural language document retrieval by showing significant improvements in bug localization performance for two large software projects using more than 4,000 queries.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624044,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624044,Bug Localization;Pseudo Relevance Feedback;Query Expansion;Query Reformulation;Software Maintenance,Animation;Context;Measurement;Search engines;Software;Software libraries;Strips,document handling;program debugging;query processing;software metrics,automatic QR;automatic query reformulation;bug localization performance improvement;code base;code search;highest-ranked artifact retrieval;initial query;natural language document retrieval;positional proximity;query design;similarity metric;software artifact retrieval;software engineering tasks;software projects;source code retrieval;under-the-hood operation;user search query,,13,,28,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
"Automatically mining software-based, semantically-similar words from comment-code mappings",M. J. Howard; S. Gupta; L. Pollock; K. Vijay-Shanker,"Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716 USA",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,377,386,"Many software development and maintenance tools involve matching between natural language words in different software artifacts (e.g., traceability) or between queries submitted by a user and software artifacts (e.g., code search). Because different people likely created the queries and various artifacts, the effectiveness of these tools is often improved by expanding queries and adding related words to textual artifact representations. Synonyms are particularly useful to overcome the mismatch in vocabularies, as well as other word relations that indicate semantic similarity. However, experience shows that many words are semantically similar in computer science situations, but not in typical natural language documents. In this paper, we present an automatic technique to mine semantically similar words, particularly in the software context. We leverage the role of leading comments for methods and programmer conventions in writing them. Our evaluation of our mined related comment-code word mappings that do not already occur in WordNet are indeed viewed as computer science, semantically-similar word pairs in high proportions.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624052,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624052,,Computer science;Context;Data mining;Maintenance engineering;Semantics;Software;Tagging,data mining;natural language processing;software maintenance;software tools;text analysis,WordNet;automatic mining software-based semantically-similar word mining;comment-code word mapping;computer science;natural language documents;natural language words;software artifacts;software development tools;software maintenance tools;software traceability;synonyms;textual artifact representation;vocabulary mismatch,,22,1,28,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Better cross company defect prediction,F. Peters; T. Menzies; A. Marcus,"Lane Department of CS & EE, West Virginia University, USA",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,409,418,"How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach? This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects. To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within-company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64% more useful predictors than both within-company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624057,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624057,Cross company;data mining;defect prediction,Companies;Estimation;Predictive models;Radio frequency;Software;Training data;Vegetation,data mining;learning (artificial intelligence);software quality,Burak filter-cross-company approach;Peters filter-cross-company approach;cross-company defect prediction;cross-company learning;local data;quality prediction;state-of-the-art relevancy filter;training data;within-company learning;within-company predictors,,26,,33,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Bug report assignee recommendation using activity profiles,H. Naguib; N. Narayan; B. BrÕ_gge; D. Helal,"Institut f&#x00FC;r Informatik, Technische Universit&#x00E4;t M&#x00FC;nchen, Boltzmannstrasse 3, 87548 Garching, Germany",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,22,30,"One question which frequently arises within the context of artifacts stored in a bug tracking repository is: _ÑÒwho should work on this bug report?_Ñù A number of approaches exist to semi-automatically identify and recommend developers, e.g. using machine learning techniques and social networking analysis. In this work, we propose a new approach for assignee recommendation leveraging user activities in a bug tracking repository. Within the bug tracking repository, an activity profile is created for each user from the history of all his activities (i.e. review, assign, and resolve). This profile, to some extent, indicates the user's role, expertise, and involvement in this project. These activities influence and contribute to the identification and ranking of suitable assignees. In order to evaluate our work, we apply it to bug reports of three different projects. Our results indicate that the proposed approach is able to achieve an average hit ratio of 88%. Comparing this result to the LDA-SVM - based assignee recommendation technique, it was found that the proposed approach performs better.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6623999,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623999,Activity profile;assignee recommendation;bug report;bug tracking,Data mining;Databases;Equations;History;Mathematical model;Open source software,program debugging;recommender systems;software engineering,average hit ratio;bug report assignee recommendation;bug report assignment;bug report resolving;bug report review;bug tracking repository;user activity profiles;user expertise;user involvement;user role,,12,,26,,,,18-19 May 2013,,IEEE,IEEE Conferences,,8
Bug resolution catalysts: Identifying essential non-committers from bug repositories,S. Mani; S. Nagar; D. Mukherjee; R. Narayanam; V. S. Sinha; A. A. Nanavati,"IBM Research - New Delhi, India",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,193,202,"Bugs are inevitable in software projects. Resolving bugs is the primary activity in software maintenance. Developers, who fix bugs through code changes, are naturally important participants in bug resolution. However, there are other participants in these projects who do not perform any code commits. They can be reporters reporting bugs; people having a deep technical know-how of the software and providing valuable insights on how to solve the bug; bug-tossers who re-assign the bugs to the right set of developers. Even though all of them act on the bugs by tossing and commenting, not all of them may be crucial for bug resolution. In this paper, we formally define essential non-committers and try to identify these bug resolution catalysts. We empirically study 98304 bug reports across 11 open source and 5 commercial software projects for validating the existence of such catalysts. We propose a network analysis based approach to construct a Minimal Essential Graph that identifies such people in a project. Finally, we suggest ways of leveraging this information for bug triaging and bug report summarization.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624027,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624027,,Accuracy;Computer bugs;History;Prediction algorithms;Software;Standards;Support vector machines,graph theory;professional aspects;program debugging;project management;public domain software;software maintenance,bug report summarization;bug repositories;bug resolution catalysts;bug triaging;bug-tossers;essential noncommitter identification;minimal essential graph;network analysis based approach;open source;software maintenance;software projects,,2,,16,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Communication in open source software development mailing lists,A. Guzzi; A. Bacchelli; M. Lanza; M. Pinzger; A. van Deursen,"Department of Software and Computer Technology - Delft University of Technology, The Netherlands",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,277,286,"Open source software (OSS) development teams use electronic means, such as emails, instant messaging, or forums, to conduct open and public discussions. Researchers investigated mailing lists considering them as a hub for project communication. Prior work focused on specific aspects of emails, for example the handling of patches, traceability concerns, or social networks. This led to insights pertaining to the investigated aspects, but not to a comprehensive view of what developers communicate about. Our objective is to increase the understanding of development mailing lists communication. We quantitatively and qualitatively analyzed a sample of 506 email threads from the development mailing list of a major OSS project, Lucene. Our investigation reveals that implementation details are discussed only in about 35% of the threads, and that a range of other topics is discussed. Moreover, core developers participate in less than 75% of the threads. We observed that the development mailing list is not the main player in OSS project communication, as it also includes other channels such as the issue repository.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624039,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624039,,Buildings;Data mining;Electronic mail;Linux;Software;Sorting,project management;public domain software;software engineering;software management,Lucene;OSS project communication;development mailing lists communication;email threads;open source software development mailing lists,,29,,31,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
"Discovering, reporting, and fixing performance bugs",A. Nistor; T. Jiang; L. Tan,University of Illinois at Urbana-Champaign,2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,237,246,"Software performance is critical for how users perceive the quality of software products. Performance bugs - programming errors that cause significant performance degradation - lead to poor user experience and low system throughput. Designing effective techniques to address performance bugs requires a deep understanding of how performance bugs are discovered, reported, and fixed. In this paper, we study how performance bugs are discovered, reported to developers, and fixed by developers, and compare the results with those for non-performance bugs. We study performance and non-performance bugs from three popular code bases: Eclipse JDT, Eclipse SWT, and Mozilla. First, we find little evidence that fixing performance bugs has a higher chance to introduce new functional bugs than fixing non-performance bugs, which implies that developers may not need to be over-concerned about fixing performance bugs. Second, although fixing performance bugs is about as error-prone as fixing nonperformance bugs, fixing performance bugs is more difficult than fixing non-performance bugs, indicating that developers need better tool support for fixing performance bugs and testing performance bug patches. Third, unlike many non-performance bugs, a large percentage of performance bugs are discovered through code reasoning, not through users observing the negative effects of the bugs (e.g., performance degradation) or through profiling. The result suggests that techniques to help developers reason about performance, better test oracles, and better profiling techniques are needed for discovering performance bugs.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624035,,Cognition;Computer bugs;Inspection;Manuals;Sociology;Software;Statistics,program debugging;program testing;reasoning about programs;software performance evaluation;software prototyping;software quality,Eclipse JDT;Eclipse SWT;Mozilla;code reasoning;nonperformance bugs;performance bug discovery;performance bug fixing;performance bug patch testing;performance bug reporting;performance bug-programming errors;profiling techniques;software performance;software product quality,,19,,47,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Fixing the _ÑÉOut of sight out of mind_Ñé problem one year of mood-based microblogging in a distributed software team,K. Dullemond; B. van Gameren; M. A. Storey; A. van Deursen,"Delft University of Technology, IHomer, The Netherlands",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,267,276,"Distributed teams face the challenge of staying connected. How do team members stay connected when they no longer see each other on a daily basis? What should be done when there is no coffee corner to share your latest exploits? In this paper we evaluate a microblogging system which makes this possible in a distributed setting. The system, WeHomer, enables the sharing of information and corresponding emotions in a fully distributed organization. We analyzed the content of over a year of usage data by 19 team members in a structured fashion, performed 5 semi-structured interviews and report our findings in this paper. We draw conclusions about the topics shared, the impact on software teams and the impact of distribution and team composition. Main findings include an increase in team-connectedness and easier access to information that is traditionally harder to consistently acquire.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624038,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624038,,Companies;Encoding;Media;Mood;Software;Software engineering;Twitter,Web sites;organisational aspects;software development management;team working,WeHomer;coffee corner;distributed setting;distributed software team;fully distributed organization;information sharing;mood-based microblogging;out-of-sight-out-of-mind problem;semistructured interviews;team composition;team members,,4,,22,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Happy Birthday! A trend analysis on past MSR papers,S. Demeyer; A. Murgia; K. Wyckmans; A. Lamkanfi,"University of Antwerp Antwerp, Belgium",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,353,362,"On the occasion of the 10th anniversary of the MSR conference, it is a worthwhile exercise to meditate on the past, present and future of our research discipline. Indeed, since the MSR community has experienced a big influx of researchers bringing in new ideas, state-of-the art technology and contemporary research methods it is unclear what the future might bring. In this paper, we report on a text mining exercise applied on the complete corpus of MSR papers to reflect on where we come from; where we are now; and where we should be going. We address issues like the trendy (and outdated) research topics; the frequently (and less frequently) cited cases; the popular (and emerging) mining infrastructure; and finally the proclaimed actionable information which we are deemed to uncover.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624049,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624049,,Communities;Computer bugs;Databases;Market research;Software;Text mining,data mining;software engineering;software management;text analysis,MSR conference;MSR papers;mining infrastructure;mining software repositories;research topics;text mining exercise;trend analysis,,3,,16,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Improving bug localization using correlations in crash reports,S. Wang; F. Khomh; Y. Zou,"School of Computing, Queen's University Kingston, ON, Canada",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,247,256,"Nowadays, many software organizations rely on automatic problem reporting tools to collect crash reports directly from users' environments. These crash reports are later grouped together into crash types. Usually, developers prioritize crash types based on the number of crash reports and file bugs for the top crash types. Because a bug can trigger a crash in different usage scenarios, different crash types are sometimes related to a same bug. Two bugs are correlated when the occurrence of one bug causes the other bug to occur. We refer to a group of crash types related to identical or correlated bugs, as a crash correlation group. In this paper, we propose three rules to identify correlated crash types automatically. We also propose an algorithm to locate and rank buggy files using crash correlation groups. Through an empirical study on Firefox and Eclipse, we show that the three rules can identify crash correlation groups with a precision of 100% and a recall of 90% for Firefox and a precision of 79% and a recall of 65% for Eclipse. On the top three buggy file candidates, the proposed bug localization algorithm achieves a recall of 62% and a precision of 42% for Firefox and a recall of 52% and a precision of 50% for Eclipse. On the top 10 buggy file candidates, the recall increases to 92% for Firefox and 90% for Eclipse. Developers can combine the proposed crash correlation rules with the new bug localization algorithm to identify and fix correlated crash types all together.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624036,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624036,Automatic Problem Reporting Tools;Bug Correlation;Bug Localization;Crash Reports;Crashes;Stack Traces,Computer bugs;Correlation;Educational institutions;Organizations;Servers;Software,program debugging,Eclipse;Firefox;automatic problem reporting tools;bug localization algorithm;buggy files;correlated bugs;crash report correlations;crash types;software organizations,,15,,26,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Intensive metrics for the study of the evolution of open source projects: Case studies from Apache Software Foundation projects,S. Gala-PÕ©rez; G. Robles; J. M. GonzÕçlez-Barahona; I. Herraiz,"Apache Software Foundation, Spain",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,159,168,"Based on the empirical evidence that the ratio of email messages in public mailing lists to versioning system commits has remained relatively constant along the history of the Apache Software Foundation (ASF), this paper has as goal to study what can be inferred from such a metric for projects of the ASF. We have found that the metric seems to be an intensive metric as it is independent of the size of the project, its activity, or the number of developers, and remains relatively independent of the technology or functional area of the project. Our analysis provides evidence that the metric is related to the technical effervescence and popularity of project, and as such can be a good candidate to measure its healthy evolution. Other, similar metrics -like the ratio of developer messages to commits and the ratio of issue tracker messages to commits- are studied for several projects as well, in order to see if they have similar characteristics.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624023,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624023,,,electronic mail;mailing systems;project management;software metrics,ASF;Apache Software Foundation projects;developer message-commits ratio;email messages;healthy evolution;intensive metric;issue tracker message-commit ratio;open source project evolution;public mailing lists;technical effervescence;versioning system,,4,,23,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Linux variability anomalies: What causes them and how do they get fixed?,S. Nadi; C. Dietrich; R. Tartler; R. C. Holt; D. Lohmann,"David R. Cheriton School of Computer Science, University of Waterloo, Canada",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,111,120,"The Linux kernel is one of the largest configurable open source software systems implementing static variability. In Linux, variability is scattered over three different artifacts: source code files, Kconfig files, and Makefiles. Previous work detected inconsistencies between these artifacts that led to anomalies in the intended variability of Linux. We call these variability anomalies. However, there has been no work done to analyze how these variability anomalies are introduced in the first place, and how they get fixed. In this work, we provide an analysis of the causes and fixes of variability anomalies in Linux. We first perform an exploratory case study that uses an existing set of patches which solve variability anomalies to identify patterns for their causes. The observations we make from this dataset allow us to develop four research questions which we then answer in a confirmatory case study on the scope of the whole Linux kernel. We show that variability anomalies exist for several releases in the kernel before they get fixed, and that contrary to our initial suspicion, typos in feature names do not commonly cause these anomalies. Our results show that variability anomalies are often introduced through incomplete patches that change Kconfig definitions without properly propagating these changes to the rest of the system. Anomalies are then commonly fixed through changes to the code rather than to Kconfig files.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624017,GIT;Linux;Mining Software Repositories;Software Variability;Variability Anomalies,Data mining;Educational institutions;Feature extraction;History;Kernel;Linux,Linux;program diagnostics;public domain software,KCONFIG files;Linux kernel;Linux variability anomalies;Makefiles;configurable open source software systems;incomplete patches;source code files;static variability,,7,,21,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Mining source code repositories at massive scale using language modeling,M. Allamanis; C. Sutton,"School of Informatics, University of Edinburgh, Edinburgh EH8 9AB, UK",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,207,216,"The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by finding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the first giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is significantly better at the code suggestion task than previous models. More broadly, our approach provides a new _ÑÒlens_Ñù for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program's core logic based solely on general information theoretic criteria.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624029,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624029,,Complexity theory;Entropy;Java;Measurement;Predictive models;Software;Training,Java;data mining;project management;software management;software metrics;source coding;statistical analysis,Java;code module complexity;code suggestion task;data-driven complexity metrics;general information theoretic criteria;giga-token probabilistic language model;module topical centrality;programs core logic;reusable utility classes;software project analysis;source code repositories mining;statistical analysis,,29,,14,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Mining succinct and high-coverage API usage patterns from source code,J. Wang; Y. Dang; H. Zhang; K. Chen; T. Xie; D. Zhang,"Tsinghua Univ., China",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,319,328,"During software development, a developer often needs to discover specific usage patterns of Application Programming Interface (API) methods. However, these usage patterns are often not well documented. To help developers to get such usage patterns, there are approaches proposed to mine client code of the API methods. However, they lack metrics to measure the quality of the mined usage patterns, and the API usage patterns mined by the existing approaches tend to be many and redundant, posing significant barriers for being practical adoption. To address these issues, in this paper, we propose two quality metrics (succinctness and coverage) for mined usage patterns, and further propose a novel approach called Usage Pattern Miner (UP-Miner) that mines succinct and high-coverage usage patterns of API methods from source code. We have evaluated our approach on a large-scale Microsoft codebase. The results show that our approach is effective and outperforms an existing representative approach MAPO. The user studies conducted with Microsoft developers confirm the usefulness of the proposed approach in practice.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624045,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624045,API usage;mining software repositories;sequence mining;software reuse;usage pattern,Clustering algorithms;Context;Data mining;Indexes;Measurement;Probabilistic logic;Redundancy,application program interfaces;data mining;software reusability,MAPO;UP-miner;application programming interface;client code mining;high-coverage API usage pattern mining;large-scale Microsoft codebase;software development;software reuse;source code;succinct API usage pattern mining;usage pattern discovery;usage pattern miner,,24,3,18,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Rendezvous: A search engine for binary code,W. M. Khoo; A. Mycroft; R. Anderson,"University of Cambridge, UK",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,329,338,"The problem of matching between binaries is important for software copyright enforcement as well as for identifying disclosed vulnerabilities in software. We present a search engine prototype called Rendezvous which enables indexing and searching for code in binary form. Rendezvous identifies binary code using a statistical model comprising instruction mnemonics, control flow sub-graphs and data constants which are simple to extract from a disassembly, yet normalising with respect to different compilers and optimisations. Experiments show that Rendezvous achieves F<sub>2</sub> measures of 86.7% and 83.0% on the GNU C library compiled with different compiler optimisations and the GNU coreutils suite compiled with gcc and clang respectively. These two code bases together comprise more than one million lines of code. Rendezvous will bring significant changes to the way patch management and copyright enforcement is currently performed.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624046,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624046,,Accuracy;Binary codes;Indexing;Libraries;Optimization;Search engines,copyright;optimising compilers;search engines;software reusability;statistical analysis,F<sub>2</sub> measures;GNU C library;GNU coreutils suite;binary code search engine;clang;code bases;compiler optimisations;control flow subgraphs;gcc;instruction mnemonics;rendezvous;search engine prototype;software copyright enforcement;statistical model,,13,,30,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Replicating mining studies with SOFAS,G. Ghezzi; H. C. Gall,"s.e.a.l. - software evolution and architecture lab, Department of Informatics, University of Zurich, Switzerland",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,363,372,"The replication of studies in mining software repositories (MSR) is essential to compare different mining techniques or assess their findings across many projects. However, it has been shown that very few of these studies can be easily replicated. Their replication is just as fundamental as the studies themselves and is one of the main threats to validity that they suffer from. In this paper, we show how we can alleviate this problem with our SOFAS framework. SOFAS is a platform that enables a systematic and repeatable analysis of software projects by providing extensible and composable analysis workflows. These workflows can be applied on a multitude of software projects, facilitating the replication and scaling of mining studies. In this paper, we show how and to which degree replication can be achieved. We investigated the mining studies of MSR from 2004 to 2011 and found that from 88 studies published in the MSR proceedings so far, we can fully replicate 25 empirical studies. Additionally, we can replicate 27 additional studies to a large extent. These studies account for 30% and 32%, respectively, of the mining studies published. To support our claim we describe in detail one large study that we replicated and discuss how replication with SOFAS works for the other studies investigated. To discuss the potential of our platform we also characterise how studies can be easily enriched to deliver even more comprehensive answers by extending the analysis workflows provided by the platform.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624050,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624050,,Catalogs;Data mining;History;Ontologies;Standards;Web services,data mining;project management;software architecture,MSR;SOFAS framework;composable analysis workflows;extensible analysis workflows;mining software repositories;mining study replication;repeatable software project analysis;systematic software project analysis,,4,,38,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Revisiting software development effort estimation based on early phase development activities,M. Tsunoda; Y. Kamei; K. Toda; M. Nagappan; K. Fushida; N. Ubayashi,"Toyo University Saitama, Japan",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,429,438,"Many research projects on software estimation use software size as a major explanatory variable. However, practitioners sometimes use the ratio of effort for early phase activities such as planning and requirement analysis, to the effort for the whole development phase of the software in order to estimate effort. In this paper, we focus on effort estimation based on the effort for early phase activities. The goal of the research is to examine the relationship of early phase effort and software size with software development effort. To achieve the goal, we built effort estimation models using early phase effort as an explanatory variable, and compared the estimation accuracies of these models to the effort estimation models based on software size. In addition, we built estimation models using both early phase effort and software size. In our experiment, we used ISBSG dataset, which was collected from software development companies, and regarded planning phase effort and requirement analysis effort as early phase effort. The result of the experiment showed that when both software size and sum of planning and requirement analysis phase effort were used as explanatory variables, the estimation accuracy was most improved (Average Balanced Relative Error was improved to 75.4% from 148.4%). Based on the result, we recommend that both early phase effort and software size be used as explanatory variables, because that combination showed the high accuracy, and did not have multicollinearity issues.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624059,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624059,Effort prediction;early phase effort;estimation accuracy;function point;linear regression analysis,Accuracy;Estimation;Linear regression;Mathematical model;Planning;Productivity;Software,software development management;systems analysis,ISBSG dataset;early phase development activities;early phase effort;planning phase effort;requirement analysis effort;software development companies;software development effort estimation;software estimation;software size,,2,,23,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Search-based duplicate defect detection: An industrial experience,M. Amoui; N. Kaushik; A. Al-Dabbagh; L. Tahvildari; S. Li; W. Liu,"University of Waterloo Waterloo, Canada",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,173,182,"Duplicate defects put extra overheads on software organizations, as the cost and effort of managing duplicate defects are mainly redundant. Due to the use of natural language and various ways to describe a defect, it is usually hard to investigate duplicate defects automatically. This problem is more severe in large software organizations with huge defect repositories and massive number of defect reporters. Ideally, an efficient tool should prevent duplicate reports from reaching developers by automatically detecting and/or filtering duplicates. It also should be able to offer defect triagers a list of top-N similar bug reports and allow them to compare the similarity of incoming bug reports with the suggested duplicates. This demand has motivated us to design and develop a search-based duplicate bug detection framework at BlackBerry. The approach follows a generalized process model to evaluate and tune the performance of the system in a systematic way. We have applied the framework on software projects at BlackBerry, in addition to the Mozilla defect repository. The experimental results exhibit the performance of the developed framework and highlight the high impact of parameter tuning on its performance.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624025,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624025,Duplicate Defect Detection;Information Retrieval;Parameter Tuning;Search-based Software Engineering,Electronic mail;Indexing;Noise;Search problems;Software;Tuning,program debugging;project management,BlackBerry;Mozilla defect repository;bug reports;duplicate detection;duplicate filtering;generalized process model;natural language;search-based duplicate bug detection framework;search-based duplicate defect detection;software organizations;software projects,,3,,24,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Strategies for avoiding text fixture smells during software evolution,M. Greiler; A. Zaidman; A. van Deursen; M. A. Storey,"Delft University of Technology, The Netherlands",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,387,396,"An important challenge in creating automated tests is how to design test fixtures, i.e., the setup code that initializes the system under test before actual automated testing can start. Test designers have to choose between different approaches for the setup, trading off maintenance overhead with slow test execution. Over time, test code quality can erode and test smells can develop, such as the occurrence of overly general fixtures, obscure inline code and dead fields. In this paper, we investigate how fixture-related test smells evolve over time by analyzing several thousand revisions of five open source systems. Our findings indicate that setup management strategies strongly influence the types of test fixture smells that emerge in code, and that several types of fixture smells often emerge at the same time. Based on this information, we recommend important guidelines for setup strategies, and suggest how tool support can be improved to help in both avoiding the emergence of such smells as well as how to refactor code when test smells do appear.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624053,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624053,maintenance;test evolution;test fixture smells,Dispersion;Fixtures;Java;Market research;Software systems;Testing,configuration management;program testing;public domain software;software maintenance,automated testing;automated tests;code refactoring;fixture-related test smells;open source systems;setup code;setup management strategies;software evolution;test code quality;text fixture smell avoidance,,7,,25,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Tag recommendation in software information sites,X. Xia; D. Lo; X. Wang; B. Zhou,"College of Computer Science and Technology, Zhejiang University",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,287,296,"Nowadays, software engineers use a variety of online media to search and become informed of new and interesting technologies, and to learn from and help one another. We refer to these kinds of online media which help software engineers improve their performance in software development, maintenance and test processes as software information sites. It is common to see tags in software information sites and many sites allow users to tag various objects with their own words. Users increasingly use tags to describe the most important features of their posted contents or projects. In this paper, we propose TagCombine, an automatic tag recommendation method which analyzes objects in software information sites. TagCombine has 3 different components: 1. multilabel ranking component which considers tag recommendation as a multi-label learning problem; 2. similarity based ranking component which recommends tags from similar objects; 3. tag-term based ranking component which considers the relationship between different terms and tags, and recommends tags after analyzing the terms in the objects. We evaluate TagCombine on 2 software information sites, StackOverflow and Freecode, which contain 47,668 and 39,231 text documents, respectively, and 437 and 243 tags, respectively. Experiment results show that for StackOverflow, our TagCombine achieves recall@5 and recall@10 scores of 0.5964 and 0.7239, respectively; For Freecode, it achieves recall@5 and recall@10 scores of 0.6391 and 0.7773, respectively. Moreover, averaging over StackOverflow and Freecode results, we improve TagRec proposed by Al-Kofahi et al. by 22.65% and 14.95%, and the tag recommendation method proposed by Zangerle et al. by 18.5% and 7.35% for recall@5 and recall@10 scores.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624040,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624040,Online Media;Software Information Sites;Tag Recommendation;TagCombine,Educational institutions;Media;Prediction algorithms;Search problems;Software;Software algorithms;Vectors,learning (artificial intelligence);program testing;recommender systems;social networking (online);software maintenance;text analysis,Freecode;StackOverflow;TagCombine;TagRec;automatic tag recommendation method;multilabel learning problem;multilabel ranking component;online media;similarity based ranking component;software development;software engineers;software information sites;software maintenance;software test processes;tag-term based ranking component;text documents,,40,,37,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
"Testing principles, current practices, and effects of change localization",S. Raemaekers; G. F. Nane; A. van Deursen; J. Visser,"Software Improvement Group, Amsterdam, The Netherlands",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,257,266,"Best practices in software development state that code that is likely to change should be encapsulated to localize possible modifications. In this paper, we investigate the application and effects of this design principle. We investigate the relationship between the stability, encapsulation and popularity of libraries on a dataset of 148,253 Java libraries. We find that bigger systems with more rework in existing methods have less stable interfaces and that bigger systems tend to encapsulate dependencies better. Additionally, there are a number of factors that are associated with change in library interfaces, such as rework in existing methods, system size, encapsulation of dependencies and the number of dependencies. We find that current encapsulation practices are not targeted at libraries that change the most. We also investigate the strength of ripple effects caused by instability of dependencies and we find that libraries cause ripple effects in systems using them and that these effects can be mitigated by encapsulation.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624037,Software libraries;encapsulation;ripple effects,Correlation;Encapsulation;Java;Libraries;Measurement;Software;Stability analysis,Java;data encapsulation;program testing;software libraries,Java libraries;change localization;design principle;encapsulation practices;less stable interfaces;library interfaces;ripple effects;software development;testing principles,,1,,18,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
The impact of tangled code changes,K. Herzig; A. Zeller,"Microsoft Research, Cambridge, UK",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,121,130,"When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15% of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6% of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624018,Mining software repositories;bias;data quality;noise;tangled code changes,Accuracy;History;Manuals;Noise;Open source software;Partitioning algorithms,Java;configuration management;program debugging;public domain software,bug fixes;bug reports;change organization;multiple tangled changes;multipredictor approach;open-source JAVA projects;source files;version control systems,,46,,31,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
The MSR Cookbook: Mining a decade of research,H. Hemmati; S. Nadi; O. Baysal; O. Kononenko; W. Wang; R. Holmes; M. W. Godfrey,"Software Architecture Group, David R. Cheriton School of Computer Science, University of Waterloo, Canada",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,343,352,"The Mining Software Repositories (MSR) research community has grown significantly since the first MSR workshop was held in 2004. As the community continues to broaden its scope and deepens its expertise, it is worthwhile to reflect on the best practices that our community has developed over the past decade of research. We identify these best practices by surveying past MSR conferences and workshops. To that end, we review all 117 full papers published in the MSR proceedings between 2004 and 2012. We extract 268 comments from these papers, and categorize them using a grounded theory methodology. From this evaluation, four high-level themes were identified: data acquisition and preparation, synthesis, analysis, and sharing/replication. Within each theme we identify several common recommendations, and also examine how these recommendations have evolved over the past decade. In an effort to make this survey a living artifact, we also provide a public forum that contains the extracted recommendations in the hopes that the MSR community can engage in a continuing discussion on our evolving best practices.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624048,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624048,,Best practices;Communities;Context;Data acquisition;Data mining;Electronic mail;Software,data acquisition;data analysis;data mining;recommender systems,MSR cookbook;Mining Software Repositories research community;data acquisition;data analysis;data preparation;data replication;data sharing;data synthesis;grounded theory methodology;recommendation,,8,,66,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Understanding the evolution of Type-3 clones: An exploratory study,R. K. Saha; C. K. Roy; K. A. Schneider; D. E. Perry,"The University of Texas at Austin, USA",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,139,148,"Understanding the evolution of clones is important both for understanding the maintenance implications of clones and building a robust clone management system. To this end, researchers have already conducted a number of studies to analyze the evolution of clones, mostly focusing on Type-1 and Type-2 clones. However, although there are a significant number of Type-3 clones in software systems, we know a little how they actually evolve. In this paper, we perform an exploratory study on the evolution of Type-1, Type-2, and Type-3 clones in six open source software systems written in two different programming languages and compare the result with a previous study to better understand the evolution of Type-3 clones. Our results show that although Type-3 clones are more likely to change inconsistently, the absolute number of consistently changed Type-3 clone classes is higher than that of Type-1 and Type-2. Type-3 clone classes also have a lifespan similar to that of Type-1 and Type-2 clones. In addition, a considerable number of Type-1 and Type-2 clones convert into Type-3 clones during evolution. Therefore, it is important to manage type-3 clones properly to limit their negative impact. However, various automated clone management techniques such as notifying developers about clone changes or linked editing should be chosen carefully due to the inconsistent nature of Type-3 clones.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624021,Type-3 clones;clone evolution;clone genealogy,Cloning;History;Maintenance engineering;Robustness;Software systems;Syntactics,programming languages;public domain software;software maintenance,clone maintenance implications;open source software systems;programming languages;robust clone management system;type-1 clone;type-2 clones;type-3 clone evolution,,4,,28,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Using citation influence to predict software defects,W. Hu; K. Wong,"Department of Computing Science, University of Alberta, Edmonton, Canada",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,419,428,"The software dependency network reflects structure and the developer contribution network reflects process. Previous studies have used social network properties over these networks to predict whether a software component is defect-prone. However, these studies do not consider the strengths of the dependencies in the networks. In our approach, we use a citation influence topic model to determine dependency strengths among components and developers, analyze weak and strong dependencies separately, and apply social network properties to predict defect-prone components. In experiments on Eclipse and NetBeans, our approach has higher accuracy than prior work.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624058,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624058,,Accuracy;Couplings;Predictive models;Social network services;Software;Software measurement,citation analysis;program debugging;program diagnostics;program testing;software fault tolerance,Eclipse;NetBeans;citation influence topic model;defect-prone software component prediction;dependency strengths;developer contribution network;social network properties;software defect prediction;software dependency network,,3,,45,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Which work-item updates need your response?,D. Mukherjee; M. Garg,"IBM-Research-India, New Delhi, India",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,12,21,"Work-item notifications alert the team collaborating on a work-item about any update to the work-item (e.g., addition of comments, change in status). However, as software professionals get involved with multiple tasks in project(s), they are inundated by too many notifications from the work-item tool. Users are upset that they often miss the notifications that solicit their response in the crowd of mostly useless ones. We investigate the severity of this problem by studying the work-item repositories of two large collaborative projects and conducting a user study with one of the project teams. We find that, on an average, only 1 out of every 5 notifications that are received by the users require a response from them. We propose TWINY - a machine learning based approach to predict whether a notification will prompt any action from its recipient. Such a prediction can help to suitably mark up notifications and to decide whether a notification needs to be sent out immediately or be bundled in a message digest. We conduct empirical studies to evaluate the efficacy of different classification techniques in this setting. We find that incremental learning algorithms are ideally suited, and ensemble methods appear to give the best results in terms of prediction accuracy.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6623998,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623998,,Adaptation models;Collaboration;Electronic mail;History;Labeling;Software;Training,learning (artificial intelligence);pattern classification;project management;software development management,TWINY;classification techniques;collaborative projects;ensemble methods;incremental learning algorithms;machine learning;project teams;work-item repositories;work-item update,,0,,22,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Why so complicated? Simple term filtering and weighting for location-based bug report assignment recommendation,R. Shokripour; J. Anvik; Z. M. Kasirun; S. Zamani,"Faculty of Computer Science & Information Technology, University of Malaya, Kuala Lumpur, Malaysia",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,2,11,"Large software development projects receive many bug reports and each of these reports needs to be triaged. An important step in the triage process is the assignment of the report to a developer. Most previous efforts towards improving bug report assignment have focused on using an activity-based approach. We address some of the limitations of activity-based approaches by proposing a two-phased location-based approach where bug report assignment recommendations are based on the predicted location of the bug. The proposed approach utilizes a noun extraction process on several information sources to determine bug location information and a simple term weighting scheme to provide a bug report assignment recommendation. We found that by using a location-based approach, we achieved an accuracy of 89.41% and 59.76% when recommending five developers for the Eclipse and Mozilla projects, respectively.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6623997,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623997,Bug Report Assignment;File Activity Histories;Mining Software Artifacts;Named Entity Recognition;POS Filtering,Accuracy;Computer bugs;Data mining;Indexes;Logic gates;Noise;Software,information filtering;program debugging;project management;software management,Eclipse project;Mozilla project;activity-based approach;bug location information;bug report assignment recommendations;information sources;location-based bug report assignment recommendation;noun extraction process;term filtering;term weighting scheme;two-phased location-based approach,,29,,30,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
Will my patch make it? And how fast? Case study on the Linux kernel,Y. Jiang; B. Adams; D. M. German,"MCIS, Polytechnique Montr&#x00E9;al, Canada",2013 10th Working Conference on Mining Software Repositories (MSR),20131010,2013,,,101,110,"The Linux kernel follows an extremely distributed reviewing and integration process supported by 130 developer mailing lists and a hierarchy of dozens of Git repositories for version control. Since not every patch can make it and of those that do, some patches require a lot more reviewing and integration effort than others, developers, reviewers and integrators need support for estimating which patches are worthwhile to spend effort on and which ones do not stand a chance. This paper crosslinks and analyzes eight years of patch reviews from the kernel mailing lists and committed patches from the Git repository to understand which patches are accepted and how long it takes those patches to get to the end user. We found that 33% of the patches makes it into a Linux release, and that most of them need 3 to 6 months for this. Furthermore, that patches developed by more experienced developers are more easily accepted and faster reviewed and integrated. Additionally, reviewing time is impacted by submission time, the number of affected subsystems by the patch and the number of requested reviewers.",2160-1852;21601852,Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7,10.1109/MSR.2013.6624016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624016,,Electronic mail;Guidelines;Joining processes;Kernel;Linux;Measurement,Linux;configuration management,Linux kernel;Linux release;committed patches;developer mailing lists;extremely distributed reviewing process;git repositories;integration process;kernel mailing lists;patch reviews;requested reviewers;submission time;version control,,24,,23,,,,18-19 May 2013,,IEEE,IEEE Conferences,,9
An empirical study of dormant bugs,T. H. Chen; M. Nagappan; E. Shihab; A. E. Hassan,"Queen&#039;s University, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,82,91,"<p> Over the past decade, several research efforts have studied the quality of software systems by looking at post-release bugs. However, these studies do not account for bugs that remain dormant (i.e., introduced in a version of the software system, but are not found until much later) for years and across many versions. Such dormant bugs skew our under- standing of the software quality. In this paper we study dormant bugs against non-dormant bugs using data from 20 different open-source Apache foundation software systems. We find that 33% of the bugs introduced in a version are not reported till much later (i.e., they are reported in future versions as dormant bugs). Moreover, we find that 18.9% of the reported bugs in a version are not even introduced in that version (i.e., they are dormant bugs from prior versions). In short, the use of reported bugs to judge the quality of a specific version might be misleading. Exploring the fix process for dormant bugs, we find that they are fixed faster (median fix time of 5 days) than non- dormant bugs (median fix time of 8 days), and are fixed by more experienced developers (median commit counts of developers who fix dormant bug is 169% higher). Our results highlight that dormant bugs are different from non-dormant bugs in many perspectives and that future research in software quality should carefully study and consider dormant bugs. </p>",,,10.1145/2597073.2597108,,,Empirical Study;Software Bugs;Software Quality,,,,,11,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
An empirical study of just-in-time defect prediction using cross-project models,T. Fukushima; Y. Kamei; S. McIntosh; K. Yamashita; N. Ubayashi,"Kyushu University, Japan",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,172,181,"<p> Prior research suggests that predicting defect-inducing changes, i.e., Just-In-Time (JIT) defect prediction is a more practical alternative to traditional defect prediction techniques, providing immediate feedback while design decisions are still fresh in the minds of developers. Unfortunately, similar to traditional defect prediction models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this flaw in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from older projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT cross-project models. Through a case study on 11 open source projects, we find that in a JIT cross-project context: (1) high performance within-project models rarely perform well; (2) models trained on projects that have similar correlations between predictor and dependent variables often perform well; and (3) ensemble learning techniques that leverage historical data from several other projects (e.g., voting experts) often perform well. Our findings empirically confirm that JIT cross-project models learned using other projects are a viable solution for projects with little historical data. However, JIT cross-project models perform best when the data used to learn them is carefully selected. </p>",,,10.1145/2597073.2597075,,,Empirical study;software quality,,,,,11,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
An industrial case study of automatically identifying performance regression-causes,T. H. D. Nguyen; M. Nagappan; A. E. Hassan; M. Nasser; P. Flora,"Queen&#039;s University, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,232,241,"<p> Even the addition of a single extra field or control statement in the source code of a large-scale software system can lead to performance regressions. Such regressions can considerably degrade the user experience. Working closely with the members of a performance engineering team, we observe that they face a major challenge in identifying the cause of a performance regression given the large number of performance counters (e.g., memory and CPU usage) that must be analyzed. We propose the mining of a regression-causes repository (where the results of performance tests and causes of past regressions are stored) to assist the performance team in identifying the regression-cause of a newly-identified regression. We evaluate our approach on an open-source system, and a commercial system for which the team is responsible. The results show that our approach can accurately (up to 80% accuracy) identify performance regression-causes using a reasonably small number of historical test runs (sometimes as few as four test runs per regression-cause). </p>",,,10.1145/2597073.2597092,,,Control charts;load testing;performance regression;performance testing,,,,,4,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Do developers feel emotions? an exploratory analysis of emotions in software artifacts,A. Murgia; P. Tourani; B. Adams; M. Ortu,"University of Antwerp, Belgium",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,262,271,"<p> Software development is a collaborative activity in which developers interact to create and maintain a complex software system. Human collaboration inevitably evokes emotions like joy or sadness, which can affect the collaboration either positively or negatively, yet not much is known about the individual emotions and their role for software development stakeholders. In this study, we analyze whether development artifacts like issue reports carry any emotional information about software development. This is a first step towards verifying the feasibility of an automatic tool for emotion mining in software development artifacts: if humans cannot determine any emotion from a software artifact, neither can a tool. Analysis of the Apache Software Foundation issue tracking system shows that developers do express emotions (in particular gratitude, joy and sadness). However, the more context is provided about an issue report, the more human raters start to doubt and nuance their interpretation of emotions. More investigation is needed before building a fully automatic emotion mining tool. </p>",,,10.1145/2597073.2597086,,,Emotion Mining;Empirical Software Engineer- ing;Issue Report,,,,,21,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Estimating development effort in Free/Open source software projects by mining software repositories: a case study of OpenStack,G. Robles; J. M. GonzÕçlez-Barahona; C. CervigÕ_n; A. Capiluppi; D. Izquierdo-CortÕçzar,"Universidad Rey Juan Carlos, Spain",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,222,231,"<p> Because of the distributed and collaborative nature of free / open source software (FOSS) projects, the development effort invested in a project is usually unknown, even after the software has been released. However, this information is becoming of major interest, especially ---but not only--- because of the growth in the number of companies for which FOSS has become relevant for their business strategy. In this paper we present a novel approach to estimate effort by considering data from source code management repositories. We apply our model to the OpenStack project, a FOSS project with more than 1,000 authors, in which several tens of companies cooperate. Based on data from its repositories and together with the input from a survey answered by more than 100 developers, we show that the model offers a simple, but sound way of obtaining software development estimations with bounded margins of error. </p>",,,10.1145/2597073.2597107,,,Effort estimation;free software;mining software repositories;open source,,,,,9,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Finding patterns in static analysis alerts: improving actionable alert ranking,Q. Hanam; L. Tan; R. Holmes; P. Lam,"University of Waterloo, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,152,161,"<p> Static analysis (SA) tools that find bugs by inferring programmer beliefs (e.g., FindBugs) are commonplace in today's software industry. While they find a large number of actual defects, they are often plagued by high rates of alerts that a developer would not act on (unactionable alerts) because they are incorrect, do not significantly affect program execution, etc. High rates of unactionable alerts decrease the utility of static analysis tools in practice. </p> <p> We present a method for differentiating actionable and unactionable alerts by finding alerts with similar code patterns. To do so, we create a feature vector based on code characteristics at the site of each SA alert. With these feature vectors, we use machine learning techniques to build an actionable alert prediction model that is able to classify new SA alerts. </p> <p> We evaluate our technique on three subject programs using the FindBugs static analysis tool and the Faultbench benchmark methodology. For a developer inspecting the top 5% of all alerts for three sample projects, our approach is able to identify 57 of 211 actionable alerts, which is 38 more than the FindBugs priority measure. Combined with previous actionable alert identification techniques, our method finds 75 actionable alerts in the top 5%, which is four more actionable alerts (a 6% improvement) than previous actionable alert identification techniques. </p>",,,10.1145/2597073.2597100,,,Static analysis;alert classification;alert patterns;bug detection;machine learning,,,,,4,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
GreenMiner: a hardware based mining software repositories software energy consumption framework,,,Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,12,21,"<p> Green Mining is a field of MSR that studies software energy consumption and relies on software performance data. Unfortunately there is a severe lack of publicly available software power use performance data. This means that green mining researchers must generate this data themselves by writing tests, building multiple revisions of a product, and then running these tests multiple times (10+) for each software revision while measuring power use. Then, they must aggregate these measurements to estimate the energy consumed by the tests for each software revision. This is time consuming and is made more difficult by the constraints of mobile devices and their OSes. In this paper we propose, implement, and demonstrate Green Miner: the first dedicated hardware mining software repositories testbed. The Green Miner physically measures the energy consumption of mobile devices (Android phones) and automates the testing of applications, and the reporting of measurements back to developers and researchers. The Green Miner has already produced valuable results for commercial Android application developers, and has been shown to replicate other power studies' results. </p>",,,10.1145/2597073.2597097,,,Android;Software Change;Software Energy Consumption,,,,,20,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
How does a typical tutorial for mobile development look like?,R. Tiarks; W. Maalej,"University of Hamburg, Germany",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,272,281,"<p> We report on an exploratory study, which aims at understanding how development tutorials are structured, what types of tutorials exist, and how official tutorials differ from tutorials written by development communities. We analyzed over 1.200 tutorials for mobile application development provided by six different sources for the three major platforms: Android, Apple iOS, and Windows Phone. We found that a typical tutorial contains around 2700 words distributed over 4 pages and including a list of instructions with 18 items. Overall, 70% of the tutorials contain source code examples and a similar fraction contain images. On average, one tutorial has 6 images. When analyzing the images, we found that the studied iOS community posted the largest number of images, 14 images per tutorial, on average, from which 74% are plain images, i.e., mainly screenshots without stencils, diagrams, or highlights. In contrast, 36% of the images included in the official tutorials by Apple were diagrams or images with stencils. Community sites seem to follow a similar structure to the official sites but include items and images which are rather underrepresented in the official tutorials. From the analysis of the tutorials content by means of natural language processing combined with manual content analysis, we derived four categories for mobile development tutorials: infrastructure and design, application and services, distribution and maintenance, and development platform. Our categorization can help tutorial writers to better organize and evaluate the content of their tutorials and identify missing tutorials. </p>",,,10.1145/2597073.2597106,,,Data mining;Knowledge sharing;Software documentation,,,,,1,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Impact analysis of change requests on source code based on interaction and commit histories,M. B. Zanjani; G. Swartzendruber; H. Kagdi,"Wichita State University, USA",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,162,171,"<p> The paper presents an approach to perform impact analysis (IA) of an incoming change request on source code. The approach is based on a combination of interaction (e.g., Mylyn) and commit (e.g., CVS) histories. The source code entities (i.e., files and methods) that were interacted or changed in the resolution of past change requests (e.g., bug fixes) were used. Information retrieval, machine learning, and lightweight source code analysis techniques were employed to form a corpus from these source code entities. Additionally, the corpus was augmented with the textual descriptions of the previously resolved change requests and their associated commit messages. Given a textual description of a change request, this corpus is queried to obtain a ranked list of relevant source code entities that are most likely change prone. Such an approach that combines information from interactions and commits for IA at the change request level was not previously investigated. Furthermore, the approach requires only the entities that were interacted and/or committed in the past, which differs from the previous solutions that require indexing of a complete snapshot (e.g., a release). </p> <p> An empirical study on 3272 interactions and 5093 commits from Mylyn, an open source task management tool, was conducted. The results show that the combined approach outperforms an individual approach based on commits. Moreover, it also outperformed an approach based on indexing a single, complete snapshot of a software system. </p>",,,10.1145/2597073.2597096,,,Impact Analysis;Information Retrieval;Interaction History;Machine Learning,,,,,9,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Improving the effectiveness of test suite through mining historical data,J. Anderson; S. Salem; H. Do,"Microsoft, USA / North Dakota State University, USA",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,142,151,"<p> Software regression testing is an integral part of most major software projects. As projects grow larger and the number of tests increases, performing regression testing becomes more costly. If software engineers can identify and run tests that are more likely to detect failures during regression testing, they may be able to better manage their regression testing activities. In this paper, to help identify such test cases, we developed techniques that utilizes various types of information in software repositories. To assess our techniques, we conducted an empirical study using an industrial software product, Microsoft Dynamics AX, which contains real faults. Our results show that the proposed techniques can be effective in identifying test cases that are likely to detect failures. </p>",,,10.1145/2597073.2597084,,,Test failure prediction;empirical study;mining software repositories;regression testing,,,,,7,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Incremental origin analysis of source code files,D. Steidl; B. Hummel; E. Juergens,"CQSE, Germany",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,42,51,"<p> The history of software systems tracked by version control systems is often incomplete because many file movements are not recorded. However, static code analyses that mine the file history, such as change frequency or code churn, produce precise results only if the complete history of a source code file is available. In this paper, we show that up to 38.9% of the files in open source systems have an incomplete history, and we propose an incremental, commit-based approach to reconstruct the history based on clone information and name similarity. With this approach, the history of a file can be reconstructed across repository boundaries and thus provides accurate information for any source code analysis. We evaluate the approach in terms of correctness, completeness, performance, and relevance with a case study among seven open source systems and a developer survey. </p>",,,10.1145/2597073.2597111,,,Clone Detection;Origin Analysis;Software Evolution,,,,,5,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Mining energy-greedy API usage patterns in Android apps: an empirical study,M. Linares-VÕçsquez; G. Bavota; C. Bernal-CÕçrdenas; R. Oliveto; M. Di Penta; D. Poshyvanyk,"College of William and Mary, USA",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,2,11,"<p> Energy consumption of mobile applications is nowadays a hot topic, given the widespread use of mobile devices. The high demand for features and improved user experience, given the available powerful hardware, tend to increase the apps_Ñé energy consumption. However, excessive energy consumption in mobile apps could also be a consequence of energy greedy hardware, bad programming practices, or particular API usage patterns. We present the largest to date quantitative and qualitative empirical investigation into the categories of API calls and usage patterns that_ÑÓin the context of the Android development framework_ÑÓexhibit particularly high energy consumption profiles. By using a hardware power monitor, we measure energy consumption of method calls when executing typical usage scenarios in 55 mobile apps from different domains. Based on the collected data, we mine and analyze energy-greedy APIs and usage patterns. We zoom in and discuss the cases where either the anomalous energy consumption is unavoidable or where it is due to suboptimal usage or choice of APIs. Finally, we synthesize our findings into actionable knowledge and recipes for developers on how to reduce energy consumption while using certain categories of Android APIs and patterns </p>",,,10.1145/2597073.2597085,,,Empirical Study;Energy consumption;Mobile applications,,,,,40,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Mining questions about software energy consumption,G. Pinto; F. Castor; Y. D. Liu,"Federal University of Pernambuco, Brazil / SUNY Binghamton, USA",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,22,31,"<p> A growing number of software solutions have been proposed to address application-level energy consumption problems in the last few years. However, little is known about how much software developers are concerned about energy consumption, what aspects of energy consumption they consider important, and what solutions they have in mind for improving energy efficiency. In this paper we present the first empirical study on understanding the views of application programmers on software energy consumption problems. Using StackOverflow as our primary data source, we analyze a carefully curated sample of more than 300 questions and 550 answers from more than 800 users. With this data, we observed a number of interesting findings. Our study shows that practitioners are aware of the energy consumption problems: the questions they ask are not only diverse -- we found 5 main themes of questions -- but also often more interesting and challenging when compared to the control question set. Even though energy consumption-related questions are popular when considering a number of different popularity measures, the same cannot be said about the quality of their answers. In addition, we observed that some of these answers are often flawed or vague. We contrast the advice provided by these answers with the state-of-the-art research on energy consumption. Our summary of software energy consumption problems may help researchers focus on what matters the most to software developers and end users. </p>",,,10.1145/2597073.2597110,,,Practitioners;Q&A;Software Energy Consumption,,,,,31,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Mining questions asked by web developers,K. Bajaj; K. Pattabiraman; A. Mesbah,"University of British Columbia, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,112,121,"<p> Modern web applications consist of a significant amount of client- side code, written in JavaScript, HTML, and CSS. In this paper, we present a study of common challenges and misconceptions among web developers, by mining related questions asked on Stack Over- flow. We use unsupervised learning to categorize the mined questions and define a ranking algorithm to rank all the Stack Overflow questions based on their importance. We analyze the top 50 questions qualitatively. The results indicate that (1) the overall share of web development related discussions is increasing among developers, (2) browser related discussions are prevalent; however, this share is decreasing with time, (3) form validation and other DOM related discussions have been discussed consistently over time, (4) web related discussions are becoming more prevalent in mobile development, and (5) developers face implementation issues with new HTML5 features such as Canvas. We examine the implications of the results on the development, research, and standardization communities. </p>",,,10.1145/2597073.2597083,,,Stack Overflow;Text Mining;Topic Modeling;Web Developers,,,,,11,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Mining StackOverflow to turn the IDE into a self-confident programming prompter,L. Ponzanelli; G. Bavota; M. Di Penta; R. Oliveto; M. Lanza,"University of Lugano, Switzerland",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,102,111,"<p> Developers often require knowledge beyond the one they possess, which often boils down to consulting sources of information like Application Programming Interfaces (API) documentation, forums, Q&A websites, etc. Knowing what to search for and how is non- trivial, and developers spend time and energy to formulate their problems as queries and to peruse and process the results. We propose a novel approach that, given a context in the IDE, automatically retrieves pertinent discussions from Stack Overflow, evaluates their relevance, and, if a given confidence threshold is surpassed, notifies the developer about the available help. We have implemented our approach in Prompter, an Eclipse plug-in. Prompter has been evaluated through two studies. The first was aimed at evaluating the devised ranking model, while the second was conducted to evaluate the usefulness of Prompter. </p>",,,10.1145/2597073.2597077,,,Developers Support;Empirical Studies;Recommender Systems,,,,,31,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Modern code reviews in open-source projects: which problems do they fix?,M. Beller; A. Bacchelli; A. Zaidman; E. Juergens,"Delft University of Technology, Netherlands",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,202,211,"<p> Code review is the manual assessment of source code by humans, mainly intended to identify defects and quality problems. Modern Code Review (MCR), a lightweight variant of the code inspections investigated since the 1970s, prevails today both in industry and open-source software (OSS) systems. The objective of this paper is to increase our understanding of the practical benefits that the MCR process produces on reviewed source code. To that end, we empirically explore the problems fixed through MCR in OSS systems. We manually classified over 1,400 changes taking place in reviewed code from two OSS projects into a validated categorization scheme. Surprisingly, results show that the types of changes due to the MCR process in OSS are strikingly similar to those in the industry and academic systems from literature, featuring the similar 75:25 ratio of maintainability-related to functional problems. We also reveal that 7_Ñ_35% of review comments are discarded and that 10_Ñ_22% of the changes are not triggered by an explicit review comment. Patterns emerged in the review data; we investigated them revealing the technical factors that influence the number of changes due to the MCR process. We found that bug-fixing tasks lead to fewer changes and tasks with more altered files and a higher code churn have more changes. Contrary to intuition, the person of the reviewer had no impact on the number of changes. </p>",,,10.1145/2597073.2597082,,,Code Review;Defects;Open Source Software,,,,,24,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
MUX: algorithm selection for software model checkers,V. Tulsian; A. Kanade; R. Kumar; A. Lal; A. V. Nori,"Indian Institute of Science, India",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,132,141,"<p> With the growing complexity of modern day software, software model checking has become a critical technology for ensuring correctness of software. As is true with any promising technology, there are a number of tools for software model checking. However, their respective performance trade-offs are difficult to characterize accurately _Ñ_ making it difficult for practitioners to select a suitable tool for the task at hand. This paper proposes a technique called MUX that addresses the problem of selecting the most suitable software model checker for a given input instance. MUX performs machine learning on a repository of software verification instances. The algorithm selector, synthesized through machine learning, uses structural features from an input instance, comprising a program-property pair, at runtime and determines which tool to use. </p> <p> We have implemented MUX for Windows device drivers and evaluated it on a number of drivers and model checkers. Our results are promising in that the algorithm selector not only avoids a significant number of timeouts but also improves the total runtime by a large margin, compared to any individual model checker. It also outperforms a portfolio-based algorithm selector being used in Microsoft at present. Besides, MUX identifies structural features of programs that are key factors in determining performance of model checkers. </p>",,,10.1145/2597073.2597080,,,Algorithm selection;machine learning;software model checking,,,,,,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Oops! where did that code snippet come from?,L. Guo; J. Lawall; G. Muller,"INRIA, France / LIP6, France / Sorbonne, France / UPMC, France",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,52,61,"<p> A kernel oops is an error report that logs the status of the Linux kernel at the time of a crash. Such a report can provide valuable first-hand information for a Linux kernel maintainer to conduct postmortem debugging. Recently, a repository has been created that systematically collects kernel oopses from Linux users. However, debugging based on only the information in a kernel oops is difficult. We consider the initial problem of finding the offending line, i.e., the line of source code that incurs the crash. For this, we propose a novel algorithm based on approximate sequence matching, as used in bioinformatics, to automatically pinpoint the offending line based on information about nearby machine-code instructions, as found in a kernel oops. Our algorithm achieves 92% accuracy compared to 26% for the traditional approach of using only the oops instruction pointer. </p>",,,10.1145/2597073.2597094,,,Linux kernel;debugging;oops;sequence alignment,,,,,2,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Prediction and ranking of co-change candidates for clones,M. Mondal; C. K. Roy; K. A. Schneider,"University of Saskatchewan, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,32,41,"<p> Code clones are identical or similar code fragments scattered in a code-base. A group of code fragments that are similar to one another form a clone group. Clones in a particular group often need to be changed together (i.e., co-changed) consistently. However, all clones in a group might not require consistent changes, because some clone fragments might evolve independently. Thus, while changing a particular clone fragment, it is important for a programmer to know which other clone fragments in the same group should be consistently co-changed with that particular clone fragment. </p> <p> In this research work, we empirically investigate whether we can automatically predict and rank these other clone fragments (i.e., the co-change candidates) from a clone group while making changes to a particular clone fragment in this group. For prediction and ranking we automatically retrieve and infer evolutionary coupling among clones by mining the past clone evolution history. Our experimental result on six subject systems written in two different programming languages (C, and Java) considering both exact and near-miss clones implies that we can automatically predict and rank co-change candidates for clones by analyzing evolutionary coupling. Our ranking mechanism can help programmers pinpoint the likely co-change candidates while changing a particular clone fragment and thus, can help us to better manage software clones. </p>",,,10.1145/2597073.2597104,,,Co-change Candidates;Co-change Frequency;Co-change Recency;Code Clones;Evolutionary Coupling;Ranking,,,,,4,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Process mining multiple repositories for software defect resolution from control and organizational perspective,M. Gupta; A. Sureka; S. Padmanabhuni,"IIIT Delhi, India",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,122,131,"<p> Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities. </p>",,,10.1145/2597073.2597081,,,Empirical Software Engineering and Measurements;Issue Tracking System;Peer Code Review System;Process Mining;Social Network Analysis;Software Maintenance,,,,,2,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Revisiting Android reuse studies in the context of code obfuscation and library usages,M. Linares-VÕçsquez; A. Holtzhauer; C. Bernal-CÕçrdenas; D. Poshyvanyk,"College of William and Mary, USA",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,242,251,"<p> In the recent years, studies of design and programming practices in mobile development are gaining more attention from researchers. Several such empirical studies used Android applications (paid, free, and open source) to analyze factors such as size, quality, dependencies, reuse, and cloning. Most of the studies use executable files of the apps (APK files), instead of source code because of availability issues (most of free apps available at the Android official market are not open-source, but still can be downloaded and analyzed in APK format). However, using only APK files in empirical studies comes with some threats to the validity of the results. In this paper, we analyze some of these pertinent threats. In particular, we analyzed the impact of third-party libraries and code obfuscation practices on estimating the amount of reuse by class cloning in Android apps. When including and excluding third-party libraries from the analysis, we found statistically significant differences in the amount of class cloning 24,379 free Android apps. Also, we found some evidence that obfuscation is responsible for increasing a number of false positives when detecting class clones. Finally, based on our findings, we provide a list of actionable guidelines for mining and analyzing large repositories of Android applications and minimizing these threats to validity </p>",,,10.1145/2597073.2597109,,,Android;class cloning;obfuscated code;reuse;third-party libraries,,,,,12,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Syntax errors just aren't natural: improving error reporting with language models,,,Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,252,261,"<p> A frustrating aspect of software development is that compiler error messages often fail to locate the actual cause of a syntax error. An errant semicolon or brace can result in many errors reported throughout the file. We seek to find the actual source of these syntax errors by relying on the consistency of software: valid source code is usually repetitive and unsurprising. We exploit this consistency by constructing a simple N-gram language model of lexed source code tokens. We implemented an automatic Java syntax-error locator using the corpus of the project itself and evaluated its performance on mutated source code from several projects. Our tool, trained on the past versions of a project, can effectively augment the syntax error locations produced by the native compiler. Thus we provide a methodology and tool that exploits the naturalness of software source code to detect syntax errors alongside the parser. </p>",,,10.1145/2597073.2597102,,,NLP;error location;language;n-grams;naturalness;syntax,,,,,6,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
"The impact of code review coverage and code review participation on software quality: a case study of the qt, VTK, and ITK projects",S. McIntosh; Y. Kamei; B. Adams; A. E. Hassan,"Queen&#039;s University, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,192,201,"<p> Software code review, i.e., the practice of having third-party team members critique changes to a software system, is a well-established best practice in both open source and proprietary software domains. Prior work has shown that the formal code inspections of the past tend to improve the quality of software delivered by students and small teams. However, the formal code inspection process mandates strict review criteria (e.g., in-person meetings and reviewer checklists) to ensure a base level of review quality, while the modern, lightweight code reviewing process does not. Although recent work explores the modern code review process qualitatively, little research quantitatively explores the relationship between properties of the modern code review process and software quality. Hence, in this paper, we study the relationship between software quality and: (1) code review coverage, i.e., the proportion of changes that have been code reviewed, and (2) code review participation, i.e., the degree of reviewer involvement in the code review process. Through a case study of the Qt, VTK, and ITK projects, we find that both code review coverage and participation share a significant link with software quality. Low code review coverage and participation are estimated to produce components with up to two and five additional post-release defects respectively. Our results empirically confirm the intuition that poorly reviewed code has a negative impact on software quality in large systems using modern reviewing tools. </p>",,,10.1145/2597073.2597076,,,Code reviews;software quality,,,,,29,1,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
The promises and perils of mining GitHub,E. Kalliamvakou; G. Gousios; K. Blincoe; L. Singer; D. M. German; D. Damian,"University of Victoria, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,92,101,"<p> With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub. </p>",,,10.1145/2597073.2597074,,,Mining software repositories;bias;code reviews;git;github,,,,,43,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Thesaurus-based automatic query expansion for interface-driven code search,O. A. L. Lemos; A. C. de Paula; F. C. Zanichelli; C. V. Lopes,"Federal University of S&#227;o Paulo, Brazil",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,212,221,"<p> Software engineers often resort to code search practices to support software maintenance and evolution tasks, in particular code reuse. An issue that affects code search is the vocabulary mismatch problem: while searching for a particular function, users have to guess the exact words that were chosen by original developers to name code entities. In this paper we present an automatic query expansion (AQE) approach that uses word relations to increase the chances of finding relevant code. The approach is applied on top of Test-Driven Code Search (TDCS), a promising code retrieval technique that uses test cases as inputs to formulate the search query, but can also be used with other techniques that handle interface definitions to produce queries (interface-driven code search). Since these techniques rely on keywords and types, the vocabulary mismatch problem is also relevant. AQE is carried out by leveraging WordNet, a type thesaurus for expanding types, and another thesaurus containing only software-related word relations. Our approach is general but was specifically designed for non-native English speakers, who are frequently unaware of the most common terms used to name functions in software. Our evaluation with 36 non-native subjects - including developers and senior Computer Science students - provides evidence that our approach can improve the chances of finding relevant functions by 41% (recall improvement of 30%, on average), without hurting precision. </p>",,,10.1145/2597073.2597087,,,automatic query expansion;code search;software reuse,,,,,6,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Towards building a universal defect prediction model,F. Zhang; A. Mockus; I. Keivanloo; Y. Zou,"Queen&#039;s University, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,182,191,"<p> To predict files with defects, a suitable prediction model must be built for a software project from either itself (within-project) or other projects (cross-project). A universal defect prediction model that is built from the entire set of diverse projects would relieve the need for building models for an individual project. A universal model could also be interpreted as a basic relationship between software metrics and defects. However, the variations in the distribution of predictors pose a formidable obstacle to build a universal model. Such variations exist among projects with different context factors (e.g., size and programming language). To overcome this challenge, we propose context-aware rank transformations for predictors. We cluster projects based on the similarity of the distribution of 26 predictors, and derive the rank transformations using quantiles of predictors for a cluster. We then fit the universal model on the transformed data of 1,398 open source projects hosted on SourceForge and GoogleCode. Adding context factors to the universal model improves the predictive power. The universal model obtains prediction performance comparable to the within-project models and yields similar results when applied on five external projects (one Apache and four Eclipse projects). These results suggest that a universal defect prediction model may be an achievable goal. </p>",,,10.1145/2597073.2597078,,,Universal defect prediction model;bug;context factors;defect;defect prediction;large scale;quality;rank transformation,,,,,18,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Unsupervised discovery of intentional process models from event logs,G. Khodabandelou; C. Hug; R. DeneckÕŒre; C. Salinesi,"Sorbonne, France",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,282,291,"<p> Research on guidance and method engineering has highlighted that many method engineering issues, such as lack of flexibility or adaptation, are solved more effectively when intentions are explicitly specified. However, software engineering process models are most often described in terms of sequences of activities. This paper presents a novel approach, so-called Map Miner Method (MMM), designed to automate the construction of intentional process models from process logs. To do so, MMM uses Hidden Markov Models to model users' activities logs in terms of users' strategies. MMM also infers users' intentions and constructs fine-grained and coarse-grained intentional process models with respect to the Map metamodel syntax (i.e., metamodel that specifies intentions and strategies of process actors). These models are obtained by optimizing a new precision-fitness metric. The result is a software engineering method process specification aligned with state of the art of method engineering approaches. As a case study, the MMM is used to mine the intentional process associated to the Eclipse platform usage. Observations show that the obtained intentional process model offers a new understanding of software processes, and could readily be used for recommender systems. </p>",,,10.1145/2597073.2597101,,,Event logs Mining;Intentional Process Modeling;Software Development Process,,,,,3,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
Works for me! characterizing non-reproducible bug reports,M. Erfani Joorabchi; M. Mirzaaghaei; A. Mesbah,"University of British Columbia, Canada",Proceedings of the 11th Working Conference on Mining Software Repositories,20160129,2014,,,62,71,"<p> Bug repository systems have become an integral component of software development activities. Ideally, each bug report should help developers to find and fix a software fault. However, there is a subset of reported bugs that is not (easily) reproducible, on which developers spend considerable amounts of time and effort. We present an empirical analysis of non-reproducible bug reports to characterize their rate, nature, and root causes. We mine one industrial and five open-source bug repositories, resulting in 32K non-reproducible bug reports. We (1) compare properties of non-reproducible reports with their counterparts such as active time and number of authors, (2) investigate their life-cycle patterns, and (3) examine 120 Fixed non-reproducible reports. In addition, we qualitatively classify a set of randomly selected non-reproducible bug reports (1,643) into six common categories. Our results show that, on average, non-reproducible bug reports pertain to 17% of all bug reports, remain active three months longer than their counterparts, can be mainly (45%) classified as ""Interbug Dependencies'', and 66% of Fixed non-reproducible reports were indeed reproduced and fixed. </p>",,,10.1145/2597073.2597098,,,Non-reproducible bugs;bug tracking systems;mining bug reports,,,,,5,,,,,,May 31 2014-June 1 2014,,ACM,ACM Conferences,,9
A Historical Analysis of Debian Package Incompatibilities,M. Claes; T. Mens; R. Di Cosmo; J. Vouillon,"COMPLEXYS Res. Inst., Univ. of Mons, Mons, Belgium",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,212,223,"Users and developers of software distributions are often confronted with installation problems due to conflicting packages. A prototypical example of this are the Linux distributions such as Debian. Conflicts between packages have been studied under different points of view in the literature, in particular for the Debian operating system, but little is known about how these package conflicts evolve over time. This article presents an extensive analysis of the evolution of package incompatibilities, spanning a decade of the life of the Debian stable and testing distributions for its most popular architecture, i386. Using the technique of survival analysis, this empirical study sheds some light on the origin and evolution of package incompatibilities, and provides the basis for building indicators that may be used to improve the quality of package-based distributions.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180081,analysis;conflict;debian;dependency;distribution;empirical;evolution;maintenance;package;software,Computer architecture;Kernel;Linux;Market research;Metadata;Testing,Linux;history;software packages,Debian operating system;Debian package incompatibilities;Linux;historical analysis;survival analysis,,6,1,32,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
A Method to Detect License Inconsistencies in Large-Scale Open Source Projects,Y. Wu; Y. Manabe; T. Kanda; D. M. German; K. Inoue,"Grad. Sch. of Inf. Sci. & Technol., Osaka Univ., Suita, Japan",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,324,333,"The reuse of free and open source software (FOSS) components is becoming more and more popular. They usually contain one or more software licenses describing the requirements and conditions which should be followed when been reused. Licenses are usually written in the header of source code files as program comments. Removing or modifying the license header by re-distributors will result in the inconsistency of license with its ancestor, and may potentially cause license infringement. But to the best of our knowledge, no research has been devoted to investigate such kind of license infringements nor license inconsistencies. In this paper, we describe and categorize different types of license inconsistencies and propose a feasible method to detect them. Then we apply this method to Debian 7.5 and present the license inconsistencies found in it. With a manual analysis, we summarized various reasons behind these license inconsistencies, some of which imply license infringement and require the attention from the developers. This analysis also exposes the difficulty to discover license infringements, highlighting the usefulness of finding and maintaining source code provenance.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180091,,Electronic mail;Libraries;Licenses;Measurement;Open source software;Software reusability,law;software reusability;source code (software),Debian 7.5;FOSS;free and open source software;license inconsistencies detection;license infringements;software licenses;software reuse;source code provenance,,6,,22,,,,16-17 May 2015,,IEEE,IEEE Conferences,,9
A Study on the Role of Software Architecture in the Evolution and Quality of Software,E. Kouroshfar; M. Mirakhorli; H. Bagheri; L. Xiao; S. Malek; Y. Cai,"Comput. Sci. Dept., George Mason Univ., Fairfax, VA, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,246,257,"Conventional wisdom suggests that a software system's architecture has a significant impact on its evolution. Prior research has studied the evolution of software using the information of how its files have changed together in their revision history. No prior study, however, has investigated the impact of architecture on the evolution of software from its change history. This is mainly because most open-source software systems do not document their architectures. We have overcome this challenge using several architecture recovery techniques. We used the recovered models to examine if co-changes spanning multiple architecture modules are more likely to introduce bugs than co-changes that are within modules. The results show that the co-changes that cross architectural module boundaries are more correlated with defects than co-changes within modules, implying that, to improve accuracy, bug predictors should also take the software architecture of the system into consideration.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180084,Defects;Software Architecture;Software Repositories,Computer architecture;Couplings;Data mining;History;Measurement;Software systems,public domain software;software architecture;software quality,architecture recovery techniques;cross architectural module boundaries;open-source software systems;software architecture;software evolution;software quality,,1,,45,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
An Empirical Study of Architectural Change in Open-Source Software Systems,D. M. Le; P. Behnamghader; J. Garcia; D. Link; A. Shahbazian; N. Medvidovic,"Comput. Sci. Dept., Univ. of Southern California, Los Angeles, CA, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,235,245,"From its very inception, the study of software architecture has recognized architectural decay as a regularly occurring phenomenon in long-lived systems. Architectural decay is caused by repeated changes to a system during its lifespan. Despite decay's prevalence, there is a relative dearth of empirical data regarding the nature of architectural changes that may lead to decay, and of developers' understanding of those changes. In this paper, we take a step toward addressing that scarcity by conducting an empirical study of changes found in software architectures spanning several hundred versions of 14 open-source systems. Our study reveals several new findings regarding the frequency of architectural changes in software systems, the common points of departure in a system's architecture during maintenance and evolution, the difference between system-level and component-level architectural change, and the suitability of a system's implementation-level structure as a proxy for its architecture.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180083,architectural change;architecture recovery;open-source systems;software architecture;software evolution,Accuracy;Clustering algorithms;Computer architecture;Measurement;Software systems;System implementation,object-oriented programming;public domain software;software architecture;software maintenance,architectural decay;component-level architectural change;long-lived systems;open-source software systems;software architecture;software evolution;software maintenance;system-level architectural change,,9,,37,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
An Empirical Study of End-User Programmers in the Computer Music Community,G. Burlet; A. Hindle,"Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,292,302,"Computer musicians are a community of end-user programmers who often use visual programming languages such as Max/MSP or Pure Data to realize their musical compositions. This research study conducts a multifaceted analysis of the software development practices of computer musicians when programming in these visual music-oriented languages. A statistical analysis of project metadata harvested from software repositories hosted on GitHub reveals that in comparison to the general population of software developers, computer musicians' repositories have less commits, less frequent commits, more commits on weekends, yet similar numbers of bug reports and similar numbers of contributing authors. Analysis of source code in these repositories reveals that the vast majority of code can be reconstructed from duplicate fragments. Finally, these results are corroborated by a survey of computer musicians and interviews with individuals in this end-user community. Based on this analysis and feedback from computer musicians we find that there are many avenues where software engineering can be applied to help aid this community of end-user programmers.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180088,computer music;end user;visual programming,Cloning;Communities;Computer languages;Computers;Music;Software;Visualization,meta data;music;personal computing;program debugging;source code (software);statistical analysis;visual languages,GitHub;Max/MSP;Pure Data;computer music community;computer musician repositories;end-user community;end-user programmers;multifaceted analysis;musical compositions;project metadata;software developers;software development practices;software engineering;software repositories;source code;statistical analysis;visual music-oriented languages;visual programming languages,,0,,45,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
An Empirical Study of the Copy and Paste Behavior during Development,T. M. Ahmed; W. Shang; A. E. Hassan,"Sch. of Comput., Queen's Univ., Kingston, ON, Canada",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,99,110,"Developers frequently employ Copy and Paste. However, little is known about the copy and paste behavior during development. To better understand the copy and paste behavior, automated approaches are proposed to identify cloned code. However, such automated approaches can only identify the location of the code that has been copied and pasted, but little is known about the context of the copy and paste. On the other hand, prior research studying actual copy and paste behavior is based on a small number of users in an experimental setup. In this paper, we study the behavior of developers copying and pasting code while using the Eclipse IDE. We mine the usage data of over 20,000 Eclipse users. We aim to explore the different patterns of Copy and Paste (C&P) that are used by Eclipse users during development. We compare such usage patterns to the regular users' usage of copy and paste during non-development tasks reported in earlier studies. Our findings instruct builders of future IDEs. We find that developers' C&P behavior is considerably different from the behavior of regular users. For example, developers tend to perform more frequent C&P in the same file contrary to regular users, who tend to perform C&P across different windows. Moreover, we find that C&P across different programming languages is a common behavior as we extracted more than 75,000 C&P incidents across different programming languages. Such a finding highlights the need for code cloning tools that can detect code clones across different programming languages.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180071,Copy and Paste;Eclipse;Mining Software Repositories;UDC;code cloning,Cloning;Context;Data mining;Java;Software;Standards,program diagnostics,C&P;C&P incidents;Eclipse IDE;Eclipse users;clone detection techniques;cloned code;copy-and-paste behavior;nondevelopment tasks;programming languages,,3,,27,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
Are Bullies More Productive? Empirical Study of Affectiveness vs. Issue Fixing Time,M. Ortu; B. Adams; G. Destefanis; P. Tourani; M. Marchesi; R. Tonelli,"DIEE, Univ. of Cagliari, Cagliari, Italy",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,303,313,"Human Affectiveness, i.e., The emotional state of a person, plays a crucial role in many domains where it can make or break a team's ability to produce successful products. Software development is a collaborative activity as well, yet there is little information on how affectiveness impacts software productivity. As a first measure of this impact, this paper analyzes the relation between sentiment, emotions and politeness of developers in more than 560K Jira comments with the time to fix a Jira issue. We found that the happier developers are (expressing emotions such as JOY and LOVE in their comments), the shorter the issue fixing time is likely to be. In contrast, negative emotions such as SADNESS, are linked with longer issue fixing time. Politeness plays a more complex role and we empirically analyze its impact on developers' productivity.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180089,,Electronic publishing;Information services;Logistics;Measurement;Software;Software engineering;Training,productivity;social aspects of automation;software development management,Jira comments;Jira issue;developer productivity;human affectiveness;negative emotions;person emotional state;politeness;software development;software productivity,,23,,34,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
"Are These Bugs Really ""Normal""?",R. K. Saha; J. Lawall; S. Khurshid; D. E. Perry,"Univ. of Texas at Austin, Austin, TX, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,258,268,"Understanding the severity of reported bugs is important in both research and practice. In particular, a number of recently proposed mining-based software engineering techniques predict bug severity, bug report quality, and bug-fix time, according to this information. Many bug tracking systems provide a field ""severity"" offering options such as ""severe"", ""normal"", and ""minor"", with ""normal"" as the default. However, there is a widespread perception that for many bug reports the label ""normal"" may not reflect the actual severity, because reporters may overlook setting the severity or may not feel confident enough to do so. In many cases, researchers ignore ""normal"" bug reports, and thus overlook a large percentage of the reports provided. On the other hand, treating them all together risks mixing reports that have very diverse properties. In this study, we investigate the extent to which ""normal"" bug reports actually have the ""normal"" severity. We find that many ""normal"" bug reports in practice are not normal. Furthermore, this misclassification can have a significant impact on the accuracy of mining-based tools and studies that rely on bug report severity information.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180085,Bug Severity;Bug Tracking System;Mining Software Repositories,Accuracy;Computer bugs;Data mining;Noise;Software;Software engineering;Training,data mining;program debugging,bug report quality;bug severity prediction;bug tracking systems;bug-fix time;mining-based software engineering techniques;minor-severity;normal-bug reports;normal-severity,,0,,34,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Characteristics of Useful Code Reviews: An Empirical Study at Microsoft,A. Bosu; M. Greiler; C. Bird,"Dept. of Comput. Sci., Univ. of Alabama, Tuscaloosa, AL, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,146,156,"Over the past decade, both open source and commercial software projects have adopted contemporary peer code review practices as a quality control mechanism. Prior research has shown that developers spend a large amount of time and effort performing code reviews. Therefore, identifying factors that lead to useful code reviews can benefit projects by increasing code review effectiveness and quality. In a three-stage mixed research study, we qualitatively investigated what aspects of code reviews make them useful to developers, used our findings to build and verify a classification model that can distinguish between useful and not useful code review feedback, and finally we used this classifier to classify review comments enabling us to empirically investigate factors that lead to more effective code review feedback. In total, we analyzed 1.5 millions review comments from five Microsoft projects and uncovered many factors that affect the usefulness of review feedback. For example, we found that the proportion of useful comments made by a reviewer increases dramatically in the first year that he or she is at Microsoft but tends to plateau afterwards. In contrast, we found that the more files that are in a change, the lower the proportion of comments in the code review that will be of value to the author of the change. Based on our findings, we provide recommendations for practitioners to improve effectiveness of code reviews.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180075,code review;empirical;microsoft;recommendation,Data mining;Electronic mail;Interviews;Manuals;Reliability;Software,project management;public domain software;quality control;software development management;software quality,Microsoft projects;code quality;code review feedback;commercial software projects;contemporary peer code review practices;open source;quality control mechanism,,11,,36,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Characterization and Prediction of Issue-Related Risks in Software Projects,M. Choetkiertikul; H. K. Dam; T. Tran; A. Ghose,"Sch. of Comput. Sci. & Software Eng., Univ. of Wollongong, Wollongong, NSW, Australia",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,280,291,"Identifying risks relevant to a software project and planning measures to deal with them are critical to the success of the project. Current practices in risk assessment mostly rely on high-level, generic guidance or the subjective judgements of experts. In this paper, we propose a novel approach to risk assessment using historical data associated with a software project. Specifically, our approach identifies patterns of past events that caused project delays, and uses this knowledge to identify risks in the current state of the project. A set of risk factors characterizing _ÑÒrisky_Ñù software tasks (in the form of issues) were extracted from five open source projects: Apache, Duraspace, JBoss, Moodle, and Spring. In addition, we performed feature selection using a sparse logistic regression model to select risk factors with good discriminative power. Based on these risk factors, we built predictive models to predict if an issue will cause a project delay. Our predictive models are able to predict both the risk impact (i.e. the extend of the delay) and the likelihood of a risk occurring. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 48%-81% precision, 23%-90% recall, 29%-71% F-measure, and 70%-92% Area Under the ROC Curve. Our predictive models also have low error rates: 0.39-0.75 for Macro-averaged Mean Cost-Error and 0.7-1.2 for Macro-averaged Mean Absolute Error.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180087,,Delays;Feature extraction;Logistics;Predictive models;Risk management;Software;Springs,feature selection;project management;regression analysis;risk management;software development management,Apache;Duraspace;JBoss;Moodle;Spring;feature selection;issue-related risks;macroaveraged mean absolute error;macroaveraged mean cost-error;open source projects;project delay;project delays;risk assessment;risk factor selection;risks identification;risky software tasks;software projects;sparse logistic regression model,,4,,65,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
Co-evolution of Infrastructure and Source Code - An Empirical Study,Y. Jiang; B. Adams,"MCIS Lab., Polytech. Montreal, Montreal, QC, Canada",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,45,55,"Infrastructure-as-code automates the process of configuring and setting up the environment (e.g., servers, VMs and databases) in which a software system will be tested and/or deployed, through textual specification files in a language like Puppet or Chef. Since the environment is instantiated automatically by the infrastructure languages' tools, no manual intervention is necessary apart from maintaining the infrastructure specification files. The amount of work involved with such maintenance, as well as the size and complexity of infrastructure specification files, have not yet been studied empirically. Through an empirical study of the version control system of 265 Open Stack projects, we find that infrastructure files are large and churn frequently, which could indicate a potential of introducing bugs. Furthermore, we found that the infrastructure code files are coupled tightly with the other files in a project, especially test files, which implies that testers often need to change infrastructure specifications when making changes to the test framework and tests.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180066,,Association rules;Cloud computing;Couplings;Maintenance engineering;Measurement;Servers,formal specification;source code (software),infrastructure code files;infrastructure coevolution;infrastructure specifications;source code,,5,,36,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Code Ownership and Software Quality: A Replication Study,M. Greiler; K. Herzig; J. Czerwonka,"Microsoft Corp., Redmond, WA, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,2,12,"In a traditional sense, ownership determines rights and duties in regard to an object, for example a property. The owner of source code usually refers to the person that invented the code. However, larger code artifacts, such as files, are usually composed by multiple engineers contributing to the entity over time through a series of changes. Frequently, the person with the highest contribution, e.g. The most number of code changes, is defined as the code owner and takes responsibility for it. Thus, code ownership relates to the knowledge engineers have about code. Lacking responsibility and knowledge about code can reduce code quality. In an earlier study, Bird et al. [1] showed that Windows binaries that lacked clear code ownership were more likely to be defect prone. However recommendations for large artifacts such as binaries are usually not actionable. E.g. Changing the concept of binaries and refactoring them to ensure strong ownership would violate system architecture principles. A recent replication study by Foucault et al. [2] on open source software replicate the original results and lead to doubts about the general concept of ownership impacting code quality. In this paper, we replicated and extended the previous two ownership studies [1, 2] and reflect on their findings. Further, we define several new ownership metrics to investigate the dependency between ownership and code quality on file and directory level for 4 major Microsoft products. The results confirm the original findings by Bird et al. [1] that code ownership correlates with code quality. Using new and refined code ownership metrics we were able to classify source files that contained at least one bug with a median precision of 0.74 and a median recall of 0.38. On directory level, we achieve a precision of 0.76 and a recall of 0.60.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180062,Empirical software engineering;code ownership;software quality,Atmospheric measurements;Birds;Computer bugs;Correlation;Size measurement;Software,software metrics;software quality,Microsoft products;code ownership metrics;knowledge engineers;open source software;software quality;source code;system architecture principles,,6,,32,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Do Bugs Foreshadow Vulnerabilities? A Study of the Chromium Project,F. Camilo; A. Meneely; M. Nagappan,"Dept. of Software Eng., Rochester Inst. of Technol., Rochester, NY, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,269,279,"As developers face ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. Vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. However, conceptually vulnerabilities are different from bugs: they represent abusive functionality as opposed to wrong or insufficient functionality commonly associated with traditional, non-security bugs. In this study, we performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. Using logistic regression analysis, we examined how various categories of pre-release bugs (e.g. Stability, compatibility, etc.) are associated with post-release vulnerabilities. While we found statistically significant correlations between pre-release bugs and post-release vulnerabilities, we also found the association to be weak. Number of features, SLOC, and number of pre-release security bugs are, in general, more closely associated with post-release vulnerabilities than any of our non-security bug categories. In a separate analysis, we found that the files with highest defect density did not intersect with the files of highest vulnerability density. These results indicate that bugs and vulnerabilities are empirically dissimilar groups, warranting the need for more research targeting vulnerabilities specifically.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.32,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180086,bugs;chromium;mining;vulnerabilities,Chromium;Computer bugs;Correlation;Logistics;Measurement;Security;Software,data mining;program debugging;software quality,abusive functionality;chromium project;logistic regression analysis;post-release vulnerabilities;pre-release bugs;security-sensitive bugs;software engineering;software quality;software repositories,,6,,28,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Ecosystems in GitHub and a Method for Ecosystem Identification Using Reference Coupling,K. Blincoe; F. Harrison; D. Damian,"Software Eng. Global InterAction Lab., Univ. of Victoria, Victoria, BC, Canada",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,202,211,"Software projects are not developed in isolation. Recent research has shifted to studying software ecosystems, communities of projects that depend on each other and are developed together. However, identifying technical dependencies at the ecosystem level can be challenging. In this paper, we propose a new method, known as reference coupling, for detecting technical dependencies between projects. The method establishes dependencies through user-specified cross-references between projects. We use our method to identify ecosystems in GitHub-hosted projects, and we identify several characteristics of the identified ecosystems. We find that most ecosystems are centered around one project and are interconnected with other ecosystems. The predominant type of ecosystems are those that develop tools to support software development. We also found that the project owners' social behaviour aligns well with the technical dependencies within the ecosystem, but project contributors' social behaviour does not align with these dependencies. We conclude with a discussion on future research that is enabled by our reference coupling method.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180080,Ecosystems;GitHub;Reference Coupling;Technical Dependencies;cross-reference,Communities;Couplings;Databases;Ecosystems;Encoding;Software;Visualization,project management;software management,GitHub ecosystems;ecosystem identification;identified ecosystems;project contributor social behaviour;reference coupling method;software ecosystems;software projects,,10,,,,,,16-17 May 2015,,IEEE,IEEE Conferences,,9
Extracting Facts from Performance Tuning History of Scientific Applications for Predicting Effective Optimization Patterns,M. Hashimoto; M. Terai; T. Maeda; K. Minami,"RIKEN Adv. Inst. for Comput. Sci., Kobe, Japan",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,13,23,"To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180063,abstract syntax tree differencing;application performance tuning;large-scale scientific computing;machine learning;semantic web,Arrays;Data mining;History;Kernel;Ontologies;Phylogeny;Tuning,data mining;optimisation;program compilers,EBT;combinatorial explosion;compiler options;data mining;evidence based performance tuning;large-scale scientific applications;pattern identification tool;performance tuning history;predicting effective optimization patterns;program parameters;program transformation;scientific applications;source code snapshots;syntactic structure;version control systems,,1,,29,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Identifying Software Process Management Challenges: Survey of Practitioners in a Large Global IT Company,M. Gupta; A. Sureka; S. Padmanabhuni; A. M. Asadullah,"Indraprastha Inst. of Inf. Technol., Delhi, India",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,346,356,"Process mining consists of mining event logs generated from business process execution supported by Information Systems (IS). Process mining of software repositories has diverse applications because vast data is generated during Software Development Life Cycle (SDLC) and archived in IS such as Version Control System (VCS), Peer Code Review (PCR) System, Issue Tracking System (ITS), and mail archives. There is need to explore its applications on different repositories to aid managers in process management. We conduct two phase surveys and interviews with managers in a large, global, IT company. The first survey and in-person interviews identify the process challenges encountered by them that can be addressed by novel applications of process mining. We filter, group and abstract responses formulating 30 generic problem statements. On the basis of process mining type, we classify identified problems to eight categories such as control analysis, organizational analysis, conformance analysis, and preventive analysis. The second survey asks distinct participants the importance of solving identified problems. We calculate proposed Net Importance Metric (NIM) using 1262 ratings from 43 participants. Combined analysis of NIM and first survey responses reveals that the problems mentioned by few practitioners in first survey are considered important by majority in the second survey. We elaborate on possible solutions and challenges for most frequent and important problems. We believe solving these validated problems will help managers in improving project quality and productivity.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180093,Process Mining;Qualitative Study;Software Development Life Cycle;Software Repositories,Companies;Data mining;Interviews;Measurement;Process control;Software,business process re-engineering;data mining;information systems;information technology;software process improvement,IS;NIM;SDLC;business process execution;event logs mining;global IT company;information systems;net importance metric;process mining;productivity;project quality;software development life cycle;software process management,,1,,32,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Investigating Code Review Practices in Defective Files: An Empirical Study of the Qt System,P. Thongtanunam; S. McIntosh; A. E. Hassan; H. Iida,"Nara Inst. of Sci. & Technol., Nara, Japan",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,168,179,"Software code review is a well-established software quality practice. Recently, Modern Code Review (MCR) has been widely adopted in both open source and proprietary projects. To evaluate the impact that characteristics of MCR practices have on software quality, this paper comparatively studies MCR practices in defective and clean source code files. We investigate defective files along two perspectives: 1) files that will eventually have defects (i.e., Future-defective files) and 2) files that have historically been defective (i.e., Risky files). Through an empirical study of 11,736 reviews of changes to 24,486 files from the Qt open source project, we find that both future-defective files and risky files tend to be reviewed less rigorously than their clean counterparts. We also find that the concerns addressed during the code reviews of both defective and clean files tend to enhance evolvability, i.e., Ease future maintenance (like documentation), rather than focus on functional issues (like incorrect program logic). Our findings suggest that although functionality concerns are rarely addressed during code review, the rigor of the reviewing process that is applied to a source code file throughout a development cycle shares a link with its defect proneness.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180077,Code Review;Software Quality,Data mining;Inspection;Maintenance engineering;Measurement;Software quality;Testing,program diagnostics;software development management;software maintenance;software quality,MCR;Qt system;code review practices;defect proneness;defective files;development cycle;functional issues;future maintenance;future-defective files;modern code review;risky files;software code review;software quality practice;source code file,,9,,40,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
Lessons Learned from Building and Deploying a Code Review Analytics Platform,C. Bird; T. Carnahan; M. Greiler,"Microsoft, Redmond, WA, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,191,201,"Tool-based code review is growing in popularity and has become a standard part of the development process at Mi-crosoft. Adoption of these tools makes it possible to mine data from code reviews and provide access to it. In this paper, we pre-sent an experience report for CodeFlow Analytics, a system that collects code review data, generates metrics from this data, and provides a number of ways for development teams to access the metrics and data. We discuss the design, design decisions and chal-lenges that we encountered when building CodeFlow Analytics. We contacted teams that used CodeFlow Analytics over the past two years and discuss what prompted them to use CodeFlow Ana-lytics, how they have used it, and what the impact has been. Fur-ther, we survey research that has been enabled by using the Code-Flow Analytics platform. We provide a series of lessons learned from this experience to help others embarking on a task of building an analytics platform in an enterprise setting.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180079,,Computational modeling;Data mining;Databases;Interviews;Measurement;Servers;Software,data mining;program diagnostics,CodeFlow analytics;Microsoft;analytics platform;code review analytics platform;data mining;design decisions;enterprise setting;tool-based code review,,4,,29,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Mining Android App Usages for Generating Actionable GUI-Based Execution Scenarios,M. Linares-VÕçsquez; M. White; C. Bernal-CÕçrdenas; K. Moran; D. Poshyvanyk,"Coll. of William & Mary, Williamsburg, VA, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,111,122,"GUI-based models extracted from Android app execution traces, events, or source code can be extremely useful for challenging tasks such as the generation of scenarios or test cases. However, extracting effective models can be an expensive process. Moreover, existing approaches for automatically deriving GUI-based models are not able to generate scenarios that include events which were not observed in execution (nor event) traces. In this paper, we address these and other major challenges in our novel hybrid approach, coined as MONKEYLAB. Our approach is based on the Record__êMine__êGenerate__êValidate framework, which relies on recording app usages that yield execution (event) traces, mining those event traces and generating execution scenarios using statistical language modeling, static and dynamic analyses, and validating the resulting scenarios using an interactive execution of the app on a real device. The framework aims at mining models capable of generating feasible and fully replayable (i.e., actionable) scenarios reflecting either natural user behavior or uncommon usages (e.g., corner cases) for a given app. We evaluated MONKEYLAB in a case study involving several medium-to-large open-source Android apps. Our results demonstrate that MONKEYLAB is able to mine GUI-based models that can be used to generate actionable execution scenarios for both natural and unnatural sequences of events on Google Nexus 7 tablets.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180072,GUI models;language models;mining execution traces and event logs;mobile apps,Analytical models;Androids;Graphical user interfaces;History;Humanoid robots;Testing;Vocabulary,Android (operating system);data mining;graphical user interfaces;program diagnostics;public domain software;source code (software);system monitoring,Android App usage mining;Android app execution traces;GUI-based model extraction;Google Nexus 7 tablets;MONKEYLAB;Record__êMine__êGenerate__êValidate framework;actionable GUI-based execution scenarios;dynamic analyses;medium-to-large open-source Android apps;natural user behavior;source code;static analyses;statistical language modeling,,12,,54,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
Mining Component Repositories for Installability Issues,P. Abate; R. Di Cosmo; L. Gesbert; F. Le Fessant; R. Treinen; S. Zacchiroli,,2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,24,33,"Component repositories play an increasingly relevant role in software life-cycle management, from software distribution to end-user, to deployment and upgrade management. Software components shipped via such repositories are equipped with rich metadata that describe their relationship (e.g., Dependencies and conflicts) with other components. In this practice paper we show how to use a tool, distcheck, that uses component metadata to identify all the components in a repository that cannot be installed (e.g., Due to unsatisfiable dependencies), provides detailed information to help developers understanding the cause of the problem, and fix it in the repository. We report about detailed analyses of several repositories: the Debian distribution, the OPAM package collection, and Drupal modules. In each case, distcheck is able to efficiently identify not installable components and provide valuable explanations of the issues. Our experience provides solid ground for generalizing the use of distcheck to other component repositories.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180064,component repositories;dependency solving;installability;quality assurance;software components;software packages,History;Libraries;Linux;Metadata;Quality assurance;Software;Testing,data mining;meta data;object-oriented programming;software management;software packages;software tools,Debian distribution;Drupal modules;OPAM package collection;component metadata;component repository mining;distcheck tool;end-users;installability issues;software components;software deployment;software distribution;software life-cycle management;software upgrade management,,4,,22,,,,16-17 May 2015,,IEEE,IEEE Conferences,,9
Mining Energy-Aware Commits,I. Moura; G. Pinto; F. Ebert; F. Castor,,2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,56,67,"Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of ""energy-aware commits"". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call ""energy-aware interfaces"", which are means for clients (e.g., Developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180067,,Computer bugs;Data mining;Energy consumption;Kernel;Libraries;Manuals,power aware computing;software engineering,Github;energy management;energy saving techniques;energy-aware commits;energy-aware interfaces;software development,,11,,90,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
Partitioning Composite Code Changes to Facilitate Code Review,Y. Tao; S. Kim,"Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Hong Kong, China",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,180,190,"Developers expend significant effort on reviewing source code changes. Hence, the comprehensibility of code changes directly affects development productivity. Our prior study has suggested that composite code changes, which mix multiple development issues together, are typically difficult to review. Unfortunately, our manual inspection of 453 open source code changes reveals a non-trivial occurrence (up to 29%) of such composite changes. In this paper, we propose a heuristic-based approach to automatically partition composite changes, such that each sub-change in the partition is more cohesive and self-contained. Our quantitative and qualitative evaluation results are promising in demonstrating the potential benefits of our approach for facilitating code review of composite code changes.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180078,,Cloning;Force;Inspection;Libraries;Manuals;Pattern matching;Software,public domain software;software maintenance;source code (software),code review;open source code;partitioning composite code changes;source code changes,,4,,35,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Recommending Posts concerning API Issues in Developer Q&amp;A Sites,W. Wang; H. Malik; M. W. Godfrey,"David R. Cheriton Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,224,234,"API design is known to be a challenging craft, as API designers must balance their elegant ideals against ""real-world"" concerns, such as utility, performance, backwards compatibility, and unforeseen emergent uses. However, to date, there is no principled method to collect or analyze API usability information that incorporates input from typical developers. In practice, developers often turn to Q&A websites such as stackoverflow.com (SO) when seeking expert advice on API use, the popularity of such sites has thus led to a very large volume of unstructured information that can be searched with diligence for answers to specific questions. The collected wisdom within such sites could, in principle, be of great help to API designers to better support developer needs, if only it could be collected, analyzed, and distilled for practical use. In this paper, we present a methodology that combines several techniques, including social network analysis and topic mining, to recommend SO posts that are likely to concern API design-related issues. To establish a comparison baseline, we introduce two more recommendation approaches: a reputation-based recommender and a random recommender. We have found that when applied to Q&A discussion of two popular mobile platforms, Android and iOS, our methodology achieves up to 93% accuracy and is more stable with its recommendations when compared to the two baseline techniques.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180082,API usability;application program interfaces;online Q&amp;A;recommendation systems;software ecosystems;stackoverflow,Androids;Communities;Computer bugs;Documentation;Humanoid robots;Usability,Web sites;application program interfaces;question answering (information retrieval);recommender systems;software engineering,API design;Q&A sites;SO posts;random recommender;reputation-based recommender;social network analysis;software developers;stackoverflow.com;topic mining,,3,,42,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Sameness: An Experiment in Code Search,L. Martie; A. v. d. Hoek,"Dept. of Inf. Irvine, Univ. of California, Irvine, Irvine, CA, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,76,87,"To date, most dedicated code search engines use ranking algorithms that focus only on the relevancy between the query and the results. In practice, this means that a developer may receive search results that are all drawn from the same project, all implement the same algorithm using the same external library, or all exhibit the same complexity or size, among other possibilities that are less than ideal. In this paper, we propose that code search engines should also locate both diverse and concise (brief but complete) sets of code results. We present four novel algorithms that use relevance, diversity, and conciseness in ranking code search results. To evaluate these algorithms and the value of diversity and conciseness in code search, twenty-one professional programmers were asked to compare pairs of top ten results produced by competing algorithms. We found that two of our new algorithms produce top ten results that are strongly preferred by the programmers.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180069,Code;concise;diversity;results;sameness;search;similarity;top ten,Complexity theory;Data mining;Java;Libraries;Search engines;Thesauri,search engines,code search engines;code search ranking algorithms,,2,,51,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
The App Sampling Problem for App Store Mining,W. Martin; M. Harman; Y. Jia; F. Sarro; Y. Zhang,"Dept. of Comput. Sci., Univ. Coll. London, London, UK",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,123,133,"Many papers on App Store Mining are susceptible to the App Sampling Problem, which exists when only a subset of apps are studied, resulting in potential sampling bias. We introduce the App Sampling Problem, and study its effects on sets of user review data. We investigate the effects of sampling bias, and techniques for its amelioration in App Store Mining and Analysis, where sampling bias is often unavoidable. We mine 106,891 requests from 2,729,103 user reviews and investigate the properties of apps and reviews from 3 different partitions: the sets with fully complete review data, partially complete review data, and no review data at all. We find that app metrics such as price, rating, and download rank are significantly different between the three completeness levels. We show that correlation analysis can find trends in the data that prevail across the partitions, offering one possible approach to App Store Analysis in the presence of sampling bias.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180073,App Sampling Problem;App Store Analysis;sample bias;software repository mining,Computational modeling;Correlation;Data mining;Google;Market research;Measurement;Web pages,data mining;smart phones,application metrics;application sampling problem;application store mining;completeness levels;correlation analysis;download rank metric;fully-complete review data;no-review data;partially-complete review data;price metric;rating metric;sampling bias;user review data,,22,,37,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
The Uniqueness of Changes: Characteristics and Applications,B. Ray; M. Nagappan; C. Bird; N. Nagappan; T. Zimmermann,"Univ. of California, Davis, Davis, MD, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,34,44,"Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. Adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways, they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180065,,Buildings;Cloning;Context;History;Linux;Software;Syntactics,software architecture,Linux kernel;project architecture;software development;software project history,,2,,40,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
Toward Deep Learning Software Repositories,M. White; C. Vendome; M. Linares-Vasquez; D. Poshyvanyk,"Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,334,345,"Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., Streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyper parameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180092,Software repositories;deep learning;machine learning;n-grams;neural networks;software language models,Computational modeling;Computer architecture;Context;Context modeling;Machine learning;Software;Training,Java;learning (artificial intelligence);natural language processing;program testing;project management;source code (software),Java project corpus;NLP techniques;SE community;SE task;automatic compositional representation learning;code suggestion;connectionist models;deep-learning software repositories;high-quality models;hyperparameters;lexically analyzed source code files;natural language processing;programming language;sequential data model;software artifact conceptualization;software corpora;software engineering community;software language modeling;software lexicon improvement;software token streaming,,11,,76,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
Unveiling Exception Handling Bug Hazards in Android Based on GitHub and Google Code Issues,R. Coelho; L. Almeida; G. Gousios; A. v. Deursen,"Fed. Univ. of Rio Grande do Norte, Natal, Brazil",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,134,145,"This paper reports on a study mining the exception stack traces included in 159,048 issues reported on Android projects hosted in GitHub (482 projects) and Google Code (157 projects). The goal of this study is to investigate whether stack trace information can reveal bug hazards related to exception handling code that may lead to a decrease in application robustness. Overall 6,005 exception stack traces were extracted, and subjected to source code and byte code analysis. The out-comes of this study include the identification of the following bug hazards: (i) unexpected cross-type exception wrappings (for instance, trying to handle an instance of Out Of Memory Error ""hidden"" in a checked exception) which can make the exception-related code more complex and negatively impact the application robustness, (ii) undocumented runtime exceptions thrown by both the Android platform and third party libraries, and (iii) undocumented checked exceptions thrown by the Android Platform. Such undocumented exceptions make it difficult, and most of the times infeasible for the client code to protect against ""unforeseen"" situations that may happen while calling third-party code. This study provides further insights on such bug hazards and the robustness threats they impose to Android apps as well as to other systems based on the Java exception model.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180074,,Androids;Google;Hazards;Humanoid robots;Java;Libraries;Robustness,Android (operating system);Internet;source code (software),Android platform;GitHub;Google code;Java exception model;byte code analysis;exception handling bug hazards;source code;third-party code,,3,,52,,,,16-17 May 2015,,IEEE,IEEE Conferences,,11
Using Developer-Interaction Trails to Triage Change Requests,M. B. Zanjani; H. Kagdi; C. Bird,"Dept. of Electr. Eng. & Comput. Sci., Wichita State Univ., Wichita, KS, USA",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,88,98,"The paper presents an approach, namely iHDev, to recommend developers who are most likely to implement incoming change requests. The basic premise of iHDev is that the developers who interacted with the source code relevant to a given change request are most likely to best assist with its resolution. A machine-learning technique is first used to locate source code entities relevant to the textual description of a given change request. Ihdev then mines interaction trails (i.e., Mylyn sessions) associated with these source code entities to recommend a ranked list of developers. Ihdev integrates the interaction trails in a unique way to perform its task, which was not investigated previously. An empirical study on open source systems Mylyn and Eclipse Project was conducted to assess the effectiveness of iHDev. A number of change requests were used in the evaluated bench-mark. Recall for top one to five recommended developers and Mean Reciprocal Rank (MRR) values are reported. Furthermore, a comparative study with two previous approaches that use commit histories and/or the source code authorship information for developer recommendation was performed. Results show that iHDev could provide a recall gain of up to 127.27% with equivalent or improved MRR values by up to 112.5%.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180070,,Computer bugs;Context;Data mining;History;Mathematical model;Software;XML,learning (artificial intelligence);program compilers;recommender systems;software maintenance,Eclipse Project;developer interaction trails;evaluated benchmark;iHDev;machine-learning technique;mean reciprocal rank;source code;source code authorship information;textual description,,2,,38,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
What Is the Gist? Understanding the Use of Public Gists on GitHub,W. Wang; G. Poo-CaamaÕ±o; E. Wilde; D. M. German,"Dept. of Comput. Sci., Univ. of Victoria, Victoria, BC, Canada",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,314,323,"GitHub is a popular source code hosting site which serves as a collaborative coding platform. The many features of GitHub have greatly facilitated developers' collaboration, communication, and coordination. Gists are one feature of GitHub, which defines them as ""a simple way to share snippets and pastes with others."" This three-part study explores how users are using Gists. The first part is a quantitative analysis of Gist metadata and contents. The second part investigates the information contained in a Gist: We sampled 750k users and their Gists (totalling 762k Gists), then manually categorized the contents of 398. The third part of the study investigates what users are saying Gists are for by reading the contents of web pages and twitter feeds. The results indicate that Gists are used by a small portion of GitHub users, and those that use them typically only have a few. We found that Gists are usually small and composed of a single file. However, Gists serve a wide variety of uses, from saving snippets of code, to creating reusable components for web pages.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180090,Gists;GitHub;Mining Software Repositories,Collaboration;HTML;Metadata;Software;Twitter;Web pages,meta data;software engineering;source code (software),Gist metadata;GitHub;collaborative coding platform;public Gists;source code hosting site,,4,,16,,,,16-17 May 2015,,IEEE,IEEE Conferences,,9
Will They Like This? Evaluating Code Contributions with Language Models,V. J. Hellendoorn; P. T. Devanbu; A. Bacchelli,"SORCERERS @ Software Eng. Res. Group, Delft Univ. of Technol., Delft, Netherlands",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories,20150806,2015,,,157,167,"Popular open-source software projects receive and review contributions from a diverse array of developers, many of whom have little to no prior involvement with the project. A recent survey reported that reviewers consider conformance to the project's code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively evaluate the existence and effects of this phenomenon. To this aim we use language models, which were shown to accurately capture stylistic aspects of code. We find that rejected change sets do contain code significantly less similar to the project than accepted ones, furthermore, the less similar change sets are more likely to be subject to thorough review. Armed with these results we further investigate whether new contributors learn to conform to the project style and find that experience is positively correlated with conformance to the project's code style.",2160-1852;21601852,Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3,10.1109/MSR.2015.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180076,code review;language model;pull request,Context;Context modeling;Data mining;Entropy;Java;Mathematical model;Software,public domain software;software engineering,Github;code contributions;language models;open-source software projects;software development,,4,,35,,,,16-17 May 2015,,IEEE,IEEE Conferences,,10
A Large-Scale Empirical Study on Self-Admitted Technical Debt,G. Bavota; B. Russo,"Free Univ. of Bozen-Bolzano, Bolzano, Italy",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,315,326,"Technical debt is a metaphor introduced by Cunningham to indicate ""not quite right code which we postpone making it right"". Examples of technical debt are code smells and bug hazards. Several techniques have been proposed to detect different types of technical debt. Among those, Potdar and Shihab defined heuristics to detect instances of self-admitted technical debt in code comments, and used them to perform an empirical study on five software systems to investigate the phenomenon. Still, very little is known about the diffusion and evolution of technical debt in software projects.This paper presents a differentiated replication of the work by Potdar and Shihab. We run a study across 159 software projects to investigate the diffusion and evolution of self-admitted technical debt and its relationship with software quality. The study required the mining of over 600K commits and 2 Billion comments as well as a qualitative analysis performed via open coding.Our main findings show that self-admitted technical debt (i) is diffused, with an average of 51 instances per system, (ii) is mostly represented by code (30%), defect, and requirement debt (20% each), (iii) increases over time due to the introduction of new instances that are not fixed by developers, and (iv) even when fixed, it survives long time (over 1,000 commits on average) in the system.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.040,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832911,,Couplings;Data mining;History;Java;Measurement;Software systems,project management;software maintenance;software process improvement;software quality,bug hazards;code comments;code smells;differentiated replication;open coding;self-admitted technical debt;software projects;software quality;software systems;technical debt evolution,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
"A Large-Scale Study on Repetitiveness, Containment, and Composability of Routines in Open-Source Projects",A. T. Nguyen; H. A. Nguyen; T. N. Nguyen,"ECpE Dept., Iowa State Univ., Ames, IA, USA",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,362,373,"Source code in software systems has been shown to have a good degree of repetitiveness at the lexical, syntactical, and API usage levels. This paper presents a large-scale study on the repetitiveness, containment, and composability of source code at the semantic level. We collected a large dataset consisting of 9,224 Java projects with 2.79M class files, 17.54M methods with 187M SLOCs. For each method in a project, we build the program dependency graph (PDG) to represent a routine, and compare PDGs with one another as well as the subgraphs within them. We found that within a project, 12.1% of the routines are repeated, and most of them repeat from 2-7 times. As entirety, the routines are quite project-specific with only 3.3% of them exactly repeating in 1-4 other projects with at most 8 times. We also found that 26.1% and 7.27% of the routines are contained in other routine(s), i.e., implemented as part of other routine(s) elsewhere within a project and in other projects, respectively. Except for trivial routines, their repetitiveness and containment is independent of their complexity. Defining a subroutine via a per-variable slicing subgraph in a PDG, we found that 14.3% of all routines have all of their subroutines repeated. A high percentage of subroutines in a routine can be found/reused elsewhere. We collected 8,764,971 unique subroutines (with 323,564 unique JDK subroutines) as basic units for code searching/synthesis. We also provide practical implications of our findings to automated tools.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.044,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832915,Code Reuse;Composability;Containment;Repetitiveness,Algorithms;Complexity theory;Java;Libraries;Semantics;Software;Syntactics,Java;graph theory;public domain software;source code (software);subroutines,API usage levels;JDK subroutines;Java projects;PDG;code searching;code synthesis;lexical usage levels;open-source projects;per-variable slicing subgraph;program dependency graph;semantic level;software systems;source code composability;source code containment;source code repetitiveness;syntactical usage levels,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
A Look at the Dynamics of the JavaScript Package Ecosystem,E. Wittern; P. Suter; S. Rajagopalan,,2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,351,361,"The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node.js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem's growth and activity, into conflicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832914,Javascript; software ecosystem analysis; Node.js,Data mining;Ecosystems;Measurement;Metadata;Software packages;Testing,Java;client-server systems;software libraries;software metrics;software packages,GitHub;JavaScript-based software package ecosystem;client-side JavaScript applications;node package manager;npm packages;package recommendation engines;server-side Node.js,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
Adressing Problems with External Validity of Repository Mining Studies Through a Smart Data Platform,F. Trautsch; S. Herbold; P. Makedonski; J. Grabowski,"Inst. for Comput. Sci., Georg-August-Univ. Gottingen, Gottingen, Germany",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,97,108,"Research in software repository mining has grown considerably the last decade. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the external validity of results. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created the prototype SmartSHARK that implements our approach. Using SmartSHARK, we collected data from several projects and created different analytic examples. Within this article, we present SmartSHARK and discuss our experiences regarding the use of SmartSHARK and the mentioned problems.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832890,smart data;software analytics;software mining,Data analysis;Data collection;Data mining;Prototypes;Social network services;Software;Sparks,cloud computing;data analysis;data mining;software engineering;storage management,SmartSHARK prototype;cloud-based platform;data analytics;data collection;external validity;result replication;smart data platform;software repository mining,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
An Empirical Study on the Practice of Maintaining Object-Relational Mapping Code in Java Systems,T. H. Chen; W. Shang; J. Yang; A. E. Hassan; M. W. Godfrey; M. Nasser; P. Flora,"Queen's Univ., Kingston, ON, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,165,176,"Databases have become one of the most important components in modern software systems. For example, web services, cloud computing systems, and online transaction processing systems all rely heavily on databases. To abstract the complexity of accessing a database, developers make use of Object-Relational Mapping (ORM) frameworks. ORM frameworks provide an abstraction layer between the application logic and the underlying database. Such abstraction layer automatically maps objects in Object-Oriented Languages to database records, which significantly reduces the amount of boilerplate code that needs to be written. Despite the advantages of using ORM frameworks, we observe several difficulties in maintaining ORM code (i.e., code that makes use of ORM frameworks) when cooperating with our industrial partner. After conducting studies on other open source systems, we find that such difficulties are common in other Java systems. Our study finds that i) ORM cannot completely encapsulate database accesses in objects or abstract the underlying database technology, thus may cause ORM code changes more scattered; ii) ORM code changes are more frequent than regular code, but there is a lack of tools that help developers verify ORM code at compilation time; iii) we find that changes to ORM code are more commonly due to performance or security reasons; however, traditional static code analyzers need to be extended to capture the peculiarities of ORM code in order to detect such problems. Our study highlights the hidden maintenance costs of using ORM frameworks, and provides some initial insights about potential approaches to help maintain ORM code. Future studies should carefully examine ORM code, especially given the rising use of ORM in modern software systems.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.026,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832897,,Data mining;Databases;Java;Maintenance engineering;Portals;Software systems,Java;object-oriented databases;object-oriented programming;program diagnostics;public domain software;source code (software),Java systems;ORM code verification;abstraction layer;application logic;boilerplate code;database access encapsulation;database records;object-oriented languages;object-relational mapping;object-relational mapping code;open source systems;software systems;static code analyzers,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Automatic Clustering of Code Changes,P. Kreutzer; G. Dotzler; M. Ring; B. M. Eskofier; M. Philippsen,"Program. Syst. Group, Friedrich Alexander Univ. Erlangen-Nurnberg, Erlangen, Germany",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,61,72,"Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832887,Clustering;Code Changes;Software Repositories,Algorithm design and analysis;Bars;Clustering algorithms;Computer bugs;Context;Measurement;Software,pattern clustering;software engineering;source code (software),LASE;code changes detection;open source projects;recommendation system;self-tuning clustering algorithms,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Cold-Start Software Analytics,J. Guo; M. Rahimi; J. Cleland-Huang; A. Rasin; J. H. Hayes; M. Vierhauser,"Sch. of Comput., DePaul Univ., Chicago, IL, USA",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,142,153,"Software project artifacts such as source code, requirements, and change logs represent a gold-mine of actionable information. As a result, software analytic solutions have been developed to mine repositories and answer questions such as ""who is the expert?,' ""which classes are fault prone?,' or even ""who are the domain experts for these fault-prone classes?' Analytics often require training and configuring in order to maximize performance within the context of each project. A cold-start problem exists when a function is applied within a project context without first configuring the analytic functions on project-specific data. This scenario exists because of the non-trivial effort necessary to instrument a project environment with candidate tools and algorithms and to empirically evaluate alternate configurations. We address the cold-start problem by comparatively evaluating 'best-of-breed' and 'profile-driven' solutions, both of which reuse known configurations in new project contexts. We describe and evaluate our approach against 20 project datasets for the three analytic areas of artifact connectivity, fault-prediction, and finding the expert, and show that the best-of-breed approach outperformed the profile-driven approach in all three areas; however, while it delivered acceptable results for artifact connectivity and find the expert, both techniques underperformed for cold-start fault prediction.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.024,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832895,Cold-start;Configuration;Software Analytics,Analytical models;Context;Measurement;Predictive models;Software;Software engineering;Training,data mining;project management;software fault tolerance;software libraries,artifact connectivity;best-of-breed solutions;cold-start software analytics;fault-prediction;profile-driven solutions;project datasets;project environment;software analytic solutions;software project artifacts,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Comparing Repositories Visually with RepoGrams,D. Rozenberg; I. Beschastnikh; F. Kosmale; V. Poser; H. Becker; M. Palyart; G. C. Murphy,"Univ. of British Columbia, Vancouver, BC, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,109,120,"The availability of open source software projects has created an enormous opportunity for software engineering research. However, this availability requires that researchers judiciously select an appropriate set of evaluation targets and properly document this rationale. After all, the choice of targets may have a significant effect on evaluation. We developed a tool called RepoGrams to support researchers in qualitatively comparing and contrasting software projects over time using a set of software metrics. RepoGrams uses an extensible, metrics-based, visualization model that can be adapted to a variety of analyses. Through a user study of 14 software engineering researchers we found that RepoGrams can assist researchers in filtering candidate software projects and make more reasoned choices of targets for their evaluations. The tool is open source and is available online: http://repograms.net/.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832891,Evaluation target selection;Software evolution;Software visualization,Adaptation models;Analytical models;Color;Measurement;Servers;Software;Visualization,data visualisation;project management;public domain software;software management;software metrics,RepoGrams;extensible metrics-based visualization model;open source software projects;software engineering research;software metrics,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Does Your Configuration Code Smell?,T. Sharma; M. Fragkoulis; D. Spinellis,"Dept. of Manage. Sci. & Technol., Athens Univ. of Econ. & Bus., Athens, Greece",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,189,200,"Infrastructure as Code (IaC) is the practice of specifying computing system configurations through code, and managing them through traditional software engineering methods. The wide adoption of configuration management and increasing size and complexity of the associated code, prompt for assessing, maintaining, and improving the configuration code's quality. In this context, traditional software engineering knowledge and best practices associated with code quality management can be leveraged to assess and manage configuration code quality. We propose a catalog of 13 implementation and 11 design configuration smells, where each smell violates recommended best practices for configuration code. We analyzed 4,621 Puppet repositories containing 8.9 million lines of code and detected the cataloged implementation and design configuration smells. Our analysis reveals that the design configuration smells show 9% higher average co-occurrence among themselves than the implementation configuration smells. We also observed that configuration smells belonging to a smell category tend to co-occur with configuration smells belonging to another smell category when correlation is computed by volume of identified smells. Finally, design configuration smell density shows negative correlation whereas implementation configuration smell density exhibits no correlation with the size of a configuration management system.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.028,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832899,Code quality;Configuration smells;Infrastructure as Code;Maintainability;Technical debt,Best practices;Context;Correlation;Data mining;Production;Software;Software engineering,software maintenance;software quality,IaC;code quality management;computing system configurations;configuration code;configuration management system;infrastructure as code;puppet repositories;smell category;software engineering knowledge,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Domain-Specific Cross-Language Relevant Question Retrieval,B. Xu; Z. Xing; X. Xia; D. Lo; Q. Wang; S. Li,"Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,413,424,"In software development process, developers often seek solutions to the technical problems they encounter by searching relevant questions on Q&A sites. When developers fail to find solutions on Q&A sites in their native language (e.g., Chinese), they could translate their query and search on the Q&A sites in another language (e.g., English). However, developers who are non-native English speakers often are not comfortable to ask or search questions in English, as they do not know the proper translation of the Chinese technical words into the English technical words. Furthermore, the process of manually formulating cross-language queries and determining the weight of query words is a tedious and time-consuming process. For the purpose of helping Chinese developers take advantage of the rich knowledge base of the English version of Stack Overflow and simplify the retrieval process, we propose an automated crosslanguage relevant question retrieval (CLRQR) system to retrieve relevant English questions on Stack Overflow for a given Chinese question. Our CLRQR system first extracts essential information (both Chinese and English) from the title and description of the input Chinese question, then performs domain-specific translation of the essential Chinese information into English, and formulates a query with highest-scored English words for retrieving relevant questions in a repository of 684,599 Java questions in English from Stack Overflow. To evaluate the performance of our proposed approach, we also propose four online retrieval approaches as baselines. We randomly select 80 Java questions in SegmentFault and V2EX (two Chinese Q&A websites for computer programming) as the query Chinese questions. Each approach returns top-10 most relevant questions for a given Chinese question. We invite 5 users to evaluate the relevance of the retrieved English questions. The experiment results show that CLRQR system outperforms the four baseline approaches, and the statistical tes- s show the improvements are significant.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.049,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832920,Cross-Language Question Retrieval;Domain-Specific Translation,Algorithm design and analysis;Computers;Data mining;Frequency-domain analysis;Java;Software;Vocabulary,natural language processing;query processing;question answering (information retrieval);software engineering;word processing,CLRQR system;Chinese developers;Chinese technical words;English questions;English technical words;SegmentFault;V2EX;automated cross language relevant question retrieval;cross-language queries;domain-specific cross-language relevant question retrieval;domain-specific translation;essential Chinese information;native language;non-native English speakers;query Chinese questions;query words;software development process;stack overflow;statistical tests;time-consuming process,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Externalization of Software Behavior by the Mining of Norms,D. Avery; H. K. Dam; B. T. R. Savarimuthu; A. Ghose,"Univ. of Wollongong, Wollongong, NSW, Australia",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,223,234,"Open Source Software Development (OSSD) often suffers from conflicting views and actions due to the perceived flat and open ecology of an open source community. This often manifests itself as a lack of codified knowledge that is easily accessible for community members. How decisions are made and expectations of a software system are often described in detail through the many forms of social communications that take place within a community. These social interactions form norms which are influential in dictating what behaviors are expected in a community and of the system. In this paper, we provide a tool which mines these social interactions (in the form of bug reports) and extract norms of the system, externalizing this information into a codified form that allows others within the community to be aware of without having witnessed the social interactions.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.031,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832902,,Australia;Computer bugs;Context;Contracts;Data mining;Open source software,data mining;public domain software;software engineering,information externalization;norm mining;open source community;open source software development;social communications;social interactions;software behavior;software system,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Feature Toggles: Practitioner Practices and a Case Study,M. T. Rahman; L. P. Querel; P. C. Rigby; B. Adams,"Concordia Univ., Montreal, QC, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,201,211,"Continuous delivery and rapid releases have led to innovative techniques for integrating new features and bug fixes into a new release faster. To reduce the probability of integration conflicts, major software companies, including Google, Facebook and Netflix, use feature toggles to incrementally integrate and test new features instead of integrating the feature only when it's ready. Even after release, feature toggles allow operations managers to quickly disable a new feature that is behaving erratically or to enable certain features only for certain groups of customers. Since literature on feature toggles is surprisingly slim, this paper tries to understand the prevalence and impact of feature toggles. First, we conducted a quantitative analysis of feature toggle usage across 39 releases of Google Chrome (spanning five years of release history). Then, we studied the technical debt involved with feature toggles by mining a spreadsheet used by Google developers for feature toggle maintenance. Finally, we performed thematic analysis of videos and blog posts of release engineers at major software companies in order to further understand the strengths and drawbacks of feature toggles in practice. We also validated our findings with four Google developers. We find that toggles can reconcile rapid releases with long-term feature development and allow flexible control over which features to deploy. However they also introduce technical debt and additional maintenance for developers.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.029,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832900,feature flag;feature management;feature switch;feature toggle;software engineering,Browsers;Companies;Facebook;Google;History;Maintenance engineering;Software,DP industry;Internet;data mining;program debugging;software maintenance;software management;spreadsheet programs,Facebook;Google;Netflix;bug fixes;feature toggle maintenance;feature toggles;practitioner practices;software companies;spreadsheet mining,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
FEVER: Extracting Feature-oriented Changes from Commits,N. Dintzner; A. v. Deursen; M. Pinzger,"Software Eng. Res. Group, Delft Univ. of Technol., Delft, Netherlands",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,85,96,"The study of the evolution of highly configurable systems requires a thorough understanding of thee core ingredients of such systems: (1) the underlying variability model; (2) the assets that together implement the configurable features; and (3) the mapping from variable features to actual assets. Unfortunately, to date no systematic way to obtain such information at a sufficiently fine grained level exists. To remedy this problem we propose FEVER and its instantiation for the Linux kernel. FEVER extracts detailed information on changes in variability models (KConfig files), assets (preprocessor based C code), and mappings (Make- files). We describe how FEVER works, and apply it to several releases of the Linux kernel. Our evaluation on 300 randomly selected commits, from two different releases, shows our results are accurate in 82.6% of the commits. Furthermore, we illustrate how the populated FEVER graph database thus obtained can be used in typical Linux engineering tasks.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832889,Feature interaction; Software design engineering;,Context;Data mining;Databases;Feature extraction;Kernel;Linux;Software engineering,Linux;feature extraction;operating system kernels;software engineering,FEVER graph database;KConfig files;Linux engineering;Linux kernel;Make-files;feature-oriented change extraction;highly configurable systems;preprocessor based C code;variability model,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
From Query to Usable Code: An Analysis of Stack Overflow Code Snippets,D. Yang; A. Hussain; C. V. Lopes,"Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,391,401,"Enriched by natural language texts, Stack Overflow code snippets are an invaluable code-centric knowledge base of small units of source code. Besides being useful for software developers, these annotated snippets can potentially serve as the basis for automated tools that provide working code solutions to specific natural language queries. With the goal of developing automated tools with the Stack Overflow snippets and surrounding text, this paper investigates the following questions: (1) How usable are the Stack Overflow code snippets? and (2) When using text search engines for matching on the natural language questions and answers around the snippets, what percentage of the top results contain usable code snippets? A total of 3M code snippets are analyzed across four languages: C#, Java, JavaScript, and Python. Python and JavaScript proved to be the languages for which the most code snippets are usable. Conversely, Java and C# proved to be the languages with the lowest usability rate. Further qualitative analysis on usable Python snippets shows the characteristics of the answers that solve the original question. Finally, we use Google search to investigate the alignment of usability and the natural language annotations around code snippets, and explore how to make snippets in Stack Overflow an adequate base for future automatic program generation.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832918,automatic program generation;code mining,C# languages;Google;Java;Natural languages;Syntactics;Usability;Web search,C# language;Java;authoring languages;natural language processing;query processing;question answering (information retrieval);search engines;source code (software),C# language;Google search;Java language;JavaScript language;Python language;Stack Overflow code snippets;annotated snippets;automatic program generation;code-centric knowledge base;natural language annotations;natural language queries;natural language questions and answers;source code;text search engines,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
Got Technical Debt? Surfacing Elusive Technical Debt in Issue Trackers,S. Bellomo; R. L. Nord; I. Ozkaya; M. Popeck,"Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,327,338,"Concretely communicating technical debt and its consequences is of common interest to both researchers and software engineers. In the absence of validated tools and techniques to achieve this goal with repeatable results, developers resort to ad hoc practices. Most commonly they report using issue trackers or their existing backlog management practices to capture and track technical debt. In a manual examination of 1,264 issues from four issue trackers from open source industry and government projects, we identified 109 examples of technical debt. Our study reveals that technical debt and its related concepts have entered the vernacular of developers as they discuss development tasks through issue trackers. Even when issues are not explicitly tagged as technical debt, it is possible to identify technical debt items in these issue trackers using a categorization method we developed. We use our results and data to motivate an improved definition and an approach to explicitly report technical debt in issue trackers.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.041,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832912,Technical debt;issue tracking;software anomalies;software design;text categorization,Chromium;Computer bugs;Data mining;Government;Software;Software engineering,public domain software;software development management,ad hoc practices;backlog management practices;categorization method;development tasks;elusive technical debt;government projects;issue trackers;open source industry;software engineers,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
GreenOracle: Estimating Software Energy Consumption with Energy Measurement Corpora,S. A. Chowdhury; A. Hindle,"Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,49,60,"Software energy consumption is a relatively new concern for mobile application developers. Poor energy performance can harm adoption and sales of applications. Unfortunately for the developers, the measurement of software energy con-sumption is expensive in terms of hardware and difficult in terms of expertise. Many prior models of software energy consumption assume that developers can use hardware instrumentation and thus cannot evaluate software runningwithin emulators or virtual machines. Some prior modelsrequire actual energy measurements from the previous versions of applications in order to model the energy consumption of later versions of the same application.In this paper, we take a big-data approach to software energy consumption and present a model that can estimate software energy consumption mostly within 10% error (in joules) and does not require the developer to train on energy measurements of their own applications. This model leverages a big-data approach whereby a collection of prior applications' energy measurements allows us to train, trans-mit, and apply the model to estimate any foreign application's energy consumption for a test run. Our model is based on the dynamic traces of system calls and CPU utilization.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832886,Android applications;GreenMining;Mining software repositories;Modeling energy consumption;Software energy;improving energy consumption,Androids;Batteries;Data mining;Energy consumption;Energy measurement;Humanoid robots;Software,Big Data;energy consumption;mobile computing;power aware computing;resource allocation;virtual machines,CPU utilization;GreenOracle;big-data approach;emulators;energy measurement corpora;energy performance;foreign application;software energy consumption;system calls;virtual machines,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Grouping Android Tag Synonyms on Stack Overflow,S. Beyer; M. Pinzger,"Software Eng. Res. Group, Univ. of Klagenfurt, Klagenfurt, Austria",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,430,440,"On Stack Overflow, more than 38,000 diverse tags are used to classify posts. The Stack Overflow community provides tag synonyms to reduce the number of tags that have the same or similar meaning. In our previous research, we used those synonym pairs to derive a number of strategies to create tag synonyms automatically. In this work, we continue this line of research and present an approach to group tag synonyms to meaningful topics. We represent our synonyms as directed, weighted graphs, and investigate several graph community detection algorithms to build meaningful groups of tags, also called tag communities. We apply our approach to the tags obtained from Android-related Stack Overflow posts and evaluate the resulting tag communities quantitatively with various community metrics. In addition, we evaluate our approach qualitatively through a manual inspection and comparison of a random sample of tag communities. Our results show that we can cluster the Android tags to 2,481 meaningful tag communities. We also show how these tag communities can be used to derive trends of topics of Android-related questions on Stack Overflow.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.051,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832922,Android;Clustering;Stack Overflow;Tags,Androids;Detection algorithms;Humanoid robots;Market research;Software engineering;Tagging;Terminology,Android (operating system);directed graphs;pattern clustering,Android tag synonym grouping;Stack Overflow;directed-weighted graphs;graph community detection algorithm;qualitative analysis;tag community metrics,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
How Android App Developers Manage Power Consumption? - An Empirical Study by Mining Power Management Commits,L. Bao; D. Lo; X. Xia; X. Wang; C. Tian,"Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,37,48,"As Android platform becomes more and more popular, a large amount of Android applications have been developed. When developers design and implement Android applications, power consumption management is an important factor to consider since it affects the usability of the applications. Thus, it is important to help developers adopt proper strategies to manage power consumption. Interestingly, today, there is a large number of Android application repositories made publicly available in sites such as GitHub. These repositories can be mined to help crystalize common power management activities that developers do. These in turn can be used to help other developers to perform similar tasks to improve their own Android applications.In this paper, we present an empirical study of power management commits in Android applications. Our study extends that of Moura et al. who perform an empirical studyon energy aware commits; however they do not focus on Android applications and only a few of the commits that they study come from Android applications. Android applications are often different from other applications (e.g., those running on a server) due to the issue of limited battery life and the use of specialized APIs. As subjects of our empirical study, we obtain a list of open source Android applications from F-Droid and crawl their commits from Github. We get 468 power management commits after we filter the commits using a set of keywords and by performing manual analysis. These 468 power management commits are from 154 different Android applications and belong to 15 different application categories. Furthermore, we use open card sort to categorize these power management commits and we obtain 6 groups which correspond to different power management activities. Our study also reveals that for different kinds of Android application (e.g., Games, Connectivity, Navigation, etc.), the dominant power management activities differ.For example, the percentageof power man- gement commits belonging to Power Adaptation activity is larger for Navigation applications than those belonging to other categories.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832885,Empirical Study;Mining Software Repository;Power Consumption;Power Management,Androids;Batteries;Humanoid robots;Internet;Power demand;Smart phones;Software,Android (operating system);application program interfaces;navigation;power aware computing;power consumption,API;Android applications;F-Droid;Github;battery life;mining power management;navigation applications;open card sort;power consumption management,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
How the R Community Creates and Curates Knowledge: A Comparative Study of Stack Overflow and Mailing Lists,A. Zagalsky; C. G. Teshima; D. M. German; M. A. Storey; G. Poo-CaamaÕ±o,"Univ. of Victoria, Victoria, BC, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,441,451,"One of the many effects of social media in software development is the flourishing of very large communities of practice where members share a common interest, such as programming languages, frameworks, and tools. These communities of practice use many different communication channels but little is known about how these communities create, share, and curate knowledge using such channels. In this paper, we report a qualitative study of how one community of practice-the R software development community-creates and curates knowledge associated with questions and answers (Q&A) in two of its main communication channels: the R-tag in Stack Overflow and the R-users mailing list. The results reveal that knowledge is created and curated in two main forms: participatory, where multiple members explicitly collaborate to build knowledge, and crowdsourced, where individuals work independently of each other. The contribution of this paper is a characterization of knowledge types that are exchanged by these communities of practice, including a description of the reasons why members choose one channel over the other. Finally, this paper enumerates a set of recommendations to assist practitioners in the use of multiple channels for Q&A.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.052,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832923,Case study;Knowledge curation,Communication channels;Data mining;Electronic mail;Programming;Social network services;Software,Internet;crowdsourcing;groupware;software engineering,R software development community;R-tag;R-users mailing list;Stack Overflow;crowdsourcing;knowledge creation;knowledge curation;social media,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
Improving Change Recommendation using Aggregated Association Rules,T. Rolfsnes; L. Moonen; S. D. Alesio; R. Behjati; D. Binkley,"Simula Res. Lab., Oslo, Norway",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,73,84,"Past research has proposed association rule mining as a means to uncover the evolutionary coupling from a system's change history. These couplings have various applications, such as improving system decomposition and recommending related changes during development. The strength of the coupling can be characterized using a variety of interestingness measures. Existing recommendation engines typically use only the rule with the highest interestingness value in situations where more than one rule applies. In contrast, we argue that multiple applicable rules indicate increased evidence, and hypothesize that the aggregation of such rules can be exploited to provide more accurate recommendations.To investigate this hypothesis we conduct an empirical study on the change histories of two large industrial systems and four large open source systems. As aggregators we adopt three cumulative gain functions from information retrieval. The experiments evaluate the three using 39 different rule interestingness measures. The results show that aggregation provides a significant impact on most measure's value and, furthermore, leads to a significant improvement in the resulting recommendation.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832888,,Aggregates;Algorithm design and analysis;Context;Couplings;Data mining;History;Software,data mining;information filtering;public domain software;recommender systems;software process improvement,aggregated association rules;association rule mining;change recommendation;cumulative gain functions;evolutionary coupling;industrial systems;information retrieval;large open source systems;recommendation engines;rule interestingness measures,,3,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Inter-app Communication in Android: Developer Challenges,W. Ahmad; C. KÕ_stner; J. Sunshine; J. Aldrich,"Carnegie Mellon Univ., Pittsburgh, PA, USA",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,177,188,"The Android platform is designed to support mutually un-trusted third-party apps, which run as isolated processes but may interact via platform-controlled mechanisms, called Intents. Interactions among third-party apps are intended and can contribute to a rich user experience, for example, the ability to share pictures from one app with another. The Android platform presents an interesting point in a design space of module systems that is biased toward isolation, extensibility, and untrusted contributions. The Intent mechanism essentially provides message channels among modules, in which the set of message types is extensible. However, the module system has design limitations including the lack of consistent mechanisms to document message types, very limited checking that a message conforms to its specifications, the inability to explicitly declare dependencies on other modules, and the lack of checks for backward compatibility as message types evolve over time. In order to understand the degree to which these design limitations result in real issues, we studied a broad corpus of apps and cross-validated our results against app documentation and Android support forums. Our findings suggest that design limitations do in- deed cause development problems. Based on our results, we outline further research questions and propose possible mitigation strategies.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.027,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832898,,Androids;Context;Data mining;Documentation;Ecosystems;Humanoid robots;Software,Android (operating system);human factors,Android platform;Intents;inter-app communication;message channels;mutually untrusted third-party apps;platform-controlled mechanisms;user experience,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Interactive Exploration of Developer Interaction Traces using a Hidden Markov Model,K. Damevski; H. Chen; D. Shepherd; L. Pollock,"Virginia Commonwealth Univ., Richmond, VA, USA",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,126,136,"Using IDE usage data to analyze the behavior of software developers in the field, during the course of their daily work, can lend support to (or dispute) laboratory studies of devel- opers. This paper describes a technique that leverages Hidden Markov Models (HMMs) as a means of mining high-level developer behavior from low-level IDE interaction traces of many developers in the field. HMMs use dual stochastic processes to model higher-level hidden behavior using observable input sequences of events. We propose an interactive approach of mining interpretable HMMs, based on guiding a human expert in building a high quality HMM in an iterative, one state at a time, manner. The final result is a model that is both representative of the field data and captures the field phenomena of interest. We apply our HMM construction approach to study debugging behavior, using a large IDE interaction dataset collected from nearly 200 developers at ABB, Inc. Our results highlight the different modes and constituent actions in debugging, exhibited by the developers in our dataset.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832893,field studies; IDE usage data; hidden-markov model,Analytical models;Buildings;Data mining;Data models;Debugging;Hidden Markov models;Software,data mining;hidden Markov models;interactive systems;program debugging;software engineering;stochastic processes,IDE usage data;debugging behavior;developer interaction traces;dual stochastic processes;hidden Markov model;high quality HMM;high-level developer behavior mining;high-level hidden behavior;interactive exploration;interpretable HMM mining;large IDE interaction dataset;low-level IDE interaction traces;observable input sequences,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
Logging Library Migrations: A Case Study for the Apache Software Foundation Projects,S. Kabinna; C. P. Bezemer; W. Shang; A. E. Hassan,"Software Anal. & Intell. Lab., Queen's Univ., Kingston, ON, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,154,164,"Developers leverage logs for debugging, performance monitoring and load testing. The increased dependence on logs has lead to the development of numerous logging libraries which help developers in logging their code. As new libraries emerge and current ones evolve, projects often migrate from an older library to another one.In this paper we study logging library migrations within Apache Software Foundation (ASF) projects. From our manual analysis of JIRA issues, we find that 33 out of 223 (i.e., 14%) ASF projects have undergone at least one logging library migration. We find that the five main drivers for logging library migration are: 1) to increase flexibility (i.e., the ability to use different logging libraries within a project) 2) to improve performance, 3) to reduce effort spent on code maintenance, 4) to reduce dependence on other libraries and 5) to obtain specific features from the new logging library. We find that over 70% of the migrated projects encounter on average two post-migration bugs due to the new logging library. Furthermore, our findings suggest that performance (traditionally one of the primary drivers for migrations) is rarely improved after a migration.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.025,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832896,ASF;apache software foundation migration;logging library migration;logging migration,Data mining;Software,program debugging;program testing;software libraries;software maintenance;system monitoring,Apache Software Foundation projects;JIRA;code maintenance;debugging;load testing;logging library migrations;manual analysis;performance monitoring;post-migration bugs,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
Mining Duplicate Questions of Stack Overflow,M. Ahasanuzzaman; M. Asaduzzaman; C. K. Roy; K. A. Schneider,,2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,402,412,"Stack Overflow is a popular question answering site that is focused on programming problems. Despite efforts to prevent asking questions that have already been answered, the site contains duplicate questions. This may cause developers to unnecessarily wait for a question to be answered when it has already been asked and answered. The site currently depends on its moderators and users with high reputation to manually mark those questions as duplicates, which not only results in delayed responses but also requires additional efforts. In this paper, we first perform a manual investigation to understand why users submit duplicate questions in Stack Overflow. Based on our manual investigation we propose a classification technique that uses a number of carefully chosen features to identify duplicate questions. Evaluation using a large number of questions shows that our technique can detect duplicate questions with reasonable accuracy. We also compare our technique with DupPredictor, a state-of-the-art technique for detecting duplicate questions, and we found that our proposed technique has a better recall rate than that technique.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.048,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832919,Stack Overflow;discriminative classifier;duplicate questions,Computer bugs;Data mining;Databases;Knowledge discovery;Manuals;Mars;Software,classification;data mining;question answering (information retrieval),Stack Overflow;classification technique;duplicate question identification;duplicate question mining;programming problems;question answering site,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
Mining Performance Regression Inducing Code Changes in Evolving Software,Q. Luo; D. Poshyvanyk; M. Grechanik,,2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,25,36,"During software evolution, the source code of a system frequently changes due to bug fixes or new feature requests. Some of these changes may accidentally degrade performance of a newly released software version. A notable problem of regression testing is how to find problematic changes (out of a large number of committed changes) that may be responsible for performance regressions under certain test inputs. We propose a novel recommendation system, coined as PERFIMPACT, for automatically identifying code changes that may potentially be responsible for performance regressions using a combination of search-based input profiling and change impact analysis techniques. PERFIMPACT independently sends the same input values to two releases of the application under test, and uses a genetic algorithm to mine execution traces and explore a large space of input value combinations to find specific inputs that take longer time to execute in a new release. Since these input values are likely to expose performance regressions, PERFIMPACT automatically mines the corresponding execution traces to evaluate the impact of each code change on the performance and ranks the changes based on their estimated contribution to performance regressions. We implemented PERFIMPACT and evaluated it on different releases of two open-source web applications. The results demonstrate that PERFIMPACT effectively detects input value combinations to expose performance regressions and mines the code changes are likely to be responsible for these performance regressions.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832884,Performance regression testing;change impact analysis;genetic algorithms;mining execution traces,Data mining;Degradation;Fuel processing industries;Genetic algorithms;Software;Stakeholders;Testing,configuration management;genetic algorithms;program debugging;program diagnostics;program testing;regression analysis;software maintenance;software performance evaluation;source code (software),PERFIMPACT;bug fixes;change impact analysis;code changes identification;evolving software;execution traces mining;genetic algorithm;performance regression mining;recommendation system;regression testing;search-based input profiling;software evolution;software version;source code,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Mining Test Repositories for Automatic Detection of UI Performance Regressions in Android Apps,M. Gomez; R. Rouvoy; B. Adams; L. Seinturier,"Inria, Univ. of Lille, Lille, France",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,13,24,"The reputation of a mobile app vendor is crucial to survive amongst the ever increasing competition. However this reputation largely depends on the quality of the apps, both functional and non-functional. One major non-functional requirement of mobile apps is to guarantee smooth UI interactions, since choppy scrolling or navigation caused by performance problems on a mobile device's limited hardware resources, is highly annoying for end-users. The main research challenge of automatically identifying UI performance problems on mobile devices is that the performance of an app highly varies depending on its context-i.e., the hardware and software configurations on which it runs. This paper presents DUNE, an approach to automatically detect UI performance degradations in Android apps while taking into account context differences. First, DUNE builds an ensemble model of the UI performance metrics of an app from a repository of historical test runs that are known to be acceptable, for different configurations of context. Then, DUNE uses this model to flag UI performance deviations (regressions and optimizations) in new test runs. We empirically evaluate DUNE on real UI performance defects reported in two Android apps, and one manually injected defect in a third app. We demonstrate that this toolset can be successfully used to spot UI performance regressions at a fine granularity.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832883,,Androids;Context;Degradation;Humanoid robots;Mobile communication;Performance evaluation,data mining;mobile computing;software performance evaluation;user interfaces,Android Apps;Android apps;DUNE;UI performance degradations;UI performance metrics;UI performance regressions;test repositories mining,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
"Mining Valence, Arousal, and Dominance - Possibilities for Detecting Burnout and Productivity?",M. Mantyla; B. Adams; G. Destefanis; D. Graziotin; M. Ortu,"M3S, Univ. of Oulu, Oulu, Finland",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,247,258,"Similar to other industries, the software engineering domain is plagued by psychological diseases such as burnout, which lead developers to lose interest, exhibit lower activity and/or feel powerless. Prevention is essential for such diseases, which in turn requires early identification of symptoms. The emotional dimensions of Valence, Arousal and Dominance (VAD) are able to derive a person's interest (attraction), level of activation and perceived level of control for a particular situation from textual communication, such as emails. As an initial step towards identifying symptoms of productivity loss in software engineering, this paper explores the VAD metrics and their properties on 700,000 Jira issue reports containing over 2,000,000 comments, since issue reports keep track of a developer's progress on addressing bugs or new features. Using a general-purpose lexicon of 14,000 English words with known VAD scores, our results show that issue reports of different type (e.g., Feature Request vs. Bug) have a fair variation of Valence, while increase in issue priority (e.g., from Minor to Critical) typically increases Arousal. Furthermore, we show that as an issue's resolution time increases, so does the arousal of the individual the issue is assigned to. Finally, the resolution of an issue increases valence, especially for the issue Reporter and for quickly addressed issues. The existence ofsuch relations between VAD and issue report activities shows promise that text mining in the future could offer an alternative way for work health assessment surveys.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832904,,Computer bugs;Data mining;Productivity;Psychology;Sentiment analysis;Software;Software engineering,data mining;diseases;human factors;productivity;program debugging;psychology;software engineering,"VAD emotional dimensions;VAD metrics;bugs;burnout;issue report activities;productivity loss;psychological diseases;software engineering;text mining;textual communication;valence, arousal and dominance emotional dimensions;valence, arousal and dominance mining;work health assessment surveys",,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
On Mining Crowd-Based Speech Documentation,P. Moslehi; B. Adams; J. Rilling,"Concordia Univ., Montreal, QC, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,259,268,"Despite the globalization of software development, relevant documentation of a project, such as requirements and design documents, often still is missing, incomplete or outdated. However, parts of that documentation can be found outside the project, where it is fragmented across hundreds of textual web documents like blog posts, email messages and forum posts, as well as multimedia documents such as screencasts and podcasts. Since dissecting and filtering multimedia information based on its relevancy to a given project is an inherently difficult task, it is necessary to provide an automated approach for mining this crowd-based documentation. In this paper, we are interested in mining the speech part of YouTube screencasts, since this part typically contains the rationale and insights of a screencast. We introduce a methodology that transcribes and analyzes the transcribed text using various Information Extraction (IE) techniques, and present a case study to illustrate the applicability of our mining methodology. In this case study, we extract use case scenarios from WordPress tutorial videos and show how their content can supplement existing documentation. We then evaluate how well existing rankings of video content are able to pinpoint the most relevant videos for a given scenario.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.034,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832905,Crowd-based documentation; mining video content; speech analysis; Information Extraction; software documentation,Blogs;Data mining;Documentation;Internet;Software;Speech;Videos,data mining;document handling;information retrieval,IE techniques;WordPress tutorial videos;YouTube screencasts;crowd-based speech documentation mining;information extraction techniques;multimedia documents;multimedia information;project documentation;software development;textual Web documents;use case scenario,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,9
Sentiment Analysis in Tickets for IT Support,C. C. A. Blaz; K. Becker,"Inst. de Inf., Univ. Fed. do Rio Grande do Sul, Porto Alegre, Brazil",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,235,246,"Sentiment analysis has been adopted in software engineering for problems such as software usability and sentiment of developers in open-source projects. This paper proposes a method to evaluate the sentiment contained in tickets for IT (Information Technology) support.IT tickets are broad in coverage (e.g. infrastructure, software), and involve errors, incidents, requests, etc. The main challenge is to automatically distinguish between factual information, which is intrinsically negative (e.g. error description), from the sentiment embedded in the description. Our approach is to automatically create a Domain Dictionary that contains terms with sentiment in the IT context, used to filter terms in ticket for sentiment analysis. We experiment and evaluate three approaches for calculating the polarity of terms in tickets. Our study was developed using 34,895 tickets from five organizations, from which we randomly selected 2,333 tickets to compose a Gold Standard. Our best results display an average precision and recall of 82.83% and 88.42%, which outperforms the compared sentiment analysis solutions.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.032,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832903,Domain Dictionary;IT Tickets;Opinion Mining,Data mining;Dictionaries;Organizations;Sentiment analysis;Standards organizations;Usability,public domain software;sentiment analysis;software reusability,IT support tickets;domain dictionary;open-source projects;sentiment analysis;software engineering;software usability,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Software Ingredients: Detection of Third-Party Component Reuse in Java Software Release,T. Ishio; R. G. Kula; T. Kanda; D. M. German; K. Inoue,"Osaka Univ., Suita, Japan",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,339,350,"A software product is often dependent on a large number of third-party components.To assess potential risks, such as security vulnerabilities and license violations, a list of components and their versions in a product is important for release engineers and security analysts.Since such a list is not always available, a code comparison technique named Software Bertillonage has been proposed to test whether a product likely includes a copy of a particular component or not.Although the technique can extract candidates of reused components, a user still has to manually identify the original components among the candidates.In this paper, we propose a method to automatically select the most likely origin of components reused in a product, based on an assumption that a product tends to include an entire copy of a component rather than a partial copy.More concretely, given a Java product and a repository of jar files of existing components, our method selects jar files that can provide Java classes to the product in a greedy manner.To compare the method with the existing technique, we have conducted an evaluation using randomly created jar files including up to 1,000 components.The Software Bertillonage technique reports many candidates; the precision and recall are 0.357 and 0.993, respectively.Our method reports a list of original components whose precision and recall are 0.998 and 0.997.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.042,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832913,Software reuse;origin analysis;reverse engineering,Cloning;Java;Libraries;Licenses;Security;Software reusability,Java;object-oriented programming;software reusability,Java classes;Java product;Java software release;code comparison technique;precision;randomly created jar file repository;recall;software Bertillonage;software ingredients;software product;third-party component reuse,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Studying the Effectiveness of Application Performance Management (APM) Tools for Detecting Performance Regressions for Web Applications: An Experience Report,T. M. Ahmed; C. P. Bezemer; T. H. Chen; A. E. Hassan; W. Shang,"Software Anal. & Intell. Lab., Queen's Univ., Kingston, ON, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,1,12,"Performance regressions, such as a higher CPU utilization than in the previous version of an application, are caused by software application updates that negatively affect the performance of an application.Although a plethora of mining software repository research has been done to detect such regressions, research tools are generally not readily available to practitioners. Application Performance Management (APM) tools are commonly used in practice for detecting performance issues in the field by mining operational data.In contrast to performance regression detection tools that assume a changing code base and a stable workload, APM tools mine operational data to detect performance anomalies caused by a changing workload in an otherwise stable code base. Although APM tools are widely used in practice, no research has been done to understand 1) whether APM tools can identify performance regressions caused by code changes and 2) how well these APM tools support diagnosing the root-cause of these regressions.In this paper, we explore if the readily accessible APM tools can help practitioners detect performance regressions. We perform a case study using three commercial (AppDynamics, New Relic and Dynatrace) and one open source (Pinpoint) APM tools. In particular, we examine the effectiveness of leveraging these APM tools in detecting and diagnosing injected performance regressions (excessive memory usage, high CPU utilization and inefficient database queries) in three open source applications. We find that APM tools can detect most of the injected performance regressions, making them good candidates to detect performance regressions in practice. However, there is a gap between mining approaches that are proposed in state-of-the-art performance regression detection research and the ones used by APM tools. In addition, APM tools lack the ability to be extended, which makes it hard to enhance them when exploring novel mining approaches for detecting performance regression- .",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.011,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832882,,Data mining;Data models;Measurement;Monitoring;Production;Software;Testing,Internet;data mining;public domain software;regression analysis;software performance evaluation,CPU utilization;Web applications;application performance management tools;changing code base;injected performance regressions;mining software repository research;open source APM tools;operational data mining;performance regression detection tools;performance regressions;software application updates,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
The Impact of Switching to a Rapid Release Cycle on the Integration Delay of Addressed Issues - An Empirical Study of the Mozilla Firefox Project,D. A. da Costa; S. McIntosh; U. Kulesza; A. E. Hassan,"Fed. Univ. of Rio Grande do Norte, Natal, Brazil",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,374,385,"The release frequency of software projects has increased in recent years. Adopters of so-called rapid release cycles claim that they can deliver addressed issues (i.e., bugs, enhancements, and new features) to users more quickly. However, there is little empirical evidence to support these claims. In fact, in our prior work, we found that code integration phases may introduce delays in rapidly releasing software - 98% of addressed issues in the rapidly releasing Firefox project had their integration delayed by at least one release. To better understand the impact that rapid release cycles have on the integration delay of addressed issues, we perform a comparative study of traditional and rapid release cycles. Through an empirical study of 72,114 issue reports from the Firefox system, we observe that, surprisingly, addressed issues take a median of 50 days longer to be integrated in rapid Firefox releases than the traditional ones. To investigate the factors that are related to integration delay in traditional and rapid release cycles, we train regression models that explain if an addressed issue will have its integration delayed or not. Our explanatory models achieve good discrimination (ROC areas of 0.81-0.83) and calibration scores (Brier scores of 0.05-0.16). Deeper analysis of our explanatory models indicates that traditional releases prioritize the integration of backlog issues, while rapid releases prioritize issues that were addressed during the current release cycle. Our results suggest that rapid release cycles may not be a silver bullet for the rapid delivery of addressed issues to users.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.045,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832916,Delivery delay;Empirical study;Release engineering,Computer bugs;Context;Delays;History;Marine vehicles;Software,project management;software development management,Mozilla Firefox Project;code integration phase;integration delay;rapid release cycle;software projects release frequency,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
The Unreasonable Effectiveness of Traditional Information Retrieval in Crash Report Deduplication,J. C. Campbell; E. A. Santos; A. Hindle,"Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,269,280,"Organizations like Mozilla, Microsoft, and Apple are flooded with thousands of automated crash reports per day. Although crash reports contain valuable information for debugging, there are often too many for developers to examine individually. Therefore, in industry, crash reports are often automatically grouped together in buckets. Ubuntu's repository contains crashes from hundreds of software systems available with Ubuntu. A variety of crash report bucketing methods are evaluated using data collected by Ubuntu's Apport automated crash reporting system. The trade-off between precision and recall of numerous scalable crash deduplication techniques is explored. A set of criteria that a crash deduplication method must meet is presented and several methods that meet these criteria are evaluated on a new dataset. The evaluations presented in this paper show that using off-the-shelf information retrieval techniques, that were not designed to be used with crash reports, outperform other techniques which are specifically designed for the task of crash bucketing at realistic industrial scales. This research indicates that automated crash bucketing still has a lot of room for improvement, especially in terms of identifier tokenization.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832906,Automatic Crash Re- porting;Call Stack Trace;Contextual Information;Deduplication;Duplicate Bug Reports;Duplicate Crash Report;Free/Open Source Software;Information Retrieval;Software En- gineering,Computer bugs;Debugging;Indexing;Information retrieval;Organizations;Software,information retrieval;program debugging;software maintenance,Ubuntu Apport automated crash reporting system;Ubuntu repository;crash report bucketing methods;identifier tokenization;off-the-shelf information retrieval techniques;precision factor;recall factor;scalable crash report deduplication technique;software systems,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Topic Modeling of NASA Space System Problem Reports: Research in Practice,L. Layman; A. P. Nikora; J. Meek; T. Menzies,"Fraunhofer CESE, College Park, MD, USA",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,303,314,"Problem reports at NASA are similar to bug reports: they capture defects found during test, post-launch operational anomalies, and document the investigation and corrective action of the issue. These artifacts are a rich source of lessons learned for NASA, but are expensive to analyze since problem reports are comprised primarily of natural language text. We apply {topic modeling to a corpus of NASA problem reports to extract trends in testing and operational failures. We collected 16,669 problem reports from six NASA space flight missions and applied Latent Dirichlet Allocation topic modeling to the document corpus. We analyze the most popular topics within and across missions, and how popular topics changed over the lifetime of a mission. We find that hardware material and flight software issues are common during the integration and testing phase, while ground station software and equipment issues are more common during the operations phase. We identify a number of challenges in topic modeling for trend analysis: (1) that the process of selecting the topic modeling parameters lacks definitive guidance, (2) defining semantically-meaningful topic labels requires non-trivial effort and domain expertise, (3) topic models derived from the combined corpus of the six missions were biased toward the larger missions, and (4) topics must be semantically distinct as well as cohesive to be useful. Nonetheless, topic modeling can identify problem themes within missions and across mission lifetimes, providing useful feedback to engineers and project managers.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.039,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832910,LDA;data mining;defects;natural language processing;topic modeling,Analytical models;Computer bugs;Hardware;Market research;NASA;Software;Space missions,aerospace computing;data mining;natural language processing;program debugging,NASA space flight missions;NASA space system problem reports;bug reports;data mining;document corpus;flight software;hardware material;latent Dirichlet allocation topic modeling;natural language text;trend analysis,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
Understanding the Exception Handling Strategies of Java Libraries: An Empirical Study,D. Sena; R. Coelho; U. Kulesza; R. BonifÕçcio,"Inf. & Appl. Math. Dept., Fed. Univ. of Rio Grande do Norte, Rio Grande, Brazil",2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,212,222,"This paper presents an empirical study whose goal was to investigate the exception handling strategies adopted by Java libraries and their potential impact on the client applications. In this study, exception flow analysis was used in combination with manual inspections in order: (i) to characterize the exception handling strategies of existing Java libraries from the perspective of their users; and (ii) to identify exception handling anti-patterns. We extended an existing static analysis tool to reason about exception flows and handler actions of 656 Java libraries selected from 145 categories in the Maven Central Repository. The study findings suggest a current trend of a high number of undocumented API runtime exceptions (i.e., @throws in Javadoc) and Unintended Handler problem. Moreover, we could also identify a considerable number of occurrences of exception handling anti-patterns (e.g. Catch and Ignore). Finally, we have also analyzed 647 bug issues of the 7 most popular libraries and identified that 20.71% of the reports are defects related to the problems of the exception strategies and anti-patterns identified in our study. The results of this study point to the need of tools to better understand and document the exception handling behavior of libraries.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.030,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832901,Empirical study;Exception flows analysis;Exception handling;Exception handling anti-patterns;Software libraries;Static analysis tool,Computer bugs;Documentation;Java;Runtime;Software;Software libraries,Java;application program interfaces;data flow analysis;program debugging;software libraries,Java libraries;Maven Central Repository;bug issues;client applications;exception flow analysis;exception handling antipattern identification;exception handling strategies;undocumented API runtime exceptions;unintended handler problem,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,10
Using Dynamic and Contextual Features to Predict Issue Lifetime in GitHub Projects,R. Kikas; M. Dumas; D. Pfahl,,2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR),20170126,2016,,,291,302,"Methods for predicting issue lifetime can help software project managers to prioritize issues and allocate resources accordingly. Previous studies on issue lifetime prediction have focused on models built from static features, meaning features calculated at one snapshot of the issue's lifetime based on data associated to the issue itself. However, during its lifetime, an issue typically receives comments from various stakeholders, which may carry valuable insights into its perceived priority and difficulty and may thus be exploited to update lifetime predictions. Moreover, the lifetime of an issue depends not only on characteristics of the issue itself, but also on the state of the project as a whole. Hence, issue lifetime prediction may benefit from taking into account features capturing the issue's context (contextual features). In this work, we analyze issues from more than 4000 GitHub projects and build models to predict, at different points in an issue's lifetime, whether or not the issue will close within a given calendric period, by combining static, dynamic and contextual features. The results show that dynamic and contextual features complement the predictive power of static ones, particularly for long-term predictions.",,Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7,10.1109/MSR.2016.038,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832909,,Computer bugs;Context modeling;Data models;Feature extraction;Predictive models;Software;Stakeholders,program diagnostics;project management;public domain software;software maintenance;software management,GitHub projects;calendric period;contextual features;dynamic features;issue lifetime prediction;long-term predictions;perceived priority;resource allocation;static features,,,,,,,,14-15 May 2016,,IEEE,IEEE Conferences,,11
A Large-Scale Study of the Impact of Feature Selection Techniques on Defect Classification Models,B. Ghotra; S. McIntosh; A. E. Hassan,"Queen's Univ., Kingston, QC, Canada",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,146,157,"The performance of a defect classification model depends on the features that are used to train it. Feature redundancy, correlation, and irrelevance can hinder the performance of a classification model. To mitigate this risk, researchers often use feature selection techniques, which transform or select a subset of the features in order to improve the performance of a classification model. Recent studies compare the impact of different feature selection techniques on the performance of defect classification models. However, these studies compare a limited number of classification techniques and have arrived at contradictory conclusions about the impact of feature selection techniques. To address this limitation, we study 30 feature selection techniques (11 filter-based ranking techniques, six filter based subset techniques, 12 wrapper-based subset techniques, and a no feature selection configuration) and 21 classification techniques when applied to 18 datasets from the NASA and PROMISE corpora. Our results show that a correlation-based filter-subset feature selection technique with a BestFirst search method outperforms other feature selection techniques across the studied datasets (it outperforms in 70%-87% of the PROMISE-NASA data sets) and across the studied classification techniques (it outperforms for 90% of the techniques). Hence, we recommend the application of such a selection technique when building defect classification models.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962364,Defect Classification Models;Feature Selection Techniques,Buildings;Correlation;Measurement uncertainty;NASA;Search methods;Support vector machines;Training,feature selection;pattern classification;software performance evaluation,NASA corpora;PROMISE corpora;defect classification models;feature selection techniques;filter based subset techniques;filter-based ranking techniques;wrapper-based subset techniques,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
"A Large-Scale Study on the Usage of Testing Patterns That Address Maintainability Attributes: Patterns for Ease of Modification, Diagnoses, and Comprehension",D. Gonzalez; J. C. S. Santos; A. Popovich; M. Mirakhorli; M. Nagappan,"Software Eng. Dept., Rochester Inst. of Technol., Rochester, NY, USA",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,391,401,"Test case maintainability is an important concern, especially in open source and distributed development environments where projects typically have high contributor turn-over with varying backgrounds and experience, and where code ownership changes often. Similar to design patterns, patterns for unit testing promote maintainability quality attributes such as ease of diagnoses, modifiability, and comprehension. In this paper, we report the results of a large-scale study on the usage of four xUnit testing patterns which can be used to satisfy these maintainability attributes. This is a first-of-its-kind study which developed automated techniques to investigate these issues across 82,447 open source projects, and the findings provide more insight into testing practices in open source projects. Our results indicate that only 17% of projects had test cases, and from the 251 testing frameworks we studied, 93 of them were being used. We found 24% of projects with test files implemented patterns that could help with maintainability, while the remaining did not use these patterns. Multiple qualitative analyses indicate that usage of patterns was an ad-hoc decision by individual developers, rather than motivated by the characteristics of the project, and that developers sometimes used alternative techniques to address maintainability concerns.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962388,Maintenance;Mining Software Repositories;Open Source;Unit Test Frameworks;Unit Test Patterns;Unit Testing,Context;Maintenance engineering;Measurement;Production;Software;Testing;Writing,distributed processing;program testing;public domain software;software maintenance;software quality,distributed development environments;maintainability attributes;maintainability quality;open source development environments;test case maintainability;xUnit testing patterns,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
A Study on the Energy Consumption of Android App Development Approaches,W. Oliveira; R. Oliveira; F. Castor,"Fed. Univ. of Pernambuco, Recife, Brazil",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,42,52,"Mobile devices have become ubiquitous in the recent years, but the complaints about energy consumption are almost universal. On Android, the developer can choose among several different approaches to develop an app. In this paper, we investigate the impact of some of the most popular development approaches on the energy consumption of Android apps. Our study uses a testbed of 33 different benchmarks and 3 applications on 5 different devices to compare the energy efficiency and performance of the most commonly used approaches to develop apps on Android: Java, JavaScript, and C/C++ (through the NDK tools). In our experiments, Javascript was more energy-efficient in 75% of all benchmarks, while their Java counterparts consume up to 36.27x more energy (median of 1.97x). On the other hand, both Java and C++ outperformed JavaScript in most of the benchmarks. Based on these results, four Java applications were re-engineered to use a combination of Java and either JavaScript or C/C++ functions. For one of the apps, the hybrid solution using Java and C++ spent 10x less time and almost 100x less energy than a pure Java solution. The results were not uniform, however. For another app, when we restructured its implementation so as to minimize cross-language method invocations, the hybrid solution using Java and C++ took 8% longer to execute and consumed 11% more energy than a hybrid solution using Java and JavaScript. Since most Android apps are written solely in Java, the results of this study indicate that leveraging a combination of approaches may lead to non-negligible improvements in energy-efficiency and performance.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962354,Android;Benchmark testing;Energy consumption;Hybrid apps;Performance;Smartphones,Androids;Benchmark testing;C++ languages;Energy consumption;Humanoid robots;Java;Performance evaluation,Android (operating system);C++ language;Java;energy consumption;mobile computing,Android app development;C/C++;JavaScript;energy consumption;mobile devices;ubiquitous computing,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
An Empirical Analysis of Build Failures in the Continuous Integration Workflows of Java-Based Open-Source Software,T. Rausch; W. Hummer; P. Leitner; S. Schulte,"Distrib. Syst. Group, Vienna Univ. of Technol., Vienna, Austria",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,345,355,"Continuous Integration (CI) has become a common practice in both industrial and open-source software development. While CI has evidently improved aspects of the software development process, errors during CI builds pose a threat to development efficiency. As an increasing amount of time goes into fixing such errors, failing builds can significantly impair the development process and become very costly. We perform an indepth analysis of build failures in CI environments. Our approach links repository commits to data of corresponding CI builds. Using data from 14 open-source Java projects, we first identify 14 common error categories. Besides test failures, which are by far the most common error category (up to >80% per project), we also identify noisy build data, e.g., induced by transient Git interaction errors, or general infrastructure flakiness. Second, we analyze which factors impact the build results, taking into account general process and specific CI metrics. Our results indicate that process metrics have a significant impact on the build outcome in 8 of the 14 projects on average, but the strongest influencing factor across all projects is overall stability in the recent build history. For 10 projects, more than 50% (up to 80%) of all failed builds follow a previous build failure. Moreover, the fail ratio of the last k=10 builds has a significant impact on build results for all projects in our dataset.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.54,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962384,build errors;continuous integration;correlation analysis;mining software repositories,Androids;Data mining;History;Java;Measurement;Open source software,Java;software engineering,CI environments;Continuous Integration;Java-based open-source software development;build failures;continuous integration workflows;fail ratio;open-source Java projects;transient Git interaction errors,,1,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
An Empirical Analysis of the Docker Container Ecosystem on GitHub,J. Cito; G. Schermann; J. E. Wittern; P. Leitner; S. Zumberi; H. C. Gall,"Software Evolution & Archit. Lab., Univ. of Zurich, Zurich, Switzerland",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,323,333,"Docker allows packaging an application with its dependencies into a standardized, self-contained unit (a so-called container), which can be used for software development and to run the application on any system. Dockerfiles are declarative definitions of an environment that aim to enable reproducible builds of the container. They can often be found in source code repositories and enable the hosted software to come to life in its execution environment. We conduct an exploratory empirical study with the goal of characterizing the Docker ecosystem, prevalent quality issues, and the evolution of Dockerfiles. We base our study on a data set of over 70000 Dockerfiles, and contrast this general population with samplings that contain the Top-100 and Top-1000 most popular Docker-using projects. We find that most quality issues (28.6%) arise from missing version pinning (i.e., specifying a concrete version for dependencies). Further, we were not able to build 34% of Dockerfiles from a representative sample of 560 projects. Integrating quality checks, e.g., to issue version pinning warnings, into the container build process could result into more reproducible builds. The most popular projects change more often than the rest of the Docker population, with 5.81 revisions per year and 5 lines of code changed on average. Most changes deal with dependencies, that are currently stored in a rather unstructured manner. We propose to introduce an abstraction that, for instance, could deal with the intricacies of different package managers and could improve migration to more light-weight images.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962382,Docker;GitHub;empirical software engineering,Computer languages;Containers;Ecosystems;Sociology;Software;Statistics;Tools,software engineering;source code (software),GitHub;docker container ecosystem;docker-using projects;dockerfile evolution;software development,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
An Empirical Study on Android-Related Vulnerabilities,M. Linares-VÕçsquez; G. Bavota; C. Escobar-VelÕçsquez,"Syst. & Comput. Eng. Dept., Univ. de los Andes, Bogota, Colombia",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,2,13,"Mobile devices are used more and more in everyday life. They are our cameras, wallets, and keys. Basically, they embed most of our private information in our pocket. For this and other reasons, mobile devices, and in particular the software that runs on them, are considered first-class citizens in the software-vulnerabilities landscape. Several studies investigated the software-vulnerabilities phenomenon in the context of mobile apps and, more in general, mobile devices. Most of these studies focused on vulnerabilities that could affect mobile apps, while just few investigated vulnerabilities affecting the underlying platform on which mobile apps run: the Operating System (OS). Also, these studies have been run on a very limited set of vulnerabilities. In this paper we present the largest study at date investigating Android-related vulnerabilities, with a specific focus on the ones affecting the Android OS. In particular, we (i) define a detailed taxonomy of the types of Android-related vulnerability, (ii) investigate the layers and subsystems from the Android OS affected by vulnerabilities, and (iii) study the survivability of vulnerabilities (i.e., the number of days between the vulnerability introduction and its fixing). Our findings could help OS and apps developers in focusing their verification & validation activities, and researchers in building vulnerability detection tools tailored for the mobile world.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962350,Android;operating system;taxonomy;vulnerabilities,Androids;Humanoid robots;Libraries;Malware;Mobile communication;Security;Smart phones,Android (operating system);mobile computing;operating systems (computers);program verification;security of data,Android OS;Android-related vulnerabilities;information privacy;mobile apps;mobile devices;operating system;software vulnerabilities;validation activities;verification activities;vulnerability detection tools,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
An Exploratory Study on Assessing the Impact of Environment Variations on the Results of Load Tests,R. Gao; Z. M. Jiang,"Software Constr., AnaLytics & Evaluation Lab., York Univ., Toronto, ON, Canada",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,379,390,"Large-scale software systems like Amazon and healthcare.gov are used by thousands or millions of people every day. To ensure the quality of these systems, load testing is a required testing procedure in addition to the conventional functional testing techniques like unit and system integration testing. One of the important requirements of load testing is to create a field-like test environment. Unfortunately, this task is often very challenging due to reasons like security and rapid field updates. In this paper, we have conducted an exploratory study on the impact of environment variations on the results of load tests. We have run over 110 hours load tests, which examine the system's behavior under load with various changes (e.g., installing an antivirus program) to the targeted deployment environment. We call such load tests as environment-variation-based load tests. Case studies in three open source systems have shown that there is a clear performance impact on the system's performance due to these environment changes. Different scenarios react differently to the changes in the underlying computing resources. When predicting the performance of the system under environment changes that are not previously load tested, our ensemble models out-perform (24% - 94% better) the baseline models.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962387,load testing;mining performance counters;performance analysis;software analytics,Adaptation models;Analytical models;Computational modeling;Data mining;Data models;Load modeling;Testing,program testing;public domain software,environment-variation-based load tests;environmental variations impact assessment;open source systems;software systems,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Analyzing Program Dependencies in Java EE Applications,A. Shatnawi; H. Mili; G. El Boussaidi; A. Boubaker; Y. G. GuÕ©hÕ©neuc; N. Moha; J. Privat; M. Abdellatif,"LATECE Lab., Univ. du Quebec a Montreal, Montre&#x0301;al, QC, Canada",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,64,74,"Program dependency artifacts such as call graphs help support a number of software engineering tasks such as software mining, program understanding, debugging, feature location, software maintenance and evolution. Java Enterprise Edition (JEE) applications represent a significant part of the recent legacy applications, and we are interested in modernizing them. This modernization involves, among other things, analyzing dependencies between their various components/tiers. JEE applications tend to be multilanguage, rely on JEE container services, and make extensive use of late binding techniques-all of which makes finding such dependencies difficult. In this paper, we describe some of these difficulties and how we addressed them to build a dependency call graph. We developed our tool called DeJEE (Dependencies in JEE) as an Eclipse plug-in. We applied DeJEE on two open-source JEE applications: Java PetStore and JSP Blog. The results show that DeJEE is able to identify different types of JEE dependencies.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.6,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962356,Java EE application;Program dependency;code analysis;container services;modernization;server pages,Containers;Data mining;Java;Object oriented modeling;Servers;Software;Tools,Java;program diagnostics;software maintenance,DeJEE;Eclipse plug-in;JEE applications;JEE container services;JSP Blog;Java EE applications;Java Enterprise Edition applications;Java PetStore;dependencies in JEE;dependency call graph;late binding techniques;legacy applications;multilanguage;open-source JEE applications;program dependency analysis;program dependency artifacts;software engineering tasks,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Bug Characteristics in Blockchain Systems: A Large-Scale Empirical Study,Z. Wan; D. Lo; X. Xia; L. Cai,"Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,413,424,"Bugs severely hurt blockchain system dependability. A thorough understanding of blockchain bug characteristics is required to design effective tools for preventing, detecting and mitigating bugs. We perform an empirical study on bug characteristics in eight representative open source blockchain systems. First, we manually examine 1,108 bug reports to understand the nature of the reported bugs. Second, we leverage card sorting to label the bug reports, and obtain ten bug categories in blockchain systems. We further investigate the frequency distribution of bug categories across projects and programming languages. Finally, we study the relationship between bug categories and bug fixing time. The findings include: (1) semantic bugs are the dominant runtime bug category, (2) frequency distributions of bug types show similar trends across different projects and programming languages, (3) security bugs take the longest median time to be fixed, (4) 35.71% performance bugs are fixed in more than one year, performance bugs take the longest average time to be fixed.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962390,blockchain;bug characteristics;empirical study,C++ languages;Computer bugs;Software;Sorting;Tools,program debugging;public domain software;security of data,blockchain bug characteristics;blockchain system dependability;bug fixing time;card sorting;open source blockchain systems;performance bugs;programming languages;runtime bug category;security bugs;semantic bugs,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Candoia: A Platform for Building and Sharing Mining Software Repositories Tools as Apps,N. M. Tiwari; G. Upadhyaya; H. A. Nguyen; H. Rajan,"Iowa State Univ., Ames, IA, USA",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,53,63,"We propose Candoia, a novel platform and ecosystemfor building and sharing Mining Software Repositories(MSR) tools. Using Candoia, MSR tools are built as apps, and Candoia ecosystem, acting as an appstore, allows effective sharing. Candoia platform provides, data extraction tools for curating custom datasets for user projects, and data abstractions for enabling uniform access to MSR artifacts from disparate sources, which makes apps portable and adoptable across diverse software project settings of MSR researchers and practitioners. The structured design of a Candoia app and the languages selected for building various components of a Candoia app promotes easy customization. To evaluate Candoia we have built over two dozen MSR apps for analyzing bugs, software evolution, project management aspects, and source code and programming practices showing the applicability of the platform for buildinga variety of MSR apps. For testing portability of apps acrossdiverse project settings, we tested the apps using ten popularproject repositories, such as Apache Tomcat, JUnit, Node.js, etc, and found that apps required no changes to be portable. We performed a user study to test customizability and we found that five of eight Candoia users found it very easy to customize an existing app. Candoia is available for download.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962355,Candioa ecosystem;Candoia;Candoia exchange;MSR;MSR apps;MSR data,Buildings;Computer bugs;Data mining;Ecosystems;Java;Software;Tools,program debugging;project management;software tools,Candoia app;MSR tools;app customizability;apps sharing;appstore;data abstractions;data extraction tools;mining software repositories tools;programming practices;project management;software bugs;software evolution;software project settings;source code,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Choosing an NLP Library for Analyzing Software Documentation: A Systematic Literature Review and a Series of Experiments,F. N. A. Al Omran; C. Treude,"Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,187,197,"To uncover interesting and actionable information from natural language documents authored by software developers, many researchers rely on ""out-of-the-box"" NLP libraries. However, software artifacts written in natural language are different from other textual documents due to the technical language used. In this paper, we first analyze the state of the art through a systematic literature review in which we find that only a small minority of papers justify their choice of an NLP library. We then report on a series of experiments in which we applied four state-of-the-art NLP libraries to publicly available software artifacts from three different sources. Our results show low agreement between different libraries (only between 60% and 71% of tokens were assigned the same part-of-speech tag by all four libraries) as well as differences in accuracy depending on source: For example, spaCy achieved the best accuracy on Stack Overflow data with nearly 90% of tokens tagged correctly, while it was clearly outperformed by Google's SyntaxNet when parsing GitHub ReadMe files. Our work implies that researchers should make an informed decision about the particular NLP library they choose and that customizations to libraries might be necessary to achieve good results when analyzing software artifacts written in natural language.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962368,NLP libraries;Natural language processing;Part-of-Speech tagging;Software documentation,Bibliographies;Libraries;Natural language processing;Software;Software engineering;Tools,natural language processing;software libraries;system documentation,GitHub ReadMe files;Google SyntaxNet;Stack Overflow data;natural language documents;out-of-the-box NLP libraries;part-of-speech tag;software artifacts;software documentation;spaCy,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Classifying Code Comments in Java Open-Source Software Systems,L. Pascarella; A. Bacchelli,"Delft Univ. of Technol., Delft, Netherlands",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,227,237,"Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how six diverse Java OSS projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments, subsequently, we investigate how often each category occur by manually classifying more than 2,000 code comments from the aforementioned projects. In addition, we conduct an initial evaluation on how to automatically classify code comments at line level into our taxonomy using machine learning, initial results are promising and suggest that an accurate classification is within reach.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962372,comment taxonomy;software quality;source code comments,Google;Java;Maintenance engineering;Measurement;Open source software;Taxonomy,Java;learning (artificial intelligence);object-oriented programming;pattern classification;public domain software;source code (software),Java OSS projects;automatic code comment classification;line level;machine learning;software component;source code comments,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Developer Mistakes in Writing Android Manifests: An Empirical Study of Configuration Errors,A. K. Jha; S. Lee; W. J. Lee,"Sch. of Comput. Sci. & Eng., Kyungpook Nat. Univ., Daegu, South Korea",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,25,36,"Each Android app must have an Android manifest file. It is one of the most important configuration files manually written by developers. In addition to various configuration parameters required to run an app, it also contains configuration parameters which are used to implement security, compatibility, and accessibility of an app. Any mistakes in writing the manifest file can cause serious implications in terms of security, reliability, and availability of an app. In this paper, we study and report different types of mistakes committed by developers in writing Android manifest files. The study was performed on 13,483 real-world Android apps. We also present an open source rule-based static analysis tool which detects developer mistakes in the manifest file. The tool generates a warning message if it detects any misconfigurations in the manifest file. We used the tool to perform the empirical study and it generated total 59,547 configuration errors in 11,110 apps. Only 2,373 apps, among studied apps, do not have any configuration errors.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.41,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962352,Android apps;Android manifest;configuration errors;rule-based error detection,Androids;Humanoid robots;Reliability;Security;Testing;Tools;Writing,Android (operating system);configuration management;program diagnostics;public domain software;security of data;software reliability,Android app accessibility;Android app availability;Android app compatibility;Android app reliability;Android app security;Android manifest file;configuration errors;configuration files;open source rule-based static analysis tool;software developer mistakes,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Do Not Trust Build Results at Face Value - An Empirical Study of 30 Million CPAN Builds,M. Zolfagharinia; B. Adams; Y. G. GuÕ©hÕ©nuc,"MCIS, Polytech. Montreal, Montreal, QC, Canada",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,312,322,"Continuous Integration (CI) is a cornerstone of modern quality assurance, providing on-demand builds (compilation and tests) of code changes or software releases. Despite the myriad of CI tools and frameworks, the basic activity of interpreting build results is not straightforward, due to not only the number of builds being performed but also, and especially, due to the phenomenon of build inflation, according to which one code change can be built on dozens of different operating systems, run-time environments and hardware architectures. As existing work mostly ignored this inflation, this paper performs a large-scale empirical study of the impact of OS and run-time environment on build failures on 30 million builds of the CPAN ecosystem's CI environment. We observe the evolution of build failures over time, and investigate the impact of OSes and environments on build failures. We show that distributions may fail differently on different OSes and environments and, thus, that the results of CI require careful filtering and selection to identify reliable failure data.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.7,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962381,Build;Build failure;Module;distribution;environment;operating system,Ecosystems;Electronic mail;Green products;Java;Open source software;Operating systems;Tools,integrated software;operating systems (computers);software quality;source code (software);system recovery,CPAN builds;CPAN ecosystem CI environment;build failures;build inflation;code changes;continuous integration;operating systems;quality assurance;run-time environment;software releases,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Euphony: Harmonious Unification of Cacophonous Anti-Virus Vendor Labels for Android Malware,M. Hurier; G. Suarez-Tangil; S. K. Dash; T. F. BissyandÕ©; Y. Le Traon; J. Klein; L. Cavallaro,"Univ. of Luxembourg, Luxembourg City, Luxembourg",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,425,435,"Android malware is now pervasive and evolving rapidly. Thousands of malware samples are discovered every day with new models of attacks. The growth of these threats has come hand in hand with the proliferation of collective repositories sharing the latest specimens. Having access to a large number of samples opens new research directions aiming at efficiently vetting apps. However, automatically inferring a reference ground-truth from those repositories is not straightforward and can inadvertently lead to unforeseen misconceptions. On the one hand, samples are often mis-labeled as different parties use distinct naming schemes for the same sample. On the other hand, samples are frequently mis-classified due to conceptual errors made during labeling processes. In this paper, we analyze the associations between all labels given by different vendors and we propose a system called EUPHONY to systematically unify common samples into family groups. The key novelty of our approach is that no a-priori knowledge on malware families is needed. We evaluate our approach using reference datasets and more than 0.4 million additional samples outside of these datasets. Results show that EUPHONY provides competitive performance against the state-of-the-art.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962391,android;datasets;ground-truth;labeling;malware,Androids;Electronic mail;Engines;Humanoid robots;Labeling;Trojan horses,Android (operating system);computer viruses,Android malware;Euphony;cacophonous antivirus vendor labels;distinct naming schemes,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Exception Evolution in Long-Lived Java Systems,H. Osman; A. Chis; C. Corrodi; M. Ghafari; O. Nierstrasz,"Software Composition Group, Univ. of Bern, Bern, Switzerland",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,302,311,"Exception handling allows developers to deal with abnormal situations that disrupt the execution flow of a program. There are mainly three types of exceptions: standard exceptions provided by the programming language itself, custom exceptions defined by the project developers, and third-party exceptions defined in external libraries. We conjecture that there are multiple factors that affect the use of these exception types. We perform an empirical study on long-lived Java projects to investigate these factors. In particular, we analyze how developers rely on the different types of exceptions in throw statements and exception handlers. We confirm that the domain, the type, and the development phase of a project affect the exception handling patterns. We observe that applications have significantly more error handling code than libraries and they increasingly rely on custom exceptions. Also, projects that belong to different domains have different preferences of exception types. For instance, content management systems rely more on custom exceptions than standard exceptions whereas the opposite is true in parsing frameworks.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962380,Exception handling;empirical study;software evolution,Content management;Java;Libraries;Measurement;Software;Standards,Java;content management;exception handling;software maintenance,content management systems;error handling code;exception evolution;exception handling patterns;exception types;external libraries;long-lived Java systems;parsing frameworks;program execution flow;programming language;third-party exceptions,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,9
Extracting Build Changes with BUILDDIFF,C. Macho; S. McIntosh; M. Pinzger,"Software Eng. Res. Group, Univ. of Klagenfurt, Klagenfurt, Austria",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,368,378,"Build systems are an essential part of modern software engineering projects. As software projects change continuously, it is crucial to understand how the build system changes because neglecting its maintenance can lead to expensive build breakage. Recent studies have investigated the (co-)evolution of build configurations and reasons for build breakage, but they did this only on a coarse grained level. In this paper, we present BUILDDIFF, an approach to extract detailed build changes from MAVEN build files and classify them into 95 change types. In a manual evaluation of 400 build changing commits, we show that BUILDDIFF can extract and classify build changes with an average precision and recall of 0.96 and 0.98, respectively. We then present two studies using the build changes extracted from 30 open source Java projects to study the frequency and time of build changes. The results show that the top 10 most frequent change types account for 73% of the build changes. Among them, changes to version numbers and changes to dependencies of the projects occur most frequently. Furthermore, our results show that build changes occur frequently around releases. With these results, we provide the basis for further research, such as for analyzing the (co-)evolution of build files with other artifacts or improving effort estimation approaches. Furthermore, our detailed change information enables improvements of refactoring approaches for build configurations and improvements of models to identify error-prone build files.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.65,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962386,Build Systems;Maintenance;Software Quality,Data mining;Java;Maintenance engineering;Predictive models;Software;Software engineering;Taxonomy,Java;project management;public domain software;software cost estimation;software maintenance,BUILDDIFF;MAVEN build files;build breakage;build change classification;build change extraction;build change frequency;build change time;build configuration evolution;build systems;change types;coarse grained level;effort estimation approaches;error-prone build file identification;open source Java projects;refactoring approaches;software engineering projects;version numbers,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Extracting Code Segments and Their Descriptions from Research Articles,P. Chatterjee; B. Gause; H. Hedinger; L. Pollock,"Comput. & Inf. Sci., Univ. of Delaware, Newark, DE, USA",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,91,101,"The availability of large corpora of online software-related documents today presents an opportunity to use machine learning to improve integrated development environments by first automatically collecting code examples along with associated descriptions. Digital libraries of computer science research and education conference and journal articles can be a rich source for code examples that are used to motivate or explain particular concepts or issues. Because they are used as examples in an article, these code examples are accompanied by descriptions of their functionality, properties, or other associated information expressed in natural language text. Identifying code segments in these documents is relatively straightforward, thus this paper tackles the problem of extracting the natural language text that is associated with each code segment in an article. We present and evaluate a set of heuristics that address the challenges of the text often not being colocated with the code segment as in developer communications such as online forums.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962359,code snippet description;information extraction;mining software repositories;text analysis,Computer science;Data mining;Documentation;Electronic mail;Libraries;Natural languages;Redundancy,digital libraries;information networks;information retrieval,code segments;computer science research;digital libraries;education conference;journal articles;machine learning;natural language text;online software-related documents;research articles,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
How Open Source Projects Use Static Code Analysis Tools in Continuous Integration Pipelines,F. Zampetti; S. Scalabrino; R. Oliveto; G. Canfora; M. Di Penta,"Univ. of Sannio, Benevento, Italy",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,334,344,"Static analysis tools are often used by software developers to entail early detection of potential faults, vulnerabilities, code smells, or to assess the source code adherence to coding standards and guidelines. Also, their adoption within Continuous Integration (CI) pipelines has been advocated by researchers and practitioners. This paper studies the usage of static analysis tools in 20 Java open source projects hosted on GitHub and using Travis CI as continuous integration infrastructure. Specifically, we investigate (i) which tools are being used and how they are configured for the CI, (ii) what types of issues make the build fail or raise warnings, and (iii) whether, how, and after how long are broken builds and warnings resolved. Results indicate that in the analyzed projects build breakages due to static analysis tools are mainly related to adherence to coding standards, and there is also some attention to missing licenses. Build failures related to tools identifying potential bugs or vulnerabilities occur less frequently, and in some cases such tools are activated in a ""softer"" mode, without making the build fail. Also, the study reveals that build breakages due to static analysis tools are quickly fixed by actually solving the problem, rather than by disabling the warning, and are often properly documented.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.2,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962383,Continuous Integration;Empirical Study;Open Source Projects;Static Analysis Tools,Data mining;Encoding;History;Java;Pipelines;Software;Tools,Java;program diagnostics;public domain software;software development management,CI;GitHub;Java open source projects;Travis CI;build failures;code smells;coding standards;continuous integration infrastructure;continuous integration pipelines;open source projects;potential faults;software developers;source code adherence;static code analysis tools,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Leveraging Automated Sentiment Analysis in Software Engineering,M. R. Islam; M. F. Zibran,"Univ. of New Orleans, New Orleans, LA, USA",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,203,214,"Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed in developing SentiStrength-SE, a tool for improved sentiment analysis especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both quantitative and qualitative evaluations of our tool. SentiStrength-SE achieves 73.85% precision and 85% recall, which are significantly higher than a state-of-the-art sentiment analysis tool we compare with.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962370,,Benchmark testing;Electronic mail;Gold;Sentiment analysis;Software;Software engineering;Tools,sentiment analysis;software tools,SentiStrength-SE tool;automated sentiment analysis;benchmark dataset;software engineering textual artifacts,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Mining Change Histories for Unknown Systematic Edits,T. Molderez; R. Stevens; C. De Roover,"Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,248,256,"Software developers often need to repeat similar modifications in multiple different locations of a system's source code. These repeated similar modifications, or systematic edits, can be both tedious and error-prone to perform manually. While there are tools that can be used to assist in automating systematic edits, it is not straightforward to find out where the occurrences of a systematic edit are located in an existing system. This knowledge is valuable to help decide whether refactoring is needed, or whether future occurrences of an existing systematic edit should be automated. In this paper, we tackle the problem of finding unknown systematic edits using a closed frequent itemset mining algorithm, operating on sets of distilled source code changes. This approach has been implemented for Java programs in a tool called SysEdMiner. To evaluate the tool's precision and scalability, we have applied it to an industrial use case.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962375,Systematic edits;change distilling;frequent itemset mining,Data mining;Indexes;Itemsets;Java;Software;Systematics;Tools,Java;data mining;history;software maintenance;source code (software),Java programs;SysEdMiner;automating systematic edits;change history mining;closed frequent itemset mining algorithm;distilled source code changes;industrial use case;unknown systematic edits,,1,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,8
"Oops, My Tests Broke the Build: An Explorative Analysis of Travis CI with GitHub",M. Beller; G. Gousios; A. Zaidman,"Delft Univ. of Technol., Delft, Netherlands",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,356,367,"Continuous Integration (CI) has become a best practice of modern software development. Yet, at present, we have a shortfall of insight into the testing practices that are common in CI-based software development. In particular, we seek quantifiable evidence on how central testing is to the CI process, how strongly the project language influences testing, whether different integration environments are valuable and if testing on the CI can serve as a surrogate to local testing in the IDE. In an analysis of 2,640,825 Java and Ruby builds on Travis CI, we find that testing is the single most important reason why builds fail. Moreover, the programming language has a strong influence on both the number of executed tests, their run time, and proneness to fail. The use of multiple integration environments leads to 10% more failures being caught at build time. However, testing on Travis CI does not seem an adequate surrogate for running tests locally in the IDE. To further research on Travis CI with GitHub, we introduce TravisTorrent.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.62,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962385,,Best practices;Java;Programming;Software;Testing;Tools,integrated software;program testing;software engineering,GitHub;Travis CI;TravisTorrent;continuous integration;programming language;software development;software testing practices,,4,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Predicting Likelihood of Requirement Implementation within the Planned Iteration: An Empirical Study at IBM,A. Dehghan; A. Neal; K. Blincoe; J. Linaker; D. Damian,"Comput. Sci. Dept., Univ. of Victoria, Victoria, BC, Canada",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,124,134,"There has been a significant interest in the estimation of time and effort in fixing defects among both software practitioners and researchers over the past two decades. However, most of the focus has been on prediction of time and effort in resolving bugs, without much regard to predicting time needed to complete high-level requirements, a critical step in release planning. In this paper, we describe a mixed-method empirical study on three large IBM projects in which we developed and evaluated a process of training a predictive model constituting a set of 29 features in nine categories in order to predict if a requirement will be completed within its planned iteration. We conducted feature engineering through iterative interviews with IBM practitioners as well as analysis of large development repositories of these three projects. Using machine learning techniques, we were able to make predictions on completion time of requirements at four different stages of their lifetime. Using our industrial partner's interest in high precision over recall, we then adopted a cost sensitive learning method and maximized precision of predictions (ranging from 0.8 to 0.97) while maintaining an acceptable recall. We also ranked the features based on their relative importance to the optimized predictive model. We show that although satisfying predictions can be made at early stages, performance of predictions improves over time by taking advantage of requirements' progress data. Furthermore, feature importance ranking results show that although importance of features are highly dependent on project and prediction stage, there are certain features (e.g. requirement creator, time remained to the end of iteration, time since last requirement summary change and number of times requirement has been replanned for a new iteration) that emerge as important across most projects and stages, implying future worthwhile research directions for both researchers and practitioners.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962362,completion time prediction;machine learning;mining software repositories;release planning,Computer bugs;Computer science;History;Interviews;Planning;Predictive models;Software,formal specification;formal verification;learning (artificial intelligence);program debugging,IBM projects;bugs;cost sensitive learning method;feature engineering;feature importance ranking;high-level requirements;machine learning;mixed-method empirical study;planned iteration;release planning,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Predicting Usefulness of Code Review Comments Using Textual Features and Developer Experience,M. M. Rahman; C. K. Roy; R. G. Kula,"Univ. of Saskatchewan, Saskatoon, SK, Canada",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,215,226,"Although peer code review is widely adopted in both commercial and open source development, existing studies suggest that such code reviews often contain a significant amount of non-useful review comments. Unfortunately, to date, no tools or techniques exist that can provide automatic support in improving those non-useful comments. In this paper, we first report a comparative study between useful and non-useful review comments where we contrast between them using their textual characteristics, and reviewers' experience. Then, based on the findings from the study, we develop RevHelper, a prediction model that can help the developers improve their code review comments through automatic prediction of their usefulnessduring review submission. Comparative study using 1,116 review comments suggested that useful comments share more vocabulary with the changed code, contain salient items like relevant code elements, and their reviewers are generally more experienced. Experiments using 1,482 review comments report that our model can predict comment usefulness with 66% prediction accuracy which is promising. Comparison with three variants of a baseline model using a case study validates our empirical findings and demonstrates the potential of our model.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962371,Code review quality;change triggering capability;code element;review comment usefulness;reviewing experience,Companies;Inspection;Libraries;Predictive models;Software;Tools;Vocabulary,public domain software;software quality;source code (software);text analysis,RevHelper;code review comments;commercial source development;developer experience;open source development;relevant code elements;review submission;software development;textual characteristics;textual features;usefulness prediction,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Rationale in Development Chat Messages: An Exploratory Study,R. Alkadhi; T. Lata; E. Guzmany; B. Bruegge,"Fac. of Inf., Tech. Univ. Munchen, Garching, Germany",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,436,446,"Chat messages of development teams play an increasingly significant role in software development, having replaced emails in some cases. Chat messages contain information about discussed issues, considered alternatives and argumentation leading to the decisions made during software development. These elements, defined as rationale, are invaluable during software evolution for documenting and reusing development knowledge. Rationale is also essential for coping with changes and for effective maintenance of the software system. However, exploiting the rationale hidden in the chat messages is challenging due to the high volume of unstructured messages covering a wide range of topics. This work presents the results of an exploratory study examining the frequency of rationale in chat messages, the completeness of the available rationale and the potential of automatic techniques for rationale extraction. For this purpose, we apply content analysis and machine learning techniques on more than 8,700 chat messages from three software development projects. Our results show that chat messages are a rich source of rationale and that machine learning is a promising technique for detecting rationale and identifying different rationale elements.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962392,Chat messages;Empirical Software Engineering;Rationale,Data mining;Encoding;Informatics;Logic gates;Manuals;Software systems,electronic messaging;learning (artificial intelligence);software development management;software maintenance;text analysis,content analysis;development chat messages;development knowledge;development teams;machine learning techniques;rationale extraction;software development;software development projects;software evolution;software system maintenance;unstructured messages,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
RefDiff: Detecting Refactorings in Version Histories,D. Silva; M. T. Valente,"Dept. of Comput. Sci., Univ. Fed. de Minas Gerais, Belo Horizonte, Brazil",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,269,279,"Refactoring is a well-known technique that is widely adopted by software engineers to improve the design and enable the evolution of a system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, merge code changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring types. In an evaluation using an oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100% and recall of 88%. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-the-art approaches.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962377,git;refactoring;software evolution;software repositories,Crawlers;History;Java;Measurement;Software;Syntactics;Tools,program diagnostics;software maintenance,Java projects;RefDiff;code revisions;code similarity;git repository;refactoring detection;software engineers;software evolution;static analysis;version histories,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
"Some from Here, Some from There: Cross-Project Code Reuse in GitHub",M. Gharehyazie; B. Ray; V. Filkov,,2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,291,301,"Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962379,GitHub;code reuse;cross-project clones,Cloning;Ecosystems;Encoding;Java;Software;Syntactics;Tools,software reusability;source code (software),Deckard;GitHub;clone finding tool;code foraging;code sharing;cross-project cloning;cross-project code reuse;ecosystem cloning;onion model;project repositories,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Source File Set Search for Clone-and-Own Reuse Analysis,T. Ishio; Y. Sakaguchi; K. Ito; K. Inoue,"Grad. Sch. of Inf. Sci. & Technol., Osaka Univ., Suita, Japan",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,257,268,"Clone-and-own approach is a natural way of source code reuse for software developers. To assess how known bugs and security vulnerabilities of a cloned component affect an application, developers and security analysts need to identify an original version of the component and understand how the cloned component is different from the original one. Although developers may record the original version information in a version control system and/or directory names, such information is often either unavailable or incomplete. In this research, we propose a code search method that takes as input a set of source files and extracts all the components including similar files from a software ecosystem (i.e., a collection of existing versions of software packages). Our method employs an efficient file similarity computation using b-bit minwise hashing technique. We use an aggregated file similarity for ranking components. To evaluate the effectiveness of this tool, we analyzed 75 cloned components in Firefox and Android source code. The tool took about two hours to report the original components from 10 million files in Debian GNU/Linux packages. Recall of the top-five components in the extracted lists is 0.907, while recall of a baseline using SHA-1 file hash is 0.773, according to the ground truth recorded in the source code repositories.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962376,Software reuse;file clone detection;origin analysis;source code search,Cloning;Data mining;Software,Android (operating system);cryptography;software packages;software reusability;source code (software),Android source code;Debian GNU package;Debian Linux package;Firefox source code;SHA-1 file hash;b-bit minwise hashing technique;clone-and-own reuse analysis;component extraction;directory names;efficient file similarity computation;security vulnerabilities;software ecosystem;software packages;source code repositories;source code reuse;source file set search;source files;version control system,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Spencer: Interactive Heap Analysis for the Masses,S. Brandauer; T. Wrigstad,"Uppsala Univ., Uppsala, Sweden",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,113,123,"Programming language-design and run-time-implementation require detailed knowledge about the programs that users want to implement. Acquiring this knowledge is hard, and there is little tool support to effectively estimate whether a proposed tradeoff actually makes sense in the context of real world applications. Ideally, knowledge about behaviour of ""typical"" programs is 1) easily obtainable, 2) easily reproducible, and 3) easily sharable. We present Spencer, an open source web service and APIframework for dynamic analysis of a continuously growing set of traces of standard program corpora. Users do not obtain traces on their own, but can instead send queries to the web service that will be executed on a set of program traces. Queries are built in terms of a set of query combinators that present a high level interface for working with trace data. Since the framework is high level, and there is a hosted collection of recorded traces, queries are easy to implement. Since the data sets are shared by the research community, results are reproducible. Since the actual queries run on one (or many) servers that provide analysis as a service, obtaining results is possible on commodity hardware. Data in Spencer is meant to be obtained once, and analysed often, making the overhead of data collection mostly irrelevant. This allows Spencer to collect more data than traditional tracing tools can afford within their performance budget. Results in Spencer are cached, making complicated analyses that build on cached primitive queries speedy.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962361,dynamic analysis;heap analysis;tracing,Computer languages;Data visualization;Optimization;Performance analysis;Resource management;Tools;Web services,Web services;data analysis;program diagnostics;public domain software;query processing,APIframework;Spencer;data analysis;dynamic analysis;interactive heap analysis;masses;open source Web service;program knowledge;program traces;programming language-design;query combinators;run-time-implementation,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
SpreadCluster: Recovering Versioned Spreadsheets through Similarity-Based Clustering,L. Xu; W. Dou; C. Gao; J. Wang; J. Wei; H. Zhong; T. Huang,"State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,158,169,"Version information plays an important role in spreadsheet understanding, maintaining and quality improving. However, end users rarely use version control tools to document spreadsheets' version information. Thus, the spreadsheets' version information is missing, and different versions of a spreadsheet coexist as individual and similar spreadsheets. Existing approaches try to recover spreadsheet version information through clustering these similar spreadsheets based on spreadsheet filenames or related email conversation. However, the applicability and accuracy of existing clustering approaches are limited due to the necessary information (e.g., filenames and email conversation) is usually missing. We inspected the versioned spreadsheets in VEnron, which is extracted from the Enron Corporation. In VEnron, the different versions of a spreadsheet are clustered into an evolution group. We observed that the versioned spreadsheets in each evolution group exhibit certain common features (e.g., similar table headers and worksheet names). Based on this observation, we proposed an automatic clustering algorithm, SpreadCluster. SpreadCluster learns the criteria of features from the versioned spreadsheets in VEnron, and then automatically clusters spreadsheets with the similar features into the same evolution group. We applied SpreadCluster on all spreadsheets in the Enron corpus. The evaluation result shows that SpreadCluster could cluster spreadsheets with higher precision (78.5% vs. 59.8%) and recall rate (70.7% vs. 48.7%) than the filename-based approach used by VEnron. Based on the clustering result by SpreadCluster, we further created a new versioned spreadsheet corpus VEnron2, which is much bigger than VEnron (12,254 vs. 7,294 spreadsheets). We also applied SpreadCluster on the other two spreadsheet corpora FUSE and EUSES. The results show that SpreadCluster can cluster the versioned spreadsheets in these two corpora with high precision (91.0% and 79.8%).",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962365,clustering;evolution;spreadsheet;version,Cloning;Electronic mail;Feature extraction;Fuses;Layout;Software;Tools,configuration management;pattern clustering;software maintenance;software quality;spreadsheet programs,Enron Corporation;Enron corpus;SpreadCluster;VEnron;automatic clustering algorithm;email conversation;similarity-based clustering;spreadsheet corpora EUSES;spreadsheet corpora FUSE;spreadsheet filenames;spreadsheet maintaining;spreadsheet quality;spreadsheet understanding;spreadsheet version information,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Stack Overflow in Github: Any Snippets There?,D. Yang; P. Martins; V. Saini; C. Lopes,"Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,280,290,"When programmers look for how to achieve certain programming tasks, Stack Overflow is a popular destination in search engine results. Over the years, Stack Overflow has accumulated an impressive knowledge base of snippets of code that are amply documented. We are interested in studying how programmers use these snippets of code in their projects. Can we find Stack Overflow snippets in real projects? When snippets are used, is this copy literal or does it suffer adaptations? And are these adaptations specializations required by the idiosyncrasies of the target artifact, or are they motivated by specific requirements of the programmer? The large-scale study presented on this paper analyzes 909k non-fork Python projects hosted on Github, which contain 290M function definitions, and 1.9M Python snippets captured in Stack Overflow. Results are presented as quantitative analysis of block-level code cloning intra and inter Stack Overflow and GitHub, and as an analysis of programming behaviors through the qualitative analysis of our findings.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962378,code clone;code reuse;large-scale analysis,Androids;Cloning;Cloud computing;Data mining;Humanoid robots;Pipelines;Programming,knowledge based systems;programming;search engines,Github;Python snippets;block-level code;code snippets;interStack Overflow;intraStack Overflow;knowledge base;nonfork Python projects;programming behavior analysis;quantitative analysis;search engine results,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Structure and Evolution of Package Dependency Networks,R. Kikas; G. Gousios; M. Dumas; D. Pfahl,"Univ. of Tartu, Tartu, Estonia",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,102,112,"Software developers often include available open-source software packages into their projects to minimize redundant effort. However, adding a package to a project can also introduce risks, which can propagate through multiple levels of dependencies. Currently, not much is known about the structure of open-source package ecosystems of popular programming languages and the extent to which transitive bug propagation is possible. This paper analyzes the dependency network structure and evolution of the JavaScript, Ruby, and Rust ecosystems. The reported results reveal significant differences across language ecosystems. The results indicate that the number of transitive dependencies for JavaScript has grown 60% over the last year, suggesting that developers should look more carefully into their dependencies to understand what exactly is included. The study also reveals that vulnerability to a removal of the most popular package is increasing, yet most other packages have a decreasing impact on vulnerability. The findings of this study can inform the development of dependency management tools.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.55,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962360,dependency management;mining software repositories;software ecosystems;software evolution,Computer bugs;Computer languages;Ecosystems;Libraries;Software packages;Tools,public domain software;software maintenance;software packages;system recovery,JavaScript ecosystem;Ruby ecosystem;Rust ecosystem;dependency management tool development;language ecosystems;number-of-transitive dependencies;open-source package ecosystems;package dependency network evolution;package dependency network structure;programming languages;redundant effort minimization;transitive bug propagation,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
The Impact of Using Regression Models to Build Defect Classifiers,G. K. Rajbahadur; S. Wang; Y. Kamei; A. E. Hassan,"Queen's Univ., Kingston, QC, Canada",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,135,145,"It is common practice to discretize continuous defect counts into defective and non-defective classes and use them as a target variable when building defect classifiers (discretized classifiers). However, this discretization of continuous defect counts leads to information loss that might affect the performance and interpretation of defect classifiers. Another possible approach to build defect classifiers is through the use of regression models then discretizing the predicted defect counts into defective and non-defective classes (regression-based classifiers). In this paper, we compare the performance and interpretation of defect classifiers that are built using both approaches (i.e., discretized classifiers and regression-based classifiers) across six commonly used machine learning classifiers (i.e., linear/logistic regression, random forest, KNN, SVM, CART, and neural networks) and 17 datasets. We find that: i) Random forest based classifiers outperform other classifiers (best AUC) for both classifier building approaches, ii) In contrast to common practice, building a defect classifier using discretized defect counts (i.e., discretized classifiers) does not always lead to better performance. Hence we suggest that future defect classification studies should consider building regression-based classifiers (in particular when the defective ratio of the modeled dataset is low). Moreover, we suggest that both approaches for building defect classifiers should be explored, so the best-performing classifier can be used when determining the most influential features.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.4,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962363,Bug prediction;Classification via regression;Discretization;Model Interpretation;Non-Discretization;Random forest,Buildings;Computational modeling;Correlation;Data collection;Predictive models;Redundancy;Software,learning (artificial intelligence);pattern classification;program debugging;regression analysis;software quality,CART;KNN;SVM;classifier building approach;continuous defect counts discretization;defect classification studies;defect classifiers;defective classes;discretized classifiers;linear regression;logistic regression;machine learning classifiers;neural networks;nondefective classes;random forest;regression models;regression-based classifiers;target variable,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
To Mock or Not to Mock? An Empirical Study on Mocking Practices,D. Spadini; M. Aniche; M. Bruntink; A. Bacchelli,,2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,402,412,"When writing automated unit tests, developers often deal with software artifacts that have several dependencies. In these cases, one has the possibility of either instantiating the dependencies or using mock objects to simulate the dependencies' expected behavior. Even though recent quantitative studies showed that mock objects are widely used in OSS projects, scientific knowledge is still lacking on how and why practitioners use mocks. Such a knowledge is fundamental to guide further research on this widespread practice and inform the design of tools and processes to improve it. The objective of this paper is to increase our understanding of which test dependencies developers (do not) mock and why, as well as what challenges developers face with this practice. To this aim, we create MockExtractor, a tool to mine the usage of mock objects in testing code and employ it to collect data from three OSS projects and one industrial system. Sampling from this data, we manually analyze how more than 2,000 test dependencies are treated. Subsequently, we discuss our findings with developers from these systems, identifying practices, rationales, and challenges. These results are supported by a structured survey with more than 100 professionals. The study reveals that the usage of mocks is highly dependent on the responsibility and the architectural concern of the class. Developers report to frequently mock dependencies that make testing difficult and prefer to not mock classes that encapsulate domain concepts/rules of the system. Among the key challenges, developers report that maintaining the behavior of the mock compatible with the behavior of original class is hard and that mocking increases the coupling between the test and the production code.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962389,,Databases;Interviews;Java;Software systems;Testing;Tools,program testing,MockExtractor;automated unit tests;mock dependencies;mock objects;mocking practices;production code;software artifacts;testing code,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Understanding the Origins of Mobile App Vulnerabilities: A Large-Scale Measurement Study of Free and Paid Apps,T. Watanabe; M. Akiyama; F. Kanei; E. Shioji; Y. Takata; B. Sun; Y. Ishi; T. Shibahara; T. Yagi; T. Mori,"NTT Secure Platform Labs., Japan",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,14,24,"This paper reports a large-scale study that aims to understand how mobile application (app) vulnerabilities are associated with software libraries. We analyze both free and paid apps. Studying paid apps was quite meaningful because it helped us understand how differences in app development/maintenance affect the vulnerabilities associated with libraries. We analyzed 30k free and paid apps collected from the official Android marketplace. Our extensive analyses revealed that approximately 70%/50% of vulnerabilities of free/paid apps stem from software libraries, particularly from third-party libraries. Somewhat paradoxically, we found that more expensive/popular paid apps tend to have more vulnerabilities. This comes from the fact that more expensive/popular paid apps tend to have more functionality, i.e., more code and libraries, which increases the probability of vulnerabilities. Based on our findings, we provide suggestions to stakeholders of mobile app distribution ecosystems.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962351,Mobile App;Software Library;Vulnerability,Androids;Feature extraction;Humanoid robots;Mobile communication;Software;Software libraries,mobile computing;software libraries;software maintenance;software metrics,application development;application maintenance;free application;mobile application distribution ecosystems;mobile application vulnerabilities;official Android marketplace;paid application;software libraries;third-party libraries;vulnerability probability,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10
Who Will Leave the Company?: A Large-Scale Industry Study of Developer Turnover by Mining Monthly Work Report,L. Bao; Z. Xing; X. Xia; D. Lo; S. Li,"Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,170,181,"Software developer turnover has become a big challenge for information technology (IT) companies. The departure of key software developers might cause big loss to an IT company since they also depart with important business knowledge and critical technical skills. Understanding developer turnover is very important for IT companies to retain talented developers and reduce the loss due to developers' departure. Previous studies mainly perform qualitative observations or simple statistical analysis of developers' activity data to understand developer turnover. In this paper, we investigate whether we can predict the turnover of software developers in non-open source companies by automatically analyzing monthly self-reports. The monthly work reports in our study are from two IT companies. Monthly reports in these two companies are used to report a developer's activities and working hours in a month. We would like to investigate whether a developer will leave the company after he/she enters company for one year based on his/her first six monthly reports. To perform our prediction, we extract many factors from monthly reports, which are grouped into 6 dimensions. We apply several classifiers including naive Bayes, SVM, decision tree, kNN and random forest. We conduct an experiment on about 6-years monthly reports from two companies, this data contains 3,638 developers over time. We find that random forest classifier achieves the best performance with an F1-measure of 0.86 for retained developers and an F1-measure of 0.65 for not-retained developers. We also investigate the relationship between our proposed factors and developers' departure, and the important factors that indicate a developer's departure. We find the content of task report in monthly reports, the standard deviation of working hours, and the standard deviation of working hours of project members in the first month are the top three important factors.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.58,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962366,developer turnover;mining software repositories;prediction model,Companies;Computer science;Data mining;Feature extraction;Software;Standards,Bayes methods;business data processing;decision trees;pattern classification;software development management;statistical analysis;support vector machines,IT company;SVM;decision tree;information technology;kNN;large-scale industry;monthly work report;naive Bayes;random forest classifier;software developer turnover;statistical analysis,,,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,11
Who You Gonna Call? Analyzing Web Requests in Android Applications,M. Rapoport; P. Suter; E. Wittern; O. Lhotak; J. Dolby,"Univ. of Waterloo, Waterloo, ON, Canada",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20170703,2017,,,80,90,"Relying on ubiquitous Internet connectivity, applications on mobile devices frequently perform web requests during their execution. They fetch data for users to interact with, invoke remote functionalities, or send user-generated content or meta-data. These requests collectively reveal common practices of mobile application development, like what external services are used and how, and they point to possible negative effects like security and privacy violations, or impacts on battery life. In this paper, we assess different ways to analyze what web requests Android applications make. We start by presenting dynamic data collected from running 20 randomly selected Android applications and observing their network activity. Next, we present a static analysis tool, Stringoid, that analyzes string concatenations in Android applications to estimate constructed URL strings. Using Stringoid, we extract URLs from 30, 000 Android applications, and compare the performance with a simpler constant extraction analysis. Finally, we present a discussion of the advantages and limitations of dynamic and static analyses when extracting URLs, as we compare the data extracted by Stringoid from the same 20 applications with the dynamically collected data.",,Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4,10.1109/MSR.2017.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962358,dynamic analysis;mobile applications;static analysis;web requests,Androids;Humanoid robots;Mobile applications;Mobile handsets;Protocols;Tools;Uniform resource locators,Android (operating system);Internet;data privacy;meta data;mobile computing;program diagnostics,Android applications;Stringoid;URL strings;Web requests;battery life;constant extraction analysis;dynamic analysis;meta-data;mobile application development;mobile devices;privacy violations;remote functionalities;security violations;static analysis tool;ubiquitous Internet connectivity;user-generated content,,1,,,,,,20-21 May 2017,,IEEE,IEEE Conferences,,10