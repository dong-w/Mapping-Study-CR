Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication_Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier,,page_num
A Dynamic Slicing Technique for UML Architectural Models,J. T. Lallchandani; R. Mall,"Indian Institute of Technology Kharagpur, WB INDIA",IEEE Transactions on Software Engineering,20111205,2011,37,6,737,771,"This paper proposes a technique for dynamic slicing of UML architectural models. The presence of related information in diverse model parts (or fragments) makes dynamic slicing of Unified Modeling Language (UML) models a complex problem. We first extract all relevant information from a UML model specifying a software architecture into an intermediate representation, which we call a Model Dependency Graph (MDG). For a given slicing criterion, our slicing algorithm traverses the constructed MDG to identify the relevant model parts that are directly or indirectly affected during the execution of a specified scenario. One novelty of our approach is computation of dynamic slice based on the structural and behavioral (interactions only) UML models as against independently processing separate UML models, and determining the implicit interdependencies among different model elements distributed across model views. We also briefly discuss a prototype tool named Archlice, which we have developed to implement our algorithm.",0098-5589;00985589,,10.1109/TSE.2010.112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680909,Software architecture;UML;architectural metamodel;dynamic slicing;impact analysis.,Analytical models;Computational modeling;Computer architecture;Heuristic algorithms;Software algorithms;Software architecture;Unified modeling language,Unified Modeling Language;program slicing;software architecture;software prototyping,Archlice;UML architectural models;dynamic slicing technique;model dependency graph;prototype tool;software architecture;unified modeling language models,,10,,46,,,20110106,Nov.-Dec. 2011,,IEEE,IEEE Journals & Magazines,,34
An Analysis and Survey of the Development of Mutation Testing,Y. Jia; M. Harman,"University College London, London",IEEE Transactions on Software Engineering,20110929,2011,37,5,649,678,"Mutation Testing is a fault-based software testing technique that has been widely studied for over three decades. The literature on Mutation Testing has contributed a set of approaches, tools, developments, and empirical results. This paper provides a comprehensive analysis and survey of Mutation Testing. The paper also presents the results of several development trend analyses. These analyses provide evidence that Mutation Testing techniques and tools are reaching a state of maturity and applicability, while the topic of Mutation Testing itself is the subject of increasing interest.",0098-5589;00985589,,10.1109/TSE.2010.62,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487526,Mutation testing;survey.,Automata;Books;Computer languages;Educational institutions;Fault detection;Genetic mutations;History;Java;Programming profession;Software testing,fault diagnosis;program testing,comprehensive analysis;development trend analysis;empirical results;fault-based software testing technique;mutation testing development;mutation testing technique;mutation testing tool,,317,,264,,,20100617,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines,,29
"Assessing, Comparing, and Combining State Machine-Based Testing and Structural Testing: A Series of Experiments",S. Mouchawrab; L. C. Briand; Y. Labiche; M. Di Penta,"Carleton University, Ottawa, Canada",IEEE Transactions on Software Engineering,20110324,2011,37,2,161,187,"A large number of research works have addressed the importance of models in software engineering. However, the adoption of model-based techniques in software organizations is limited since these models are perceived to be expensive and not necessarily cost-effective. Focusing on model-based testing, this paper reports on a series of controlled experiments. It investigates the impact of state machine testing on fault detection in class clusters and its cost when compared with structural testing. Based on previous work showing this is a good compromise in terms of cost and effectiveness, this paper focuses on a specific state-based technique: the round-trip paths coverage criterion. Round-trip paths testing is compared to structural testing, and it is investigated whether they are complementary. Results show that even when a state machine models the behavior of the cluster under test as accurately as possible, no significant difference between the fault detection effectiveness of the two test strategies is observed, while the two test strategies are significantly more effective when combined by augmenting state machine testing with structural testing. A qualitative analysis also investigates the reasons why test techniques do not detect certain faults and how the cost of state machine testing can be brought down.",0098-5589;00985589,,10.1109/TSE.2010.32,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416729,State-based software testing;controlled experiments;state machines.;structural testing,,fault tolerant computing;finite state machines;program testing;software engineering,fault detection;model based techniques;round trip paths testing;software engineering;software organizations;state machine based testing;structural testing,,25,,69,,,20100218,March-April 2011,,IEEE,IEEE Journals & Magazines,,26
Deriving a Slicing Algorithm via FermaT Transformations,M. P. Ward; H. Zedan,"De Montfort University, Leicester",IEEE Transactions on Software Engineering,20110128,2011,37,1,24,47,"In this paper, we present a case study in deriving an algorithm from a formal specification via FermaT transformations. The general method (which is presented in a separate paper) is extended to a method for deriving an implementation of a program transformation from a specification of the program transformation. We use program slicing as an example transformation since this is of interest outside the program transformation community. We develop a formal specification for program slicing in the form of a WSL specification statement which is refined into a simple slicing algorithm by applying a sequence of general purpose program transformations and refinements. Finally, we show how the same methods can be used to derive an algorithm for semantic slicing. The main novel contributions of this paper are: 1) developing a formal specification for slicing, 2) expressing the definition of slicing in terms of a WSL specification statement, and 3) by applying correctness preserving transformations to the specification, we can derive a simple slicing algorithm.",0098-5589;00985589,,10.1109/TSE.2010.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401170,Program slicing;algorithm derivation.;formal methods;program transformations,,formal specification;program slicing,FermaT transformations;WSL specification statement;program slicing;program transformation;semantic slicing;slicing algorithm,,3,,73,,,20100129,Jan.-Feb. 2011,,IEEE,IEEE Journals & Magazines,,23
Dynamic Analysis for Diagnosing Integration Faults,L. Mariani; F. Pastore; M. Pezze,"University of Milano Bicocca, Milan",IEEE Transactions on Software Engineering,20110728,2011,37,4,486,508,"Many software components are provided with incomplete specifications and little access to the source code. Reusing such gray-box components can result in integration faults that can be difficult to diagnose and locate. In this paper, we present Behavior Capture and Test (BCT), a technique that uses dynamic analysis to automatically identify the causes of failures and locate the related faults. BCT augments dynamic analysis techniques with model-based monitoring. In this way, BCT identifies a structured set of interactions and data values that are likely related to failures (failure causes), and indicates the components and the operations that are likely responsible for failures (fault locations). BCT advances scientific knowledge in several ways. It combines classic dynamic analysis with incremental finite state generation techniques to produce dynamic models that capture complementary aspects of component interactions. It uses an effective technique to filter false positives to reduce the effort of the analysis of the produced data. It defines a strategy to extract information about likely causes of failures by automatically ranking and relating the detected anomalies so that developers can focus their attention on the faults. The effectiveness of BCT depends on the quality of the dynamic models extracted from the program. BCT is particularly effective when the test cases sample the execution space well. In this paper, we present a set of case studies that illustrate the adequacy of BCT to analyze both regression testing failures and rare field failures. The results show that BCT automatically filters out most of the false alarms and provides useful information to understand the causes of failures in 69 percent of the case studies.",0098-5589;00985589,,10.1109/TSE.2010.93,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611554,Dynamic Analysis;diagnosis;false positive filters;fault localization;field failure analysis.;regression failure analysis,Analytical models;Automata;Engines;Inference algorithms;Monitoring;Software;Testing,fault diagnosis;object-oriented programming;program testing;software fault tolerance,BCT;Integration Faults;behavior capture and test technique;dynamic analysis;incremental finite state generation techniques;model-based monitoring;software components,,26,,71,,,20101028,July-Aug. 2011,,IEEE,IEEE Journals & Magazines,,22
Dynamic QoS Management and Optimization in Service-Based Systems,R. Calinescu; L. Grunske; M. Kwiatkowska; R. Mirandola; G. Tamburrelli,"Aston University, Birmingham",IEEE Transactions on Software Engineering,20110527,2011,37,3,387,409,"Service-based systems that are dynamically composed at runtime to provide complex, adaptive functionality are currently one of the main development paradigms in software engineering. However, the Quality of Service (QoS) delivered by these systems remains an important concern, and needs to be managed in an equally adaptive and predictable way. To address this need, we introduce a novel, tool-supported framework for the development of adaptive service-based systems called QoSMOS (QoS Management and Optimization of Service-based systems). QoSMOS can be used to develop service-based systems that achieve their QoS requirements through dynamically adapting to changes in the system state, environment, and workload. QoSMOS service-based systems translate high-level QoS requirements specified by their administrators into probabilistic temporal logic formulae, which are then formally and automatically analyzed to identify and enforce optimal system configurations. The QoSMOS self-adaptation mechanism can handle reliability and performance-related QoS requirements, and can be integrated into newly developed solutions or legacy systems. The effectiveness and scalability of the approach are validated using simulations and a set of experiments based on an implementation of an adaptive service-based system for remote medical assistance.",0098-5589;00985589,,10.1109/TSE.2010.92,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611553,QoS management;QoS optimization;Service-oriented software engineering;adaptive systems.,Analytical models;Markov processes;Optimization;Probabilistic logic;Quality of service;Scattering;Unified modeling language,health care;medical computing;optimisation;quality of service;software engineering,QoS management;QoSMOS;health care;optimization;remote medical assistance;service-based systems;software engineering,,105,,102,,,20101028,May-June 2011,,IEEE,IEEE Journals & Magazines,,22
Improving Source Code Lexicon via Traceability and Information Retrieval,A. De Lucia; M. Di Penta; R. Oliveto,"University of Salerno, Fisciano",IEEE Transactions on Software Engineering,20110324,2011,37,2,205,227,"The paper presents an approach helping developers to maintain source code identifiers and comments consistent with high-level artifacts. Specifically, the approach computes and shows the textual similarity between source code and related high-level artifacts. Our conjecture is that developers are induced to improve the source code lexicon, i.e., terms used in identifiers or comments, if the software development environment provides information about the textual similarity between the source code under development and the related high-level artifacts. The proposed approach also recommends candidate identifiers built from high-level artifacts related to the source code under development and has been implemented as an Eclipse plug-in, called COde Comprehension Nurturant Using Traceability (COCONUT). The paper also reports on two controlled experiments performed with master's and bachelor's students. The goal of the experiments is to evaluate the quality of identifiers and comments (in terms of their consistency with high-level artifacts) in the source code produced when using or not using COCONUT. The achieved results confirm our conjecture that providing the developers with similarity between code and high-level artifacts helps to improve the quality of source code lexicon. This indicates the potential usefulness of COCONUT as a feature for software development environments.",0098-5589;00985589,,10.1109/TSE.2010.89,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601742,Software traceability;empirical software engineering.;information retrieval;software development environments;source code comprehensibility;source code identifier quality,,information retrieval;program diagnostics;software quality,COCONUT;bachelor student;candidate identifier;code comprehension nurturant using traceability;high level artifact;information retrieval;master student;software development;source code lexicon;textual similarity,,25,,72,,,20101014,March-April 2011,,IEEE,IEEE Journals & Magazines,,22
A Classification Framework for Software Component Models,I. Crnkovic; S. Sentilles; A. Vulgarakis; M. R. V. Chaudron,"M&#x0E4;lardalen University, V&#x0E4;ster&#x0E5;s",IEEE Transactions on Software Engineering,20110929,2011,37,5,593,615,"In the last decade, a large number of different software component models have been developed, with different aims and using different principles and technologies. This has resulted in a number of models which have many similarities, but also principal differences, and in many cases unclear concepts. Component-based development has not succeeded in providing standard principles, as has, for example, object-oriented development. In order to increase the understanding of the concepts and to differentiate component models more easily, this paper identifies, discusses, and characterizes fundamental principles of component models and provides a Component Model Classification Framework based on these principles. Further, the paper classifies a large number of component models using this framework.",0098-5589;00985589,,10.1109/TSE.2010.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587419,Software components;component composition.;component lifecycle;extra-functional properties;software component models,Bismuth;Data models;Packaging,object-oriented programming;pattern classification,component based development;component model classification framework;object oriented development;software component models,,67,2,53,,,20100927,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines,,22
Systematic review and aggregation of empirical studies on elicitation techniques,O. Dieste; N. Juristo,"Universidad Polit&#x0E9;cnica de Madrid, Boadilla del Monte",IEEE Transactions on Software Engineering,20110324,2011,37,2,283,304,"We have located the results of empirical studies on elicitation techniques and aggregated these results to gather empirically grounded evidence. Our chosen surveying methodology was systematic review, whereas we used an adaptation of comparative analysis for aggregation because meta-analysis techniques could not be applied. The review identified 564 publications from the SCOPUS, IEEEXPLORE, and ACM DL databases, as well as Google. We selected and extracted data from 26 of those publications. The selected publications contain 30 empirical studies. These studies were designed to test 43 elicitation techniques and 50 different response variables. We got 100 separate results from the experiments. The aggregation generated 17 pieces of knowledge about the interviewing, laddering, sorting, and protocol analysis elicitation techniques. We provide a set of guidelines based on the gathered pieces of knowledge.",0098-5589;00985589,,10.1109/TSE.2010.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416730,Elicitation methods;experimentation;performance measures;systematic literature review.,Data mining;Databases;Information analysis;Protocols;Sensitivity analysis;Software measurement;Sorting,knowledge acquisition;software engineering,ACM DL databases;Google;IEEEXPLORE;SCOPUS;elicitation techniques;empirically grounded evidence;surveying methodology;systematic review,,36,,102,,,20100218,March-April 2011,,IEEE,IEEE Journals & Magazines,,21
Preventing Temporal Violations in Scientific Workflows: Where and How,X. Liu; Y. Yang; Y. Jiang; J. Chen,"Swinburne University of Technology, Melbourne",IEEE Transactions on Software Engineering,20111205,2011,37,6,805,825,"Due to the dynamic nature of the underlying high-performance infrastructures for scientific workflows such as grid and cloud computing, failures of timely completion of important scientific activities, namely, temporal violations, often take place. Unlike conventional exception handling on functional failures, nonfunctional QoS failures such as temporal violations cannot be passively recovered. They need to be proactively prevented through dynamically monitoring and adjusting the temporal consistency states of scientific workflows at runtime. However, current research on workflow temporal verification mainly focuses on runtime monitoring, while the adjusting strategy for temporal consistency states, namely, temporal adjustment, has so far not been thoroughly investigated. For this issue, two fundamental problems of temporal adjustment, namely, where and how, are systematically analyzed and addressed in this paper. Specifically, a novel minimum probability time redundancy-based necessary and sufficient adjustment point selection strategy is proposed to address the problem of where and an innovative genetic-algorithm-based effective and efficient local rescheduling strategy is proposed to tackle the problem of how. The results of large-scale simulation experiments with generic workflows and specific real-world applications demonstrate that our temporal adjustment strategy can remarkably prevent the violations of both local and global temporal constraints in scientific workflows.",0098-5589;00985589,,10.1109/TSE.2010.99,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645643,Workflow management;exception handling;reliability;software verification;statistical methods.,Decision support systems;Quality of service;Software reliability;Workflow management software,genetic algorithms;middleware;quality of service;workflow management software,adjustment point selection;cloud computing;generic workflows;grid computing;nonfunctional QoS failures;rescheduling strategy;scientific workflows;temporal violation prevention;workflow temporal verification,,23,,53,,,20101129,Nov.-Dec. 2011,,IEEE,IEEE Journals & Magazines,,20
Efficient Consistency Measurement Based on Behavioral Profiles of Process Models,M. Weidlich; J. Mendling; M. Weske,"Hasso Plattner Institute, Potsdam",IEEE Transactions on Software Engineering,20110527,2011,37,3,410,429,"Engineering of process-driven business applications can be supported by process modeling efforts in order to bridge the gap between business requirements and system specifications. However, diverging purposes of business process modeling initiatives have led to significant problems in aligning related models at different abstract levels and different perspectives. Checking the consistency of such corresponding models is a major challenge for process modeling theory and practice. In this paper, we take the inappropriateness of existing strict notions of behavioral equivalence as a starting point. Our contribution is a concept called behavioral profile that captures the essential behavioral constraints of a process model. We show that these profiles can be computed efficiently, i.e., in cubic time for sound free-choice Petri nets w.r.t. their number of places and transitions. We use behavioral profiles for the definition of a formal notion of consistency which is less sensitive to model projections than common criteria of behavioral equivalence and allows for quantifying deviation in a metric way. The derivation of behavioral profiles and the calculation of a degree of consistency have been implemented to demonstrate the applicability of our approach. We also report the findings from checking consistency between partially overlapping models of the SAP reference model.",0098-5589;00985589,,10.1109/TSE.2010.96,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611557,Process model analysis;behavioral abstraction;consistency checking;consistency measures.;process model alignment,Analytical models;Business;Computational modeling;Petri nets;Semantics;Software;Unified modeling language,Petri nets;commerce;corporate modelling,Petri nets;SAP reference model;behavioral constraints;behavioral profiles;business process modeling;business requirements;efficient consistency measurement;process-driven business applications;system specifications,,51,,82,,,20101028,May-June 2011,,IEEE,IEEE Journals & Magazines,,19
A Flowchart Language for Quantum Programming,M. Ying; Y. Feng,"University of Technology, Sydney and Tsinghua University, Beijing",IEEE Transactions on Software Engineering,20110728,2011,37,4,466,485,"Several high-level quantum programming languages have been proposed in the previous research. In this paper, we define a low-level flowchart language for quantum programming, which can be used in implementation of high-level quantum languages and in design of quantum compilers. The formal semantics of the flowchart language is given, and the notion of correctness for programs written in this language is introduced. A structured quantum programming theorem is presented, which provides a technique of translating quantum flowchart programs into programs written in a high-level language, namely, a quantum extension of the while-language.",0098-5589;00985589,,10.1109/TSE.2010.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611555,Quantum programming;flowchart language;structured programming.;while-language,Computer languages;Computers;Probabilistic logic;Programming;Quantum computing;Quantum mechanics;Semantics,flowcharting;formal languages;program compilers;program interpreters;programming language semantics;quantum computing,formal semantics;high level quantum programming language;low level flowchart language;quantum compiler design;quantum flowchart program translation;structured quantum programming theorem,,6,,34,,,20101028,July-Aug. 2011,,IEEE,IEEE Journals & Magazines,,19
Bristlecone: Language Support for Robust Software Applications,B. Demsky; S. Sundaramurthy,"University of California, Irvine, Irvine",IEEE Transactions on Software Engineering,20110128,2011,37,1,4,23,"We present Bristlecone, a programming language for robust software systems. Bristlecone applications have two components: a high-level organization specification that describes how the application's conceptual operations interact and a low-level operational specification that describes the sequence of instructions that comprise an individual conceptual operation. Bristlecone uses the high-level organization specification to recover the software system from an error to a consistent state and to reason how to safely continue the software system's execution after the error. We have implemented a compiler and runtime for Bristlecone. We have evaluated this implementation on three benchmark applications: a Web crawler, a Web server, and a multiroom chat server. We developed both a Bristlecone version and a Java version of each benchmark application. We used injected failures to evaluate the robustness of each version of the application. We found that the Bristlecone versions of the benchmark applications more successfully survived the injected failures. The Bristlecone compiler contains a static analysis that operates on the organization specification to generate a set of diagrams that graphically present the task interactions in the application. We have used the analysis to help understand the high-level structure of three Bristlecone applications: a game server, a Web server, and a chat server.",0098-5589;00985589,,10.1109/TSE.2010.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416725,Software robustness.,,Java;program compilers;program diagnostics;programming languages;software fault tolerance;specification languages,Bristlecone compiler;Java version;Web crawler;Web server;benchmark applications;game server;high-level organization specification;high-level structure;injected failures;language support;low-level operational specification;multiroom chat server;programming language;robust software applications;robust software systems;runtime;static analysis;task interactions,,1,,48,,,20100218,Jan.-Feb. 2011,,IEEE,IEEE Journals & Magazines,,19
A Comparison of Tabular Expression-Based Testing Strategies,X. Feng; D. L. Parnas; T. H. Tse; T. O'Callaghan,"Div. of Sci. &amp; Technol., United Int. Coll., Zhuhai, China",IEEE Transactions on Software Engineering,20110929,2011,37,5,616,634,"Tabular expressions have been proposed as a notation to document mathematically precise but readable software specifications. One of the many roles of such documentation is to guide testers. This paper 1) explores the application of four testing strategies (the partition strategy, decision table-based testing, the basic meaningful impact strategy, and fault-based testing) to tabular expression-based specifications, and 2) compares the strategies on a mathematical basis through formal and precise definitions of the subsumption relationship. We also compare these strategies through experimental studies. These results will help researchers improve current methods and will enable testers to select appropriate testing strategies for tabular expression-based specifications.",0098-5589;00985589,,10.1109/TSE.2011.78,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975175,Tabular expression;conditionally subsume.;subsume;test case constraint;unconditionally subsume,Documentation;Electronic mail;Redundancy;Software engineering;Software quality;Testing,formal specification;program testing;system documentation,decision table-based testing;fault-based testing;meaningful impact strategy;partition strategy;readable software specifications;tabular expression-based testing strategies,,4,,49,,,20110804,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines,,18
Putting Preemptive Time Petri Nets to Work in a V-Model SW Life Cycle,L. Carnevali; L. Ridi; E. Vicario,"Universit&#x0E0; di Firenze, Firenze",IEEE Transactions on Software Engineering,20111205,2011,37,6,826,844,"Preemptive Time Petri Nets (pTPNs) support modeling and analysis of concurrent timed SW components running under fixed priority preemptive scheduling. The model is supported by a well-established theory based on symbolic state space analysis through Difference Bounds Matrix (DBM) zones, with specific contributions on compositional modularization, trace analysis, and efficient overapproximation and cleanup in the management of suspension deriving from preemptive behavior. In this paper, we devise and implement a framework that brings the theory to application. To this end, we cast the theory into an organic tailoring of design, coding, and testing activities within a V-Model SW life cycle in respect of the principles of regulatory standards applied to the construction of safety-critical SW components. To implement the toolchain subtended by the overall approach into a Model Driven Development (MDD) framework, we complement the theory of state space analysis with methods and techniques supporting semiformal specification and automated compilation into pTPN models and real-time code, measurement-based Execution Time estimation, test case selection and execution, coverage evaluation.",0098-5589;00985589,,10.1109/TSE.2011.4,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680913,Execution Time estimation;Real-time systems;SW life cycle;V-Model;automated code generation;automated model transformation;coverage analysis.;model driven development;preemptive Time Petri Nets;real-time testing;safety-critical SW components;symbolic state space analysis;test case selection and execution,Analytical models;Computer architecture;Mathematical model;Petri nets;Real time systems;Unified modeling language,Petri nets;formal specification;program diagnostics;program testing;safety-critical software;scheduling,V-model SW life cycle;automated compilation;compositional modularization;concurrent timed SW components;coverage evaluation;difference bounds matrix zones;fixed priority preemptive scheduling;measurement-based execution time estimation;model driven development framework;overapproximation;pTPN models;preemptive time Petri nets;real-time code;safety-critical SW components;semiformal specification;symbolic state space analysis;test case execution;test case selection;trace analysis,,8,,71,,,20110106,Nov.-Dec. 2011,,IEEE,IEEE Journals & Magazines,,18
Loupe: Verifying Publish-Subscribe Architectures with a Magnifying Lens,L. Baresi; C. Ghezzi; L. Mottola,"Politecnico di Milano, Milano",IEEE Transactions on Software Engineering,20110324,2011,37,2,228,246,"The Publish-Subscribe (P/S) communication paradigm fosters high decoupling among distributed components. This facilitates the design of dynamic applications, but also impacts negatively on their verification, making it difficult to reason on the overall federation of components. In addition, existing P/S infrastructures offer radically different features to the applications, e.g., in terms of message reliability. This further complicates the verification as its outcome depends on the specific guarantees provided by the underlying P/S system. Although model checking has been proposed as a tool for the verification of P/S architectures, existing solutions overlook many characteristics of the underlying communication infrastructure to avoid state explosion problems. To overcome these limitations, the Loupe domain-specific model checker adopts a different approach. The P/S infrastructure is not modeled on top of a general-purpose model checker. Instead, it is embedded within the checking engine, and the traditional P/S operations become part of the modeling language. In this paper, we describe Loupe's design and the dedicated state abstractions that enable accurate verification without incurring state explosion problems. We also illustrate our use of state-of-the-art software verification tools to assess some key functionality in Loupe's current implementation. A complete case study shows how Loupe eases the verification of P/S architectures. Finally, we quantitatively compare Loupe's performance against alternative approaches. The results indicate that Loupe is effective and efficient in enabling accurate verification of P/S architectures.",0098-5589;00985589,,10.1109/TSE.2010.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432228,Publish-subscribe;model-checking.;verification,,formal verification;message passing;middleware,Loupe design;Loupe domain-specific model checker;P-S architectures verification;P-S infrastructures;P-S operation;communication infrastructure;dedicated state abstraction;distributed component;general purpose model checker;lens magnification;message reliability;modeling language;publish-subscribe architectures;publish-subscribe communication paradigm;state explosion problem;state-of-the-art software verification tool,,7,,64,,,20100318,March-April 2011,,IEEE,IEEE Journals & Magazines,,18
Software Module Clustering as a Multi-Objective Search Problem,K. Praditwong; M. Harman; X. Yao,"The University of Birmingham, Birmingham",IEEE Transactions on Software Engineering,20110324,2011,37,2,264,282,"Software module clustering is the problem of automatically organizing software units into modules to improve program structure. There has been a great deal of recent interest in search-based formulations of this problem in which module boundaries are identified by automated search, guided by a fitness function that captures the twin objectives of high cohesion and low coupling in a single-objective fitness function. This paper introduces two novel multi-objective formulations of the software module clustering problem, in which several different objectives (including cohesion and coupling) are represented separately. In order to evaluate the effectiveness of the multi-objective approach, a set of experiments was performed on 17 real-world module clustering problems. The results of this empirical study provide strong evidence to support the claim that the multi-objective approach produces significantly better solutions than the existing single-objective approach.",0098-5589;00985589,,10.1109/TSE.2010.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406532,SBSE;evolutionary computation.;module clustering;multi-objective optimization,,optimisation;pattern clustering;search problems;software engineering,multi-objective search problem;program structure;software module clustering,,108,,32,,,20100205,March-April 2011,,IEEE,IEEE Journals & Magazines,,18
WAM_ÑÓThe Weighted Average Method for Predicting the Performance of Systems with Bursts of Customer Sessions,D. Krishnamurthy; J. Rolia; M. Xu,"Dept. of Electr. &amp; Comput. Eng., Univ. of Calgary, Calgary, AB, Canada",IEEE Transactions on Software Engineering,20110929,2011,37,5,718,735,"Predictive performance models are important tools that support system sizing, capacity planning, and systems management exercises. We introduce the Weighted Average Method (WAM) to improve the accuracy of analytic predictive performance models for systems with bursts of concurrent customers. WAM considers the customer population distribution at a system to reflect the impact of bursts. The WAM approach is robust with respect to distribution functions, including heavy-tail-like distributions, for workload parameters. We demonstrate the effectiveness of WAM using a case study involving a multitier TPC-W benchmark system. To demonstrate the utility of WAM with multiple performance modeling approaches, we developed both Queuing Network Models and Layered Queuing Models for the system. Results indicate that WAM improves prediction accuracy for bursty workloads for QNMs and LQMs by 10 and 12 percent, respectively, with respect to a Markov Chain approach reported in the literature.",0098-5589;00985589,,10.1109/TSE.2011.65,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5953602,Performance of systems;modeling techniques;operational analysis.;queuing theory,Accuracy;Analytical models;Markov processes;Predictive models;Queueing analysis;Software;Time factors,Markov processes;queueing theory;software performance evaluation;systems analysis,Markov chain approach;analytic predictive performance models;bursty workloads;capacity planning;customer session bursts;heavy tail like distributions;layered queuing models;multitier TPC-W benchmark system;queuing network models;system performance prediction;system sizing;systems management exercises;weighted average method,,7,,41,,,20110714,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines,,17
Does Socio-Technical Congruence Have an Effect on Software Build Success? A Study of Coordination in a Software Project,I. Kwan; A. Schroter; D. Damian,"University of Victoria, Victoria",IEEE Transactions on Software Engineering,20110527,2011,37,3,307,324,"Socio-technical congruence is an approach that measures coordination by examining the alignment between the technical dependencies and the social coordination in the project. We conduct a case study of coordination in the IBM Rational Team Concert project, which consists of 151 developers over seven geographically distributed sites, and expect that high congruence leads to a high probability of successful builds. We examine this relationship by applying two congruence measurements: an unweighted congruence measure from previous literature, and a weighted measure that overcomes limitations of the existing measure. We discover that there is a relationship between socio-technical congruence and build success probability, but only for certain build types, and observe that in some situations, higher congruence actually leads to lower build success rates. We also observe that a large proportion of zero-congruence builds are successful, and that socio-technical gaps in successful builds are larger than gaps in failed builds. Analysis of the social and technical aspects in IBM Rational Team Concert allows us to discuss the effects of congruence on build success. Our findings provide implications with respect to the limits of applicability of socio-technical congruence and suggest further improvements of socio-technical congruence to study coordination.",0098-5589;00985589,,10.1109/TSE.2011.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740929,Empirical software engineering;awareness;coordination;integration.;socio-technical congruence;software quality,Collaboration;Context;Programming;Software;Software engineering;Software measurement;Weight measurement,social aspects of automation;software development management,congruence measurements;social coordination;socio-technical congruence;software build success;software project;unweighted congruence measure;weighted measure,,40,1,54,,,20110405,May-June 2011,,IEEE,IEEE Journals & Magazines,,17
Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts,D. Kim; X. Wang; S. Kim; A. Zeller; S. C. Cheung; S. Park,"Sogang University, Seoul",IEEE Transactions on Software Engineering,20110527,2011,37,3,430,447,"Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these _ÑÒtop crashes_Ñù thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.",0098-5589;00985589,,10.1109/TSE.2011.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711013,Top crash;crash reports;data mining.;machine learning;social network analysis,Computer bugs;Feature extraction;Fires;Software;Testing;Training,program debugging;software maintenance;software quality;system recovery,Firefox crash report databases;Thunderbird crash report databases;debugging;software failures;software maintenance;software quality;software systems,,28,,59,,,20110210,May-June 2011,,IEEE,IEEE Journals & Magazines,,17
FlowTalk: Language Support for Long-Latency Operations in Embedded Devices,A. Bergel; W. Harrison; V. Cahill; S. Clarke,"Comput. Sci. Dept. (DCC), Univ. of Chile, Santiago, Chile",IEEE Transactions on Software Engineering,20110728,2011,37,4,526,543,"Wireless sensor networks necessitate a programming model different from those used to develop desktop applications. Typically, resources in terms of power and memory are constrained. C is the most common programming language used to develop applications on very small embedded sensor devices. We claim that C does not provide efficient mechanisms to address the implicit asynchronous nature of sensor sampling. C applications for these devices suffer from a disruption in their control flow. In this paper, we present FlowTalk, a new object-oriented programming language aimed at making software development for wireless embedded sensor devices easier. FlowTalk is an object-oriented programming language in which dynamicity (e.g., object creation) has been traded for a reduction in memory consumption. The event model that traditionally comes from using sensors is adapted in FlowTalk with controlled disruption, a light-weight continuation mechanism. The essence of our model is to turn asynchronous long-latency operations into synchronous and blocking method calls. FlowTalk is built for TinyOS and can be used to develop applications that can fit in 4 KB of memory for a large number of wireless sensor devices.",0098-5589;00985589,,10.1109/TSE.2010.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5492692,Embedded systems;object-based programming.,Application software;Automotive engineering;Biosensors;Computer languages;Embedded software;Java;Object oriented modeling;Object oriented programming;Sampling methods;Wireless sensor networks,C language;embedded systems;intelligent sensors;object-oriented languages;object-oriented programming;software engineering;wireless sensor networks,C language;FlowTalk;TinyOS;asynchronous long-latency operations;embedded sensor devices;language support;light-weight continuation mechanism;memory consumption;memory size 4 KByte;object-oriented programming language;programming language;sensor sampling;wireless sensor networks,,0,,44,,,20100628,July-Aug. 2011,,IEEE,IEEE Journals & Magazines,,17
Measuring the Discriminative Power of Object-Oriented Class Cohesion Metrics,J. Al Dallal,"Kuwait University, Kuwait",IEEE Transactions on Software Engineering,20111205,2011,37,6,788,804,"Several object-oriented cohesion metrics have been proposed in the literature. These metrics aim to measure the relationship between class members, namely, methods and attributes. Different metrics use different models to represent the connectivity pattern of cohesive interactions (CPCI) between class members. Most of these metrics are normalized to allow for easy comparison of the cohesion of different classes. However, in some cases, these metrics obtain the same cohesion values for different classes that have the same number of methods and attributes but different CPCIs. This leads to incorrectly considering the classes to be the same in terms of cohesion, even though their CPCIs clearly indicate that the degrees of cohesion are different. We refer to this as a lack of discrimination anomaly (LDA) problem. In this paper, we list and discuss cases in which the LDA problem exists, as expressed through the use of 16 cohesion metrics. In addition, we empirically study the frequent occurrence of the LDA problem when the considered metrics are applied to classes in five open source Java systems. Finally, we propose a metric and a simulation-based methodology to measure the discriminative power of cohesion metrics. The discrimination metric measures the probability that a cohesion metric will produce distinct cohesion values for classes with the same number of attributes and methods but different CPCIs. A highly discriminating cohesion metric is more desirable because it exhibits a lower chance of incorrectly considering classes to be cohesively equal when they have different CPCIs.",0098-5589;00985589,,10.1109/TSE.2010.97,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639020,Cohesive interactions;connectivity pattern;discrimination metric;discriminative power;lack of discrimination anomaly;object-oriented class cohesion.,Object oriented modeling;Phase measurement;Power measurement;Software measurement,Java;laser Doppler anemometry;object-oriented methods;public domain software,cohesive interactions;discrimination anomaly;discriminative power measurement;object oriented class cohesion metrics;open source Java systems;simulation based methodology,,19,,41,,,20101118,Nov.-Dec. 2011,,IEEE,IEEE Journals & Magazines,,16
Zebu: A Language-Based Approach for Network Protocol Message Processing,L. Burgy; L. Reveillere; J. Lawall; G. Muller,"Princeton University, Princeton",IEEE Transactions on Software Engineering,20110728,2011,37,4,575,591,"A network application communicates with other applications according to a set of rules known as a protocol. This communication is managed by the part of the application known as the protocol-handling layer, which enables the manipulation of protocol messages. The protocol-handling layer is a critical component of a network application since it represents the interface between the application and the outside world. It must thus satisfy two constraints: It must be efficient to be able to treat a large number of messages and it must be robust to face various attacks targeting the application itself or the underlying platform. Despite these constraints, the development process of this layer still remains rudimentary and requires a high level of expertise. It includes translating the protocol specification written in a high-level formalism such as ABNF toward low-level code such as C. The gap between these abstraction levels can entail many errors. This paper proposes a new language-based approach to developing protocol-handling layers, to improve their robustness without compromising their performance. Our approach is based on the use of a domain-specific language, Zebu, to specify the protocol-handling layer of network applications that use textual HTTP-like application protocols. The Zebu syntax is very close to that of ABNF, facilitating the adoption of Zebu by domain experts. By annotating the original ABNF specification of a protocol, the Zebu user can dedicate the protocol-handling layer to the needs of a given application. The Zebu compiler first checks the annotated specification for inconsistencies, and then generates a protocol-handling layer according to the annotations. This protocol-handling layer is made up of a set of data structures that represent a message, a parser that fills in these data structures, and various stub functions to access these data structures or drive the parsing of a message.",0098-5589;00985589,,10.1109/TSE.2010.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487528,Network protocols;domain-specific languages.;message composing;message parsing,Access protocols;Computer bugs;Data structures;Domain specific languages;Electronic mail;IP networks;Network servers;Robustness;Streaming media;Web server,Internet;computational linguistics;formal specification;program compilers;protocols;specification languages;telecommunication computing,ABNF;Internet era;Zebu compiler;Zebu syntax;data structures;domain-specific languages;language-based approach;message parsing;network protocol message processing;protocol specification;protocol-handling layer;textual HTTP-like application protocols,,7,,34,,,20100617,July-Aug. 2011,,IEEE,IEEE Journals & Magazines,,16
A Controlled Experiment for Evaluating the Impact of Coupling on the Maintainability of Service-Oriented Software,M. Perepletchikov; C. Ryan,"RMIT University, Melbourne",IEEE Transactions on Software Engineering,20110728,2011,37,4,449,465,"One of the goals of Service-Oriented Computing (SOC) is to improve software maintainability as businesses become more agile, and thus underlying processes and rules change more frequently. This paper presents a controlled experiment examining the relationship between coupling in service-oriented designs, as measured using a recently proposed suite of SOC-specific coupling metrics and software maintainability in terms of the specific subcharacteristics of analyzability, changeability, and stability. The results indicate a statistically significant causal relationship between the investigated coupling metrics and the maintainability of service-oriented software. As such, the investigated metrics can facilitate coupling related design decisions with the aim of producing more maintainable service-oriented software products.",0098-5589;00985589,,10.1109/TSE.2010.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482590,Services systems;design concepts;empirical studies.;maintainability;product metrics,Application software;Costs;Logic;Product design;Programming;Software design;Software maintenance;Software measurement;Software metrics;Stability analysis,service-oriented architecture;software maintenance;software metrics,service-oriented computing;service-oriented designs;service-oriented software products;software maintainability improvement;specific coupling metrics;specific subcharacteristics;statistically significant causal relationship,,17,,48,,,20100607,July-Aug. 2011,,IEEE,IEEE Journals & Magazines,,16
Empirical Studies of Pair Programming for CS/SE Teaching in Higher Education: A Systematic Literature Review,N. Salleh; E. Mendes; J. Grundy,"International Islamic University of Malaysia, Kuala Lumpur and University of Auckland, Auckland",IEEE Transactions on Software Engineering,20110728,2011,37,4,509,525,"The objective of this paper is to present the current evidence relative to the effectiveness of pair programming (PP) as a pedagogical tool in higher education CS/SE courses. We performed a systematic literature review (SLR) of empirical studies that investigated factors affecting the effectiveness of PP for CS/SE students and studies that measured the effectiveness of PP for CS/SE students. Seventy-four papers were used in our synthesis of evidence, and 14 compatibility factors that can potentially affect PP's effectiveness as a pedagogical tool were identified. Results showed that students' skill level was the factor that affected PP's effectiveness the most. The most common measure used to gauge PP's effectiveness was time spent on programming. In addition, students' satisfaction when using PP was overall higher than when working solo. Our meta-analyses showed that PP was effective in improving students' grades on assignments. Finally, in the studies that used quality as a measure of effectiveness, the number of test cases succeeded, academic performance, and expert opinion were the quality measures mostly applied. The results of this SLR show two clear gaps in this research field: 1) a lack of studies focusing on pair compatibility factors aimed at making PP an effective pedagogical tool and 2) a lack of studies investigating PP for software design/modeling tasks in conjunction with programming tasks.",0098-5589;00985589,,10.1109/TSE.2010.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482588,Empirical studies;pair programming;systematic review.,Algorithm design and analysis;Collaborative work;Computer science;Education;Educational programs;Performance evaluation;Programming profession;Software design;Testing;Time measurement,educational technology;further education;software prototyping;teaching,CS/SE teaching;PP;higher education;pair programming;systematic literature review,,64,,75,,,20100607,July-Aug. 2011,,IEEE,IEEE Journals & Magazines,,16
Self-Supervising BPEL Processes,L. Baresi; S. Guinea,"Politecnico di Milano, Milano, Italy",IEEE Transactions on Software Engineering,20110324,2011,37,2,247,263,"Service compositions suffer changes in their partner services. Even if the composition does not change, its behavior may evolve over time and become incorrect. Such changes cannot be fully foreseen through prerelease validation, but impose a shift in the quality assessment activities. Provided functionality and quality of service must be continuously probed while the application executes, and the application itself must be able to take corrective actions to preserve its dependability and robustness. We propose the idea of self-supervising BPEL processes, that is, special-purpose compositions that assess their behavior and react through user-defined rules. Supervision consists of monitoring and recovery. The former checks the system's execution to see whether everything is proceeding as planned, while the latter attempts to fix any anomalies. The paper introduces two languages for defining monitoring and recovery and explains how to use them to enrich BPEL processes with self-supervision capabilities. Supervision is treated as a cross-cutting concern that is only blended at runtime, allowing different stakeholders to adopt different strategies with no impact on the actual business logic. The paper also presents a supervision-aware runtime framework for executing the enriched processes, and briefly discusses the results of in-lab experiments and of a first evaluation with industrial partners.",0098-5589;00985589,,10.1109/TSE.2010.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432226,Software engineering;assertion checkers;assertion languages;design tools and techniques;distributed/Internet-based software engineering tools and techniques.;performance;software/program verification,,Web services;business process re-engineering;program verification;service-oriented architecture,business logic;business process execution language;industrial partner;quality assessment;self supervising BPEL process;stakeholder;supervision aware runtime framework,,46,,41,,,20100318,March-April 2011,,IEEE,IEEE Journals & Magazines,,16
Automatically Detecting and Tracking Inconsistencies in Software Design Models,A. Egyed,"Johannes Kepler University, Linz",IEEE Transactions on Software Engineering,20110324,2011,37,2,188,204,"Software models typically contain many inconsistencies and consistency checkers help engineers find them. Even if engineers are willing to tolerate inconsistencies, they are better off knowing about their existence to avoid follow-on errors and unnecessary rework. However, current approaches do not detect or track inconsistencies fast enough. This paper presents an automated approach for detecting and tracking inconsistencies in real time (while the model changes). Engineers only need to define consistency rules-in any language-and our approach automatically identifies how model changes affect these consistency rules. It does this by observing the behavior of consistency rules to understand how they affect the model. The approach is quick, correct, scalable, fully automated, and easy to use as it does not require any special skills from the engineers using it. We evaluated the approach on 34 models with model sizes of up to 162,237 model elements and 24 types of consistency rules. Our empirical evaluation shows that our approach requires only 1.4 ms to reevaluate the consistency of the model after a change (on average); its performance is not noticeably affected by the model size and common consistency rules but only by the number of consistency rules, at the expense of a quite acceptable, linearly increasing memory consumption.",0098-5589;00985589,,10.1109/TSE.2010.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432227,Design tools and techniques;design.,,formal verification;software maintenance,automatic inconsistency detection;automatic inconsistency tracking;consistency checkers;consistency rules;empirical evaluation;memory consumption;software design model,,36,,40,,,20100318,March-April 2011,,IEEE,IEEE Journals & Magazines,,16
"Semi-Proving: An Integrated Method for Program Proving, Testing, and Debugging",T. Y. Chen; T. H. Tse; Z. Q. Zhou,"Swinburne University of Technology, Hawthorn",IEEE Transactions on Software Engineering,20110128,2011,37,1,109,125,"We present an integrated method for program proving, testing, and debugging. Using the concept of metamorphic relations, we select necessary properties for target programs. For programs where global symbolic evaluation can be conducted and the constraint expressions involved can be solved, we can either prove that these necessary conditions for program correctness are satisfied or identify all inputs that violate the conditions. For other programs, our method can be converted into a symbolic-testing approach. Our method extrapolates from the correctness of a program for tested inputs to the correctness of the program for related untested inputs. The method supports automatic debugging through the identification of constraint expressions that reveal failures.",0098-5589;00985589,,10.1109/TSE.2010.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406529,Software/program verification;symbolic execution;testing and debugging.,,formal verification;program debugging;program testing,automatic debugging;constraint expression;integrated method;metamorphic relation;program debugging;program proving;program testing;program verification;semiproving;symbolic evaluation;symbolic testing,,37,,60,,,20100205,Jan.-Feb. 2011,,IEEE,IEEE Journals & Magazines,,16
Developing a Single Model and Test Prioritization Strategies for Event-Driven Software,R. C. Bryce; S. Sampath; A. M. Memon,"Utah State University, Logan",IEEE Transactions on Software Engineering,20110128,2011,37,1,48,64,"Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.",0098-5589;00985589,,10.1109/TSE.2010.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401169,Combinatorial interaction testing;GUI testing.;Web application testing;covering arrays;event-driven software (EDS);t-way interaction coverage;test suite prioritization;user-session testing,Abstracts;Application software;Computer science;Educational institutions;Embedded software;Graphical user interfaces;Information systems;Protocols;Software testing;User interfaces,Internet;graphical user interfaces;program testing;service-oriented architecture,EDS;GUI testing;Web application testing;event-driven software;graphical user interface;test prioritization strategy,,49,,28,,,20100129,Jan.-Feb. 2011,,IEEE,IEEE Journals & Magazines,,16
"The Awareness Network, To Whom Should I Display My Actions? And, Whose Actions Should I Monitor?",C. R. B. de Souza; D. F. Redmiles,"IBM Brazil, S&#x0E3; o Paulo",IEEE Transactions on Software Engineering,20110527,2011,37,3,325,340,"The concept of awareness plays a pivotal role in research in Computer-Supported Cooperative Work. Recently, software engineering researchers interested in the collaborative nature of software development have explored the implications of this concept in the design of software development tools. A critical aspect of awareness is the associated coordinative work practices of displaying and monitoring actions. This aspect concerns how colleagues monitor one another's actions to understand how these actions impact their own work and how they display their actions in such a way that others can easily monitor them while doing their own work. In this paper, we focus on an additional aspect of awareness: the identification of the social actors who should be monitored and the actors to whom their actions should be displayed. We address this aspect by presenting software developers' work practices based on ethnographic data from three different software development teams. In addition, we illustrate how these work practices are influenced by different factors, including the organizational setting, the age of the project, and the software architecture. We discuss how our results are relevant for both CSCW and software engineering researchers.",0098-5589;00985589,,10.1109/TSE.2011.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710950,Computer-supported cooperative work;organizational management and coordination;programming environments;programming teams;tools.,Collaboration;Interviews;Monitoring;Programming;Servers;Software,groupware;software architecture,CSCW;awareness network;computer-supported cooperative work;ethnographic data;organizational setting;project age;social actor;software architecture;software development,,27,,57,,,20110210,May-June 2011,,IEEE,IEEE Journals & Magazines,,15
"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities",Y. Shin; A. Meneely; L. Williams; J. A. Osborne,"DePaul University, Chicago",IEEE Transactions on Software Engineering,20111205,2011,37,6,772,787,"Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.",0098-5589;00985589,,10.1109/TSE.2010.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680,Fault prediction;software metrics;software security;vulnerability prediction.,Charge coupled devices;Complexity theory;Fault diagnosis;Predictive models;Software security,Linux;online front-ends;program testing;public domain software;software fault tolerance;software metrics,Mozilla Firefox Web browser;Red Hat enterprise Linux kernel;code churn;developer activity metrics;open-source projects;security inspection;software metrics;software vulnerabilities;source code;vulnerable code locations,,82,1,43,,,20100902,Nov.-Dec. 2011,,IEEE,IEEE Journals & Magazines,,15
Verifying the Evolution of Probability Distributions Governed by a DTMC,Y. Kwon; G. Agha,"Microsoft Corporation, Redmond",IEEE Transactions on Software Engineering,20110128,2011,37,1,126,141,"We propose a new probabilistic temporal logic, iLTL, which captures properties of systems whose state can be represented by probability mass functions (pmfs). Using iLTL, we can specify reachability to a state (i.e., a pmf), as well as properties representing the aggregate (expected) behavior of a system. We then consider a class of systems whose transitions are governed by a Markov Chain-in this case, the set of states a system may be in is specified by the transitions of pmfs from all potential initial states to the final state. We then provide a model checking algorithm to check iLTL properties of such systems. Unlike existing model checking techniques, which either compute the portions of the computational paths that satisfy a specification or evaluate properties along a single path of pmf transitions, our model checking technique enables us to do a complete analysis on the expected behaviors of large-scale systems. Desirable system parameters may also be found as a counterexample of a negated goal. Finally, we illustrate the usefulness of iLTL model checking by means of two examples: assessing software reliability and ensuring the results of administering a drug.",0098-5589;00985589,,10.1109/TSE.2010.80,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557891,Discrete Time Markov Chain;Probabilistic model checking;linear temporal logic;pharmacokinetics.,,Markov processes;formal verification;statistical distributions;temporal logic,DTMC;discrete time Markov chain;iLTL;large scale system;model checking;probability distribution;probability mass function;temporal logic,,5,,39,,,20100826,Jan.-Feb. 2011,,IEEE,IEEE Journals & Magazines,,15
Dynamic Software Updating Using a Relaxed Consistency Model,H. Chen; J. Yu; C. Hang; B. Zang; P. C. Yew,"Fudan University, Shanghai",IEEE Transactions on Software Engineering,20110929,2011,37,5,679,694,"Software is inevitably subject to changes. There are patches and upgrades that close vulnerabilities, fix bugs, and evolve software with new features. Unfortunately, most traditional dynamic software updating approaches suffer some level of limitations; few of them can update multithreaded applications when involving data structure changes, while some of them lose binary compatibility or incur nonnegligible performance overhead. This paper presents POLUS, a software maintenance tool capable of iteratively evolving running unmodified multithreaded software into newer versions, yet with very low performance overhead. The main idea in POLUS is a relaxed consistency model that permits the concurrent activity of the old and new code. POLUS borrows the idea of cache-coherence protocol in computer architecture and uses a _Ñùbidirectional write-through_Ñù synchronization protocol to ensure system consistency. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd, and Apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead on the three applications-a less than 1 percent performance degradation (but 5 percent for one case). The time to apply an update is also minimal.",0098-5589;00985589,,10.1109/TSE.2010.79,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551162,Maintainability;reliability;runtime environments.,Bidirectional control;Protocols;Registers;Runtime;Software;Synchronization,computer architecture;hypermedia;multi-threading;program testing;software maintenance;software tools;transport protocols,HTTP server;POLUS;bidirectional write-through synchronization protocol;binary compatibility;cache-coherence protocol;computer architecture;concurrent activity;data structure;dynamic software update;iteratively evolving running unmodified multithreaded software;nonnegligible performance;prevalent server application;relaxed consistency model;software maintenance tool,,14,,42,,,20100819,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines,,15
An Attack Surface Metric,P. K. Manadhata; J. M. Wing,"Symantec Research Labs, Culver City",IEEE Transactions on Software Engineering,20110527,2011,37,3,371,386,"Measurement of software security is a long-standing challenge to the research community. At the same time, practical security metrics and measurements are essential for secure software development. Hence, the need for metrics is more pressing now due to a growing demand for secure software. In this paper, we propose using a software system's attack surface measurement as an indicator of the system's security. We formalize the notion of a system's attack surface and introduce an attack surface metric to measure the attack surface in a systematic manner. Our measurement method is agnostic to a software system's implementation language and is applicable to systems of all sizes; we demonstrate our method by measuring the attack surfaces of small desktop applications and large enterprise systems implemented in C and Java. We conducted three exploratory empirical studies to validate our method. Software developers can mitigate their software's security risk by measuring and reducing their software's attack surfaces. Our attack surface reduction approach complements the software industry's traditional code quality improvement approach for security risk mitigation and is useful in multiple phases of the software development lifecycle. Our collaboration with SAP demonstrates the use of our metric in the software development process.",0098-5589;00985589,,10.1109/TSE.2010.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482589,Code design;life cycle;product metrics;protection mechanisms;risk mitigation;software security.,Application software;Java;Pressing;Programming;Security;Size measurement;Software measurement;Software quality;Software systems;Time measurement,C language;Java;security;software metrics,C language;Java language;attack surface metric;implementation language;security metrics;software development;software security,,124,2,53,,,20100607,May-June 2011,,IEEE,IEEE Journals & Magazines,,15
GUI Interaction Testing: Incorporating Event Context,X. Yuan; M. B. Cohen; A. M. Memon,Google Kirkland,IEEE Transactions on Software Engineering,20110728,2011,37,4,559,574,"Graphical user interfaces (GUIs), due to their event-driven nature, present an enormous and potentially unbounded way for users to interact with software. During testing, it is important to _ÑÒadequately cover_Ñù this interaction space. In this paper, we develop a new family of coverage criteria for GUI testing grounded in combinatorial interaction testing. The key motivation of using combinatorial techniques is that they enable us to incorporate _ÑÒcontext_Ñù into the criteria in terms of event combinations, sequence length, and by including all possible positions for each event. Our new criteria range in both efficiency (measured by the size of the test suite) and effectiveness (the ability of the test suites to detect faults). In a case study on eight applications, we automatically generate test cases and systematically explore the impact of context, as captured by our new criteria. Our study shows that by increasing the event combinations tested and by controlling the relative positions of events defined by the new criteria, we can detect a large number of faults that were undetectable by earlier techniques.",0098-5589;00985589,,10.1109/TSE.2010.50,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444885,GUI testing;GUITAR testing system.;automated testing;combinatorial interaction testing;model-based testing,Automatic testing;Computer science;Context modeling;Fault detection;Graphical user interfaces;Logic testing;Software performance;Software testing;System testing;User interfaces,automatic test pattern generation;graphical user interfaces;program testing,GUI interaction testing;automatic test case generation;combinatorial interaction testing;event driven nature;graphical user interface,,60,2,44,,,20100408,July-Aug. 2011,,IEEE,IEEE Journals & Magazines,,15
Genetic Algorithms for Randomized Unit Testing,J. H. Andrews; T. Menzies; F. C. H. Li,"University of Western Ontario, London, Ont., Canada",IEEE Transactions on Software Engineering,20110128,2011,37,1,80,94,"Randomized testing is an effective method for testing software units. The thoroughness of randomized unit testing varies widely according to the settings of certain parameters, such as the relative frequencies with which methods are called. In this paper, we describe Nighthawk, a system which uses a genetic algorithm (GA) to find parameters for randomized unit testing that optimize test coverage. Designing GAs is somewhat of a black art. We therefore use a feature subset selection (FSS) tool to assess the size and content of the representations within the GA. Using that tool, we can reduce the size of the representation substantially while still achieving most of the coverage found using the full representation. Our reduced GA achieves almost the same results as the full system, but in only 10 percent of the time. These results suggest that FSS could significantly optimize metaheuristic search-based software engineering tools.",0098-5589;00985589,,10.1109/TSE.2010.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5704237,Software testing;feature subset selection;genetic algorithms;randomized testing;search-based optimization;testing tools.,Biological cells;Gallium;Java;Optimization;Receivers;Software;Testing,feature extraction;genetic algorithms;program testing;randomised algorithms;search problems;software engineering,Nighthawk;feature subset selection tool;genetic algorithm;metaheuristic search;optimized test coverage;randomized unit testing;relative frequency;software engineering tool;software testing,,29,,49,,,,Jan.-Feb. 2011,,IEEE,IEEE Journals & Magazines,,14
A General Software Defect-Proneness Prediction Framework,Q. Song; Z. Jia; M. Shepperd; S. Ying; J. Liu,"Xi'an Jiaotong University, Xi'an",IEEE Transactions on Software Engineering,20110527,2011,37,3,356,370,"BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.",0098-5589;00985589,,10.1109/TSE.2010.90,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611551,Software defect prediction;machine learning;scheme evaluation.;software defect-proneness prediction,Buildings;Data models;Prediction algorithms;Predictive models;Software;Training;Training data,learning (artificial intelligence);software fault tolerance;software performance evaluation,competing learning schemes;defect predictor;scheme evaluation;software defect proneness prediction framework,,72,1,44,,,20101028,May-June 2011,,IEEE,IEEE Journals & Magazines,,14
A Comparative Study of Software Model Checkers as Unit Testing Tools: An Industrial Case Study,M. Kim; Y. Kim; H. Kim,"KAIST, Daejon",IEEE Transactions on Software Engineering,20110324,2011,37,2,146,160,"Conventional testing methods often fail to detect hidden flaws in complex embedded software such as device drivers or file systems. This deficiency incurs significant development and support/maintenance cost for the manufacturers. Model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses. Whereas conventional model checkers require manual effort to create an abstract target model, modern software model checkers remove this overhead by directly analyzing a target C program, and can be utilized as unit testing tools. However, since software model checkers are not fully mature yet, they have limitations according to the underlying technologies and tool implementations, potentially critical issues when applied in industrial projects. This paper reports our experience in applying Blast and CBMC to testing the components of a storage platform software for flash memory. Through this project, we analyzed the strong and weak points of two different software model checking technologies in the viewpoint of real-world industrial application-counterexample-guided abstraction refinement with predicate abstraction and SAT-based bounded analysis.",0098-5589;00985589,,10.1109/TSE.2010.68,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510242,CEGAR-based model checking;Embedded software verification;bounded model checking;flash file systems.;software model checking,,C language;program testing;program verification;storage management,Blast;C program;CBMC;abstract target model;complex embedded software;flash memory;model checking techniques;software model checkers;storage platform software;unit testing tools,,17,,54,,,20100715,March-April 2011,,IEEE,IEEE Journals & Magazines,,14
Frameworks Generate Domain-Specific Languages: A Case Study in the Multimedia Domain,X. Amatriain; P. Arumi,"Telefonica Research, Barcelona, Spain",IEEE Transactions on Software Engineering,20110728,2011,37,4,544,558,"We present an approach to software framework development that includes the generation of domain-specific languages (DSLs) and pattern languages as goals for the process. Our model is made of three workflows-framework, metamodel, and patterns-and three phases-inception, construction, and formalization. The main conclusion is that when developing a framework, we can produce with minimal overhead-almost as a side effect-a metamodel with an associated DSL and a pattern language. Both outputs will not only help the framework evolve in the right direction, but will also be valuable in themselves. In order to illustrate these ideas, we present a case study in the multimedia domain. For several years, we have been developing a multimedia framework. The process has produced a full-fledged domain-specific metamodel for the multimedia domain, with an associated DSL and a pattern language.",0098-5589;00985589,,10.1109/TSE.2010.48,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5441292,CASE.;Domain-specific architectures;life cycle;visual programming,Best practices;Computer aided software engineering;Concrete;DSL;Domain specific languages;Metamodeling;Natural languages;Software engineering;Unified modeling language;Vocabulary,multimedia computing;software engineering;specification languages;visual programming,associated DSL;domain-specific languages;domain-specific metamodel;multimedia domain;pattern languages;software framework development;visual programming,,5,,37,,,20100401,July-Aug. 2011,,IEEE,IEEE Journals & Magazines,,14
A Controlled Experiment for Program Comprehension through Trace Visualization,B. Cornelissen; A. Zaidman; A. van Deursen,"Software Improvement Group, Amsterdam",IEEE Transactions on Software Engineering,20110527,2011,37,3,341,355,"Software maintenance activities require a sufficient level of understanding of the software at hand that unfortunately is not always readily available. Execution trace visualization is a common approach in gaining this understanding, and among our own efforts in this context is Extravis, a tool for the visualization of large traces. While many such tools have been evaluated through case studies, there have been no quantitative evaluations to the present day. This paper reports on the first controlled experiment to quantitatively measure the added value of trace visualization for program comprehension. We designed eight typical tasks aimed at gaining an understanding of a representative subject system, and measured how a control group (using the Eclipse IDE) and an experimental group (using both Eclipse and Extravis) performed these tasks in terms of time spent and solution correctness. The results are statistically significant in both regards, showing a 22 percent decrease in time requirements and a 43 percent increase in correctness for the group using trace visualization.",0098-5589;00985589,,10.1109/TSE.2010.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5441291,Program comprehension;controlled experiment.;dynamic analysis,Computer Society;Control systems;Documentation;Gain measurement;Performance evaluation;Programming;Scalability;Software maintenance;Time measurement;Visualization,data visualisation;software maintenance,execution trace visualization;program comprehension;software maintenance,,41,,56,,,20100401,May-June 2011,,IEEE,IEEE Journals & Magazines,,14
From UML to Petri Nets: The PCM-Based Methodology,S. Distefano; M. Scarpa; A. Puliafito,"University of Messina, Sicily",IEEE Transactions on Software Engineering,20110128,2011,37,1,65,79,"In this paper, we present an evaluation methodology to validate the performance of a UML model, representing a software architecture. The proposed approach is based on open and well-known standards: UML for software modeling and the OMG Profile for Schedulability, Performance, and Time Specification for the performance annotations into UML models. Such specifications are collected in an intermediate model, called the Performance Context Model (PCM). The intermediate model is translated into a performance model which is subsequently evaluated. The paper is focused on the mapping from the PCM to the performance domain. More specifically, we adopt Petri nets as the performance domain, specifying a mapping process based on a compositional approach we have entirely implemented in the ArgoPerformance tool. All of the rules to derive a Petri net from a PCM and the performance measures assessable from the former are carefully detailed. To validate the proposed technique, we provide an in-depth analysis of a web application for music streaming.",0098-5589;00985589,,10.1109/TSE.2010.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5396344,Petri nets;Software engineering;UML;performances evaluation;software performance engineering.,,Petri nets;Unified Modeling Language;Web services;media streaming;software architecture;software metrics;software performance evaluation,ArgoPerformance tool;OMG profile;PCM;Petri nets;UML;Web application;mapping process;music streaming;performance context model;schedulability;software architecture;software modeling;time specification,,23,,31,,,20100122,Jan.-Feb. 2011,,IEEE,IEEE Journals & Magazines,,14
A Risk Management Methodology for Project Risk Dependencies,T. W. Kwan; H. K. N. Leung,"The Hong Kong Polytechnic University, Hong Kong",IEEE Transactions on Software Engineering,20110929,2011,37,5,635,648,"Project risks are not always independent, yet current risk management practices do not clearly manage dependencies between risks. If dependencies can be explicitly identified and analyzed, project managers will be able to develop better risk management strategies and make more effective risk planning decisions. This paper proposes a management methodology to address risk dependency issues. Through the study of three IT projects, we confirm that risk dependencies do exist in projects and can be identified and systematically managed. We also observed that, as project teams needed to deal with risk dependency issues, communications between projects were improved, and there were synergetic effects in managing risks and risk dependencies among projects.",0098-5589;00985589,,10.1109/TSE.2010.108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5696725,Project risk management;metrics.;risk assessment;risk dependencies,Analytical models;Delta modulation;Fault trees;Lead;Measurement;Monitoring;Risk management,project management;risk analysis,IT projects;project risk management;risk dependencies;risk planning decisions,,25,,33,,,20110120,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines,,13
Tuning Temporal Features within the Stochastic ñÑ-Calculus,L. Pauleve; M. Magnin; O. Roux,"IRCCyN, &#x0C9;cole Centrale de Nantes",IEEE Transactions on Software Engineering,20111205,2011,37,6,858,871,"The stochastic ñÑ-calculus is a formalism that has been used for modeling complex dynamical systems where the stochasticity and the delay of transitions are important features, such as in the case of biochemical reactions. Commonly, durations of transitions within stochastic ñÑ-calculus models follow an exponential law. The underlying dynamics of such models are expressed in terms of continuous-time Markov chains, which can then be efficiently simulated and model-checked. However, the exponential law comes with a huge variance, making it difficult to model systems with accurate temporal constraints. In this paper, a technique for tuning temporal features within the stochastic ñÑ-calculus is presented. This method relies on the introduction of a stochasticity absorption factor by replacing the exponential distribution with the Erlang distribution, which is a sum of exponential random variables. This paper presents a construction of the stochasticity absorption factor in the classical stochastic ñÑ-calculus with exponential rates. Tools for manipulating the stochasticity absorption factor and its link with timed intervals for firing transitions are also presented. Finally, the model-checking of such designed models is tackled by supporting the stochasticity absorption factor in a translation from the stochastic ñÑ-calculus to the probabilistic model checker PRISM.",0098-5589;00985589,,10.1109/TSE.2010.95,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611556,Markov processes;Temporal parameters;model-checking;pi-calculus;stochastic processes.,Analytical models;Exponential distribution;Random variables;Stochastic processes,exponential distribution;formal verification;pi calculus;stochastic processes,Erlang distribution;PRISM;biochemical reactions;complex dynamical system modeling;continuous-time Markov chains;exponential distribution;exponential random variables;probabilistic model checker;stochastic ñÑ-calculus;stochasticity absorption factor;temporal feature tuning,,1,,36,,,20101028,Nov.-Dec. 2011,,IEEE,IEEE Journals & Magazines,,13
Plat_Forms: A Web Development Platform Comparison by an Exploratory Experiment Searching for Emergent Platform Properties,L. Prechelt,"Freie Universit&#x0E4;t Berlin, Berlin",IEEE Transactions on Software Engineering,20110128,2011,37,1,95,108,"Background: For developing Web-based applications, there exist several competing and widely used technological platforms (consisting of a programming language, framework(s), components, and tools), each with an accompanying development culture and style. Research question: Do Web development projects exhibit emergent process or product properties that are characteristic and consistent within a platform, but show relevant substantial differences across platforms or do team-to-team individual differences outweigh such differences, if any? Such a property could be positive (i.e., a platform advantage), negative, or neutral, and it might be unobvious which is which. Method: In a nonrandomized, controlled experiment, framed as a public contest called _ÑÒPlat_Forms,_Ñù top-class teams of three professional programmers competed to implement the same requirements for a Web-based application within 30 hours. Three different platforms (Java EE, PHP, or Perl) were used by three teams each. We compare the resulting nine products and process records along many dimensions, both external (usability, functionality, reliability, security, etc.) and internal (size, structure, modifiability, etc.). Results: The various results obtained cover a wide spectrum: First, there are results that many people would have called _ÑÒobvious_Ñù or _ÑÒwell known,_Ñù say, that Perl solutions tend to be more compact than Java solutions. Second, there are results that contradict conventional wisdom, say, that our PHP solutions appear in some (but not all) respects to be actually at least as secure as the others. Finally, one result makes a statement we have not seen discussed previously: Along several dimensions, the amount of within-platform variation between the teams tends to be smaller for PHP than for the other platforms. Conclusion: The results suggest that substantial characteristic platform differences do indeed exist in some dimensions, but possibly not in o thers.",0098-5589;00985589,,10.1109/TSE.2010.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406528,Emergent properties;Java;PHP;Perl.;design structure;functionality;modifiability;product size;reliability;security;usability,Buildings;Cascading style sheets;Computer languages;Ecosystems;Java;Libraries;Product design;Programming profession;Security;Usability,Internet;Java;Perl;Web design;emergent phenomena,Java EE;PHP solutions;Perl solutions;Web based applications;Web development platform;emergent platform properties,,2,,26,,,20100205,Jan.-Feb. 2011,,IEEE,IEEE Journals & Magazines,,13
Swarm Verification Techniques,G. J. Holzmann; R. Joshi; A. Groce,"California Institute of Technology, Pasadena",IEEE Transactions on Software Engineering,20111205,2011,37,6,845,857,"The range of verification problems that can be solved with logic model checking tools has increased significantly in the last few decades. This increase in capability is based on algorithmic advances and new theoretical insights, but it has also benefitted from the steady increase in processing speeds and main memory sizes on standard computers. The steady increase in processing speeds, though, ended when chip-makers started redirecting their efforts to the development of multicore systems. For the near-term future, we can anticipate the appearance of systems with large numbers of CPU cores, but without matching increases in clock-speeds. We will describe a model checking strategy that can allow us to leverage this trend and that allows us to tackle significantly larger problem sizes than before.",0098-5589;00985589,,10.1109/TSE.2010.110,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661793,Software engineering tools and techniques;distributed algorithms;logic model checking;software verification.,Computational modeling;Data models;Formal verification;Memory management;Multicore processing;Parallel processing;Search problems,formal verification;multiprocessing systems,CPU cores;logic model checking tools;multicore system development;swarm verification techniques,,20,,22,,,20101210,Nov.-Dec. 2011,,IEEE,IEEE Journals & Magazines,,12
The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment,M. Jorgensen; S. Grimstad,"University of Oslo and Simula Research Laboratory, Lysaker",IEEE Transactions on Software Engineering,20110929,2011,37,5,695,707,"Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field settings were much smaller than those found for similar manipulations in laboratory settings. Our findings suggest that we should be careful about generalizing to field settings the effect sizes found in laboratory settings. While laboratory settings can be useful to demonstrate the existence of an effect and better understand it, field studies may be needed to study the size and importance of these effects.",0098-5589;00985589,,10.1109/TSE.2010.78,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551161,Cost estimation;requirements/specifications.;software psychology,Companies;Estimation;Laboratories;Materials;Programming;Project management;Software,formal specification;software cost estimation,irrelevant information impact;laboratory settings;misleading information impact;original requirement specification;randomized controlled field experiment;software development effort estimates;software development projects;software psychology,,13,,31,,,20100819,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines,,12
Toward a Formalism for Conservative Claims about the Dependability of Software-Based Systems,P. Bishop; R. Bloomfield; B. Littlewood; A. Povyakalo; D. Wright,"City University, London and Adelard LLP, London",IEEE Transactions on Software Engineering,20110929,2011,37,5,708,717,"In recent work, we have argued for a formal treatment of confidence about the claims made in dependability cases for software-based systems. The key idea underlying this work is ""the inevitability of uncertainty"": It is rarely possible to assert that a claim about safety or reliability is true with certainty. Much of this uncertainty is epistemic in nature, so it seems inevitable that expert judgment will continue to play an important role in dependability cases. Here, we consider a simple case where an expert makes a claim about the probability of failure on demand (pfd) of a subsystem of a wider system and is able to express his confidence about that claim probabilistically. An important, but difficult, problem then is how such subsystem (claim, confidence) pairs can be propagated through a dependability case for a wider system, of which the subsystems are components. An informal way forward is to justify, at high confidence, a strong claim, and then, conservatively, only claim something much weaker: ""I'm 99 percent confident that the pfd is less than 10<sup>-5</sup>, so it's reasonable to be 100 percent confident that it is less than 10<sup>-3</sup>."" These conservative pfds of subsystems can then be propagated simply through the dependability case of the wider system. In this paper, we provide formal support for such reasoning.",0098-5589;00985589,,10.1109/TSE.2010.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5492693,Bayesian probability;safety case;software reliability.,Battery powered vehicles;Phase frequency detector;Power engineering and energy;Power engineering computing;Programming;Reliability engineering;Software reliability;Software safety;Software systems;Uncertainty,probability;software fault tolerance;uncertainty handling,conservative claims;formal support;probability of failure on demand;software-based system dependability;software-based system reliability;software-based system safety,,11,,24,,,20100628,Sept.-Oct. 2011,,IEEE,IEEE Journals & Magazines,,9
A Systematic Literature Review on Fault Prediction Performance in Software Engineering,T. Hall; S. Beecham; D. Bowes; D. Gray; S. Counsell,"Brunel University, Uxbridge",IEEE Transactions on Software Engineering,20121129,2012,38,6,1276,1304,"Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.",0098-5589;00985589,,10.1109/TSE.2011.103,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035727,Systematic literature review;software fault prediction,Analytical models;Context modeling;Data models;Fault diagnosis;Predictive models;Software testing;Systematics,Bayes methods;regression analysis;software fault tolerance;software quality,contextual information;cost reduction;fault prediction models;fault prediction performance;fault prediction study;feature selection;independent variables;logistic regression;methodological information;naive Bayes;predictive performance;reliable methodology;simple modeling techniques;software engineering;software quality;systematic literature review,,171,,,,,20111006,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,28
StakeRare: Using Social Networks and Collaborative Filtering for Large-Scale Requirements Elicitation,S. L. Lim; A. Finkelstein,"University College London, London",IEEE Transactions on Software Engineering,20120529,2012,38,3,707,735,"Requirements elicitation is the software engineering activity in which stakeholder needs are understood. It involves identifying and prioritizing requirements-a process difficult to scale to large software projects with many stakeholders. This paper proposes StakeRare, a novel method that uses social networks and collaborative filtering to identify and prioritize requirements in large software projects. StakeRare identifies stakeholders and asks them to recommend other stakeholders and stakeholder roles, builds a social network with stakeholders as nodes and their recommendations as links, and prioritizes stakeholders using a variety of social network measures to determine their project influence. It then asks the stakeholders to rate an initial list of requirements, recommends other relevant requirements to them using collaborative filtering, and prioritizes their requirements using their ratings weighted by their project influence. StakeRare was evaluated by applying it to a software project for a 30,000-user system, and a substantial empirical study of requirements elicitation was conducted. Using the data collected from surveying and interviewing 87 stakeholders, the study demonstrated that StakeRare predicts stakeholder needs accurately and arrives at a more complete and accurately prioritized list of requirements compared to the existing method used in the project, taking only a fraction of the time.",0098-5589;00985589,,10.1109/TSE.2011.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740931,Requirements/specifications;elicitation methods;experimentation;human factors;recommender systems;requirements prioritization;social network analysis;stakeholder analysis.,Business;Collaboration;Filtering;Size measurement;Social network services;Software;Software engineering,collaborative filtering;data acquisition;project management;recommender systems;social networking (online);software management,StakeRare;collaborative filtering;data collection;recommender system;requirement elicitation;social network;software engineering;software project;stakeholder,,37,,113,,,20110405,May-June 2012,,IEEE,IEEE Journals & Magazines,,28
An Extensible Framework for Improving a Distributed Software System's Deployment Architecture,S. Malek; N. Medvidovic; M. Mikic-Rakic,"George Mason University, Fairfax",IEEE Transactions on Software Engineering,20120130,2012,38,1,73,100,"A distributed system's allocation of software components to hardware nodes (i.e., deployment architecture) can have a significant impact on its quality of service (QoS). For a given system, there may be many deployment architectures that provide the same functionality, but with different levels of QoS. The parameters that influence the quality of a system's deployment architecture are often not known before the system's initial deployment and may change at runtime. This means that redeployment of the software system may be necessary to improve the system's QoS properties. This paper presents and evaluates a framework aimed at finding the most appropriate deployment architecture for a distributed software system with respect to multiple, possibly conflicting QoS dimensions. The framework supports formal modeling of the problem and provides a set of tailorable algorithms for improving a system's deployment. We have realized the framework on top of a visual deployment architecture modeling and analysis environment. The framework has been evaluated for precision and execution-time complexity on a large number of simulated distributed system scenarios, as well as in the context of two third-party families of distributed applications.",0098-5589;00985589,,10.1109/TSE.2011.3,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680912,Software architecture;quality of service;self-adaptive software.;software deployment,Distributed processing;Quality of service;Software architecture,computational complexity;distributed processing;object-oriented methods;quality of service;resource allocation,QoS;distributed software system;execution-time complexity;extensible framework;hardware nodes;precision-time complexity;quality of service;software component allocation;system deployment;tailorable algorithms;visual deployment architecture analysis environment;visual deployment architecture modeling environment,,30,1,66,,,20110106,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,27
Modeling Product Line Software Assets Using Domain-Specific Kits,N. I. Altintas; S. Cetin; A. H. Dogru; H. Oguztuzun,"Cybersoft Information Technologies, Istanbul",IEEE Transactions on Software Engineering,20121129,2012,38,6,1376,1402,"Software Product Line Engineering (SPLE) is a prominent paradigm for the assembly of a family of products using product line core assets. The modeling of software assets that together form the actual products is critical for achieving the strategic benefits of Software Product Lines (SPLs). We propose a feature-based approach to software asset modeling based on abstractions provided by Domain-Specific Kits (DSKs). This approach involves a software Asset Metamodel (AMM) used to derive Asset Modeling Languages (AMLs) that define reusable software assets in domain-specific terms. The approach also prescribes a roadmap for modeling these software assets in conjunction with the product line reference architecture. Asset capabilities can be modeled using feature diagrams as the external views of the software assets. Internal views can be expressed in terms of Domain-Specific Artifacts (DSAs) with Variability Points (VPs), where the domain-specific artifacts are created using Domain-Specific Kits. This approach produces loosely coupled and highly cohesive software assets that are reusable for multiple product lines. The approach is validated by assessing software asset reuse in two different product lines in the finance domain. We also evaluated the productivity gains in large-scale complex projects, and found that the approach yielded a significant reduction in the total project effort.",0098-5589;00985589,,10.1109/TSE.2011.109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065739,Asset modeling;domain-specific kits;feature models;reuse;software asset;software product lines,Complexity theory;Computer architecture;Productivity;Programming;Software reliability;Systematics,financial data processing;product development;project management;simulation languages;software reusability,AML;AMM;DSA;DSK;SPLE;VP;asset modeling languages;domain-specific artifacts;domain-specific kits;domain-specific terms;feature diagrams;feature-based approach;finance domain;internal views;large-scale complex projects;product line core assets;product line reference architecture;product line software asset modelling;productivity gains;software asset metamodel;software asset reusability;software product line engineering;variability points,,5,,86,,,20111101,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,26
Evaluation and Measurement of Software Process Improvement_ÑÓA Systematic Literature Review,M. Unterkalmsteiner; T. Gorschek; A. K. M. M. Islam; C. K. Cheng; R. B. Permadi; R. Feldt,"Blekinge Institute of Technology, Karlskrona",IEEE Transactions on Software Engineering,20120403,2012,38,2,398,424,"BACKGROUND-Software Process Improvement (SPI) is a systematic approach to increase the efficiency and effectiveness of a software development organization and to enhance software products. OBJECTIVE-This paper aims to identify and characterize evaluation strategies and measurements used to assess the impact of different SPI initiatives. METHOD-The systematic literature review includes 148 papers published between 1991 and 2008. The selected papers were classified according to SPI initiative, applied evaluation strategies, and measurement perspectives. Potential confounding factors interfering with the evaluation of the improvement effort were assessed. RESULTS-Seven distinct evaluation strategies were identified, wherein the most common one, _ÑÒPre-Post Comparison,_Ñù was applied in 49 percent of the inspected papers. Quality was the most measured attribute (62 percent), followed by Cost (41 percent), and Schedule (18 percent). Looking at measurement perspectives, _ÑÒProject_Ñù represents the majority with 66 percent. CONCLUSION-The evaluation validity of SPI initiatives is challenged by the scarce consideration of potential confounding factors, particularly given that _ÑÒPre-Post Comparison_Ñù was identified as the most common evaluation strategy, and the inaccurate descriptions of the evaluation context. Measurements to assess the short and mid-term impact of SPI initiatives prevail, whereas long-term measurements in terms of customer satisfaction and return on investment tend to be less used.",0098-5589;00985589,,10.1109/TSE.2011.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728832,Process implementation and change;metrics/measurement;process measurement;systematic literature review.,Current measurement;Data mining;Organizations;Software;Software measurement;Systematics,software process improvement,SPI;customer satisfaction;return on investment;software development organization;software process improvement,,62,,266,,,20110310,March-April 2012,,IEEE,IEEE Journals & Magazines,,26
A Comprehensive Approach to Naming and Accessibility in Refactoring Java Programs,M. SchÕ_fer; A. Thies; F. Steimann; F. Tip,"IBM T.J. Watson Research Center, Hawthorne",IEEE Transactions on Software Engineering,20121129,2012,38,6,1233,1257,"Automated tool support for refactoring is now widely available for mainstream programming languages such as Java. However, current refactoring tools are still quite fragile in practice and often fail to preserve program behavior or compilability. This is mainly because analyzing and transforming source code requires consideration of many language features that complicate program analysis, in particular intricate name lookup and access control rules. This paper introduces J<sub>L</sub>, a lookup-free, access control-free representation of Java programs. We present algorithms for translating Java programs into J<sub>L</sub> and vice versa, thereby making it possible to formulate refactorings entirely at the level of J<sub>L</sub> and to rely on the translations to take care of naming and accessibility issues. We demonstrate how complex refactorings become more robust and powerful when lifted to J<sub>L</sub>. Our approach has been implemented using the JastAddJ compiler framework, and evaluated by systematically performing two commonly used refactorings on an extensive suite of real-world Java applications. The evaluation shows that our tool correctly handles many cases where current refactoring tools fail to handle the complex rules for name binding and accessibility in Java.",0098-5589;00985589,,10.1109/TSE.2012.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152131,Java;Restructuring;and reengineering;object-oriented languages;reverse engineering,Access control;Feature extraction;Java;Object oriented programming;Program processors;Reverse engineering;Shadow mapping,Java;authorisation;naming services;program compilers;program diagnostics;software maintenance,JL;JastAddJ compiler framework;Java program refactoring;access control rules;accessibility issues;comprehensive approach;language features;lookup-free access control-free representation;mainstream programming languages;name lookup;naming issues;program analysis;source code analysis;source code transformation,,6,,43,,,20120214,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,24
"Input Domain Reduction through Irrelevant Variable Removal and Its Effect on Local, Global, and Hybrid Search-Based Structural Test Data Generation",P. McMinn; M. Harman; K. Lakhotia; Y. Hassoun; J. Wegener,"University of Sheffield, Sheffield",IEEE Transactions on Software Engineering,20120403,2012,38,2,453,477,"Search-Based Test Data Generation reformulates testing goals as fitness functions so that test input generation can be automated by some chosen search-based optimization algorithm. The optimization algorithm searches the space of potential inputs, seeking those that are _ÑÒfit for purpose,_Ñù guided by the fitness function. The search space of potential inputs can be very large, even for very small systems under test. Its size is, of course, a key determining factor affecting the performance of any search-based approach. However, despite the large volume of work on Search-Based Software Testing, the literature contains little that concerns the performance impact of search space reduction. This paper proposes a static dependence analysis derived from program slicing that can be used to support search space reduction. The paper presents both a theoretical and empirical analysis of the application of this approach to open source and industrial production code. The results provide evidence to support the claim that input domain reduction has a significant effect on the performance of local, global, and hybrid search, while a purely random search is unaffected.",0098-5589;00985589,,10.1109/TSE.2011.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710949,Search-based software testing;automated test data generation;evolutionary testing;input domain reduction.,Algorithm design and analysis;Input variables;Optimization;Search problems;Software algorithms;Software testing,automatic test pattern generation;optimisation;program compilers;program slicing;program testing;public domain software;search problems,fitness functions;hybrid search-based structural test data generation;industrial production code;input domain reduction;irrelevant variable removal;key determining factor;open source approach;program slicing;search space reduction;search-based optimization algorithm;search-based software testing;static dependence analysis;test input generation,,20,,52,,,20110210,March-April 2012,,IEEE,IEEE Journals & Magazines,,24
Two Studies of Framework-Usage Templates Extracted from Dynamic Traces,A. Heydarnoori; K. Czarnecki; W. Binder; T. T. Bartolomei,"University of Lugano, Lugano",IEEE Transactions on Software Engineering,20121129,2012,38,6,1464,1487,"Object-oriented frameworks are widely used to develop new applications. They provide reusable concepts that are instantiated in application code through potentially complex implementation steps such as subclassing, implementing interfaces, and calling framework operations. Unfortunately, many modern frameworks are difficult to use because of their large and complex APIs and frequently incomplete user documentation. To cope with these problems, developers often use existing framework applications as a guide. However, locating concept implementations in those sample applications is typically challenging due to code tangling and scattering. To address this challenge, we introduce the notion of concept-implementation templates, which summarize the necessary concept-implementation steps and identify them in the sample application code, and a technique, named FUDA, to automatically extract such templates from dynamic traces of sample applications. This paper further presents the results of two experiments conducted to evaluate the quality and usefulness of FUDA templates. The experimental evaluation of FUDA with 14 concepts in five widely used frameworks suggests that the technique is effective in producing templates with relatively few false positives and false negatives for realistic concepts by using two sample applications. Moreover, we observed in a user study with 28 programmers that the use of templates reduced the concept-implementation time compared to when documentation was used.",0098-5589;00985589,,10.1109/TSE.2011.77,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975174,Object-oriented application frameworks;application programming interface (API);concept location;concept-implementation templates;dynamic analysis;feature identification;framework comprehension;framework documentation,Application programming interfaces;Documentation;Dynamic programming;Feature extraction;Java;Runtime,application program interfaces;object-oriented methods,FUDA templates;application code;calling framework operations;code scattering;code tangling;concept-implementation steps;concept-implementation templates;dynamic traces;framework API understanding through dynamic analysis;framework-usage template extraction;interface implementation;object-oriented frameworks;subclassing operation;user documentation,,3,,68,,,20110804,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,23
Formal Specification-Based Inspection for Verification of Programs,S. Liu; Y. Chen; F. Nagoya; J. A. McDermid,"Hosei University, Koganei-shi",IEEE Transactions on Software Engineering,20120924,2012,38,5,1100,1122,"Software inspection is a static analysis technique that is widely used for defect detection, but which suffers from a lack of rigor. In this paper, we address this problem by taking advantage of formal specification and analysis to support a systematic and rigorous inspection method. The aim of the method is to use inspection to determine whether every functional scenario defined in the specification is implemented correctly by a set of program paths and whether every program path of the program contributes to the implementation of some functional scenario in the specification. The method is comprised of five steps: deriving functional scenarios from the specification, deriving paths from the program, linking scenarios to paths, analyzing paths against the corresponding scenarios, and producing an inspection report, and allows for a systematic and automatic generation of a checklist for inspection. We present an example to show how the method can be used, and describe an experiment to evaluate its performance by comparing it to perspective-based reading (PBR). The result shows that our method may be more effective in detecting function-related defects than PBR but slightly less effective in detecting implementation-related defects. We also describe a prototype tool to demonstrate the supportability of the method, and draw some conclusions about our work.",0098-5589;00985589,,10.1109/TSE.2011.102,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035726,Specification-based program inspection;formal specification;program verification;software inspection,DH-HEMTs;High definition video;Three dimensional displays,formal specification;formal verification,PBR;automatic generation;defect detection;formal specification based inspection;perspective based reading;program verification;prototype tool;software inspection;systematic generation,,9,,80,,,20111006,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,22
Adaptation of Service Protocols Using Process Algebra and On-the-Fly Reduction Techniques,R. Mateescu; P. Poizat; G. SalaÕ_n,"Inria Grenoble-Rh&#x00F4;ne-Alpes/CONVECS, Montbonnot Saint-Martin",IEEE Transactions on Software Engineering,20120726,2012,38,4,755,777,"Reuse and composition are increasingly advocated and put into practice in modern software engineering. However, the software entities that are to be reused to build an application, e.g., services, have seldom been developed to integrate and to cope with the application requirements. As a consequence, they present mismatch, which directly hampers their reusability and the possibility of composing them. Software Adaptation has become a hot topic as a nonintrusive solution to work mismatch out using corrective pieces named adaptors. However, adaptation is a complex issue, especially when behavioral interfaces, or conversations, are taken into account. In this paper, we present state-of-the-art techniques to generate adaptors given the description of reused entities' conversations and an abstract specification of the way mismatch can be solved. We use a process algebra to encode the adaptation problem, and propose on-the-fly exploration and reduction techniques to compute adaptor protocols. Our approach follows the model-driven engineering paradigm, applied to service-oriented computing as a representative field of composition-based software engineering. We take service description languages as inputs of the adaptation process and we implement adaptors as centralized service compositions, i.e., orchestrations. Our approach is completely tool supported.",0098-5589;00985589,,10.1109/TSE.2011.62,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928357,Service composition;adaptation contracts;interfaces;mismatch;on--the-fly generation;process algebra;protocols;software adaptation;tools;verification,Adaptation model;Algebra;Computational modeling;Contracts;Encoding;Protocols;Semantics,formal specification;process algebra;protocols;service-oriented architecture;software reusability;specification languages,abstract specification;adaptors;centralized service compositions;composition;composition-based software engineering;model-driven engineering paradigm;on-the-fly exploration;on-the-fly reduction techniques;process algebra;reusability;reused entity conversations;service description languages;service protocol adaptation;service-oriented computing;software adaptation;software entities,,20,,65,,,20110623,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,22
Data Mining Techniques for Software Effort Estimation: A Comparative Study,K. Dejaeger; W. Verbeke; D. Martens; B. Baesens,"Katholieke Universiteit Leuven, Leuven",IEEE Transactions on Software Engineering,20120403,2012,38,2,375,397,"A predictive model is required to be accurate and comprehensible in order to inspire confidence in a business setting. Both aspects have been assessed in a software effort estimation setting by previous studies. However, no univocal conclusion as to which technique is the most suited has been reached. This study addresses this issue by reporting on the results of a large scale benchmarking study. Different types of techniques are under consideration, including techniques inducing tree/rule-based models like M5 and CART, linear models such as various types of linear regression, nonlinear models (MARS, multilayered perceptron neural networks, radial basis function networks, and least squares support vector machines), and estimation techniques that do not explicitly induce a model (e.g., a case-based reasoning approach). Furthermore, the aspect of feature subset selection by using a generic backward input selection wrapper is investigated. The results are subjected to rigorous statistical testing and indicate that ordinary least squares regression in combination with a logarithmic transformation performs best. Another key finding is that by selecting a subset of highly predictive attributes such as project size, development, and environment related attributes, typically a significant increase in estimation accuracy can be obtained.",0098-5589;00985589,,10.1109/TSE.2011.55,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928350,Data mining;regression.;software effort estimation,Artificial neural networks;Cognition;Data mining;Data models;Estimation;Regression tree analysis;Software,data mining;program testing;regression analysis;software cost estimation,CART;M5;data mining techniques;estimation techniques;feature subset selection;generic backward input selection wrapper;linear regression;logarithmic transformation;nonlinear models;ordinary least squares regression;predictive model;rigorous statistical testing;rule-based models;software effort estimation,,64,,108,,,20110623,March-April 2012,,IEEE,IEEE Journals & Magazines,,22
A Theoretical and Empirical Analysis of the Role of Test Sequence Length in Software Testing for Structural Coverage,A. Arcuri,"Simula Research Laboratory, Lysaker",IEEE Transactions on Software Engineering,20120529,2012,38,3,497,519,"In the presence of an internal state, often a sequence of function calls is required to test software. In fact, to cover a particular branch of the code, a sequence of previous function calls might be required to put the internal state in the appropriate configuration. Internal states are not only present in object-oriented software, but also in procedural software (e.g., static variables in C programs). In the literature, there are many techniques to test this type of software. However, to the best of our knowledge, the properties related to the choice of the length of these sequences have received only a little attention in the literature. In this paper, we analyze the role that the length plays in software testing, in particular branch coverage. We show that, on _ÑÒdifficult_Ñù software testing benchmarks, longer test sequences make their testing trivial. Hence, we argue that the choice of the length of the test sequences is very important in software testing. Theoretical analyses and empirical studies on widely used benchmarks and on an industrial software are carried out to support our claims.",0098-5589;00985589,,10.1109/TSE.2011.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750005,Evolutionary testing;length;object-oriented software;search-based software engineering;software testing;state problem;test sequence.,Algorithm design and analysis;Containers;Search problems;Software;Software algorithms;Software testing,C language;object-oriented programming;program testing,C programs;empirical analysis;function calls;internal state;object-oriented software;procedural software;software testing;static variables;structural coverage;test sequence length;test sequences;theoretical analysis,,10,,40,,,20110415,May-June 2012,,IEEE,IEEE Journals & Magazines,,22
Aspect-Oriented Refactoring of Legacy Applications: An Evaluation,M. Mortensen; S. Ghosh; J. Bieman,"Google, Boulder",IEEE Transactions on Software Engineering,20120130,2012,38,1,118,140,"The primary claimed benefits of aspect-oriented programming (AOP) are that it improves the understandability and maintainability of software applications by modularizing crosscutting concerns. Before there is widespread adoption of AOP, developers need further evidence of the actual benefits as well as costs. Applying AOP techniques to refactor legacy applications is one way to evaluate costs and benefits. We replace crosscutting concerns with aspects in three industrial applications to examine the effects on qualities that affect the maintainability of the applications. We study several revisions of each application, identifying crosscutting concerns in the initial revision and also crosscutting concerns that are added in later revisions. Aspect-oriented refactoring reduced code size and improved both change locality and concern diffusion. Costs include the effort required for application refactoring and aspect creation, as well as a decrease in performance.",0098-5589;00985589,,10.1109/TSE.2010.109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661792,Aspect-oriented programming;crosscutting concerns;legacy systems;maintainability.;refactoring,Aspect-oriented programming;Java;Legacy systems;Maintenance engineering;Programming;Software measurement,aspect-oriented programming;software maintenance,AOP techniques;application refactoring;aspect creation;aspect-oriented programming;aspect-oriented refactoring;benefits evaluation;change locality;code size;concern diffusion;cost evaluation;crosscutting concerns;legacy applications;primary claimed benefits;software maintainability;software understandability,,11,,37,,,20101210,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,22
Fault Localization for Dynamic Web Applications,S. Artzi; J. Dolby; F. Tip; M. Pistoia,"IBM Software Group, Littleton",IEEE Transactions on Software Engineering,20120403,2012,38,2,314,335,"In recent years, there has been significant interest in fault-localization techniques that are based on statistical analysis of program constructs executed by passing and failing executions. This paper shows how the Tarantula, Ochiai, and Jaccard fault-localization algorithms can be enhanced to localize faults effectively in web applications written in PHP by using an extended domain for conditional and function-call statements and by using a source mapping. We also propose several novel test-generation strategies that are geared toward producing test suites that have maximal fault-localization effectiveness. We implemented various fault-localization techniques and test-generation strategies in Apollo, and evaluated them on several open-source PHP applications. Our results indicate that a variant of the Ochiai algorithm that includes all our enhancements localizes 87.8 percent of all faults to within 1 percent of all executed statements, compared to only 37.4 percent for the unenhanced Ochiai algorithm. We also found that all the test-generation strategies that we considered are capable of generating test suites with maximal fault-localization effectiveness when given an infinite time budget for test generation. However, on average, a directed strategy based on path-constraint similarity achieves this maximal effectiveness after generating only 6.5 tests, compared to 46.8 tests for an undirected test-generation strategy.",0098-5589;00985589,,10.1109/TSE.2011.76,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975173,Fault localization;PHP.;program analysis;statistical debugging;web applications,Algorithm design and analysis;Browsers;Concrete;Databases;HTML;Open source software;Servers,program testing;software fault tolerance;statistical analysis,Apollo;Jaccard;Ochiai;Tarantula;dynamic Web applications;fault localization;fault localization effectiveness;open-source PHP applications;path constraint;source mapping;statistical analysis;test generation strategies,,13,,50,,,20110804,March-April 2012,,IEEE,IEEE Journals & Magazines,,21
MOSES: A Framework for QoS Driven Runtime Adaptation of Service-Oriented Systems,V. Cardellini; E. Casalicchio; V. Grassi; S. Iannucci; F. L. Presti; R. Mirandola,"University of Roma &#x0022;Tor Vergata&#x0022;, Roma",IEEE Transactions on Software Engineering,20120924,2012,38,5,1138,1159,"Architecting software systems according to the service-oriented paradigm and designing runtime self-adaptable systems are two relevant research areas in today's software engineering. In this paper, we address issues that lie at the intersection of these two important fields. First, we present a characterization of the problem space of self-adaptation for service-oriented systems, thus providing a frame of reference where our and other approaches can be classified. Then, we present MOSES, a methodology and a software tool implementing it to support QoS-driven adaptation of a service-oriented system. It works in a specific region of the identified problem space, corresponding to the scenario where a service-oriented system architected as a composite service needs to sustain a traffic of requests generated by several users. MOSES integrates within a unified framework different adaptation mechanisms. In this way it achieves greater flexibility in facing various operating environments and the possibly conflicting QoS requirements of several concurrent users. Experimental results obtained with a prototype implementation of MOSES show the effectiveness of the proposed approach.",0098-5589;00985589,,10.1109/TSE.2011.68,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963694,Service-oriented architecture;quality of service;runtime adaptation,Adaptation models;Concrete;Quality of service;Runtime;Semiconductor optical amplifiers;Service oriented architecture;Software systems,service-oriented architecture,MOSES;QoS driven runtime adaptation;QoS-driven adaptation;runtime self adaptable system;self adaptation;service oriented paradigm;service oriented system;service-oriented system;software engineering;software system architecture,,56,,62,,,20110728,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,21
Automated Abstractions for Contract Validation,G. de Caso; V. Braberman; D. Garbervetsky; S. Uchitel,"FCEyN, Universidad de Buenos Aires, Buenos Aires",IEEE Transactions on Software Engineering,20120130,2012,38,1,141,162,"Pre/postcondition-based specifications are commonplace in a variety of software engineering activities that range from requirements through to design and implementation. The fragmented nature of these specifications can hinder validation as it is difficult to understand if the specifications for the various operations fit together well. In this paper, we propose a novel technique for automatically constructing abstractions in the form of behavior models from pre/postcondition-based specifications. Abstraction techniques have been used successfully for addressing the complexity of formal artifacts in software engineering; however, the focus has been, up to now, on abstractions for verification. Our aim is abstraction for validation and hence, different and novel trade-offs between precision and tractability are required. More specifically, in this paper, we define and study enabledness-preserving abstractions, that is, models in which concrete states are grouped according to the set of operations that they enable. The abstraction results in a finite model that is intuitive to validate and which facilitates tracing back to the specification for debugging. The paper also reports on the application of the approach to two industrial strength protocol specifications in which concerns were identified.",0098-5589;00985589,,10.1109/TSE.2010.98,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639021,Requirements/specifications;automated abstraction.;validation,Buffer storage;Object oriented modeling;Protocols;Software engineering;Validation,formal specification;software engineering,automated abstractions;behavior models;contract validation;formal artifacts;industrial strength;postcondition based specifications;precondition based specifications;software engineering,,14,,38,,,20101118,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,21
Matching and Merging of Variant Feature Specifications,S. Nejati; M. Sabetzadeh; M. Chechik; S. Easterbrook; P. Zave,"Simula Research Laboratory, Lysaker",IEEE Transactions on Software Engineering,20121129,2012,38,6,1355,1375,"Model Management addresses the problem of managing an evolving collection of models by capturing the relationships between models and providing well-defined operators to manipulate them. In this paper, we describe two such operators for manipulating feature specifications described using hierarchical state machine models: Match, for finding correspondences between models, and Merge, for combining models with respect to known or hypothesized correspondences between them. Our Match operator is heuristic, making use of both static and behavioral properties of the models to improve the accuracy of matching. Our Merge operator preserves the hierarchical structure of the input models, and handles differences in behavior through parameterization. This enables us to automatically construct merges that preserve the semantics of hierarchical state machines. We report on tool support for our Match and Merge operators, and illustrate and evaluate our work by applying these operators to a set of telecommunication features built by AT&T.",0098-5589;00985589,,10.1109/TSE.2011.112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6086550,Model management;behavior preservation;hierarchical state machines;match;merge;parameterization;statecharts;variability modeling,Computational modeling;Electronic mail;Hierarchical systems;Parameterization;Pragmatics;Semantics;Voice mail,finite state machines;formal specification,behavioral property;hierarchical state machine model;hierarchical structure;match operator;merge operator;model management;static property;telecommunication feature;tool support;variant feature specification,,6,,83,,,20111122,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,20
Automatic Detection of Unsafe Dynamic Component Loadings,T. Kwon; Z. Su,"University of California, Davis, Davis",IEEE Transactions on Software Engineering,20120403,2012,38,2,293,313,"Dynamic loading of software components (e.g., libraries or modules) is a widely used mechanism for an improved system modularity and flexibility. Correct component resolution is critical for reliable and secure software execution. However, programming mistakes may lead to unintended or even malicious components being resolved and loaded. In particular, dynamic loading can be hijacked by placing an arbitrary file with the specified name in a directory searched before resolving the target component. Although this issue has been known for quite some time, it was not considered serious because exploiting it requires access to the local file system on the vulnerable host. Recently, such vulnerabilities have started to receive considerable attention as their remote exploitation became realistic. It is now important to detect and fix these vulnerabilities. In this paper, we present the first automated technique to detect vulnerable and unsafe dynamic component loadings. Our analysis has two phases: 1) apply dynamic binary instrumentation to collect runtime information on component loading (online phase), and 2) analyze the collected information to detect vulnerable component loadings (offline phase). For evaluation, we implemented our technique to detect vulnerable and unsafe component loadings in popular software on Microsoft Windows and Linux. Our evaluation results show that unsafe component loading is prevalent in software on both OS platforms, and it is more severe on Microsoft Windows. In particular, our tool detected more than 4,000 unsafe component loadings in our evaluation, and some can lead to remote code execution on Microsoft Windows.",0098-5589;00985589,,10.1109/TSE.2011.108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065738,Unsafe component loading;dynamic analysis.,Image resolution;Instruments;Linux;Loading;Operating systems;Security,Linux;object-oriented programming;operating systems (computers);security of data;software libraries;software reliability;system monitoring,Linux;Microsoft Windows;OS platforms;arbitrary file;automated technique;automatic detection;component resolution;dynamic binary instrumentation;dynamic loading;file system;malicious components;remote code execution;remote exploitation;runtime information;software components;software execution;software libraries;software modules;system flexibility;system modularity;unsafe component loading;unsafe dynamic component loadings;vulnerable component loadings;vulnerable dynamic component loadings;vulnerable host,,2,,50,,,20111101,March-April 2012,,IEEE,IEEE Journals & Magazines,,20
Architecture-Based Reliability Prediction with the Palladio Component Model,F. Brosch; H. Koziolek; B. Buhnova; R. Reussner,"FZI Forschungszentrum Informatik, Karlsruhe",IEEE Transactions on Software Engineering,20121129,2012,38,6,1319,1339,"With the increasing importance of reliability in business and industrial software systems, new techniques of architecture-based reliability engineering are becoming an integral part of the development process. These techniques can assist system architects in evaluating the reliability impact of their design decisions. Architecture-based reliability engineering is only effective if the involved reliability models reflect the interaction and usage of software components and their deployment to potentially unreliable hardware. However, existing approaches either neglect individual impact factors on reliability or hard-code them into formal models, which limits their applicability in component-based development processes. This paper introduces a reliability modeling and prediction technique that considers the relevant architectural factors of software systems by explicitly modeling the system usage profile and execution environment and automatically deriving component usage profiles. The technique offers a UML-like modeling notation whose models are automatically transformed into a formal analytical model. Our work builds upon the Palladio Component Model (PCM), employing novel techniques of information propagation and reliability assessment. We validate our technique with sensitivity analyses and simulation in two case studies. The case studies demonstrate effective support of usage profile analysis and architectural configuration ranking, together with the employment of reliability-improving architecture tactics.",0098-5589;00985589,,10.1109/TSE.2011.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018968,Software architectures;design tools and techniques;quality analysis and evaluation;reliability,Design methodology;Markov processes;Phase change materials;Software architecture;Software quality;Software reliability;Unified modeling language,Unified Modeling Language;object-oriented programming;software architecture;software reliability,UML like modeling notation;architectural configuration ranking;architecture based reliability engineering;architecture based reliability prediction;architecture tactics;assist system architects;component based development process;component usage profiles;execution environment;formal analytical model;industrial software system;information propagation;palladio component model;reliability assessment;reliability impact;reliability modeling;sensitivity analysis;software component;system usage profile;usage profile analysis,,22,,63,,,20110915,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,20
Size-Constrained Regression Test Case Selection Using Multicriteria Optimization,S. Mirarab; S. Akhlaghi; L. Tahvildari,"University of Texas at Austin, Austin",IEEE Transactions on Software Engineering,20120726,2012,38,4,936,956,"To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.",0098-5589;00985589,,10.1109/TSE.2011.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928351,Pareto optimality;Software regression testing;integer programming;test case selection,Estimation;Fault detection;IP networks;Optimization;Software;Testing;Time factors,greedy algorithms;integer programming;linear programming;program testing;regression analysis,greedy algorithm;integer linear programming problem;iterative manner;modified software system;multicriteria optimization;size constrained regression test case selection;voting mechanism,,20,,61,,,20110623,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,20
A UML/MARTE Model Analysis Method for Uncovering Scenarios Leading to Starvation and Deadlocks in Concurrent Systems,M. Shousha; L. Briand; Y. Labiche,"Carleton University, Ottawa",IEEE Transactions on Software Engineering,20120403,2012,38,2,354,374,"Concurrency problems such as starvation and deadlocks should be identified early in the design process. As larger, more complex concurrent systems are being developed, this is made increasingly difficult. We propose here a general approach based on the analysis of specialized design models expressed in the Unified Modeling Language (UML) that uses a specifically designed genetic algorithm to detect concurrency problems. Though the current paper addresses deadlocks and starvation, we will show how the approach can be easily tailored to other concurrency issues. Our main motivations are 1) to devise solutions that are applicable in the context of the UML design of concurrent systems without requiring additional modeling and 2) to use a search technique to achieve scalable automation in terms of concurrency problem detection. To achieve the first objective, we show how all relevant concurrency information is extracted from systems' UML models that comply with the UML Modeling and Analysis of Real-Time and Embedded Systems (MARTE) profile. For the second objective, a tailored genetic algorithm is used to search for execution sequences exhibiting deadlock or starvation problems. Scalability in terms of problem detection is achieved by showing that the detection rates of our approach are, in general, high and are not strongly affected by large increases in the size of complex search spaces.",0098-5589;00985589,,10.1109/TSE.2010.107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661791,MARTE;MDD;Search-based software engineering;UML;concurrent systems;deadlock;genetic algorithms.;model analysis;starvation,Analytical models;Computational modeling;Concurrent computing;Data mining;Real time systems;System recovery;Unified modeling language,Unified Modeling Language;concurrency control;embedded systems;genetic algorithms;search problems;software engineering,MARTE profile;UML design;UML modeling and analysis;UML models;UML-MARTE model analysis method;Unified Modeling Language;complex concurrent systems;complex search spaces;concurrency information;concurrency problem detection;concurrency problems;deadlocks;design process;embedded systems;execution sequences;genetic algorithm;real-time systems;scalable automation;search technique;specialized design models;starvation,,9,,46,,,20101210,March-April 2012,,IEEE,IEEE Journals & Magazines,,20
Random Testing: Theoretical Results and Practical Implications,A. Arcuri; M. Z. Iqbal; L. Briand,"Simula, Oslo",IEEE Transactions on Software Engineering,20120403,2012,38,2,258,277,"A substantial amount of work has shed light on whether random testing is actually a useful testing technique. Despite its simplicity, several successful real-world applications have been reported in the literature. Although it is not going to solve all possible testing problems, random testing appears to be an essential tool in the hands of software testers. In this paper, we review and analyze the debate about random testing. Its benefits and drawbacks are discussed. Novel results addressing general questions about random testing are also presented, such as how long does random testing need, on average, to achieve testing targets (e.g., coverage), how does it scale, and how likely is it to yield similar results if we rerun it on the same testing problem (predictability). Due to its simplicity that makes the mathematical analysis of random testing tractable, we provide precise and rigorous answers to these questions. Results show that there are practical situations in which random testing is a viable option. Our theorems are backed up by simulations and we show how they can be applied to most types of software and testing criteria. In light of these results, we then assess the validity of empirical analyzes reported in the literature and derive guidelines for both practitioners and scientists.",0098-5589;00985589,,10.1109/TSE.2011.121,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104067,Coupon collector;Schur function;adaptive random testing.;partition testing;predictability;random testing;theory,Algorithm design and analysis;Color;Generators;Random variables;Software;Testing;Upper bound,program testing;software tools,mathematical analysis;partition testing;random testing;software testing;software tool,,42,,52,,,20111213,March-April 2012,,IEEE,IEEE Journals & Magazines,,19
Palantir: Early Detection of Development Conflicts Arising from Parallel Code Changes,A. Sarma; D. F. Redmiles; A. van der Hoek,"University of Nebraska- Lincoln, Lincoln",IEEE Transactions on Software Engineering,20120726,2012,38,4,889,908,"The earlier a conflict is detected, the easier it is to resolve-this is the main precept of workspace awareness. Workspace awareness seeks to provide users with information of relevant ongoing parallel changes occurring in private workspaces, thereby enabling the early detection and resolution of potential conflicts. The key approach is to unobtrusively inform developers of potential conflicts arising because of concurrent changes to the same file and dependency violations in ongoing parallel work. This paper describes our research goals, approach, and implementation of workspace awareness through PalantiÍÅr and includes a comprehensive evaluation involving two laboratory experiments. We present both quantitative and qualitative results from the experiments, which demonstrate that the use of PalantiÍÅr, as compared to not using PalantiÍÅr 1) leads to both earlier detection and earlier resolution of a larger number of conflicts, 2) leaves fewer conflicts unresolved in the code base that was ultimately checked in, and 3) involves reasonable overhead. Furthermore, we report on interesting changes in users' behavior, especially how conflict resolution strategies changed among PalantiÍÅr users.",0098-5589;00985589,,10.1109/TSE.2011.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928359,Software engineering;computer-supported collaborative work;configuration management;programmer workbench,Computer architecture;Context;Databases;Instant messaging;Laboratories;Measurement;Monitoring,configuration management;groupware;parallel processing;software management,PalantiÍÅr;computer-supported collaborative work;conflict resolution strategies;dependency violations;development conflict early detection;laboratory experiments;parallel code changes;software configuration management system;software engineering;workspace awareness,,20,,55,,,20110623,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,19
On the Evolution of Services,V. Andrikopoulos; S. Benbernou; M. P. Papazoglou,"IAAS, University of Stuttgart, Stuttgart",IEEE Transactions on Software Engineering,20120529,2012,38,3,609,628,"In an environment of constant change and variation driven by competition and innovation, a software service can rarely remain stable. Being able to manage and control the evolution of services is therefore an important goal for the Service-Oriented paradigm. This work extends existing and widely adopted theories from software engineering, programming languages, service-oriented computing, and other related fields to provide the fundamental ingredients required to guarantee that spurious results and inconsistencies that may occur due to uncontrolled service changes are avoided. The paper provides a unifying theoretical framework for controlling the evolution of services that deals with structural, behavioral, and QoS level-induced service changes in a type-safe manner, ensuring correct versioning transitions so that previous clients can use a versioned service in a consistent manner.",0098-5589;00985589,,10.1109/TSE.2011.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728828,Services engineering;service compatibility.;service evolution;versioning,Availability;Business;Guidelines;Protocols;Quality of service;Software;XML,Web services;service-oriented architecture,programming languages;service-oriented computing;service-oriented paradigm;software engineering;software service,,51,,55,,,20110310,May-June 2012,,IEEE,IEEE Journals & Magazines,,19
Toward a Tool-Based Development Methodology for Pervasive Computing Applications,D. Cassou; J. Bruneau; C. Consel; E. Balland,"University of Bordeaux and INRIA, Talence",IEEE Transactions on Software Engineering,20121129,2012,38,6,1445,1463,"Despite much progress, developing a pervasive computing application remains a challenge because of a lack of conceptual frameworks and supporting tools. This challenge involves coping with heterogeneous devices, overcoming the intricacies of distributed systems technologies, working out an architecture for the application, encoding it in a program, writing specific code to test the application, and finally deploying it. This paper presents a design language and a tool suite covering the development life-cycle of a pervasive computing application. The design language allows us to define a taxonomy of area-specific building-blocks, abstracting over their heterogeneity. This language also includes a layer to define the architecture of an application, following an architectural pattern commonly used in the pervasive computing domain. Our underlying methodology assigns roles to the stakeholders, providing separation of concerns. Our tool suite includes a compiler that takes design artifacts written in our language as input and generates a programming framework that supports the subsequent development stages, namely, implementation, testing, and deployment. Our methodology has been applied on a wide spectrum of areas. Based on these experiments, we assess our approach through three criteria: expressiveness, usability, and productivity.",0098-5589;00985589,,10.1109/TSE.2011.107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051438,Methodology;domain-specific language;generative programming;pervasive computing;programming support;simulation;toolkit,Computational modeling;Computer architecture;Domain specific languages;Pervasive computing;Programming;Software architecture;Taxonomy,program compilers;software architecture;ubiquitous computing,architectural pattern;area-specific building-blocks;compiler;design artifacts;development life-cycle;distributed systems technologies;pervasive computing applications;tool-based development methodology,,15,,53,,,20111018,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,18
GenProg: A Generic Method for Automatic Software Repair,C. Le Goues; T. Nguyen; S. Forrest; W. Weimer,"University of Virginia, Charlottesville",IEEE Transactions on Software Engineering,20120130,2012,38,1,54,72,"This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality.",0098-5589;00985589,,10.1109/TSE.2011.104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035728,Automatic programming;corrections;testing and debugging.,Automatic programming;Computer bugs;Debugging;Encoding;Maintenance engineering;Syntactics,formal specification;genetic algorithms;software maintenance,GenProg;automatic software repair;defects repair;formal specifications;genetic programming;legacy programs;program annotations;special coding practices,,134,1,75,,,20111006,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,18
Clone Management for Evolving Software,H. A. Nguyen; T. T. Nguyen; N. H. Pham; J. Al-Kofahi; T. N. Nguyen,"Iowa State University, Ames",IEEE Transactions on Software Engineering,20120924,2012,38,5,1008,1026,"Recent research results suggest a need for code clone management. In this paper, we introduce JSync, a novel clone management tool. JSync provides two main functions to support developers in being aware of the clone relation among code fragments as software systems evolve and in making consistent changes as they create or modify cloned code. JSync represents source code and clones as (sub)trees in Abstract Syntax Trees, measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of JSync include the algorithms to compute tree editing scripts, to detect and update code clones and their groups, to analyze the changes of cloned code to validate their consistency, and to recommend relevant clone synchronization and merging. Our empirical study on several real-world systems shows that JSync is efficient and accurate in clone detection and updating, and provides the correct detection of the defects resulting from inconsistent changes to clones and the correct recommendations for change propagation across cloned code.",0098-5589;00985589,,10.1109/TSE.2011.90,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007141,Clone management;clone consistency analysis;clone merging;clone synchronization,Cloning;Databases;Feature extraction;Merging;Software systems;Synchronization;Vegetation,Java;program compilers,JSync;abstract syntax trees;change propagation;clone management tool;code clone management;code fragments;evolving software;software systems;source code;structural characteristic vectors;tree editing scripts,,27,,50,,,20110901,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,18
Exemplar: A Source Code Search Engine for Finding Highly Relevant Applications,C. McMillan; M. Grechanik; D. Poshyvanyk; C. Fu; Q. Xie,"College of William and Mary, Williamsburg",IEEE Transactions on Software Engineering,20120924,2012,38,5,1069,1087,"A fundamental problem of finding software applications that are highly relevant to development tasks is the mismatch between the high-level intent reflected in the descriptions of these tasks and low-level implementation details of applications. To reduce this mismatch we created an approach called EXEcutable exaMPLes ARchive (Exemplar) for finding highly relevant software projects from large archives of applications. After a programmer enters a natural-language query that contains high-level concepts (e.g., MIME, datasets), Exemplar retrieves applications that implement these concepts. Exemplar ranks applications in three ways. First, we consider the descriptions of applications. Second, we examine the Application Programming Interface (API) calls used by applications. Third, we analyze the dataflow among those API calls. We performed two case studies (with professional and student developers) to evaluate how these three rankings contribute to the quality of the search results from Exemplar. The results of our studies show that the combined ranking of application descriptions and API documents yields the most-relevant search results. We released Exemplar and our case study data to the public.",0098-5589;00985589,,10.1109/TSE.2011.84,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989838,Source code search engines;concept location;information retrieval;mining software repositories;open source software;software reuse,Cryptography;Data mining;Engines;Java;Search engines;Software;Vocabulary,application program interfaces;data flow analysis;document handling;natural language processing;project management;query processing;software management;software reusability;system documentation,API call;API document;Exemplar;application description ranking;application programming interface;dataflow;development task;executable examples archive;natural-language query;search quality;software application;software project;software reuse;source code search engine,,26,1,52,,,20110818,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,18
What Do We Know about the Effectiveness of Software Design Patterns?,C. Zhang; D. Budgen,"Durham University, Durham",IEEE Transactions on Software Engineering,20120924,2012,38,5,1213,1231,"Context. Although research in software engineering largely seeks to improve the practices and products of software development, many practices are based upon codification of expert knowledge, often with little or no underpinning from objective empirical evidence. Software design patterns seek to codify expert knowledge to share experience about successful design structures. Objectives. To investigate how extensively the use of software design patterns has been subjected to empirical study and what evidence is available about how and when their use can provide an effective mechanism for knowledge transfer about design. Method. We conducted a systematic literature review in the form of a mapping study, searching the literature up to the end of 2009 to identify relevant primary studies about the use of the 23 patterns catalogued in the widely referenced book by the _ÑÒGang of Four._Ñù These studies were then categorized according to the forms of study employed, the patterns that were studied, as well as the context within which the study took place. Results. Our searches identified 611 candidate papers. Applying our inclusion/exclusion criteria resulted in a final set of 10 papers that described 11 instances of _ÑÒformal_Ñù experimental studies of object-oriented design patterns. We augmented our analysis by including seven _ÑÒexperience_Ñù reports that described application of patterns using less rigorous observational forms. We report and review the profiles of the empirical evidence for those patterns for which multiple studies exist. Conclusions. We could not identify firm support for any of the claims made for patterns in general, although there was some support for the usefulness of patterns in providing a framework for maintenance, and some qualitative indication that they do not help novices learn about design. For future studies we recommend that researchers use case studies that focus upon some key patterns, and seek to id- ntify the impact that their use can have upon maintenance.",0098-5589;00985589,,10.1109/TSE.2011.79,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975176,Design patterns;empirical software engineering;systematic literature review,Maintenance engineering;Search engines;Software design;Software engineering;Systematics;Terminology,object-oriented programming;software maintenance,Gang-of-Four;design structures;knowledge transfer;object-oriented design patterns;software design patterns;software development;software engineering,,25,,66,,,20110804,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,18
QoS Assurance for Dynamic Reconfiguration of Component-Based Software Systems,W. Li,"Centerfor Intell. &amp; Networked Syst., Central Queensland Univ., Rockhampton, QLD, Australia",IEEE Transactions on Software Engineering,20120529,2012,38,3,658,676,"A major challenge of dynamic reconfiguration is Quality of Service (QoS) assurance, which is meant to reduce application disruption to the minimum for the system's transformation. However, this problem has not been well studied. This paper investigates the problem for component-based software systems from three points of view. First, the whole spectrum of QoS characteristics is defined. Second, the logical and physical requirements for QoS characteristics are analyzed and solutions to achieve them are proposed. Third, prior work is classified by QoS characteristics and then realized by abstract reconfiguration strategies. On this basis, quantitative evaluation of the QoS assurance abilities of existing work and our own approach is conducted through three steps. First, a proof-of-concept prototype called the reconfigurable component model is implemented to support the representation and testing of the reconfiguration strategies. Second, a reconfiguration benchmark is proposed to expose the whole spectrum of QoS problems. Third, each reconfiguration strategy is tested against the benchmark and the testing results are evaluated. The most important conclusion from our investigation is that the classified QoS characteristics can be fully achieved under some acceptable constraints.",0098-5589;00985589,,10.1109/TSE.2011.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740932,Change management;componentware;dynamic reconfiguration;modeling the QoS assurance process;system evolution.,Benchmark testing;Connectors;Encryption;Protocols;Quality of service;Receivers,quality of service;software quality,QoS assurance;Quality of Service;abstract reconfiguration;application disruption;component based software systems;dynamic reconfiguration;logical requirements;physical requirements;quantitative evaluation;reconfigurable component;reconfiguration benchmark,,13,,39,,,20110405,May-June 2012,,IEEE,IEEE Journals & Magazines,,18
PerLa: A Language and Middleware Architecture for Data Management and Integration in Pervasive Information Systems,F. A. Schreiber; R. Camplani; M. Fortunato; M. Marelli; G. Rota,"Politecnico di Milano, Milano",IEEE Transactions on Software Engineering,20120403,2012,38,2,478,496,"A declarative SQL-like language and a middleware infrastructure are presented for collecting data from different nodes of a pervasive system. Data management is performed by hiding the complexity due to the large underlying heterogeneity of devices, which can span from passive RFID(s) to ad hoc sensor boards to portable computers. An important feature of the presented middleware is to make the integration of new device types in the system easy through the use of device self-description. Two case studies are described for PerLa usage, and a survey is made for comparing our approach with other projects in the area.",0098-5589;00985589,,10.1109/TSE.2011.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728831,Declarative language;SQL;device heterogeneity;functionality proxy;middleware infrastructure;pervasive system;wireless sensor networks.,Context;Databases;Hardware;Middleware;Monitoring;Software;Wireless sensor networks,SQL;data integration;information systems;middleware;software architecture;ubiquitous computing,PerLa;ad hoc sensor boards;data collection;data integration;data management;declarative SQL-like language;device self-description;language architecture;middleware architecture;passive RFID;pervasive information systems;portable computers,,16,,53,,,20110310,March-April 2012,,IEEE,IEEE Journals & Magazines,,18
Invariant-Based Automatic Testing of Modern Web Applications,A. Mesbah; A. van Deursen; D. Roest,"University of British Columbia, Vancouver",IEEE Transactions on Software Engineering,20120130,2012,38,1,35,53,"Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.",0098-5589;00985589,,10.1109/TSE.2011.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728834,Ajax.;Automated testing;web applications,Browsers;Robots;Servers;User interfaces;Web and internet services,Internet;Java;XML;automatic testing;client-server systems;program testing;trees (mathematics);user interfaces,AJAX-based Web 2.0 application;AJAX-specific fault identification;DOM-tree invariant;application-specific state validator;client-side runtime manipulation;fault detection;fault revealing capability;generic invariant checking component;invariant-based automatic testing;state-flow graph;stateful asynchronous client-server communication;user interface,,42,,40,,,20110310,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,18
A Static Approach to Prioritizing JUnit Test Cases,H. Mei; D. Hao; L. Zhang; L. Zhang; J. Zhou; G. Rothermel,"Peking University, Beijing",IEEE Transactions on Software Engineering,20121129,2012,38,6,1258,1275,"Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the program under test to estimate the ability of each test case to achieve code coverage, and then schedules the order of these test cases based on those estimates. To evaluate the effectiveness of JUPTA, we conducted an empirical study on 19 versions of four Java programs ranging from 2K-80K lines of code, and compared several variants of JUPTA with three control techniques, and several other existing dynamic coverage-based test case prioritization techniques, assessing the abilities of the techniques to increase the rate of fault detection of test suites. Our results show that the test suites constructed by JUPTA are more effective than those in random and untreated test orders in terms of fault-detection effectiveness. Although the test suites constructed by dynamic coverage-based techniques retain fault-detection effectiveness advantages, the fault-detection effectiveness of the test suites constructed by JUPTA is close to that of the test suites constructed by those techniques, and the fault-detection effectiveness of the test suites constructed by some of - UPTA's variants is better than that of the test suites constructed by several of those techniques.",0098-5589;00985589,,10.1109/TSE.2011.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363461,JUnit;Software testing;call graph;regression testing;test case prioritization,Regression analysis;Scheduling;Software testing,Java;program testing;regression analysis;software fault tolerance,JUPTA;JUnit test case prioritization techniques operating in the absence of coverage information;Java programs;dynamic code coverage information;dynamic coverage-based techniques;fault-detection effectiveness;regression testing;static approach;static call graphs;test case prioritization techniques,,35,,44,,,,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,17
Solving the Large Scale Next Release Problem with a Backbone-Based Multilevel Algorithm,J. Xuan; H. Jiang; Z. Ren; Z. Luo,"Dalian University of Technology, Dalian",IEEE Transactions on Software Engineering,20120924,2012,38,5,1195,1212,"The Next Release Problem (NRP) aims to optimize customer profits and requirements selection for the software releases. The research on the NRP is restricted by the growing scale of requirements. In this paper, we propose a Backbone-based Multilevel Algorithm (BMA) to address the large scale NRP. In contrast to direct solving approaches, the BMA employs multilevel reductions to downgrade the problem scale and multilevel refinements to construct the final optimal set of customers. In both reductions and refinements, the backbone is built to fix the common part of the optimal customers. Since it is intractable to extract the backbone in practice, the approximate backbone is employed for the instance reduction while the soft backbone is proposed to augment the backbone application. In the experiments, to cope with the lack of open large requirements databases, we propose a method to extract instances from open bug repositories. Experimental results on 15 classic instances and 24 realistic instances demonstrate that the BMA can achieve better solutions on the large scale NRP instances than direct solving approaches. Our work provides a reduction approach for solving large scale problems in search-based requirements engineering.",0098-5589;00985589,,10.1109/TSE.2011.92,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261327,The next release problem;backbone;multilevel algorithm;requirements instance generation;search-based requirements engineering;soft backbone,Algorithm design and analysis;Approximation algorithms;Optimization;Polynomials;Search problems;Software;Software algorithms,customer services;formal specification;program debugging;software development management;systems analysis,BMA;backbone extraction;backbone-based multilevel algorithm;customer profit optimization;instance reduction;large scale NRP instances;large scale next release problem;multilevel reductions;open bug repositories;open large requirements databases;optimal customers;requirements selection;search-based requirements engineering;soft backbone;software releases,,24,,69,,,20120806,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,17
Precise Calling Context Encoding,W. N. Sumner; Y. Zheng; D. Weeratunge; X. Zhang,"Purdue University, West Lafayette",IEEE Transactions on Software Engineering,20120924,2012,38,5,1160,1177,"Calling contexts (CCs) are very important for a wide range of applications such as profiling, debugging, and event logging. Most applications perform expensive stack walking to recover contexts. The resulting contexts are often explicitly represented as a sequence of call sites and hence are bulky. We propose a technique to encode the current calling context of any point during an execution. In particular, an acyclic call path is encoded into one number through only integer additions. Recursive call paths are divided into acyclic subsequences and encoded independently. We leverage stack depth in a safe way to optimize encoding: If a calling context can be safely and uniquely identified by its stack depth, we do not perform encoding. We propose an algorithm to seamlessly fuse encoding and stack depth-based identification. The algorithm is safe because different contexts are guaranteed to have different IDs. It also ensures contexts can be faithfully decoded. Our experiments show that our technique incurs negligible overhead (0-6.4 percent). For most medium-sized programs, it can encode all contexts with just one number. For large programs, we are able to encode most calling contexts to a few numbers. We also present our experience of applying context encoding to debugging crash-based failures.",0098-5589;00985589,,10.1109/TSE.2011.70,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963696,Calling context;call graph;calling context encoding;context sensitivity;path encoding;profiling,Context;Decoding;Encoding;Image edge detection;Instruments;Runtime;Software algorithms,optimisation;program compilers;program debugging,CC;ID;acyclic subsequences;call sites;context recovery;crash-based failure debugging;encoding optimization;event logging;medium-sized programs;precise calling context encoding;profiling;recursive call paths;stack depth-based identification;stack walking,,6,,54,,,20110728,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,17
Does Software Process Improvement Reduce the Severity of Defects? A Longitudinal Field Study,D. E. Harter; C. F. Kemerer; S. A. Slaughter,"Syracuse University, Syracuse",IEEE Transactions on Software Engineering,20120726,2012,38,4,810,827,"As firms increasingly rely on information systems to perform critical functions, the consequences of software defects can be catastrophic. Although the software engineering literature suggests that software process improvement can help to reduce software defects, the actual evidence is equivocal. For example, improved development processes may only remove the _ÑÒeasier_Ñù syntactical defects, while the more critical defects remain. Rigorous empirical analyses of these relationships have been very difficult to conduct due to the difficulties in collecting the appropriate data on real systems from industrial organizations. This field study analyzes a detailed data set consisting of 7,545 software defects that were collected on software projects completed at a major software firm. Our analyses reveal that higher levels of software process improvement significantly reduce the likelihood of high severity defects. In addition, we find that higher levels of process improvement are even more beneficial in reducing severe defects when the system developed is large or complex, but are less beneficial in development when requirements are ambiguous, unclear, or incomplete. Our findings reveal the benefits and limitations of software process improvement for the removal of severe defects and suggest where investments in improving development processes may have their greatest effects.",0098-5589;00985589,,10.1109/TSE.2011.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928358,CMM;Software complexity;defect severity;requirements ambiguity;software process,Complexity theory;Coordinate measuring machines;Production;Programming;Software quality;Testing,program debugging;software process improvement,critical defects;information systems;longitudinal field study;severe defects;software defects;software engineering literature;software process improvement;syntactical defects,,16,,68,,,20110623,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,17
SMT-Based Bounded Model Checking for Embedded ANSI-C Software,L. Cordeiro; B. Fischer; J. Marques-Silva,"Federal University of Amazonas, Brazil",IEEE Transactions on Software Engineering,20120726,2012,38,4,957,974,"Propositional bounded model checking has been applied successfully to verify embedded software, but remains limited by increasing propositional formula sizes and the loss of high-level information during the translation preventing potential optimizations to reduce the state space to be explored. These limitations can be overcome by encoding high-level information in theories richer than propositional logic and using SMT solvers for the generated verification conditions. Here, we propose the application of different background theories and SMT solvers to the verification of embedded software written in ANSI-C in order to improve scalability and precision in a completely automatic way. We have modified and extended the encodings from previous SMT-based bounded model checkers to provide more accurate support for variables of finite bit width, bit-vector operations, arrays, structures, unions, and pointers. We have integrated the CVC3, Boolector, and Z3 solvers with the CBMC front-end and evaluated them using both standard software model checking benchmarks and typical embedded software applications from telecommunications, control systems, and medical devices. The experiments show that our ESBMC model checker can analyze larger problems than existing tools and substantially reduce the verification time.",0098-5589;00985589,,10.1109/TSE.2011.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928354,Software engineering;formal methods;model checking;verification,Electronic mail;Embedded software;Encoding;Optimization;Safety;Space exploration,embedded systems;formal verification,SMT based bounded model checking;SMT solvers;embedded ANSI-C software;embedded software verification;model checkers;software model checking benchmarks,,35,,64,,,20110623,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,17
A Semi-Automatic Approach for Extracting Software Product Lines,M. T. Valente; V. Borges; L. Passos,"University of Minas Gerais, Belo Horizonte",IEEE Transactions on Software Engineering,20120726,2012,38,4,737,754,"The extraction of nontrivial software product lines (SPL) from a legacy application is a time-consuming task. First, developers must identify the components responsible for the implementation of each program feature. Next, they must locate the lines of code that reference the components discovered in the previous step. Finally, they must extract those lines to independent modules or annotate them in some way. To speed up product line extraction, this paper describes a semi-automatic approach to annotate the code of optional features in SPLs. The proposed approach is based on an existing tool for product line development, called CIDE, that enhances standard IDEs with the ability to associate background colors with the lines of code that implement a feature. We have evaluated and successfully applied our approach to the extraction of optional features from three nontrivial systems: Prevayler (an in-memory database system), JFreeChart (a chart library), and ArgoUML (a UML modeling tool).",0098-5589;00985589,,10.1109/TSE.2011.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928352,Software product lines;annotations;refactoring tools;virtual separation of concerns,Color;Context;Feature extraction;Image color analysis;Multithreading;Semantics;Software,Unified Modeling Language;feature extraction;product development;software maintenance;software reusability,ArgoUML nontrivial systems;CIDE;JFreeChart nontrivial systems;Prevayler nontrivial systems;SPL;background colors;code lines localization;legacy application;optional feature code annotation;optional feature extraction;product line development;program feature;semiautomatic approach;software product lines extraction,,9,,46,,,20110623,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,17
DEC: Service Demand Estimation with Confidence,A. Kalbasi; D. Krishnamurthy; J. Rolia; S. Dawson,"University of Calgary, Calgary",IEEE Transactions on Software Engineering,20120529,2012,38,3,561,578,"We present a new technique for predicting the resource demand requirements of services implemented by multitier systems. Accurate demand estimates are essential to ensure the efficient provisioning of services in an increasingly service-oriented world. The demand estimation technique proposed in this paper has several advantages compared with regression-based demand estimation techniques, which many practitioners employ today. In contrast to regression, it does not suffer from the problem of multicollinearity, it provides more reliable aggregate resource demand and confidence interval predictions, and it offers a measurement-based validation test. The technique can be used to support system sizing and capacity planning exercises, costing and pricing exercises, and to predict the impact of changes to a service upon different service customers.",0098-5589;00985589,,10.1109/TSE.2011.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728829,Benchmarking;resource demand prediction;statistical regression.,Benchmark testing;Computers;Equations;Estimation;Frequency modulation;Mathematical model;Software,multiprocessing systems;regression analysis;service-oriented architecture,DEC;capacity planning;multicollinearity;multitier systems;regression-based demand estimation techniques;resource demand requirements;service demand estimation technique;service-oriented world;system sizing,,18,,33,,,20110310,May-June 2012,,IEEE,IEEE Journals & Magazines,,17
A Model of Data Warehousing Process Maturity,A. Sen; K. Ramamurthy; A. P. Sinha,"Texas A&M University, College Station",IEEE Transactions on Software Engineering,20120403,2012,38,2,336,353,"Even though data warehousing (DW) requires huge investments, the data warehouse market is experiencing incredible growth. However, a large number of DW initiatives end up as failures. In this paper, we argue that the maturity of a data warehousing process (DWP) could significantly mitigate such large-scale failures and ensure the delivery of consistent, high quality, _ÑÒsingle-version of truth_Ñù data in a timely manner. However, unlike software development, the assessment of DWP maturity has not yet been tackled in a systematic way. In light of the critical importance of data as a corporate resource, we believe that the need for a maturity model for DWP could not be greater. In this paper, we describe the design and development of a five-level DWP maturity model (DWP-M) over a period of three years. A unique aspect of this model is that it covers processes in both data warehouse development and operations. Over 20 key DW executives from 13 different corporations were involved in the model development process. The final model was evaluated by a panel of experts; the results strongly validate the functionality, productivity, and usability of the model. We present the initial and final DWP-M model versions, along with illustrations of several key process areas at different levels of maturity.",0098-5589;00985589,,10.1109/TSE.2011.2,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680911,Data warehousing process;design-science research;model validation;software maturity models.,Business;Data mining;Data warehouses;Programming;Software;Standards organizations;Warehousing,data warehouses,DWP-M model;data warehouse market;data warehousing process maturity;large-scale failures,,12,,75,,,20110106,March-April 2012,,IEEE,IEEE Journals & Magazines,,17
Software Development Estimation Biases: The Role of Interdependence,M. Jorgensen; S. Grimstad,"University of Oslo, Lysaker",IEEE Transactions on Software Engineering,20120529,2012,38,3,677,693,"Software development effort estimates are frequently too low, which may lead to poor project plans and project failures. One reason for this bias seems to be that the effort estimates produced by software developers are affected by information that has no relevance for the actual use of effort. We attempted to acquire a better understanding of the underlying mechanisms and the robustness of this type of estimation bias. For this purpose, we hired 374 software developers working in outsourcing companies to participate in a set of three experiments. The experiments examined the connection between estimation bias and developer dimensions: self-construal (how one sees oneself), thinking style, nationality, experience, skill, education, sex, and organizational role. We found that estimation bias was present along most of the studied dimensions. The most interesting finding may be that the estimation bias increased significantly with higher levels of interdependence, i.e., with stronger emphasis on connectedness, social context, and relationships. We propose that this connection may be enabled by an activation of one's self-construal when engaging in effort estimation, and a connection between a more interdependent self-construal and increased search for indirect messages, lower ability to ignore irrelevant context, and a stronger emphasis on socially desirable responses.",0098-5589;00985589,,10.1109/TSE.2011.40,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6193066,Effort estimation;cultural differences;estimation bias;software engineering.,Companies;Context;Estimation;Instruments;Outsourcing;Programming;Software,estimation theory;software management,interdependence role;outsourcing companies;project failures;project plans;software development estimation,,4,,48,,,20120501,May-June 2012,,IEEE,IEEE Journals & Magazines,,16
"Reasoning about the Reliability of Diverse Two-Channel Systems in Which One Channel Is ""Possibly Perfect""",B. Littlewood; J. Rushby,"City University, London",IEEE Transactions on Software Engineering,20120924,2012,38,5,1178,1194,"This paper refines and extends an earlier one by the first author [1]. It considers the problem of reasoning about the reliability of fault-tolerant systems with two _ÑÒchannels_Ñù (i.e., components) of which one, A, because it is conventionally engineered and presumed to contain faults, supports only a claim of reliability, while the other, B, by virtue of extreme simplicity and extensive analysis, supports a plausible claim of _ÑÒperfection._Ñù We begin with the case where either channel can bring the system to a safe state. The reasoning about system probability of failure on demand (pfd) is divided into two steps. The first concerns aleatory uncertainty about 1) whether channel A will fail on a randomly selected demand and 2) whether channel B is imperfect. It is shown that, conditional upon knowing p<sub>A</sub> (the probability that A fails on a randomly selected demand) and p<sub>B</sub> (the probability that channel B is imperfect), a conservative bound on the probability that the system fails on a randomly selected demand is simply p<sub>A</sub> X p<sub>B</sub>. That is, there is conditional independence between the events _ÑÒA fails_Ñù and _ÑÒB is imperfect._Ñù The second step of the reasoning involves epistemic uncertainty, represented by assessors' beliefs about the distribution of (p<sub>A</sub>, p<sub>B</sub>), and it is here that dependence may arise. However, we show that under quite plausible assumptions, a conservative bound on system pfd can be constructed from point estimates for just three parameters. We discuss the feasibility of establishing credible estimates for these parameters. We extend our analysis from faults of omission to those of commission, and then combine these to yield an analysis for monitored architectures of a kind proposed for aircraft.",0098-5589;00985589,,10.1109/TSE.2011.80,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975177,Software reliability;assurance case;program correctness;software diversity;software fault tolerance,Cognition;Phase frequency detector;Safety;Software;Software reliability;Uncertainty,aircraft;probability;reasoning about programs;software fault tolerance;uncertainty handling,PFD;aircraft;aleatory uncertainty;assessors belief;conditional independence;diverse two-channel system;epistemic uncertainty;fault tolerant system;probability of failure on demand;randomly selected demand;reasoning about the reliability,,8,,47,,,20110804,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,16
Tools for the Rapid Prototyping of Provably Correct Ambient Intelligence Applications,A. Coronato; G. De Pietro,"National Research Council (CNR), Naples",IEEE Transactions on Software Engineering,20120726,2012,38,4,975,991,"Ambient Intelligence technologies have not yet been widely adopted in safety critical scenarios. This principally has been due to fact that acceptable degrees of dependability have not been reached for the applications that rely on such technologies. However, the new critical application domains, like Ambient Assisted Living and Smart Hospitals, which are currently emerging, are increasing the need for methodologies and tools that can improve the reliability of the final systems. This paper presents a middleware architecture for safety critical Ambient Intelligence applications which provides the developer with services for runtime verification. It is now possible to continuously monitor and check the running system against correctness properties defined at design time. Moreover, a visual tool which allows the formal design of several of the characteristics of an Ambient Intelligence application and the automatic generation of setting up parameters and code for the middleware infrastructure is also presented.",0098-5589;00985589,,10.1109/TSE.2011.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963693,Safety critical ambient intelligence systems;designing tools;middleware infrastructures,Ambient intelligence;Biomembranes;Calculus;Middleware;Mobile communication;Monitoring;Runtime,middleware;safety-critical software;software prototyping;ubiquitous computing,ambient assisted living;ambient intelligence applications;automatic generation;formal design;middleware architecture;new critical application domains;pervasive computing;rapid prototyping;runtime verification;safety critical scenarios;smart hospitals;visual tool,,12,,48,,,20110728,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,16
Finding Atomicity-Violation Bugs through Unserializable Interleaving Testing,S. Lu; S. Park; Y. Zhou,"University of Wisconsin-Madison, Madison",IEEE Transactions on Software Engineering,20120726,2012,38,4,844,860,"Multicore hardware is making concurrent programs pervasive. Unfortunately, concurrent programs are prone to bugs. Among different types of concurrency bugs, atomicity violations are common and important. How to test the interleaving space and expose atomicity-violation bugs is an open problem. This paper makes three contributions. First, it designs and evaluates a hierarchy of four interleaving coverage criteria using 105 real-world concurrency bugs. This study finds a coverage criterion (Unserializable Interleaving Coverage) that balances the complexity and the capability of exposing atomicity-violation bugs well. Second, it studies stress testing to understand why this common practice cannot effectively expose atomicity-violation bugs from the perspective of unserializable interleaving coverage. Third, it designs CTrigger following the unserializable interleaving coverage criterion. CTrigger uses trace analysis to identify feasible unserializable interleavings, and then exercises low-probability interleavings to expose atomicity-violation bugs. We evaluate CTrigger with real-world atomicity-violation bugs from seven applications. CTrigger efficiently exposes these bugs within 1-235 seconds, two to four orders of magnitude faster than stress testing. Without CTrigger, some of these bugs do not manifest even after seven days of stress testing. Furthermore, once a bug is exposed, CTrigger can reliably reproduce it, usually within 5 seconds, for diagnosis.",0098-5589;00985589,,10.1109/TSE.2011.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740930,Testing and debugging;bug characteristics;concurrent programming;debugging aids;diagnostics;test coverage of code;testing strategies,Complexity theory;Computer bugs;Concurrent computing;Instruction sets;Stress;Synchronization;Testing,formal verification;multiprocessing systems;probability;program debugging;ubiquitous computing,concurrency bugs;finding atomicity violation bugs;low probability interleavings;multicore hardware;pervasive concurrent programs;trace analysis;unserializable interleaving coverage;unserializable interleaving testing,,10,,48,,,20110405,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,16
An Autonomous Engine for Services Configuration and Deployment,F. Cuadrado; J. C. Duenas; R. Garcia-Carmona,"Queen Mary University of London, London",IEEE Transactions on Software Engineering,20120529,2012,38,3,520,536,"The runtime management of the infrastructure providing service-based systems is a complex task, up to the point where manual operation struggles to be cost effective. As the functionality is provided by a set of dynamically composed distributed services, in order to achieve a management objective multiple operations have to be applied over the distributed elements of the managed infrastructure. Moreover, the manager must cope with the highly heterogeneous characteristics and management interfaces of the runtime resources. With this in mind, this paper proposes to support the configuration and deployment of services with an automated closed control loop. The automation is enabled by the definition of a generic information model, which captures all the information relevant to the management of the services with the same abstractions, describing the runtime elements, service dependencies, and business objectives. On top of that, a technique based on satisfiability is described which automatically diagnoses the state of the managed environment and obtains the required changes for correcting it (e.g., installation, service binding, update, or configuration). The results from a set of case studies extracted from the banking domain are provided to validate the feasibility of this proposal.",0098-5589;00985589,,10.1109/TSE.2011.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728830,Autonomic systems;model-based management;satisfiability;service configuration.,Business;Containers;Context;Engines;Runtime;Servers;Web services,computability;fault tolerant computing;service-oriented architecture,automated closed control loop;autonomous engine;banking domain;business objectives;distributed elements;dynamically composed distributed services;generic information model;runtime elements;runtime management;runtime resources;satisfiability;service dependencies;service-based systems;services configuration,,7,,38,,,20110310,May-June 2012,,IEEE,IEEE Journals & Magazines,,16
Model Checking Semantically Annotated Services,I. Di Pietro; F. Pagliarecci; L. Spalazzi,"Universit&#x0E0; Politecnica delle Marche, Ancona",IEEE Transactions on Software Engineering,20120529,2012,38,3,592,608,"Model checking is a formal verification method widely accepted in the web service world because of its capability to reason about service behavior at process level. It has been used as a basic tool in several scenarios such as service selection, service validation, and service composition. The importance of semantics is also widely recognized. Indeed, there are several solutions to the problem of providing semantics to web services, most of them relying on some form of Description Logic. This paper presents an integration of model checking and semantic reasoning technologies in an efficient way. This can be considered the first step toward the use of semantic model checking in problems of selection, validation, and composition. The approach relies on a representation of services at process level that is based on semantically annotated state transition systems (asts) and a representation of specifications based on a semantically annotated version of computation tree logic (anctl). This paper proves that the semantic model checking algorithm is sound and complete and can be accomplished in polynomial time. This approach has been evaluated with several experiments.",0098-5589;00985589,,10.1109/TSE.2011.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680919,Formal methods;description logic;intelligent web services;model checking;semantic web;temporal logic;web services.,Biological system modeling;Computational modeling;Ontologies;Semantics;Switches;Syntactics;Web services,Web services;computational complexity;formal verification;semantic Web;temporal logic;trees (mathematics),Web service;annotated state transition systems;computation tree logic;description logic;formal verification method;polynomial time;semantic model checking;semantic reasoning technologies;semantically annotated services;temporal logic,,10,,63,,,20110106,May-June 2012,,IEEE,IEEE Journals & Magazines,,16
Aspectizing Java Access Control,R. Toledo; A. Nunez; E. Tanter; J. Noye,"University of Chile, Santiago",IEEE Transactions on Software Engineering,20120130,2012,38,1,101,117,"It is inevitable that some concerns crosscut a sizeable application, resulting in code scattering and tangling. This issue is particularly severe for security-related concerns: It is difficult to be confident about the security of an application when the implementation of its security-related concerns is scattered all over the code and tangled with other concerns, making global reasoning about security precarious. In this study, we consider the case of access control in Java, which turns out to be a crosscutting concern with a nonmodular implementation based on runtime stack inspection. We describe the process of modularizing access control in Java by means of Aspect-Oriented Programming (AOP). We first show a solution based on AspectJ, the most popular aspect-oriented extension to Java, that must rely on a separate automata infrastructure. We then put forward a novel solution via dynamic deployment of aspects and scoping strategies. Both solutions, apart from providing a modular specification of access control, make it possible to easily express other useful policies such as the Chinese wall policy. However, relying on expressive scope control results in a compact implementation, which, at the same time, permits the straightforward expression of even more interesting policies. These new modular implementations allowed by AOP alleviate maintenance and evolution issues produced by the crosscutting nature of access control.",0098-5589;00985589,,10.1109/TSE.2011.6,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680915,Programming languages;access control.;aspect-oriented programming;security,Access control;Aspect-oriented programming;Computer architecture;Computer security;Java;Programming,Java;aspect-oriented programming;authorisation;automata theory,AspectJ;Chinese wall policy;Java access control aspectization;aspect-oriented programming;aspects strategies;automata infrastructure;code scattering;code tangling;runtime stack inspection;scoping strategies;security-related concerns,,5,,45,,,20110106,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,16
Domain-Specific Service Selection for Composite Services,O. Moser; F. Rosenberg; S. Dustdar,"Vienna University of Technology, Vienna",IEEE Transactions on Software Engineering,20120726,2012,38,4,828,843,"We propose a domain-specific service selection mechanism and system implementation to address the issue of runtime adaptation of composite services that implement mission-critical business processes. To this end, we leverage quality of service (QoS) as a means to specify rigid dependability requirements. QoS does not include only common attributes such as availability or response time but also attributes specific to certain business domains and processes. Therefore, we combine both domain-agnostic and domain-specific QoS attributes in an adaptive QoS model. For specifying the service selection strategy, we propose a domain-specific language called VieDASSL to specify so-called selectors. This language can be used to specify selector implementations based on the available QoS attributes. Both the QoS model implementation and the selectors can be adapted at runtime to deal with changing business and QoS requirements. Our approach is implemented on top of an existing WS-BPEL engine. We demonstrate its feasibility by implementing a case study from the telecommunication domain.",0098-5589;00985589,,10.1109/TSE.2011.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231591,Service composition;domain specific languages;monitoring;quality of service;service selection,Adaptation models;Availability;Business;Engines;Quality of service;Runtime;Time factors,Web services;business data processing;quality of service;reliability;specification languages,QoS requirements;VieDASSL;WS-BPEL engine;Web services;adaptive QoS model;business requirements;composite services;domain-agnostic QoS attributes;domain-specific QoS attributes;domain-specific language;domain-specific service selection mechanism;mission-critical business processes;quality of service;runtime adaptation;selectors;telecommunication,,15,,56,,,20120703,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,15
"DESSERT: a DividE-and-conquer methodology for identifying categorieS, choiceS, and choicE Relations for Test case generation",T. Y. Chen; P. L. Poon; S. F. Tang; T. H. Tse,"Swinburne University of Technology, Melbourne",IEEE Transactions on Software Engineering,20120726,2012,38,4,794,809,"This paper extends the choce relation framework, abbreviated as choc'late, which assists software testers in the application of category/choice methods to testing. choc'late assumes that the tester is able to construct a single choice relation table from the entire specification; this table then forms the basis for test case generation using the associated algorithms. This assumption, however, may not hold true when the specification is complex and contains many specification components. For such a specification, the tester may construct a preliminary choice relation table from each specification component, and then consolidate all the preliminary tables into a final table to be processed by choc'late for test case generation. However, it is often difficult to merge these preliminary tables because such merging may give rise to inconsistencies among choice relations or overlaps among choices. To alleviate this problem, we introduce a DividE-and-conquer methodology for identifying categorieS, choiceS, and choicE Relations for Test case generation, abbreviated as dessert. The theoretical framework and the associated algorithms are discussed. To demonstrate the viability and effectiveness of our methodology, we describe case studies using the specifications of three real-life commercial software systems.",0098-5589;00985589,,10.1109/TSE.2011.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963695,Black-box testing;category-partition method;choice relation framework;choice relation table;software testing;test case generation,Awards activities;Electronic mail;Encoding;Software systems;Software testing,divide and conquer methods;formal specification;program testing,CHOC'LATE;DESSERT;black-box testing;category identification;choice identification;choice relation identification;divide-and-conquer methodology;real-life commercial software systems;software testing;specification components;test case generation,,9,,18,,,20110728,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,15
BURN: Enabling Workload Burstiness in Customized Service Benchmarks,G. Casale; A. Kalbasi; D. Krishnamurthy; J. Rolia,"Imperial College London, London",IEEE Transactions on Software Engineering,20120726,2012,38,4,778,793,"We introduce BURN, a methodology to create customized benchmarks for testing multitier applications under time-varying resource usage conditions. Starting from a set of preexisting test workloads, BURN finds a policy that interleaves their execution to stress the multitier application and generate controlled burstiness in resource consumption. This is useful to study, in a controlled way, the robustness of software services to sudden changes in the workload characteristics and in the usage levels of the resources. The problem is tackled by a model-based technique which first generates Markov models to describe resource consumption patterns of each test workload. Then, a policy is generated using an optimization program which sets as constraints a target request mix and user-specified levels of burstiness at the different resources in the system. Burstiness is quantified using a novel metric called overdemand, which describes in a natural way the tendency of a workload to keep a resource congested for long periods of time and across multiple requests. A case study based on a three-tier application testbed shows that our method is able to control and predict burstiness for session service demands at a fine-grained scale. Furthermore, experiments demonstrate that for any given request mix our approach can expose latency and throughput degradations not found with nonbursty workloads having the same request mix.",0098-5589;00985589,,10.1109/TSE.2011.58,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928353,Benchmarking;bottleneck migration;burstiness;overdemand;performance,Aggregates;Analytical models;Benchmark testing;Computational modeling;Linear regression;Markov processes;Servers,Markov processes;benchmark testing,BURN;Markov models;controlled burstiness;customized benchmarks;customized service benchmarks;fine-grained scale;latency;model-based technique;multitier application;nonbursty workloads;optimization program;resource consumption pattern;session service demands;software services;target request mix;three-tier application testbed;throughput degradation;time-varying resource usage condition;user-specified levels;workload burstiness,,6,,38,,,20110623,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,15
Pointcut Rejuvenation: Recovering Pointcut Expressions in Evolving Aspect-Oriented Software,R. Khatchadourian; P. Greenwood; A. Rashid; G. Xu,"Ohio State University, Columbus",IEEE Transactions on Software Engineering,20120529,2012,38,3,642,657,"Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base code can lead to join points incorrectly falling in or out of the scope of pointcuts. In this paper, we present an automated approach that limits fragility problems by providing mechanical assistance in pointcut maintenance. The approach is based on harnessing arbitrarily deep structural commonalities between program elements corresponding to join points selected by a pointcut. The extracted patterns are then applied to later versions to offer suggestions of new join points that may require inclusion. To illustrate that the motivation behind our proposal is well founded, we first empirically establish that join points captured by a single pointcut typically portray a significant amount of unique structural commonality by analyzing patterns extracted from 23 AspectJ programs. Then, we demonstrate the usefulness of our technique by rejuvenating pointcuts in multiple versions of three of these programs. The results show that our parameterized heuristic algorithm was able to accurately and automatically infer the majority of new join points in subsequent software versions that were not captured by the original pointcuts.",0098-5589;00985589,,10.1109/TSE.2011.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710951,Software development environments;software maintenance;software tools.,Fuels;Observers;Programming;Proposals;Robustness;Software;Software engineering,aspect-oriented programming,AspectJ programs;aspect-oriented programming;aspect-oriented software;deep structural commonalities harnessing;join points;parameterized heuristic algorithm;pattern analysis;pointcut expression recovery;pointcut fragility;pointcut maintenance;pointcut rejuvenation;program elements,,2,,43,,,20110210,May-June 2012,,IEEE,IEEE Journals & Magazines,,15
Schedule of Bad Smell Detection and Resolution: A New Way to Save Effort,H. Liu; Z. Ma; W. Shao; Z. Niu,"Beijing Institute of Technology and Ministry of Education, Beijing",IEEE Transactions on Software Engineering,20120130,2012,38,1,220,235,"Bad smells are signs of potential problems in code. Detecting and resolving bad smells, however, remain time-consuming for software engineers despite proposals on bad smell detection and refactoring tools. Numerous bad smells have been recognized, yet the sequences in which the detection and resolution of different kinds of bad smells are performed are rarely discussed because software engineers do not know how to optimize sequences or determine the benefits of an optimal sequence. To this end, we propose a detection and resolution sequence for different kinds of bad smells to simplify their detection and resolution. We highlight the necessity of managing bad smell resolution sequences with a motivating example, and recommend a suitable sequence for commonly occurring bad smells. We evaluate this recommendation on two nontrivial open source applications, and the evaluation results suggest that a significant reduction in effort ranging from 17.64 to 20 percent can be achieved when bad smells are detected and resolved using the proposed sequence.",0098-5589;00985589,,10.1109/TSE.2011.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680918,Scheme;bad smell;detection;effort;schedule.;software refactoring,Distance measurement;Feature extraction;Refactoring;Scheduling,scheduling;software engineering,bad smell detection;bad smell resolution;detection sequence;open source applications;optimal sequence;resolution sequence;software engineers,,29,,54,,,20110106,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,15
Measuring Code Quality to Improve Specification Mining,C. Le Goues; W. Weimer,"University of Virginia, Charlottesville",IEEE Transactions on Software Engineering,20120130,2012,38,1,175,190,"Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few false positives in practice (63 percent when balancing precision and recall, 3 percent when focused on precision), while still finding useful specifications (e.g., those that find many bugs) on over 1.5 million lines of code.",0098-5589;00985589,,10.1109/TSE.2011.5,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680914,Specification mining;code metrics;machine learning;program understanding.;software engineering,Cloning;Data mining;Maintenance engineering;Optimization;Refactoring;Software measurement,data mining;formal specification;program debugging;program testing;software quality,automatic mining techniques;code quality metrics;debugging;documentation;formal specifications;optimization;program testing;refactoring;repair;software engineering process;specification mining;temporal-property miner,,10,,61,,,20110106,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,15
Work Item Tagging: Communicating Concerns in Collaborative Software Development,C. Treude; M. A. Storey,"University of Victoria, Victoria",IEEE Transactions on Software Engineering,20120130,2012,38,1,19,34,"In collaborative software development projects, work items are used as a mechanism to coordinate tasks and track shared development work. In this paper, we explore how _ÑÒtagging,_Ñù a lightweight social computing mechanism, is used to communicate matters of concern in the management of development tasks. We present the results from two empirical studies over 36 and 12 months, respectively, on how tagging has been adopted and what role it plays in the development processes of several professional development projects with more than 1,000 developers in total. Our research shows that the tagging mechanism was eagerly adopted by the teams, and that it has become a significant part of many informal processes. Different kinds of tags are used by various stakeholders to categorize and organize work items. The tags are used to support finding of tasks, articulation work, and information exchange. Implicit and explicit mechanisms have evolved to manage the tag vocabulary. Our findings indicate that lightweight informal tool support, prevalent in the social computing domain, may play an important role in improving team-based software development practices.",0098-5589;00985589,,10.1109/TSE.2010.91,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611552,Tagging;articulation work;collaboration;software development;task management;work items.,Collaboration;Data mining;Programming;Software engineering;Tagging,groupware;information retrieval;software development management,collaborative software development project;lightweight informal tool support;social computing mechanism;tag vocabulary;team-based software development practice;work item tagging,,17,,44,,,20101028,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,15
Programmer-Friendly Refactoring Errors,E. Murphy-Hill; A. P. Black,"North Carolina State University, Raleigh",IEEE Transactions on Software Engineering,20121129,2012,38,6,1417,1431,"Refactoring tools, common to many integrated development environments, can help programmers to restructure their code. These tools sometimes refuse to restructure the programmer's code, instead giving the programmer a textual error message that she must decode if she wishes to understand the reason for the tool's refusal and what corrective action to take. This paper describes a graphical alternative to textual error messages called Refactoring Annotations. It reports on two experiments, one using an integrated development environment and the other using paper mockups, that show that programmers can use Refactoring Annotations to quickly and accurately understand the cause of refactoring errors.",0098-5589;00985589,,10.1109/TSE.2011.110,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072219,Refactoring;programmers;refactoring errors;tools;usability,Java;Programming;Prototypes;Taxonomy;Visualization,computer graphics;software maintenance,graphical alternative;integrated development environment;paper mockups;programmer-friendly refactoring errors;refactoring annotations;refactoring tools;textual error message;textual error messages,,3,,22,,,20111108,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,14
Automatically Generating Test Cases for Specification Mining,V. Dallmeier; N. Knopp; C. Mallon; G. Fraser; S. Hack; A. Zeller,"Universit&#x0E4;t des Saarlandes, Saarbr&#x0FC;cken",IEEE Transactions on Software Engineering,20120403,2012,38,2,243,257,"Dynamic specification mining observes program executions to infer models of normal program behavior. What makes us believe that we have seen sufficiently many executions? The TAUTOKO (_ÑÒTautoko_Ñù is the MaÍóori word for _ÑÒenhance, enrich._Ñù) typestate miner generates test cases that cover previously unobserved behavior, systematically extending the execution space, and enriching the specification. To our knowledge, this is the first combination of systematic test case generation and typestate mining-a combination with clear benefits: On a sample of 800 defects seeded into six Java subjects, a static typestate verifier fed with enriched models would report significantly more true positives and significantly fewer false positives than the initial models.",0098-5589;00985589,,10.1109/TSE.2011.105,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6044587,Specification mining;test case generation;typestate analysis.,Fault detection;Heuristic algorithms;Instruments;Java;Schedules;Software;Testing,Java;automatic test pattern generation;data mining;formal specification;program verification,Java;TAUTOKO;automatic test case generation;dynamic specification mining;normal program behavior;program executions;static typestate verifier;typestate mining,,25,,31,,,20111013,March-April 2012,,IEEE,IEEE Journals & Magazines,,14
Evaluating Dynamic Software Update Safety Using Systematic Testing,C. M. Hayden; E. K. Smith; E. A. Hardisty; M. Hicks; J. S. Foster,"University of Maryland, College Park, College Park",IEEE Transactions on Software Engineering,20121129,2012,38,6,1340,1354,"Dynamic software updating (DSU) systems patch programs on the fly without incurring downtime. To avoid failures due to the updating process itself, many DSU systems employ timing restrictions. However, timing restrictions are theoretically imperfect, and their practical effectiveness is an open question. This paper presents the first significant empirical evaluation of three popular timing restrictions: activeness safety (AS), which prevents updates to active functions, con-freeness safety (CFS), which only allows modifications to active functions when doing so is provably type-safe, and manual identification of the event-handling loops during which an update may occur. We evaluated these timing restrictions using a series of DSU patches to three programs: OpenSSH, vsftpd, and ngIRCd. We systematically applied updates at each distinct update point reached during execution of a suite of system tests for these programs to determine which updates pass and which fail. We found that all three timing restrictions prevented most failures, but only manual identification allowed none. Further, although CFS and AS allowed many more update points, manual identification still supported updates with minimal delay. Finally, we found that manual identification required the least developer effort. Overall, we conclude that manual identification is most effective.",0098-5589;00985589,,10.1109/TSE.2011.101,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035725,Dynamic software updating (DSU);hot-swapping;program tracing;software reliability;testing,Servers;Software reliability;Software testing,program testing;safety-critical software;software fault tolerance;software maintenance,AS;CFS;DSU systems;OpenSSH;active functions;activeness safety;con-freeness safety;dynamic software updating safety evaluation;event-handling loop identification;failure prevention;manual identification;ngIRCd;systematic testing;timing restrictions;vsftpd,,9,,35,,,20111006,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,14
Mutation-Driven Generation of Unit Tests and Oracles,G. Fraser; A. Zeller,"Saarland University, Saarbr&#x0FC;cken",IEEE Transactions on Software Engineering,20120403,2012,38,2,278,292,"To assess the quality of test suites, mutation analysis seeds artificial defects (mutations) into programs; a nondetected mutation indicates a weakness in the test suite. We present an automated approach to generate unit tests that detect these mutations for object-oriented classes. This has two advantages: First, the resulting test suite is optimized toward finding defects modeled by mutation operators rather than covering code. Second, the state change caused by mutations induces oracles that precisely detect the mutants. Evaluated on 10 open source libraries, our ï_test prototype generates test suites that find significantly more seeded defects than the original manually written test suites.",0098-5589;00985589,,10.1109/TSE.2011.93,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019060,Mutation analysis;assertions;search-based testing.;test case generation;test oracles;unit testing,Biological cells;Generators;Genetic algorithms;Libraries;Software;Software algorithms;Testing,automatic test pattern generation;object-oriented programming;optimisation;program testing,artificial defects;automated test case generation;mutation driven generation;mutation operators;object-oriented classes;open source libraries;optimization;oracle;test suites quality;unit test,,64,,52,,,20110915,March-April 2012,,IEEE,IEEE Journals & Magazines,,14
A Practical Approach to Size Estimation of Embedded Software Components,K. Lind; R. Heldal,"Saab Automobile AB, Trollh&#x00E4;ttan",IEEE Transactions on Software Engineering,20120924,2012,38,5,993,1007,"To estimate software code size early in the development process is important for developing cost-efficient embedded systems. We have applied the COSMIC Functional Size Measurement (FSM) method for size estimation of embedded software components in the automotive industry. Correlational studies were conducted using data from two automotive companies. The studies show strong correlation between functional size and software code size, which is important for obtaining accurate estimation results. This paper presents the characteristics and results of our work, and aims to provide a practical framework for how to use COSMIC FSM for size estimation purposes. We investigate the results from our earlier correlational studies, and conduct further studies to identify such a framework. Based on these activities, we conclude that a clear purpose of the estimation process, a well-defined domain allowing categorization of software, consistent content and quality of requirements, and historical data from implemented software are key factors for size estimation of embedded software components.",0098-5589;00985589,,10.1109/TSE.2011.86,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999672,COSMIC FSM;Real-time and embedded systems;software components;software product metrics,Automotive engineering;Estimation;Industries;Memory management;Size measurement;Software;Vehicles,automotive engineering;object-oriented programming;production engineering computing;software cost estimation;software metrics,COSMIC functional size measurement method;FSM method;automotive companies;automotive industry;consistent content;cost-efficient embedded systems;development process;domain allowing categorization;embedded software components;historical data;requirements quality;software code size estimation;software product metrics,,5,,48,,,20110825,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,14
Structural Complexity and Programmer Team Strategy: An Experimental Test,N. Ramasubbu; C. F. Kemerer; J. Hong,"University of Pittsburgh, Pittsburgh",IEEE Transactions on Software Engineering,20120924,2012,38,5,1054,1068,"This study develops and empirically tests the idea that the impact of structural complexity on perfective maintenance of object-oriented software is significantly determined by the team strategy of programmers (independent or collaborative). We analyzed two key dimensions of software structure, coupling and cohesion, with respect to the maintenance effort and the perceived ease-of-maintenance by pairs of programmers. Hypotheses based on the distributed cognition and task interdependence theoretical frameworks were tested using data collected from a controlled lab experiment employing professional programmers. The results show a significant interaction effect between coupling, cohesion, and programmer team strategy on both maintenance effort and perceived ease-of-maintenance. Highly cohesive and low-coupled programs required lower maintenance effort and were perceived to be easier to maintain than the low-cohesive programs and high-coupled programs. Further, our results would predict that managers who strategically allocate maintenance tasks to either independent or collaborative programming teams depending on the structural complexity of software could lower their team's maintenance effort by as much as 70 percent over managers who use simple uniform resource allocation policies. These results highlight the importance of achieving congruence between team strategies employed by collaborating programmers and the structural complexity of software.",0098-5589;00985589,,10.1109/TSE.2011.88,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999673,CK metrics;Object-oriented programming;complexity measures;maintenance process;programming teams;software management;software productivity;software quality,Collaboration;Complexity theory;Couplings;Maintenance engineering;Programming profession;Software,computational complexity;object-oriented programming;resource allocation;software maintenance,collaborative programming teams;controlled lab experiment;distributed cognition;high-coupled programs;low-cohesive programs;object-oriented software;perceived ease-of-maintenance;perfective maintenance;professional programmers;programmer team strategy;resource allocation policies;significant interaction;software structure;structural complexity;task interdependence,,6,,62,,,20110825,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,14
Mining Crosscutting Concerns through Random Walks,C. Zhang; H. A. Jacobsen,"The Hong Kong University of Science and Technology, Hong Kong",IEEE Transactions on Software Engineering,20120924,2012,38,5,1123,1137,"Inspired by our past manual aspect mining experiences, this paper describes a probabilistic random walk model to approximate the process of discovering crosscutting concerns (CCs) in the absence of the domain knowledge about the investigated application. The random walks are performed on the concept graphs extracted from the program sources to calculate metrics of _ÑÒutilization_Ñù and _ÑÒaggregation_Ñù for each of the program elements. We rank all the program elements based on these metrics and use a threshold to produce a set of candidates that represent crosscutting concerns. We implemented the algorithm as the Prism CC miner (PCM) and evaluated PCM on Java applications ranging from a small-scale drawing application to a medium-sized middleware application and to a large-scale enterprise application server. Our quantification shows that PCM is able to produce comparable results (95 percent accuracy for the top 125 candidates) with respect to the manual mining effort. PCM is also significantly more effective as compared to the conventional approach.",0098-5589;00985589,,10.1109/TSE.2011.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989837,Aspect mining;mining crosscutting concerns,Algorithm design and analysis;Computational modeling;Data mining;Manuals;Mathematical model;Phase change materials;Radiation detectors,Java;aspect-oriented programming;data mining;graph theory;middleware;probability;small-to-medium enterprises,Java applications;Prism CC miner;aggregation metric;aspect mining;concept graphs;crosscutting concerns mining;large-scale enterprise application server;medium-sized middleware application;probabilistic random walk model;program elements;program sources;small-scale drawing application;utilization metric,,5,,37,,,20110818,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,14
Scalable Differential Analysis of Process Algebra Models,M. Tribastone; S. Gilmore; J. Hillston,"The University of Edinburgh, Edinburgh",IEEE Transactions on Software Engineering,20120130,2012,38,1,205,219,"The exact performance analysis of large-scale software systems with discrete-state approaches is difficult because of the well-known problem of state-space explosion. This paper considers this problem with regard to the stochastic process algebra PEPA, presenting a deterministic approximation to the underlying Markov chain model based on ordinary differential equations. The accuracy of the approximation is assessed by means of a substantial case study of a distributed multithreaded application.",0098-5589;00985589,,10.1109/TSE.2010.82,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5567115,Markov processes.;Modeling and prediction;ordinary differential equations,Approximation methods;Computational modeling;Markov methods;Mathematical model;Numerical models;Semantics;Stochastic processes,Markov processes;differential equations;multi-threading;process algebra;software engineering,Markov chain model;PEPA;discrete-state approach;distributed multithreaded application;large-scale software systems;ordinary differential equations;scalable differential analysis;stochastic process algebra,,39,,26,,,20100909,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,14
Dealing with Burstiness in Multi-Tier Applications: Models and Their Parameterization,G. Casale; N. Mi; L. Cherkasova; E. Smirni,"Imperial College London, London",IEEE Transactions on Software Engineering,20120924,2012,38,5,1040,1053,"Workloads and resource usage patterns in enterprise applications often show burstiness resulting in large degradation of the perceived user performance. In this paper, we propose a methodology for detecting burstiness symptoms in multi-tier applications but, rather than identifying the root cause of burstiness, we incorporate this information into models for performance prediction. The modeling methodology is based on the index of dispersion of the service process at a server, which is inferred by observing the number of completions within the concatenated busy times of that server. The index of dispersion is used to derive a Markov-modulated process that captures burstiness and variability of the service process at each resource well and that allows us to define queueing network models for performance prediction. Experimental results and performance model predictions are in excellent agreement and argue for the effectiveness of the proposed methodology under both bursty and nonbursty workloads. Furthermore, we show that the methodology extends to modeling flash crowds that create burstiness in the stream of requests incoming to the application.",0098-5589;00985589,,10.1109/TSE.2011.87,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311395,Capacity planning;bottleneck switch;bursty workload;index of dispersion;multi-tier applications,Dispersion;Estimation;Indexes;Predictive models;Servers;Switches,Markov processes;client-server systems;queueing theory;resource allocation;software architecture,Markov-modulated process;burstiness root cause identification;burstiness symptom detection;enterprise applications;flash crowd model;multitier applications;nonbursty workloads;performance prediction model;queueing network models;resource usage patterns;server busy times;service process variability;user performance degradation;workload patterns,,18,,25,,,,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,13
"How We Refactor, and How We Know It",E. Murphy-Hill; C. Parnin; A. P. Black,"North Carolina State University, Raleigh",IEEE Transactions on Software Engineering,20120130,2012,38,1,5,18,"Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.",0098-5589;00985589,,10.1109/TSE.2011.41,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6112738,Refactoring;floss refactoring;refactoring tools;root-canal refactoring.,Java;Refactoring;Software tools,software maintenance;software tools,Eclipse developers;Mylyn developers;randomly sampled code;refactoring tools;research and development,,99,,22,,,20111227,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,13
On the Value of Ensemble Effort Estimation,E. Kocaguneli; T. Menzies; J. W. Keung,"West Virginia University, Morgantown",IEEE Transactions on Software Engineering,20121129,2012,38,6,1403,1416,"Background: Despite decades of research, there is no consensus on which software effort estimation methods produce the most accurate models. Aim: Prior work has reported that, given M estimation methods, no single method consistently outperforms all others. Perhaps rather than recommending one estimation method as best, it is wiser to generate estimates from ensembles of multiple estimation methods. Method: Nine learners were combined with 10 preprocessing options to generate 9 Õ‹ 10 = 90 solo methods. These were applied to 20 datasets and evaluated using seven error measures. This identified the best n (in our case n = 13) solo methods that showed stable performance across multiple datasets and error measures. The top 2, 4, 8, and 13 solo methods were then combined to generate 12 multimethods, which were then compared to the solo methods. Results: 1) The top 10 (out of 12) multimethods significantly outperformed all 90 solo methods. 2) The error rates of the multimethods were significantly less than the solo methods. 3) The ranking of the best multimethod was remarkably stable. Conclusion: While there is no best single effort estimation method, there exist best combinations of such effort estimation methods.",0098-5589;00985589,,10.1109/TSE.2011.111,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081882,Software cost estimation;analogy;ensemble;k-NN;machine learning;neural nets;regression trees;support vector machines,Costs;Machine learning;Measurement uncertainty;Neural networks;Regression tree analysis;Software performance;Support vector machines;Taxonomy,software development management,ensemble effort estimation;error measures;multiple estimation method;single method;software effort estimation,,55,,82,,,20111115,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,13
Analyzing the Effect of Gain Time on Soft-Task Scheduling Policies in Real-Time Systems,L. BÕ_rdalo; A. Terrasa; A. Espinosa; A. GarcÕ_a-Fornes,"Universitat Polit&#x00E8;ecnica de Val&#x00E8;ncia, Valencia",IEEE Transactions on Software Engineering,20121129,2012,38,6,1305,1318,"In hard real-time systems, gain time is defined as the difference between the Worst Case Execution Time (WCET) of a hard task and its actual processor consumption at runtime. This paper presents the results of an empirical study about how the presence of a significant amount of gain time in a hard real-time system questions the advantages of using the most representative scheduling algorithms or policies for aperiodic or soft tasks in fixed-priority preemptive systems. The work presented here refines and complements many other studies in this research area in which such policies have been introduced and compared. This work has been performed by using the authors' testing framework for soft scheduling policies, which produces actual, synthetic, randomly generated applications, executes them in an instrumented Real-Time Operating System (RTOS), and finally processes this information to obtain several statistical outcomes. The results show that, in general, the presence of a significant amount of gain time reduces the performance benefit of the scheduling policies under study when compared to serving the soft tasks in background, which is considered the theoretical worst case. In some cases, this performance benefit is so small that the use of a specific scheduling policy for soft tasks is questionable.",0098-5589;00985589,,10.1109/TSE.2011.95,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025357,RT-Linux;Real-time systems;scheduling policies,Decision support systems;Generators;Heuristic algorithms;Real time systems;Scheduling;Servers;Time factors,operating systems (computers);real-time systems;scheduling,RTOS;WCET;aperiodic tasks;author testing framework;fixed-priority preemptive systems;gain time;hard real-time systems;instrumented real-time operating system;processor consumption;representative scheduling algorithms;soft-task scheduling policies;statistical outcomes;worst case execution time,,1,,31,,,20110923,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,13
Fluid Rewards for a Stochastic Process Algebra,M. Tribastone; J. Ding; S. Gilmore; J. Hillston,"Ludwig-Maximilians-Universit&#x00E4;t, M&#x00FC;nchen",IEEE Transactions on Software Engineering,20120726,2012,38,4,861,874,"Reasoning about the performance of models of software systems typically entails the derivation of metrics such as throughput, utilization, and response time. If the model is a Markov chain, these are expressed as real functions of the chain, called reward models. The computational complexity of reward-based metrics is of the same order as the solution of the Markov chain, making the analysis infeasible when evaluating large-scale systems. In the context of the stochastic process algebra PEPA, the underlying continuous-time Markov chain has been shown to admit a deterministic (fluid) approximation as a solution of an ordinary differential equation, which effectively circumvents state-space explosion. This paper is concerned with approximating Markovian reward models for PEPA with fluid rewards, i.e., functions of the solution of the differential equation problem. It shows that (1) the Markovian reward models for typical metrics of performance enjoy asymptotic convergence to their fluid analogues, and that (2) via numerical tests, the approximation yields satisfactory accuracy in practice.",0098-5589;00985589,,10.1109/TSE.2011.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975178,Markov processes;Modeling and prediction;ordinary differential equations,Approximation methods;Computational modeling;Convergence;Markov processes;Mathematical model;Servers,Markov processes;computational complexity;mathematics computing;process algebra;stochastic processes,Markov chain;computational complexity;fluid rewards;metric derivation;ordinary differential equation;software systems;state-space explosion;stochastic process algebra,,13,,33,,,20110804,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,13
Pert: The Application-Aware Tailoring of Java Object Persistence,P. Liu; C. Zhang,"Hong Kong University of Science and Technology, Hong Kong",IEEE Transactions on Software Engineering,20120726,2012,38,4,909,922,"Persistence is a widely used technique which allows the objects that represent the results of lengthy computations to outlive the process that creates it in order to considerably speed up subsequent program executions. We observe that conventional persistence techniques usually do not consider the application contexts of the persistence operations, where not all of the object states need to be persisted. Leveraging this observation, we have designed and implemented a framework called Pert, which first performs static program analysis to estimate the actual usage of the persisted object, given the context of its usage in the program. The Pert runtime uses the statically computed information to efficiently make tailoring decisions to prune the redundant and unused object states during the persistence operations. Our evaluation result shows that the Pert-based optimization can speed up the conventional persistence operations by 1 to 45 times. The amount of persisted data is also dramatically reduced, as the result of the application-aware tailoring.",0098-5589;00985589,,10.1109/TSE.2011.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963692,Object persistence;performance optimization;program analysis,Algorithm design and analysis;Anodes;Context;Java;Libraries;Optimization;Runtime,Java;optimisation,Java object persistence;Pert based optimization;application aware tailoring;lengthy computations;persistence techniques;static program analysis;subsequent program executions,,0,,20,,,20110728,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,13
Mutable Protection Domains: Adapting System Fault Isolation for Reliability and Efficiency,G. Parmer; R. West,"The George Washington University, Washtington, DC",IEEE Transactions on Software Engineering,20120726,2012,38,4,875,888,"As software systems are becoming increasingly complex, the likelihood of faults and unexpected behaviors will naturally increase. Today, mobile devices to large-scale servers feature many millions of lines of code. Compile-time checks and offline verification methods are unlikely to capture all system states and control flow interactions of a running system. For this reason, many researchers have developed methods to contain faults at runtime by using software and hardware-based techniques to define protection domains. However, these approaches tend to impose isolation boundaries on software components that are static, and thus remain intact while the system is running. An unfortunate consequence of statically structured protection domains is that they may impose undue overhead on the communication between separate components. This paper proposes a new runtime technique that trades communication cost for fault isolation. We describe Mutable Protection Domains (MPDs) in the context of our Composite operating system. MPD dynamically adapts hardware isolation between interacting software components, depending on observed communication _ÑÒhot-paths,_Ñù with the purpose of maximizing fault isolation where possible. In this sense, MPD naturally tends toward a system of maximal component isolation, while collapsing protection domains where costs are prohibitive. By increasing isolation for low-cost interacting components, MPD limits the scope of impact of future unexpected faults. We demonstrate the utility of MPD using a webserver, and identify different hot-paths for different workloads that dictate adaptations to system structure. Experiments show up to 40 percent improvement in throughput compared to a statically organized system, while maintaining high-fault isolation.",0098-5589;00985589,,10.1109/TSE.2011.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928356,Component-based;fault isolation;operating systems;performance;reliability,Hardware;Kernel;Reliability;Servers;Switches,fault tolerant computing;mobile computing;object-oriented programming;operating systems (computers),MPD;compile-time checks;composite operating system;control flow interactions;fault isolation;hardware-based techniques;large-scale servers;maximal component isolation;mobile computing;mobile devices;mutable protection domains;offline verification methods;software -based techniques;software components;software systems;system fault isolation,,3,,40,,,20110623,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,13
Comparing the Defect Reduction Benefits of Code Inspection and Test-Driven Development,J. W. Wilkerson; J. F. Nunamaker; R. Mercer,"Pennsylvania State University, Erie, Erie",IEEE Transactions on Software Engineering,20120529,2012,38,3,547,560,"This study is a quasi experiment comparing the software defect rates and implementation costs of two methods of software defect reduction: code inspection and test-driven development. We divided participants, consisting of junior and senior computer science students at a large Southwestern university, into four groups using a two-by-two, between-subjects, factorial design and asked them to complete the same programming assignment using either test-driven development, code inspection, both, or neither. We compared resulting defect counts and implementation costs across groups. We found that code inspection is more effective than test-driven development at reducing defects, but that code inspection is also more expensive. We also found that test-driven development was no more effective at reducing defects than traditional programming methods.",0098-5589;00985589,,10.1109/TSE.2011.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750007,Agile programming;code inspections and walk throughs;empirical study.;reliability;test-driven development;testing strategies,Inspection;Java;Programming profession;Software;Testing;Writing,program testing;system recovery,code inspection;defect reduction benefits;junior computer science students;programming assignment;quasi experiment;senior computer science students;software defect rates;software defect reduction;test driven development,,6,,47,,,20110415,May-June 2012,,IEEE,IEEE Journals & Magazines,,13
Exploiting the Essential Assumptions of Analogy-Based Effort Estimation,E. Kocaguneli; T. Menzies; A. Bener; J. W. Keung,"West Virginia University, Morgantown",IEEE Transactions on Software Engineering,20120403,2012,38,2,425,438,"Background: There are too many design options for software effort estimators. How can we best explore them all? Aim: We seek aspects on general principles of effort estimation that can guide the design of effort estimators. Method: We identified the essential assumption of analogy-based effort estimation, i.e., the immediate neighbors of a project offer stable conclusions about that project. We test that assumption by generating a binary tree of clusters of effort data and comparing the variance of supertrees versus smaller subtrees. Results: For 10 data sets (from Coc81, Nasa93, Desharnais, Albrecht, ISBSG, and data from Turkish companies), we found: 1) The estimation variance of cluster subtrees is usually larger than that of cluster supertrees; 2) if analogy is restricted to the cluster trees with lower variance, then effort estimates have a significantly lower error (measured using MRE, AR, and Pred(25) with a Wilcoxon test, 95 percent confidence, compared to nearest neighbor methods that use neighborhoods of a fixed size). Conclusion: Estimation by analogy can be significantly improved by a dynamic selection of nearest neighbors, using only the project data from regions with small variance.",0098-5589;00985589,,10.1109/TSE.2011.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728833,Software cost estimation;analogy;k-NN.,Estimation;Euclidean distance;Humans;Linear regression;Software;Training;Training data,pattern clustering;program testing;project management;software cost estimation;trees (mathematics),Albrecht data set;Coc81 data set;Desharnais data set;ISBSG data set;Nasa93 data set;Turkish companies;analogy-based effort estimation;binary cluster tree;cluster subtrees;dynamic selection;essential assumption;estimation variance;nearest neighbor selection;project data;software effort estimator design;subtree variance;supertree variance,,66,,69,,,20110310,March-April 2012,,IEEE,IEEE Journals & Magazines,,13
Forecasting Risk Impact on ERP Maintenance with Augmented Fuzzy Cognitive Maps,J. L. Salmeron; C. Lopez,"University Pablo de Olavide, Seville",IEEE Transactions on Software Engineering,20120403,2012,38,2,439,452,"Worldwide, firms have made great efforts to implement Enterprise Resource Planning (ERP) systems. Despite these efforts, ERP adoption success is not guaranteed. Successful adoption of an ERP system also depends on proper system maintenance. For this reason, companies should follow a maintenance strategy that drives the ERP system toward success. However, in general, ERP maintenance managers do not know what conditions they should target to successfully maintain their ERP systems. Furthermore, numerous risks threaten these projects, but they are normally dealt with intuitively. To date, there has been limited literature published regarding ERP maintenance risks or ERP maintenance success. To address this need, we have built a dynamic simulation tool that allows ERP managers to foresee the impact of risks on maintenance goals. This research would help professionals manage their ERP maintenance projects. Moreover, it covers a significant gap in the literature.",0098-5589;00985589,,10.1109/TSE.2011.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680917,ERP;fuzzy cognitive maps;risk management;simulation;software maintenance.,Decision support systems,cognition;enterprise resource planning;forecasting theory;fuzzy set theory;project management;risk analysis;software maintenance;software management,ERP adoption success;ERP maintenance project management;ERP maintenance risks;ERP system maintenance;augmented fuzzy cognitive maps;dynamic simulation tool;enterprise resource planning system;risk impact forecasting,,25,,111,,,20110106,March-April 2012,,IEEE,IEEE Journals & Magazines,,13
Reducing Unauthorized Modification of Digital Objects,P. C. Van Oorschot; G. Wurster,"Carleton University, Ottawa",IEEE Transactions on Software Engineering,20120130,2012,38,1,191,204,"We consider the problem of malicious modification of digital objects. We present a protection mechanism designed to protect against unauthorized replacement or modification of digital objects while still allowing authorized updates transparently. We use digital signatures without requiring any centralized public key infrastructure. To explore the viability of our proposal, we apply the approach to file-system binaries, implementing a prototype in Linux which protects operating system and application binaries on disk. To test the prototype and related kernel modifications, we show that it protects against various rootkits currently available while incurring minimal overhead costs. The general approach can be used to restrict updates to general digital objects.",0098-5589;00985589,,10.1109/TSE.2011.7,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680916,Protection mechanisms;access controls;file organization;operating systems.;software release management and delivery;system integration and implementation,Access controls;Digital signatures;File organization;Malware;Operating systems;Public key,authorisation;digital signatures;file organisation;industrial property;operating system kernels,Linux;digital signatures;file-system binaries;kernel modification;malicious modification problem;operating system protection;overhead cost minimisation;unauthorized digital object replacement;unauthorized modification reduction,,0,1,63,,,20110106,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,13
The Link between Dependency and Cochange: Empirical Evidence,M. M. Geipel; F. Schweitzer,"ETH Zurich, Zurich",IEEE Transactions on Software Engineering,20121129,2012,38,6,1432,1444,"We investigate the relationship between class dependency and change propagation (cochange) in software written in Java. On the one hand, we find a strong correlation between dependency and cochange. Furthermore, we provide empirical evidence for the propagation of change along paths of dependency. These findings support the often alleged role of dependencies as propagators of change. On the other hand, we find that approximately half of all dependencies are never involved in cochanges and that the vast majority of cochanges pertain to only a small percentage of dependencies. This means that inferring the cochange characteristics of a software architecture solely from its dependency structure results in a severely distorted approximation of cochange characteristics. Any metric which uses dependencies alone to pass judgment on the evolvability of a piece of Java software is thus unreliable. As a consequence, we suggest to always take both the change characteristics and the dependency structure into account when evaluating software architecture.",0098-5589;00985589,,10.1109/TSE.2011.91,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363462,Modularity;class dependency;open source,Java;Open source software;Software development,Java;software architecture,Java software;change propagation;class dependency;cochange characteristic;software architecture,,5,,48,,,,Nov.-Dec. 2012,,IEEE,IEEE Journals & Magazines,,12
Exploiting Dynamic Information in IDEs Improves Speed and Correctness of Software Maintenance Tasks,D. Rothlisberger; M. Harry; W. Binder; P. Moret; D. Ansaloni; A. Villazon; O. Nierstrasz,"Universit&#x0E1;t Bern, Bern",IEEE Transactions on Software Engineering,20120529,2012,38,3,579,591,"Modern IDEs such as Eclipse offer static views of the source code, but such views ignore information about the runtime behavior of software systems. Since typical object-oriented systems make heavy use of polymorphism and dynamic binding, static views will miss key information about the runtime architecture. In this paper, we present an approach to gather and integrate dynamic information in the Eclipse IDE with the goal of better supporting typical software maintenance activities. By means of a controlled experiment with 30 professional developers, we show that for typical software maintenance tasks, integrating dynamic information into the Eclipse IDE yields a significant 17.5 percent decrease of time spent while significantly increasing the correctness of the solutions by 33.5 percent. We also provide a comprehensive performance evaluation of our approach.",0098-5589;00985589,,10.1109/TSE.2011.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178187,Object-oriented programming;complexity measures;integrated environments;performance measures.;reengineering;restructuring;reverse engineering,Concrete;Context;Java;Measurement;Runtime;Software maintenance;Weaving,dynamic programming;object-oriented programming;program compilers;software maintenance,Eclipse;IDE;dynamic binding;dynamic information;exploiting dynamic information;object-oriented systems;runtime architecture;runtime behavior;software maintenance tasks;software systems;source code,,8,,35,,,20120405,May-June 2012,,IEEE,IEEE Journals & Magazines,,12
Coping with Existing Systems in Information Systems Development,F. Zickert; R. Beck,"Goethe University Frankfurt, Frankfurt",IEEE Transactions on Software Engineering,20120924,2012,38,5,1027,1039,"Determining how to cope with existing systems is an important issue for information systems development (ISD). In this paper, we investigate how well different ISD patterns are suited for coping with existing systems. Empirical results, gathered from three software development projects undertaken by a financial institution, suggest propositions regarding how ISD patterns and existing systems affect the characteristics of objective ISD complexity, which in turn determine overall experienced complexity. Existing systems increase complexity due to conflicting interdependencies, but ISD patterns that reduce this complexity, such as those that employ bottom-up or concurrent consideration patterns, are best suited for coping with existing systems. In contrast, top-down and iterative focusing patterns, as classically used in new development, increase the complexity associated with conflicting interdependency, which makes them particularly unsuited for coping with existing systems in ISD.",0098-5589;00985589,,10.1109/TSE.2011.89,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999674,Complexity measures;improvements;software engineering process;software maintenance,Complexity theory;File systems;Information systems;Maintenance engineering;Programming;Servers;Software maintenance,financial data processing;information systems;investment;project management;software maintenance;software metrics,ISD patterns;bottom-up patterns;complexity measures;complexity reduction;concurrent consideration patterns;financial institution;information systems development;information systems portfolio;iterative focusing patterns;software development projects;software maintenance;top-down focusing patterns,,0,,73,,,20110825,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,12
Quality Requirements in Industrial Practice&#x2014;An Extended Interview Study at Eleven Companies,R. Berntsson Svensson; T. Gorschek; B. Regnell; R. Torkar; A. Shahrokni; R. Feldt,"Lund University, Lund",IEEE Transactions on Software Engineering,20120726,2012,38,4,923,935,"In order to create a successful software product and assure its quality, it is not enough to fulfill the functional requirements, it is also crucial to find the right balance among competing quality requirements (QR). An extended, previously piloted, interview study was performed to identify specific challenges associated with the selection, tradeoff, and management of QR in industrial practice. Data were collected through semistructured interviews with 11 product managers and 11 project leaders from 11 software companies. The contribution of this study is fourfold: First, it compares how QR are handled in two cases, companies working in business-to-business markets and companies that are working in business-to-consumer markets. These two are also compared in terms of impact on the handling of QR. Second, it compares the perceptions and priorities of QR by product and project management, respectively. Third, it includes an examination of the interdependencies among quality requirements perceived as most important by the practitioners. Fourth, it characterizes the selection and management of QR in downstream development activities.",0098-5589;00985589,,10.1109/TSE.2011.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753901,Management;process;requirements/specifications,Companies;Industries;Interviews;Reliability;Telecommunications;Usability,DP industry;project management;software management;software quality,QR handling;QR management;QR selection;QR tradeoff;business-to-business market;business-to-consumer market;industrial practice;project management;quality requirements;software company;software product;software quality,,20,,40,,,20110421,July-Aug. 2012,,IEEE,IEEE Journals & Magazines,,12
Oracles for Distributed Testing,R. M. Hierons,"Brunel University, Middlesex",IEEE Transactions on Software Engineering,20120529,2012,38,3,629,641,"The problem of deciding whether an observed behavior is acceptable is the oracle problem. When testing from a finite state machine (FSM), it is easy to solve the oracle problem and so it has received relatively little attention for FSMs. However, if the system under test has physically distributed interfaces, called ports, then in distributed testing, we observe a local trace at each port and we compare the set of local traces with the set of allowed behaviors (global traces). This paper investigates the oracle problem for deterministic and nondeterministic FSMs and for two alternative definitions of conformance for distributed testing. We show that the oracle problem can be solved in polynomial time for the weaker notion of conformance (___<sub>w</sub>) but is NP-hard for the stronger notion of conformance (___), even if the FSM is deterministic. However, when testing from a deterministic FSM with controllable input sequences, the oracle problem can be solved in polynomial time and similar results hold for nondeterministic FSMs. Thus, in some cases, the oracle problem can be efficiently solved when using ___<sub>s</sub> and where this is not the case, we can use the decision procedure for ___<sub>w</sub> as a sound approximation.",0098-5589;00985589,,10.1109/TSE.2011.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750006,Software engineering/software/program verification;controllability;distributed systems;finite state machine;local observability.;nondeterminism;software engineering/testing and debugging;systems and software;test oracle,Controllability;Observability;Polynomials;Software;Software engineering;Testing,distributed processing;finite state machines;polynomials;program testing,FSM;NP-hard problem;decision procedure;distributed testing;finite state machine;oracle problem;physically distributed interfaces;polynomial time;sound approximation;weaker notion,,15,,37,,,20110415,May-June 2012,,IEEE,IEEE Journals & Magazines,,12
Specifying Dynamic Analyses by Extending Language Semantics,A. Lienhard; T. Girba; O. Nierstrasz,"University of Bern, Bern",IEEE Transactions on Software Engineering,20120529,2012,38,3,694,706,"Dynamic analysis is increasingly attracting attention for debugging, profiling, and program comprehension. Ten to twenty years ago, many dynamic analyses investigated only simple method execution traces. Today, in contrast, many sophisticated dynamic analyses exist, for instance, for detecting memory leaks, analyzing ownership properties, measuring garbage collector performance, or supporting debugging tasks. These analyses depend on complex program instrumentations and analysis models, making it challenging to understand, compare, and reproduce the proposed approaches. While formal specifications and proofs are common in the field of static analysis, most dynamic analyses are specified using informal, textual descriptions. In this paper, we propose a formal framework using operational semantics that allows researchers to precisely specify their dynamic analysis. Our goal is to provide an accessible and reusable basis on which researchers who may not be familiar with rigorous specifications of dynamic analyses can build. By extending the provided semantics, one can concisely specify how runtime events are captured and how this data is transformed to populate the analysis model. Furthermore, our approach provides the foundations to reason about properties of a dynamic analysis.",0098-5589;00985589,,10.1109/TSE.2011.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740933,Dynamic analysis;debugging.;formal definitions and theory;tracing,Analytical models;Arrays;Context;Performance analysis;Runtime;Semantics;Syntactics,formal specification;program debugging;programming language semantics,complex program instrumentations;dynamic analysis specification;formal specifications;garbage collector performance;informal descriptions;language semantics extending;memory leaks detection;ownership properties;program comprehension;program debugging;program profiling;textual descriptions,,0,,26,,,20110405,May-June 2012,,IEEE,IEEE Journals & Magazines,,12
Defining and Evaluating a Measure of Open Source Project Survivability,U. Raja; M. J. Tretter,"The University of Alabama, Tuscaloosa",IEEE Transactions on Software Engineering,20120130,2012,38,1,163,174,"In this paper, we define and validate a new multidimensional measure of Open Source Software (OSS) project survivability, called Project Viability. Project viability has three dimensions: vigor, resilience, and organization. We define each of these dimensions and formulate an index called the Viability Index (VI) to combine all three dimensions. Archival data of projects hosted at SourceForge.net are used for the empirical validation of the measure. An Analysis Sample (n=136) is used to assign weights to each dimension of project viability and to determine a suitable cut-off point for VI. Cross-validation of the measure is performed on a hold-out Validation Sample (n=96). We demonstrate that project viability is a robust and valid measure of OSS project survivability that can be used to predict the failure or survival of an OSS project accurately. It is a tangible measure that can be used by organizations to compare various OSS projects and to make informed decisions regarding investment in the OSS domain.",0098-5589;00985589,,10.1109/TSE.2011.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6127835,Evaluation framework;external validity;open source software;project evaluation;software measurement;software survivability.,Indexes;Maintenance engineering;Software measurement,project management;public domain software;software metrics,multidimensional measure;open source project survivability;open source software project survivability;organization;project viability;resilience;viability index;vigor,,12,,69,,,20120110,Jan.-Feb. 2012,,IEEE,IEEE Journals & Magazines,,11
Formal Analysis of the Probability of Interaction Fault Detection Using Random Testing,A. Arcuri; L. Briand,"Simula Research Laboratory, Lysaker",IEEE Transactions on Software Engineering,20120924,2012,38,5,1088,1099,"Modern systems are becoming highly configurable to satisfy the varying needs of customers and users. Software product lines are hence becoming a common trend in software development to reduce cost by enabling systematic, large-scale reuse. However, high levels of configurability entail new challenges. Some faults might be revealed only if a particular combination of features is selected in the delivered products. But testing all combinations is usually not feasible in practice, due to their extremely large numbers. Combinatorial testing is a technique to generate smaller test suites for which all combinations of t features are guaranteed to be tested. In this paper, we present several theorems describing the probability of random testing to detect interaction faults and compare the results to combinatorial testing when there are no constraints among the features that can be part of a product. For example, random testing becomes even more effective as the number of features increases and converges toward equal effectiveness with combinatorial testing. Given that combinatorial testing entails significant computational overhead in the presence of hundreds or thousands of features, the results suggest that there are realistic scenarios in which random testing may outperform combinatorial testing in large systems. Furthermore, in common situations where test budgets are constrained and unlike combinatorial testing, random testing can still provide minimum guarantees on the probability of fault detection at any interaction level. However, when constraints are present among features, then random testing can fare arbitrarily worse than combinatorial testing. As a result, in order to have a practical impact, future research should focus on better understanding the decision process to choose between random testing and combinatorial testing, and improve combinatorial testing in the presence of feature constraints.",0098-5589;00985589,,10.1109/TSE.2011.85,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999671,Combinatorial testing;constraint;feature diagram;interaction testing;lower bound;random testing;theory,Benchmark testing;Context;Fault detection;Feature extraction;Scalability;Software,cost reduction;customer satisfaction;probability;program testing;program verification;random processes;software cost estimation;software fault tolerance,combinatorial testing;computational overhead;cost reduction;customer satisfaction;feature constraints;formal analysis;interaction fault detection probability;large-scale reusability;random testing;software development;software product lines;user satisfaction,,12,,37,,,20110825,Sept.-Oct. 2012,,IEEE,IEEE Journals & Magazines,,11
Comparing Semi-Automated Clustering Methods for Persona Development,J. Brickey; S. Walczak; T. Burgess,"US Army, Combating Terrorism Center, West Point",IEEE Transactions on Software Engineering,20120529,2012,38,3,537,546,"Current and future information systems require a better understanding of the interactions between users and systems in order to improve system use and, ultimately, success. The use of personas as design tools is becoming more widespread as researchers and practitioners discover its benefits. This paper presents an empirical study comparing the performance of existing qualitative and quantitative clustering techniques for the task of identifying personas and grouping system users into those personas. A method based on Factor (Principal Components) Analysis performs better than two other methods which use Latent Semantic Analysis and Cluster Analysis as measured by similarity to expert manually defined clusters.",0098-5589;00985589,,10.1109/TSE.2011.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928355,Clustering;interaction styles;personas;user interfaces.;user-centered design,Clustering methods;Humans;Interviews;Manuals;Organizations;Principal component analysis;Software,information systems;pattern clustering;principal component analysis,design tools;information systems;latent semantic analysis;persona development;principal components analysis;qualitative clustering techniques;quantitative clustering techniques;semi-automated clustering methods,,4,,47,,,20110623,May-June 2012,,IEEE,IEEE Journals & Magazines,,9
Verifying Protocol Conformance Using Software Model Checking for the Model-Driven Development of Embedded Systems,Y. Moffett; J. Dingel; A. Beaulieu,"CF 18 Avionics System Engineering, Ottawa",IEEE Transactions on Software Engineering,20130828,2013,39,9,1307,13256,"To facilitate modular development, the use of state machines has been proposed to specify the protocol (i.e., the sequence of messages) that each port of a component can engage in. The protocol conformance checking problem consists of determining whether the actual behavior of a component conforms to the protocol specifications on its ports. In this paper, we consider this problem in the context of the model-driven development (MDD) of embedded systems based on UML 2, in which UML 2 state machines are used to specify component behavior. We provide a definition of conformance which slightly extends those found in the literature and reduce the conformance check to a state space exploration. We describe a tool implementing the approach using the Java PathFinder software model checker and the MDD tool IBM Rational RoseRT, discuss its application to three case studies, and show how the tool repeatedly allowed us to find unexpected conformance errors with encouraging performance. We conclude that the approach is promising for supporting the modular development of embedded components in the context of industrial applications of MDD.",0098-5589;00985589,,10.1109/TSE.2013.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482140,Component-based software engineering;behavioral interface specifications;formal specification and verification;model-driven development;software model checking;software modeling,Context;Java;Ports (Computers);Protocols;Safety;Software;Unified modeling language,Unified Modeling Language;embedded systems;formal verification;object-oriented programming,IBM Rational RoseRT MDD tool;Java PathFinder software model checker;MDD industrial application;UML 2 state machines;Unified Modeling Language;embedded system;model-driven development;protocol conformance checking problem;protocol conformance verification;software model checking;state space exploration,,1,,47,,,20130319,Sept. 2013,,IEEE,IEEE Journals & Magazines,,11949
A Study of Variability Models and Languages in the Systems Software Domain,T. Berger; S. She; R. Lotufo; A. Wasowski; K. Czarnecki,"IT Univ. of Copenhagen, Copenhagen, Denmark",IEEE Transactions on Software Engineering,20131122,2013,39,12,1611,1640,"Variability models represent the common and variable features of products in a product line. Since the introduction of FODA in 1990, several variability modeling languages have been proposed in academia and industry, followed by hundreds of research papers on variability models and modeling. However, little is known about the practical use of such languages. We study the constructs, semantics, usage, and associated tools of two variability modeling languages, Kconfig and CDL, which are independently developed outside academia and used in large and significant software projects. We analyze 128 variability models found in 12 open--source projects using these languages. Our study 1) supports variability modeling research with empirical data on the real-world use of its flagship concepts. However, we 2) also provide requirements for concepts and mechanisms that are not commonly considered in academic techniques, and 3) challenge assumptions about size and complexity of variability models made in academic papers. These results are of interest to researchers working on variability modeling and analysis techniques and to designers of tools, such as feature dependency checkers and interactive product configurators.",0098-5589;00985589,,10.1109/TSE.2013.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572787,Empirical software engineering;configuration;feature modeling;open source;software product lines;variability modeling,Analytical models;Biological system modeling;Computational modeling;Computer architecture;Product line;Semantics;Software products,public domain software;simulation languages;software engineering,CDL language;FODA;Kconfig language;associated language tools;feature dependency checkers;interactive product configurators;language constructs;language semantics;language usage;open-source projects;software projects;systems software domain;variability analysis techniques;variability modeling languages;variability models,,22,,88,,,20130731,Dec. 2013,,IEEE,IEEE Journals & Magazines,,29
Generating Domain-Specific Visual Language Tools from Abstract Visual Specifications,J. C. Grundy; J. Hosking; K. N. Li; N. M. Ali; J. Huh; R. L. Li,"Swinburne University University of Technology, Hawthorn",IEEE Transactions on Software Engineering,20130326,2013,39,4,487,515,"Domain-specific visual languages support high-level modeling for a wide range of application domains. However, building tools to support such languages is very challenging. We describe a set of key conceptual requirements for such tools and our approach to addressing these requirements, a set of visual language-based metatools. These support definition of metamodels, visual notations, views, modeling behaviors, design critics, and model transformations and provide a platform to realize target visual modeling tools. Extensions support collaborative work, human-centric tool interaction, and multiplatform deployment. We illustrate application of the metatoolset on tools developed with our approach. We describe tool developer and cognitive evaluations of our platform and our exemplar tools, and summarize key future research directions.",0098-5589;00985589,,10.1109/TSE.2012.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6205768,Metatool;domain-specific visual language;model-driven engineering;software tool;visual specification,Abstracts;Business;Computational modeling;Electronic mail;Software;Unified modeling language;Visualization,,,,6,,79,,,20120529,13-Apr,,IEEE,IEEE Journals & Magazines,,28
A Learning-Based Framework for Engineering Feature-Oriented Self-Adaptive Software Systems,N. Esfahani; A. Elkhodary; S. Malek,"Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA",IEEE Transactions on Software Engineering,20131028,2013,39,11,1467,1493,"Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation.",0098-5589;00985589,,10.1109/TSE.2013.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574860,Self-adaptive software;autonomic computing;feature-orientation;machine learning,Adaptation models;Authentication;Measurement;Quality of service;Runtime;Software systems,inference mechanisms;learning (artificial intelligence);quality of service;software engineering,adaptation decision assessment;adaptation decision reasoning;engineering feature-oriented self-adaptive software systems;feature-models;learning-based framework;online learning-based approach;quality-of-service goals;runtime conditions,,26,,67,,,20130805,Nov. 2013,,IEEE,IEEE Journals & Magazines,,26
A Taxonomy and Mapping of Computer-Based Critiquing Tools,N. M. Ali; J. Hosking; J. Grundy,"Fac. of Comput. Sci. & Inf. Technol., Univ. Putra Malaysia, Serdang, Malaysia",IEEE Transactions on Software Engineering,20131028,2013,39,11,1494,1520,"Critics have emerged in recent times as a specific tool feature to support users in computer-mediated tasks. These computer-supported critics provide proactive guidelines or suggestions for improvement to designs, code, and other digital artifacts. The concept of a critic has been adopted in various domains, including medical, programming, software engineering, design sketching, and others. Critics have been shown to be an effective mechanism for providing feedback to users. We propose a new critic taxonomy based on extensive review of the critic literature. The groups and elements of our critic taxonomy are presented and explained collectively with examples, including the mapping of 13 existing critic tools, predominantly for software engineering and programming education tasks to the taxonomy. We believe this critic taxonomy will assist others in identifying, categorizing, developing, and deploying computer-supported critics in a range of domains.",0098-5589;00985589,,10.1109/TSE.2013.32,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570472,Design critics;critic taxonomy;critiquing systems;software tool support;survey,Java;Programming;Recommender systems;Software;Software engineering;Taxonomy;Unified modeling language,computer science education;programming;software engineering,computer-based critiquing tool mapping;computer-based critiquing tool taxonomy;computer-mediated tasks;computer-supported critics;critic taxonomy;digital artifacts;programming education tasks;software engineering,,3,,68,,,20130726,Nov. 2013,,IEEE,IEEE Journals & Magazines,,26
Generating Test Data from OCL Constraints with Search Techniques,S. Ali; M. Zohaib Iqbal; A. Arcuri; L. C. Briand,"Simula Research Lab, Norway",IEEE Transactions on Software Engineering,20130925,2013,39,10,1376,1402,"Model-based testing (MBT) aims at automated, scalable, and systematic testing solutions for complex industrial software systems. To increase chances of adoption in industrial contexts, software systems can be modeled using well-established standards such as the Unified Modeling Language (UML) and the Object Constraint Language (OCL). Given that test data generation is one of the major challenges to automate MBT, we focus on test data generation from OCL constraints in this paper. This endeavor is all the more challenging given the numerous OCL constructs and operations that are designed to facilitate the definition of constraints. Though search-based software testing has been applied to test data generation for white-box testing (e.g., branch coverage), its application to the MBT of industrial software systems has been limited. In this paper, we propose a set of search heuristics targeted to OCL constraints to guide test data generation and automate MBT in industrial applications. We evaluate these heuristics for three search algorithms: Genetic Algorithm, (1+1) Evolutionary Algorithm, and Alternating Variable Method. We empirically evaluate our heuristics using complex artificial problems, followed by empirical analyses of the feasibility of our approach on one industrial system in the context of robustness testing. Our approach is also compared with the most widely referenced OCL solver (UMLtoCSP) in the literature and shows to be significantly more efficient.",0098-5589;00985589,,10.1109/TSE.2013.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6491405,OCL;empirical evaluation;model-based testing;search-based software engineering;search-based testing;test data generation,Genetic algorithms;Search problems;Software algorithms;Software testing;Standards;Unified modeling language,genetic algorithms;program testing;specification languages,MBT;OCL constraints;OCL constructs;OCL operations;UML;UMLtoCSP OCL solver;alternating variable method;empirical analysis;genetic algorithm;industrial software systems;model-based testing;object constraint language;one-plus-one evolutionary algorithm;robustness testing context;search techniques;search-based software testing;test data generation;unified modeling language;white-box testing,,30,,75,,,20130401,Oct. 2013,,IEEE,IEEE Journals & Magazines,,26
Synthesizing Modal Transition Systems from Triggered Scenarios,G. E. Sibay; V. Braberman; S. Uchitel; J. Kramer,"Imperial College London, London",IEEE Transactions on Software Engineering,20130626,2013,39,7,975,1001,"Synthesis of operational behavior models from scenario-based specifications has been extensively studied. The focus has been mainly on either existential or universal interpretations. One noteworthy exception is Live Sequence Charts (LSCs), which provides expressive constructs for conditional universal scenarios and some limited support for nonconditional existential scenarios. In this paper, we propose a scenario-based language that supports both existential and universal interpretations for conditional scenarios. Existing model synthesis techniques use traditional two-valued behavior models, such as Labeled Transition Systems. These are not sufficiently expressive to accommodate specification languages with both existential and universal scenarios. We therefore shift the target of synthesis to Modal Transition Systems (MTS), an extension of labeled Transition Systems that can distinguish between required, unknown, and proscribed behavior to capture the semantics of existential and universal scenarios. Modal Transition Systems support elaboration of behavior models through refinement, which complements an incremental elicitation process suitable for specifying behavior with scenario-based notations. The synthesis algorithm that we define constructs a Modal Transition System that uses refinement to characterize all the Labeled Transition Systems models that satisfy a mixed, conditional existential and universal scenario-based specification. We show how this combination of scenario language, synthesis, and Modal Transition Systems supports behavior model elaboration.",0098-5589;00985589,,10.1109/TSE.2012.62,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311408,MTS;Scenarios;partial behavior models;synthesis,Analytical models;Cognition;Indexes;Merging;Online banking;Semantics;Unified modeling language,formal verification;reasoning about programs,LSC;MTS;behavior model elaboration;conditional existential scenario-based specification;conditional universal scenarios;incremental elicitation process;labeled transition system models;live sequence charts;modal transition systems;model synthesis techniques;nonconditional existential scenarios;operational behavior models;scenario-based language;scenario-based notations;scenario-based specifications;specification languages;triggered scenarios;two-valued behavior models;universal scenario-based specification,,6,,40,,,20120924,13-Jul,,IEEE,IEEE Journals & Magazines,,26
Empirical Principles and an Industrial Case Study in Retrieving Equivalent Requirements via Natural Language Processing Techniques,D. Falessi; G. Cantone; G. Canfora,"Simula Research Laboratory, Lysaker and University of Rome &#x0022;TorVergata&#x0022;, Rome",IEEE Transactions on Software Engineering,20121228,2013,39,1,18,44,"Though very important in software engineering, linking artifacts of the same type (clone detection) or different types (traceability recovery) is extremely tedious, error-prone, and effort-intensive. Past research focused on supporting analysts with techniques based on Natural Language Processing (NLP) to identify candidate links. Because many NLP techniques exist and their performance varies according to context, it is crucial to define and use reliable evaluation procedures. The aim of this paper is to propose a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. In this paper, we conjecture, and verify, that NLP techniques perform on a given dataset according to both ability and the odds of identifying equivalent requirements correctly. For instance, when the odds of identifying equivalent requirements are very high, then it is reasonable to expect that NLP techniques will result in good performance. Our key idea is to measure this random factor of the specific dataset(s) in use and then adjust the observed performance accordingly. To support the application of the principles we report their practical application to a case study that evaluates the performance of a large number of NLP techniques for identifying equivalent requirements in the context of an Italian company in the defense and aerospace domain. The current application context is the evaluation of NLP techniques to identify equivalent requirements. However, most of the proposed principles seem applicable to evaluating any estimation technique aimed at supporting a binary decision (e.g., equivalent/nonequivalent), with the estimate in the range [0,1] (e.g., the similarity provided by the NLP), when the dataset(s) is used as a benchmark (i.e., testbed), independently of the type of estimator (i.e., requirements text) and of the estimation method (e.g., NLP).",0098-5589;00985589,,10.1109/TSE.2011.122,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6112783,Empirical software engineering;equivalent requirements;metrics and measurement;natural language processing;traceability recovery,Context;Matrix decomposition;Measurement;Monitoring;Natural language processing;Semantics;Thesauri,information retrieval;natural language processing;program diagnostics;software engineering,Italian company;NLP techniques;aerospace domain;binary decision;clone detection;defense domain;equivalent requirement retrieval;estimation technique;industrial case study;natural language processing techniques;reliable evaluation procedures;software engineering;traceability recovery,,29,,116,,,20111227,Jan. 2013,,IEEE,IEEE Journals & Magazines,,26
"Identification, Impact, and Refactoring of Smells in Pipe-Like Web Mashups",K. T. Stolee; S. Elbaum,"Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA",IEEE Transactions on Software Engineering,20131122,2013,39,12,1654,1679,"With the emergence of tools to support visual mashup creation, tens of thousands of users have started to access, manipulate, and compose data from web sources. We have observed, however, that mashups created by these users tend to suffer from deficiencies that propagate as mashups are reused, which happens frequently. To address these deficiencies, we would like to bring some of the benefits of software engineering techniques to the end users creating these programs. In this work, we focus on identifying code smells indicative of the deficiencies we observed in web mashups programmed in the popular Yahoo! Pipes environment. Through an empirical study, we explore the impact of those smells on the preferences of 61 users, and observe that a significant majority of users prefer mashups without smells. We then introduce refactorings targeting those smells. These refactorings reduce the complexity of the mashup programs, increase their abstraction, update broken data sources and dated components, and standardize their structures to fit the community development patterns. Our assessment of a sample of over 8,000 mashups shows that smells are present in 81 percent of them and that the proposed refactorings can reduce the number of smelly mashups to 16 percent, illustrating the potential of refactoring to support the thousands of end-users programming mashups. Further, we explore how the smells and refactorings can apply to other end-user programming domains to show the generalizability of our approach.",0098-5589;00985589,,10.1109/TSE.2013.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6589568,End-user software engineering;code smells;empirical studies;end-user programming;refactoring;web mashups,Factoring;Generators;Mashups;Programming;Visualization,Internet;software maintenance,Web sources;Yahoo! Pipes environment;end-users programming mashups;pipe-like Web mashups;smell identification;smell impact;smell refactoring;software engineering techniques;visual mashup creation,,7,,56,,,20130906,Dec. 2013,,IEEE,IEEE Journals & Magazines,,25
Software Architecture Optimization Methods: A Systematic Literature Review,A. Aleti; B. Buhnova; L. Grunske; A. Koziolek; I. Meedeniya,"Monash University, Australia",IEEE Transactions on Software Engineering,20130429,2013,39,5,658,683,"Due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving. In the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated. However, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes. To integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities. Based on this survey, a taxonomy has been created which is used to classify the existing research. Furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments.",0098-5589;00985589,,10.1109/TSE.2012.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311410,Software architecture optimization;optimization methods;problem overview;systematic literature review,Computer architecture;Optimization methods;Software;Software architecture;Systematics;Taxonomy,software architecture;software quality,software architecture design;software architecture optimization method;software quality attribute;software system,,83,,245,,,20120924,13-May,,IEEE,IEEE Journals & Magazines,,25
TACO: Efficient SAT-Based Bounded Verification Using Symmetry Breaking and Tight Bounds,J. P. Galeotti; N. Rosner; C. G. LÕ_pez Pombo; M. F. Frias,"Universidad de Buenos Aires and CONICET, Argentina",IEEE Transactions on Software Engineering,20130828,2013,39,9,1283,1307,"SAT-based bounded verification of annotated code consists of translating the code together with the annotations to a propositional formula, and analyzing the formula for specification violations using a SAT-solver. If a violation is found, an execution trace exposing the failure is exhibited. Code involving linked data structures with intricate invariants is particularly hard to analyze using these techniques. In this paper, we present Translation of Annotated COde (TACO), a prototype tool which implements a novel, general, and fully automated technique for the SAT-based analysis of JML-annotated Java sequential programs dealing with complex linked data structures. We instrument code analysis with a symmetry-breaking predicate which, on one hand, reduces the size of the search space by ignoring certain classes of isomorphic models and, on the other hand, allows for the parallel, automated computation of tight bounds for Java fields. Experiments show that the translations to propositional formulas require significantly less propositional variables, leading to an improvement of the efficiency of the analysis of orders of magnitude, compared to the noninstrumented SAT--based analysis. We show that in some cases our tool can uncover bugs that cannot be detected by state-of-the-art tools based on SAT-solving, model checking, or SMT-solving.",0098-5589;00985589,,10.1109/TSE.2013.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482141,Alloy;DynAlloy;KodKod;SAT-based code analysis;Static analysis,Analytical models;Context;Contracts;Cost accounting;Instruments;Java;Metals,computability;formal specification;formal verification;program diagnostics;program interpreters,JML-annotated Java sequential program;SAT-based bounded verification;SAT-solving;SMT-solving;TACO tool;automated tight bound computation;code analysis;code translation;data structure;isomorphic model;model checking;satisfiability;specification violation;symmetry-breaking predicate;translation-of-annotated code tool,,7,,47,,,20130319,Sept. 2013,,IEEE,IEEE Journals & Magazines,,24
Automated API Property Inference Techniques,M. P. Robillard; E. Bodden; D. Kawrykow; M. Mezini; T. Ratchford,"McGill University, Montr&#x00E9;al",IEEE Transactions on Software Engineering,20130429,2013,39,5,613,637,"Frameworks and libraries offer reusable and customizable functionality through Application Programming Interfaces (APIs). Correctly using large and sophisticated APIs can represent a challenge due to hidden assumptions and requirements. Numerous approaches have been developed to infer properties of APIs, intended to guide their use by developers. With each approach come new definitions of API properties, new techniques for inferring these properties, and new ways to assess their correctness and usefulness. This paper provides a comprehensive survey of over a decade of research on automated property inference for APIs. Our survey provides a synthesis of this complex technical field along different dimensions of analysis: properties inferred, mining techniques, and empirical results. In particular, we derive a classification and organization of over 60 techniques into five different categories based on the type of API property inferred: unordered usage patterns, sequential usage patterns, behavioral specifications, migration mappings, and general information.",0098-5589;00985589,,10.1109/TSE.2012.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311409,API evolution;API property;API usage pattern;data mining;interface;pattern mining;programming rules;protocols;specifications,Association rules;Context;Itemsets;Programming;Protocols;Software engineering,application program interfaces;data mining;pattern classification,application programming interfaces;automated API property inference technique;behavioral specifications;empirical results;general information;migration mappings;mining techniques;properties inferred;sequential usage patterns;technique classification;unordered usage patterns,,36,,109,,,20120924,13-May,,IEEE,IEEE Journals & Magazines,,24
Monitoring Data Usage in Distributed Systems,D. Basin; M. Harvan; F. Klaedtke; E. Zalinescu,"ETH Zurich, Zurich",IEEE Transactions on Software Engineering,20130925,2013,39,10,1403,1426,"IT systems manage increasing amounts of sensitive data and there is a growing concern that they comply with policies that regulate data usage. In this paper, we use temporal logic to express policies and runtime monitoring to check system compliance. While well-established methods for monitoring linearly ordered system behavior exist, a major challenge is monitoring distributed and concurrent systems where actions are locally observed in the different system parts. These observations can only be partially ordered, while policy compliance may depend on the actions' actual order of appearance. Technically speaking, it is in general intractable to check compliance of partially ordered traces. We identify fragments of our policy specification language for which compliance can be checked efficiently, namely, by monitoring a single representative trace in which the observed actions are totally ordered. Through a case study we show that the fragments are capable of expressing nontrivial policies and that monitoring representative traces is feasible on real-world data.",0098-5589;00985589,,10.1109/TSE.2013.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493331,Monitors;distributed systems;regulation;temporal logic;verification,Cost accounting;Distributed databases;Finite element analysis;Monitoring;Periodic structures;Semantics;Standards,concurrency control;formal verification;specification languages;temporal logic,IT systems;concurrent systems;data usage monitoring;data usage regulation;distributed systems;information technology systems;policy compliance;policy specification language;runtime monitoring;system compliance;temporal logic,,7,,34,,,20130403,Oct. 2013,,IEEE,IEEE Journals & Magazines,,23
"Pair Programming and Software Defects--A Large, Industrial Case Study",E. di Bella; I. Fronza; N. Phaphoom; A. Sillitti; G. Succi; J. Vlasenko,"Dept. of Econ. & Quantitative Methods, Univ. of Genova, Genoa, Italy",IEEE Transactions on Software Engineering,20130626,2013,39,7,930,953,"In the last decade, there has been increasing interest in pair programming (PP). However, despite the existing work, there is still a lack of substantial evidence of the effects of PP in industrial environments. To address this issue, we have analyzed the work of a team of 17 industrial developers for 14 months. The team is part of the IT department of a large Italian manufacturing company; it adopts a customized version of extreme programming (XP). We have investigated the effects of PP on software quality in five different scenarios. The results show that PP appears to provide a perceivable but small effect on the reduction of defects in these settings.",0098-5589;00985589,,10.1109/TSE.2012.68,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6331491,Pair programming;case study;software defects,Programming;Software,configuration management;manufacturing industries;production engineering computing;software fault tolerance;software prototyping;software quality;team working,IT department;customized version;extreme programming;industrial case study;large Italian manufacturing company;pair programming;software defects;software quality,,27,,94,,,20121016,13-Jul,,IEEE,IEEE Journals & Magazines,,23
What Industry Needs from Architectural Languages: A Survey,I. Malavolta; P. Lago; H. Muccini; P. Pelliccione; A. Tang,"Universit&#x00E0; dell'Aquila, Italy",IEEE Transactions on Software Engineering,20130523,2013,39,6,869,891,"Many times we are faced with the proliferation of definitions, concepts, languages, and tools in certain (research) topics. But often there is a gap between what is provided by existing technologies and what is needed by their users. The strengths, limitations, and needs of the available technologies can be dubious. The same applies to software architectures, and specifically to languages designed to represent architectural models. Tens of different architectural languages have been introduced by the research and industrial communities in the last two decades. However, it is unclear if they fulfill the user's perceived needs in architectural description. As a way to plan for next generation languages for architectural description, this study analyzes practitioners' perceived strengths, limitations, and needs associated with existing languages for software architecture modeling in industry. We run a survey by interviewing 48 practitioners from 40 different IT companies in 15 countries. Each participant is asked to fill in a questionnaire of 51 questions. By analyzing the data collected through this study, we have concluded that 1) while practitioners are generally satisfied with the design capabilities provided by the languages they use, they are dissatisfied with the architectural language analysis features and their abilities to define extra-functional properties; 2) architectural languages used in practice mostly originate from industrial development instead of from academic research; 3) more formality and better usability are required of an architectural language.",0098-5589;00985589,,10.1109/TSE.2012.74,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374194,ADL;Software architecture;architecture description languages;empirical study;survey,Communities;Computer architecture;Google;Industries;Software;Software architecture;Unified modeling language,data analysis;software architecture;specification languages,architectural description;architectural languages;architectural models;data collection analysis;industrial development;next generation languages;software architectures,,70,,64,,,20121204,13-Jun,,IEEE,IEEE Journals & Magazines,,22
Test Case-Aware Combinatorial Interaction Testing,C. Yilmaz,"Sabanci University, Istanbul",IEEE Transactions on Software Engineering,20130429,2013,39,5,684,706,"The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations by using a battery of test cases. Traditional covering arrays, while taking system-wide interoption constraints into account, do not provide a systematic way of handling test case-specific interoption constraints. The basic justification for t-way covering arrays is that they can cost effectively exercise all system behaviors caused by the settings of t or fewer options. In this paper, we hypothesize, however, that in the presence of test case-specific interoption constraints, many such behaviors may not be tested due to masking effects caused by the overlooked test case-specific constraints. For example, if a test case refuses to run in a configuration due to an unsatisfied test case-specific constraint, none of the valid option setting combinations appearing in the configuration will be tested by that test case. To account for test case-specific constraints, we introduce a new combinatorial object, called a test case-aware covering array. A t-way test case-aware covering array is not just a set of configurations, as is the case in traditional covering arrays, but a set of configurations, each of which is associated with a set of test cases such that all test case-specific constraints are satisfied and that, for each test case, each valid combination of option settings for every combination of t options appears at least once in the set of configurations that the test case is associated with. We furthermore present three algorithms to compute test case-aware covering arrays. Two of the algorithms aim to minimize the number of configurations required (one is fast, but produces larger arrays, the other is slower, but produces smaller arrays), whereas the remaining algorithm aims to minimize the number of test runs requ- red. The results of our empirical studies conducted on two widely used highly configurable software systems suggest that test case-specific constraints do exist in practice, that traditional covering arrays suffer from masking effects caused by ignorance of such constraints, and that test case-aware covering arrays are better than other approaches in handling test case-specific constraints, thus avoiding masking effects.",0098-5589;00985589,,10.1109/TSE.2012.65,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311411,Software quality assurance;combinatorial interaction testing;covering arrays,Computational modeling;Servers;Software algorithms;Software systems;Systematics;Testing,program testing;software quality,CIT approach;highly configurable software system;interoption constraint;masking effect;software system;t-way test case-aware covering array;test case-aware combinatorial interaction testing,,5,,34,,,20120924,13-May,,IEEE,IEEE Journals & Magazines,,22
Elaborating Requirements Using Model Checking and Inductive Learning,D. Alrajeh; J. Kramer; A. Russo; S. Uchitel,"Imperial College London, London",IEEE Transactions on Software Engineering,20130225,2013,39,3,361,383,"The process of Requirements Engineering (RE) includes many activities, from goal elicitation to requirements specification. The aim is to develop an operational requirements specification that is guaranteed to satisfy the goals. In this paper, we propose a formal, systematic approach for generating a set of operational requirements that are complete with respect to given goals. We show how the integration of model checking and inductive learning can be effectively used to do this. The model checking formally verifies the satisfaction of the goals and produces counterexamples when incompleteness in the operational requirements is detected. The inductive learning process then computes operational requirements from the counterexamples and user-provided positive examples. These learned operational requirements are guaranteed to eliminate the counterexamples and be consistent with the goals. This process is performed iteratively until no goal violation is detected. The proposed framework is a rigorous, tool-supported requirements elaboration technique which is formally guided by the engineer's knowledge of the domain and the envisioned system.",0098-5589;00985589,,10.1109/TSE.2012.41,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216384,Requirements elaboration;behavior model refinement;goal operationalization;inductive learning;model checking,Adaptation models;Calculus;Computational modeling;Semantics;Software;Switches;Wheels,formal specification;formal verification;learning by example,RE;formal verification;goal elicitation;inductive learning process;model checking;operational requirements specification;requirement engineering;requirement specification;tool-supported requirements elaboration,,5,,52,,,20120612,13-Mar,,IEEE,IEEE Journals & Magazines,,22
Self-Organizing Roles on Agile Software Development Teams,R. Hoda; J. Noble; S. Marshall,"The University of Auckland, Auckland",IEEE Transactions on Software Engineering,20130225,2013,39,3,422,444,"Self-organizing teams have been recognized and studied in various forms-as autonomous groups in socio-technical systems, enablers of organizational theories, agents of knowledge management, and as examples of complex-adaptive systems. Over the last decade, self-organizing teams have taken center stage in software engineering when they were incorporated as a hallmark of Agile methods. Despite the long and rich history of self-organizing teams and their recent popularity with Agile methods, there has been little research on the topic within software wngineering. Particularly, there is a dearth of research on how Agile teams organize themselves in practice. Through a Grounded Theory research involving 58 Agile practitioners from 23 software organizations in New Zealand and India over a period of four years, we identified informal, implicit, transient, and spontaneous roles that make Agile teams self-organizing. These roles-Mentor, Coordinator, Translator, Champion, Promoter, and Terminator-are focused toward providing initial guidance and encouraging continued adherence to Agile methods, effectively managing customer expectations and coordinating customer collaboration, securing and sustaining senior management support, and identifying and removing team members threatening the self-organizing ability of the team. Understanding these roles will help software development teams and their managers better comprehend and execute their roles and responsibilities as a self-organizing team.",0098-5589;00985589,,10.1109/TSE.2012.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197202,Agile software development;Self-organizing;grounded theory;software engineering;team roles,Collaboration;Organizations;Organizing;Programming;Software;Software engineering,knowledge management;software management;software prototyping;team working,India;New Zealand;agile software development teams;autonomous groups;champion role;complex-adaptive system examples;coordinator role;customer collaboration coordination;customer expectation management;grounded theory research;knowledge management agents;mentor role;organizational theories enablers;promoter role;self-organizing roles;self-organizing teams;senior management support security;senior management support sustainability;socio-technical systems;software engineering;terminator role;translator role,,31,,115,,,20120508,13-Mar,,IEEE,IEEE Journals & Magazines,,22
"Proactive Self-Adaptation for Improving the Reliability of Mission-Critical, Embedded, and Mobile Software",D. Cooray; E. Kouroshfar; S. Malek; R. Roshandel,,IEEE Transactions on Software Engineering,20131122,2013,39,12,1714,1735,"Embedded and mobile software systems are marked with a high degree of unpredictability and dynamism in the execution context. At the same time, such systems are often mission-critical, meaning that they need to satisfy strict reliability requirements. Most current software reliability analysis approaches are not suitable for these types of software systems, as they do not take the changes in the execution context of the system into account. We propose an approach geared to such systems which continuously furnishes refined reliability predictions at runtime by incorporating various sources of information, including the execution context of the system. The reliability predictions are leveraged to proactively place the software in the (near-)optimal configuration with respect to changing conditions. Our approach considers two representative architectural reconfiguration decisions that impact the system's reliability: reallocation of components to processes and changing the number of component replicas. We have realized the approach as part of a framework intended for mission-critical settings, called REsilient SItuated SofTware system (RESIST), and evaluated it using a mobile emergency response system.",0098-5589;00985589,,10.1109/TSE.2013.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574866,Context awareness;mobility;reliability;self-adaptive systems;software architecture,Computer architecture;Context awareness;Mobile communication;Reliability engineering;Software architecture;Software reliability,embedded systems;mobile computing;software architecture;software reliability,RESIST approach;architectural reconfiguration decisions;component reallocation;component replicas;dynamism degree;embedded software;execution context;mission-critical software;mobile emergency response system;mobile software;proactive self-adaptation;reliability requirements;resilient situated software system;software reliability analysis approach;unpredictability degree,,5,,52,,,20130805,Dec. 2013,,IEEE,IEEE Journals & Magazines,,21
Whitening SOA Testing via Event Exposure,C. Ye; H. A. Jacobsen,"University of Toronto, Toronto",IEEE Transactions on Software Engineering,20130925,2013,39,10,1444,1465,"Whitening the testing of service-oriented applications can provide service consumers confidence on how well an application has been tested. However, to protect business interests of service providers and to prevent information leakage, the implementation details of services are usually invisible to service consumers. This makes it challenging to determine the test coverage of a service composition as a whole and design test cases effectively. To address this problem, we propose an approach to whiten the testing of service compositions based on events exposed by services. By deriving event interfaces to explore only necessary test coverage information from service implementations, our approach allows service consumers to determine test coverage based on selected events exposed by services at runtime without releasing the service implementation details. We also develop an approach to design test cases effectively based on event interfaces concerning both effectiveness and information leakage. The experimental results show that our approach outperforms existing testing approaches for service compositions with up to 49 percent more test coverage and an up to 24 percent higher fault-detection rate. Moreover, our solution can trade off effectiveness, efficiency, and information leakage for test case generation.",0098-5589;00985589,,10.1109/TSE.2013.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495456,Web service composition;event interface;events;white-box testing,Books;Catalogs;Jacobian matrices;Runtime;Service-oriented architecture;Testing,Web services;program testing;service-oriented architecture,SOA testing whitening;Web services;event exposure;fault-detection rate;information leakage;service composition;service consumers;service providers;service-oriented architecture;test case design approach;test case generation;test coverage,,9,,84,,,20130408,Oct. 2013,,IEEE,IEEE Journals & Magazines,,21
MADMatch: Many-to-Many Approximate Diagram Matching for Design Comparison,S. Kpodjedo; F. Ricca; P. Galinier; G. Antoniol; Y. G. GuÕ©hÕ©neuc,"Ecole Polytechnique de Montreal, Montreal",IEEE Transactions on Software Engineering,20130725,2013,39,8,1090,1111,"Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms.",0098-5589;00985589,,10.1109/TSE.2013.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464271,Diagram differencing;approximate graph matching;identifier splitting;search-based software engineering,Algorithm design and analysis;Optimization;Scalability;Software;Software algorithms;Software engineering;Unified modeling language,graph theory;optimisation;search problems;software engineering,AURA algorithm;ETGM;MADMatch approach;PLTSDiff algorithm;UMLDiff algorithm;design comparison;design evolution analysis;error-tolerant graph matching;lexical information;many-to-many approximate diagram matching approach;model comparison;optimization problem;software engineering;structural information;tabu search,,6,,32,,,20130218,Aug. 2013,,IEEE,IEEE Journals & Magazines,,21
Session Reliability of Web Systems under Heavy-Tailed Workloads: An Approach Based on Design and Analysis of Experiments,N. Janevski; K. Goseva-Popstojanova,"West Virginia University, Morgantown",IEEE Transactions on Software Engineering,20130725,2013,39,8,1157,1178,"While workload characterization and performance of web systems have been studied extensively, reliability has received much less attention. In this paper, we propose a framework for session reliability modeling which integrates the user view represented by the session layer and the system view represented by the service layer. A unique characteristic of the session layer is that, in addition to the user navigation patterns, it incorporates the session length in number of requests and allows us to account for heavy-tailed workloads shown to exist in real web systems. The service layer is focused on the request reliability as it is observed at the service provider side. It considers the multifier web server architecture and the way components interact in serving each request. Within this framework, we develop a session reliability model and solve it using simulation. Instead of the traditional one-factor-at-a-time sensitivity analysis, we use statistical design and analysis of experiments, which allow us to identify the factors and interactions that have statistically significant effect on session reliability. Our findings indicate that session reliability, which accounts for the distribution of failed requests within sessions, provides better representation of the user perceived quality than the request-based reliability.",0098-5589;00985589,,10.1109/TSE.2013.3,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6409359,Internet applications;Reliability;modeling and prediction;simulation;statistical methods;web servers,Analytical models;Availability;Navigation;Reliability engineering;Software reliability;Web servers,Internet;design of experiments;sensitivity analysis;user interfaces,Web system;analysis-of-experiment;design-of-experiment;heavy-tailed workload;multifier Web server architecture;request reliability;request-based reliability;sensitivity analysis;service layer;session layer;session reliability;system view;user navigation pattern;user perceived quality;user view;workload characterization,,2,,58,,,20130110,Aug. 2013,,IEEE,IEEE Journals & Magazines,,21
Verifying Linearizability via Optimized Refinement Checking,Y. Liu; W. Chen; Y. A. Liu; J. Sun; S. J. Zhang; J. S. Dong,"Nanyang Technological University, Singapore",IEEE Transactions on Software Engineering,20130626,2013,39,7,1018,1039,"Linearizability is an important correctness criterion for implementations of concurrent objects. Automatic checking of linearizability is challenging because it requires checking that: (1) All executions of concurrent operations are serializable, and (2) the serialized executions are correct with respect to the sequential semantics. In this work, we describe a method to automatically check linearizability based on refinement relations from abstract specifications to concrete implementations. The method does not require that linearization points in the implementations be given, which is often difficult or impossible. However, the method takes advantage of linearization points if they are given. The method is based on refinement checking of finite-state systems specified as concurrent processes with shared variables. To tackle state space explosion, we develop and apply symmetry reduction, dynamic partial order reduction, and a combination of both for refinement checking. We have built the method into the PAT model checker, and used PAT to automatically check a variety of implementations of concurrent objects, including the first algorithm for scalable nonzero indicators. Our system is able to find all known and injected bugs in these implementations.",0098-5589;00985589,,10.1109/TSE.2012.82,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363443,Linearizability;PAT;model checking;refinement,Educational institutions;Electronic mail;History;Optimization;Semantics;Sun,concurrency control;formal specification;program debugging;program verification;programming language semantics;refinement calculus,PAT model checker;automatic linearizability checking;bug detection;concurrent objects;concurrent operations;dynamic partial-order reduction;finite-state systems;linearizability verification;linearization points;optimized refinement checking;refinement relations;scalable nonzero indicators;sequential semantics;serialized executions;shared variables;symmetry reduction,,8,,62,,,20121129,13-Jul,,IEEE,IEEE Journals & Magazines,,21
A decentralized self-adaptation mechanism for service-based applications in the cloud,V. Nallur; R. Bahsoon,"University of Birmingham, Birmingham",IEEE Transactions on Software Engineering,20130429,2013,39,5,591,612,"Cloud computing, with its promise of (almost) unlimited computation, storage, and bandwidth, is increasingly becoming the infrastructure of choice for many organizations. As cloud offerings mature, service-based applications need to dynamically recompose themselves to self-adapt to changing QoS requirements. In this paper, we present a decentralized mechanism for such self-adaptation, using market-based heuristics. We use a continuous double-auction to allow applications to decide which services to choose, among the many on offer. We view an application as a multi-agent system and the cloud as a marketplace where many such applications self-adapt. We show through a simulation study that our mechanism is effective for the individual application as well as from the collective perspective of all applications adapting at the same time.",0098-5589;00985589,,10.1109/TSE.2012.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249687,Self-adaptation;market based;multi-agent systems,Adaptation models;Cloud computing;Measurement;Pricing;Quality of service;Reliability;Resource management,cloud computing;electronic commerce;multi-agent systems;quality of service,QoS requirements;cloud computing;continuous double-auction;decentralized self-adaptation mechanism;market-based heuristics;multiagent system;service-based applications,,28,,56,,,20120726,13-May,,IEEE,IEEE Journals & Magazines,,21
The Effects of Test-Driven Development on External Quality and Productivity: A Meta-Analysis,Y. Rafique; V. B. MiÅçi—_,"Ryerson University, Toronto",IEEE Transactions on Software Engineering,20130523,2013,39,6,835,856,"This paper provides a systematic meta-analysis of 27 studies that investigate the impact of Test-Driven Development (TDD) on external code quality and productivity. The results indicate that, in general, TDD has a small positive effect on quality but little to no discernible effect on productivity. However, subgroup analysis has found both the quality improvement and the productivity drop to be much larger in industrial studies in comparison with academic studies. A larger drop of productivity was found in studies where the difference in test effort between the TDD and the control group's process was significant. A larger improvement in quality was also found in the academic studies when the difference in test effort is substantial; however, no conclusion could be derived regarding the industrial studies due to the lack of data. Finally, the influence of developer experience and task size as moderator variables was investigated, and a statistically significant positive correlation was found between task size and the magnitude of the improvement in quality.",0098-5589;00985589,,10.1109/TSE.2012.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197200,Test-driven development;agile software development;code quality;meta-analysis;programmer productivity,Computational modeling;Process control;Productivity;Programming;Size measurement;Testing,program testing;software development management;software quality,TDD;code productivity;code quality;quality improvement;subgroup analysis;systematic meta analysis;test driven development,,19,,71,,,20120508,13-Jun,,IEEE,IEEE Journals & Magazines,,21
Alloy Meets the Algebra of Programming: A Case Study,J. N. Oliveira; M. A. Ferreira,"University of Minho, Braga",IEEE Transactions on Software Engineering,20130225,2013,39,3,305,326,"Relational algebra offers to software engineering the same degree of conciseness and calculational power as linear algebra in other engineering disciplines. Binary relations play the role of matrices with similar emphasis on multiplication and transposition. This matches with Alloy's lemma _ÑÒeverything is a relation_Ñù and with the relational basis of the Algebra of Programming (AoP). Altogether, it provides a simple and coherent approach to checking and calculating programs from abstract models. In this paper, we put Alloy and the Algebra of Programming together in a case study originating from the Verifiable File System mini-challenge put forward by Joshi and Holzmann: verifying the refinement of an abstract file store model into a journaled (Flash) data model catering to wear leveling and recovery from power loss. Our approach relies on diagrams to graphically express typed assertions. It interweaves model checking (in Alloy) with calculational proofs in a way which offers the best of both worlds. This provides ample evidence of the positive impact in software verification of Alloy's focus on relations, complemented by induction-free proofs about data structures such as stores and lists.",0098-5589;00985589,,10.1109/TSE.2012.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6155724,Model checking;algebra of programming;grand challenges in computing;software verification,Calculus;Cognition;Matrices;Metals;Programming;Software,formal verification;mathematics computing;relational algebra;software engineering,Alloys lemma;AoP;algebra of programming;calculating programs;data structures;linear algebra;model checking;relational algebra;relational basis;software engineering;software verification;verifiable file system,,2,,53,,,20120221,13-Mar,,IEEE,IEEE Journals & Magazines,,21
Systematic Elaboration of Scalability Requirements through Goal-Obstacle Analysis,L. Duboc; E. Letier; D. S. Rosenblum,"State University of Rio de Janeiro, Rio de Janeiro",IEEE Transactions on Software Engineering,20121228,2013,39,1,119,140,"Scalability is a critical concern for many software systems. Despite the recognized importance of considering scalability from the earliest stages of development, there is currently little support for reasoning about scalability at the requirements level. This paper presents a goal-oriented approach for eliciting, modeling, and reasoning about scalability requirements. The approach consists of systematically identifying scalability-related obstacles to the satisfaction of goals, assessing the likelihood and severity of these obstacles, and generating new goals to deal with them. The result is a consolidated set of requirements in which important scalability concerns are anticipated through the precise, quantified specification of scaling assumptions and scalability goals. The paper presents results from applying the approach to a complex, large-scale financial fraud detection system.",0098-5589;00985589,,10.1109/TSE.2012.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152130,KAOS;Requirements/specifications;analysis;goal-oriented requirements engineering;performance measures;quality analysis and evaluation;scalability,Analytical models;Batch production systems;Educational institutions;Natural languages;Scalability;Software,financial data processing;fraud;large-scale systems;reasoning about programs;systems analysis,complex large-scale financial fraud detection system;goal satisfaction;goal-obstacle analysis;goal-oriented requirements engineering;reasoning about scalability;scalability requirement elicitation;scalability requirement modeling;software systems;systematic elaboration;systematic scalability-related obstacle identification,,10,,56,,,20120214,Jan. 2013,,IEEE,IEEE Journals & Magazines,,21
Performance Specification and Evaluation with Unified Stochastic Probes and Fluid Analysis,R. A. Hayden; J. T. Bradley; A. Clark,"Imperial College London, London",IEEE Transactions on Software Engineering,20121228,2013,39,1,97,118,"Rapid and accessible performance evaluation of complex software systems requires two critical features: the ability to specify useful performance metrics easily and the capability to analyze massively distributed architectures, without recourse to large compute clusters. We present the unified stochastic probe, a performance specification mechanism for process algebra models that combines many existing ideas: state and action-based activation, location-based specification, many-probe specification, and immediate signaling. These features, between them, allow the precise and compositional construction of complex performance measurements. The paper shows how a subset of the stochastic probe language can be used to specify common response-time measures in massive process algebra models. The second contribution of the paper is to show how these response-time measures can be analyzed using so-called fluid techniques to produce rapid results. In doing this, we extend the fluid approach to incorporate immediate activities and a new type of response-time measure. Finally, we calculate various response-time measurements on a complex distributed wireless network of O(10<sup>129</sup>) states in size.",0098-5589;00985589,,10.1109/TSE.2012.1,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133297,Performance modeling;fluid approximation;measurement probes;passage-time analysis;performance evaluation tools;stochastic process algebra,Algebra;Analytical models;Computational modeling;Probes;Semantics;Stochastic processes;Syntactics,formal specification;process algebra;software metrics;software performance evaluation,action-based activation;common response-time measure;complex distributed wireless network;fluid analysis;immediate signaling;location-based specification;many-probe specification;performance evaluation mechanism;performance metrics;performance specification mechanism;process algebra model;software system;state-based activation;stochastic probe language;unified stochastic probes,,7,,29,,,20120117,Jan. 2013,,IEEE,IEEE Journals & Magazines,,21
Featured Transition Systems: Foundations for Verifying Variability-Intensive Systems and Their Application to LTL Model Checking,A. Classen; M. Cordy; P. Y. Schobbens; P. Heymans; A. Legay; J. F. Raskin,"University of Namur (FUNDP), Namur",IEEE Transactions on Software Engineering,20130725,2013,39,8,1069,1089,"The premise of variability-intensive systems, specifically in software product line engineering, is the ability to produce a large family of different systems efficiently. Many such systems are critical. Thorough quality assurance techniques are thus required. Unfortunately, most quality assurance techniques were not designed with variability in mind. They work for single systems, and are too costly to apply to the whole system family. In this paper, we propose an efficient automata-based approach to linear time logic (LTL) model checking of variability-intensive systems. We build on earlier work in which we proposed featured transitions systems (FTSs), a compact mathematical model for representing the behaviors of a variability-intensive system. The FTS model checking algorithms verify all products of a family at once and pinpoint those that are faulty. This paper complements our earlier work, covering important theoretical aspects such as expressiveness and parallel composition as well as more practical things like vacuity detection and our logic feature LTL. Furthermore, we provide an in-depth treatment of the FTS model checking algorithm. Finally, we present SNIP, a new model checker for variability-intensive systems. The benchmarks conducted with SNIP confirm the speedups reported previously.",0098-5589;00985589,,10.1109/TSE.2012.86,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389685,Formal methods;features;model checking;software product lines;variability;verification,Automata;Labeling;Quality assurance;Semantics;Software;Unified modeling language,automata theory;formal logic;formal verification;software quality,FTS model checking algorithm;LTL model checking;SNIP;automata-based approach;featured transition systems;linear time logic model checking;mathematical model;model checker;quality assurance techniques;software product line engineering;variability-intensive system verification,,48,,74,,,20121220,Aug. 2013,,IEEE,IEEE Journals & Magazines,,20
Proactive and Reactive Runtime Service Discovery: A Framework and Its Evaluation,A. Zisman; G. Spanoudakis; J. Dooley; I. Siveroni,"City University London, London",IEEE Transactions on Software Engineering,20130626,2013,39,7,954,974,"The identification of services during the execution of service-based applications to replace services in them that are no longer available and/or fail to satisfy certain requirements is an important issue. In this paper, we present a framework to support runtime service discovery. This framework can execute service discovery queries in pull and push mode. In pull mode, it executes queries when a need for finding a replacement service arises. In push mode, queries are subscribed to the framework to be executed proactively and, in parallel with the operation of the application, to identify adequate services that could be used if the need for replacing a service arises. Hence, the proactive (push) mode of query execution makes it more likely to avoid interruptions in the operation of service-based applications when a service in them needs to be replaced at runtime. In both modes of query execution, the identification of services relies on distance-based matching of structural, behavioral, quality, and contextual characteristics of services and applications. A prototype implementation of the framework has been developed and an evaluation was carried out to assess the performance of the framework. This evaluation has shown positive results, which are discussed in the paper.",0098-5589;00985589,,10.1109/TSE.2012.84,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6375710,Web-services discovery;application development in services;composite web services;context-aware QoS model,Context;Database languages;Educational institutions;Informatics;Runtime;Servers;Unified modeling language,Web services;quality of service;query processing;software quality;ubiquitous computing,behavioral characteristics;composite Web-services discovery;context-aware QoS model;contextual characteristics;distance-based matching;proactive runtime service discovery;pull mode;push mode;quality characteristics;reactive runtime service discovery;replacement service;service discovery queries execution;service identification;service-based applications;structural characteristics,,12,,62,,,20121210,13-Jul,,IEEE,IEEE Journals & Magazines,,20
Locating Need-to-Externalize Constant Strings for Software Internationalization with Generalized String-Taint Analysis,X. Wang; L. Zhang; T. Xie; H. Mei; J. Sun,"Peking University, Beijing",IEEE Transactions on Software Engineering,20130326,2013,39,4,516,536,"Nowadays, a software product usually faces a global market. To meet the requirements of different local users, the software product must be internationalized. In an internationalized software product, user-visible hard-coded constant strings are externalized to resource files so that local versions can be generated by translating the resource files. In many cases, a software product is not internationalized at the beginning of the software development process. To internationalize an existing product, the developers must locate the user-visible constant strings that should be externalized. This locating process is tedious and error-prone due to 1) the large number of both user-visible and non-user-visible constant strings and 2) the complex data flows from constant strings to the Graphical User Interface (GUI). In this paper, we propose an automatic approach to locating need-to-externalize constant strings in the source code of a software product. Given a list of precollected API methods that output values of their string argument variables to the GUI and the source code of the software product under analysis, our approach traces from the invocation sites (within the source code) of these methods back to the need-to-externalize constant strings using generalized string-taint analysis. In our empirical evaluation, we used our approach to locate need-to-externalize constant strings in the uninternationalized versions of seven real-world open source software products. The results of our evaluation demonstrate that our approach is able to effectively locate need-to-externalize constant strings in uninternationalized software products. Furthermore, to help developers understand why a constant string requires translation and properly translate the need-to-externalize strings, we provide visual representation of the string dependencies related to the need-to-externalize strings.",0098-5589;00985589,,10.1109/TSE.2012.40,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216383,Software internationalization;need-to-externalize constant strings;string-taint analysis,Globalization;Graphical user interfaces;Java;Libraries;Production;Prototypes;Software,,,,4,,31,,,20120612,13-Apr,,IEEE,IEEE Journals & Magazines,,20
Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers,K. Dejaeger; T. Verbraken; B. Baesens,"Katholieke Universiteit Leuven, Leuven",IEEE Transactions on Software Engineering,20130124,2013,39,2,237,257,"Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of DemsÍÔar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.",0098-5589;00985589,,10.1109/TSE.2012.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175912,Bayesian networks;Software fault prediction;classification;comprehensibility,Bayesian methods;Capability maturity model;Machine learning;Measurement;Predictive models;Probability distribution;Software,Markov processes;belief networks;feature extraction;learning (artificial intelligence);pattern classification;prediction theory;program testing;software fault tolerance;statistical analysis,AUC;BN classifiers;BN theory;Bayesian network classifiers;Demsar;Markov blanket principle;Naive Bayes classifier;citing predictive performance;faulty software code;feature selection;introduced H-measure;machine learning literature;model selection;predictive performance;software development;software fault prediction models;software testing;statistical framework,,49,,109,,,20120403,Feb. 2013,,IEEE,IEEE Journals & Magazines,,20
Centroidal Voronoi Tessellations- A New Approach to Random Testing,A. Shahbazi; A. F. Tappenden; J. Miller,"University of Alberta, Edmonton",IEEE Transactions on Software Engineering,20130124,2013,39,2,163,183,"Although Random Testing (RT) is low cost and straightforward, its effectiveness is not satisfactory. To increase the effectiveness of RT, researchers have developed Adaptive Random Testing (ART) and Quasi-Random Testing (QRT) methods which attempt to maximize the test case coverage of the input domain. This paper proposes the use of Centroidal Voronoi Tessellations (CVT) to address this problem. Accordingly, a test case generation method, namely, Random Border CVT (RBCVT), is proposed which can enhance the previous RT methods to improve their coverage of the input space. The generated test cases by the other methods act as the input to the RBCVT algorithm and the output is an improved set of test cases. Therefore, RBCVT is not an independent method and is considered as an add-on to the previous methods. An extensive simulation study and a mutant-based software testing investigation have been performed to demonstrate the effectiveness of RBCVT against the ART and QRT methods. Results from the experimental frameworks demonstrate that RBCVT outperforms previous methods. In addition, a novel search algorithm has been incorporated into RBCVT reducing the order of computational complexity of the new approach. To further analyze the RBCVT method, randomness analysis was undertaken demonstrating that RBCVT has the same characteristics as ART methods in this regard.",0098-5589;00985589,,10.1109/TSE.2012.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165316,Adaptive random testing;P-measure;centroidal Voronoi tessellation;random testing;software testing;test case generation;test strategies,Generators;Power capacitors;Runtime;Software algorithms;Software testing;Subspace constraints,computational complexity;computational geometry;program testing,ART method;QRT method;RBCVT algorithm;adaptive random testing method;centroidal Voronoi tessellations;computational complexity;mutant-based software testing;quasi-random testing method;random border CVT;randomness analysis;search algorithm;software defects;test case generation method,,20,,68,,,20120306,Feb. 2013,,IEEE,IEEE Journals & Magazines,,20
Integer Linear Programming-Based Property Checking for Asynchronous Reactive Systems,S. Leue; W. Wei,"University of Konstanz, Konstanz",IEEE Transactions on Software Engineering,20130124,2013,39,2,216,236,"Asynchronous reactive systems form the basis of a wide range of software systems, for instance in the telecommunications domain. It is highly desirable to rigorously show that these systems are correctly designed. However, traditional formal approaches to the verification of these systems are often difficult because asynchronous reactive systems usually possess extremely large or even infinite state spaces. We propose an integer linear program (ILP) solving-based property checking framework that concentrates on the local analysis of the cyclic behavior of each individual component of a system. We apply our framework to the checking of the buffer boundedness and livelock freedom properties, both of which are undecidable for asynchronous reactive systems with an infinite state space. We illustrate the application of the proposed checking methods to Promela, the input language of the SPIN model checker. While the precision of our framework remains an issue, we propose a counterexample guided abstraction refinement procedure based on the discovery of dependences among control flow cycles. We have implemented prototype tools with which we obtained promising experimental results on real-life system models.",0098-5589;00985589,,10.1109/TSE.2011.1,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680910,Promela;Software verification;UML;abstraction;asynchronous communication;buffer boundedness;control flow cycles;counterexamples;cycle dependences;formal methods;integer linear programming;livelock freedom;property checking;refinement;static analysis,Analytical models;Complexity theory;Cost accounting;Integer linear programming;Mathematical model;Message passing;Unified modeling language,data structures;formal languages;formal verification;integer programming;linear programming;program diagnostics;state-space methods,ILP solving-based property checking framework;Promela;SPIN model checker input language;asynchronous reactive systems;buffer boundedness;control flow cycles;counterexample guided abstraction refinement procedure;cyclic behavior;formal approaches;individual component;infinite state spaces;integer linear programming-based property checking;livelock freedom properties;real-life system models;software systems;telecommunications domain,,0,,73,,,20110106,Feb. 2013,,IEEE,IEEE Journals & Magazines,,20
Compositional Verification for Hierarchical Scheduling of Real-Time Systems,L. Carnevali; A. Pinzuti; E. Vicario,Universit&#x00E0; di Firenze,IEEE Transactions on Software Engineering,20130429,2013,39,5,638,657,"Hierarchical Scheduling (HS) techniques achieve resource partitioning among a set of real-time applications, providing reduction of complexity, confinement of failure modes, and temporal isolation among system applications. This facilitates compositional analysis for architectural verification and plays a crucial role in all industrial areas where high-performance microprocessors allow growing integration of multiple applications on a single platform. We propose a compositional approach to formal specification and schedulability analysis of real-time applications running under a Time Division Multiplexing (TDM) global scheduler and preemptive Fixed Priority (FP) local schedulers, according to the ARINC-653 standard. As a characterizing trait, each application is made of periodic, sporadic, and jittering tasks with offsets, jitters, and nondeterministic execution times, encompassing intra-application synchronizations through semaphores and mailboxes and interapplication communications among periodic tasks through message passing. The approach leverages the assumption of a TDM partitioning to enable compositional design and analysis based on the model of preemptive Time Petri Nets (pTPNs), which is expressly extended with a concept of Required Interface (RI) that specifies the embedding environment of an application through sequencing and timing constraints. This enables exact verification of intra-application constraints and approximate but safe verification of interapplication constraints. Experimentation illustrates results and validates their applicability on two challenging workloads in the field of safety-critical avionic systems.",0098-5589;00985589,,10.1109/TSE.2012.54,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264049,ARINC-653;Real-time systems;compositional verification;hierarchical scheduling;preemptive fixed priority;preemptive time Petri nets;symbolic state-space analysis;time division multiplexing,Complexity theory;Job shop scheduling;Petri nets;Real time systems;Resource management;Time division multiplexing;Timing,Petri nets;aerospace computing;formal verification;message passing;microprocessor chips;resource allocation;safety-critical software;scheduling,ARINC-653 standard;FP local scheduler;HS technique;RI concept;TDM global scheduler;architectural verification;compositional analysis;compositional design;compositional verification;formal specification;hierarchical scheduling;high-performance microprocessor;interapplication communication;intra-application synchronization;jittering task;mailbox;message passing;nondeterministic execution time;periodic task;preemptive fixed priority local scheduler;preemptive time Petri nets model;realtime system;required interface concept;resource partitioning;safety-critical avionic system;schedulability analysis;semaphore;sequencing constraint;sporadic task;time division multiplexing;timing constraint,,14,,41,,,20120809,13-May,,IEEE,IEEE Journals & Magazines,,19
"Task Environment Complexity, Global Team Dispersion, Process Capabilities, and Coordination in Software Development",G. Lee; J. A. Espinosa; W. H. DeLone,"American Univ., Washington, DC, USA",IEEE Transactions on Software Engineering,20131122,2013,39,12,1753,1771,"Software development teams are increasingly global. Team members are separated by multiple boundaries such as geographic location, time zone, culture, and organization, presenting substantial coordination challenges. Global software development becomes even more challenging when user requirements change dynamically. However, little empirical research has investigated how team dispersion across multiple boundaries and user requirements dynamism, which collectively increase task environment complexity, influence team coordination and software development success in the global context. Further, we have a limited understanding of how software process capabilities such as rigor, standardization, agility, and customizability mitigate the negative effects of global team dispersion and user requirements dynamism. To address these important issues, we test a set of relevant hypotheses using field survey data obtained from both project managers and stakeholders. Our results show that global team dispersion and user requirements dynamism have a negative effect on coordination effectiveness. We find that the negative effect of global team dispersion on coordination effectiveness decreases as process standardization increases and that the negative effect of user requirements dynamism on coordination effectiveness decreases as process agility increases. We find that coordination effectiveness has a positive effect on global software development success in terms of both process and product aspects.",0098-5589;00985589,,10.1109/TSE.2013.40,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583162,Global boundaries;global software development;software process capability;task environment complexity;team coordination;team dispersion;user requirements dynamism,Complexity theory;Dispersion;Global communication;Globalization;Process capability;Software development;User centered design,software development management;team working,agility capability;coordination;coordination effectiveness;culture;customizability capability;geographic location;global software development;global team dispersion;organization;process aspect;process capabilities;product aspect;rigor capability;software development teams;standardization capability;task environment complexity;time zone;user requirements dynamism,,5,,101,,,20130821,Dec. 2013,,IEEE,IEEE Journals & Magazines,,18
OBEY: Optimal Batched Refactoring Plan Execution for Class Responsibility Redistribution,H. C. Jiau; L. W. Mar; J. C. Chen,"National Cheng Kung University, Tainan",IEEE Transactions on Software Engineering,20130828,2013,39,9,1245,1263,"The redistribution of class responsibilities is a common reengineering practice in object-oriented (OO) software evolution. During the redistribution, developers frequently construct batched refactoring plans for moving multiple methods and fields among various classes. With an objective of carefully maintaining the cohesion and coupling degree of the class design, executing a batched refactoring plan without introducing any objective-violating side effect into the refactored code is essential. However, using most refactoring engines for batched refactoring plan execution introduces coupling-increasing Middle Man bad smell in the final refactored code and therefore makes the refactoring execution suboptimal in achieving the redistribution objective. This work proposes Obey, a methodology for optimal batched refactoring plan execution. Obey analyzes a batched refactoring plan, identifies Middle Man symptoms that cause suboptimal execution, and renovates the plan for optimal execution. We have conducted an empirical study on three open-source software projects to confirm the effectiveness of Obey in a practical context.",0098-5589;00985589,,10.1109/TSE.2013.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493333,Reengineering;batched refactoring execution;change impact analysis;class responsibility redistribution;optimization,Context;Couplings;Engines;Measurement;Optimization;Software systems,object-oriented programming;software maintenance,OBEY methodology;OO software evolution;class responsibility redistribution;coupling-increasing middle man bad smell;object-oriented software evolution;open-source software project;optimal batched refactoring plan execution;refactoring engine;software reengineering practice,,1,,74,,,20130403,Sept. 2013,,IEEE,IEEE Journals & Magazines,,18
A Uniform Representation of Hybrid Criteria for Regression Testing,S. Sampath; R. Bryce; A. M. Memon,"University of Maryland, Baltimore",IEEE Transactions on Software Engineering,20130925,2013,39,10,1326,1344,"Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.",0098-5589;00985589,,10.1109/TSE.2013.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6484067,GUI testing;Test case prioritization;hybrid test criteria;test criteria;web testing,Educational institutions;Fault detection;Genetic algorithms;Loss measurement;Minimization;Testing;Vectors,program testing;regression analysis,hybrid test criteria;merge-and-rank formulations;rank-merge-and-choice hybrid combination;regression testing;test case prioritization;uniform representation,,10,,74,,,20130321,Oct. 2013,,IEEE,IEEE Journals & Magazines,,18
Patterns of Knowledge in API Reference Documentation,W. Maalej; M. P. Robillard,"University of Hamburg, Germany",IEEE Transactions on Software Engineering,20130828,2013,39,9,1264,1282,"Reading reference documentation is an important part of programming with application programming interfaces (APIs). Reference documentation complements the API by providing information not obvious from the API syntax. To improve the quality of reference documentation and the efficiency with which the relevant information it contains can be accessed, we must first understand its content. We report on a study of the nature and organization of knowledge contained in the reference documentation of the hundreds of APIs provided as a part of two major technology platforms: Java SDK 6 and .NET 4.0. Our study involved the development of a taxonomy of knowledge types based on grounded methods and independent empirical validation. Seventeen trained coders used the taxonomy to rate a total of 5,574 randomly sampled documentation units to assess the knowledge they contain. Our results provide a comprehensive perspective on the patterns of knowledge in API documentation: observations about the types of knowledge it contains and how this knowledge is distributed throughout the documentation. The taxonomy and patterns of knowledge we present in this paper can be used to help practitioners evaluate the content of their API documentation, better organize their documentation, and limit the amount of low-value content. They also provide a vocabulary that can help structure and facilitate discussions about the content of APIs.",0098-5589;00985589,,10.1109/TSE.2013.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6473801,.NET;API documentation;Java;content analysis;data mining;empirical study;grounded method;pattern mining;software documentation,Documentation;Encoding;Java;Reliability;Sociology;Software;Taxonomy,application program interfaces;learning (artificial intelligence);pattern classification,.NET 4.0 API;API reference documentation;Java SDK 6 API;application program interface;knowledge nature;knowledge organization;knowledge pattern;knowledge taxonomy;reference documentation efficiency;reference documentation quality,,22,,35,,,20130307,Sept. 2013,,IEEE,IEEE Journals & Magazines,,18
An Empirical Evaluation of Mutation Testing for Improving the Test Quality of Safety-Critical Software,R. Baker; I. Habli,"Aero Engine Controls, Birmingham",IEEE Transactions on Software Engineering,20130523,2013,39,6,787,805,"Testing provides a primary means for assuring software in safety-critical systems. To demonstrate, particularly to a certification authority, that sufficient testing has been performed, it is necessary to achieve the test coverage levels recommended or mandated by safety standards and industry guidelines. Mutation testing provides an alternative or complementary method of measuring test sufficiency, but has not been widely adopted in the safety-critical industry. In this study, we provide an empirical evaluation of the application of mutation testing to airborne software systems which have already satisfied the coverage requirements for certification. Specifically, we apply mutation testing to safety-critical software developed using high-integrity subsets of C and Ada, identify the most effective mutant types, and analyze the root causes of failures in test cases. Our findings show how mutation testing could be effective where traditional structural coverage analysis and manual peer review have failed. They also show that several testing issues have origins beyond the test activity, and this suggests improvements to the requirements definition and coding process. Our study also examines the relationship between program characteristics and mutation survival and considers how program size can provide a means for targeting test areas most likely to have dormant faults. Industry feedback is also provided, particularly on how mutation testing can be integrated into a typical verification life cycle of airborne software.",0098-5589;00985589,,10.1109/TSE.2012.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298894,Mutation;certification;safety-critical software;testing;verification,Certification;Guidelines;Industries;Safety;Software systems;Testing,Ada;C language;aerospace computing;certification;integrated software;program testing;program verification;safety-critical software;software quality,Ada;C;airborne software system;certification;coding process;coverage requirement satisfaction;empirical evaluation;industry guideline;mutant type;mutation testing;safety standard;safety-critical software;software failure;software integration;software test quality;structural coverage analysis;test coverage level;test sufficiency measurement;verification life cycle,,9,,41,,,20120911,13-Jun,,IEEE,IEEE Journals & Magazines,,18
Self-Management of Adaptable Component-Based Applications,L. Rosa; L. Rodrigues; A. Lopes; M. Hiltunen; R. Schlichting,"INESC-ID and Universidade T&#x00E9;cnica de Lisboa, Lisboa",IEEE Transactions on Software Engineering,20130225,2013,39,3,403,421,"The problem of self-optimization and adaptation in the context of customizable systems is becoming increasingly important with the emergence of complex software systems and unpredictable execution environments. Here, a general framework for automatically deciding on when and how to adapt a system whenever it deviates from the desired behavior is presented. In this framework, the system's target behavior is described as a high-level policy that establishes goals for a set of performance indicators. The decision process is based on information provided independently for each component that describes the available adaptations, their impact on performance indicators, and any limitations or requirements. The technique consists of both offline and online phases. Offline, rules are generated specifying component adaptations that may help to achieve the established goals when a given change in the execution context occurs. Online, the corresponding rules are evaluated when a change occurs to choose which adaptations to perform. Experimental results using a prototype framework in the context of a web-based application demonstrate the effectiveness of this approach.",0098-5589;00985589,,10.1109/TSE.2012.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197201,Adaptive systems;autonomic computing;goal policies;self-management,Catalogs;Context;Optimization;Runtime;Software systems,fault tolerant computing;optimisation,Web based application;adaptable component based applications;autonomic computing;complex software systems;customizable systems;decision process;desired behavior;performance indicators;target behavior;unpredictable execution environments,,9,,28,,,20120508,13-Mar,,IEEE,IEEE Journals & Magazines,,18
"Resource Management for Complex, Dynamic Environments",M. S. Raunak; L. J. Osterweil,"Loyola University MD, Baltimore",IEEE Transactions on Software Engineering,20130225,2013,39,3,384,402,"This paper describes an approach to the specification and management of the agents and resources that are required to support the execution of complex systems and processes. The paper suggests that a resource should be viewed as a provider of a set of capabilities that are needed by a system or process, where that set may vary dynamically over time and with circumstances. This view of resources is defined and then made the basis for the framework of an approach to specifying, managing, and allocating resources in the presence of real-world complexity and dynamism. The ROMEO prototype resource management system is presented as an example of how this framework can be instantiated. Some case studies of the use of ROMEO to support system execution are presented and used to evaluate the framework, the ROMEO prototype, and our view of the nature of resources.",0098-5589;00985589,,10.1109/TSE.2012.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197203,Resources management;discrete event simulation;healthcare processes;process modeling,Context;Databases;Hospitals;Resource management;Software;Surgery,formal specification;software agents;software engineering,ROMEO prototype resource management system;complex environments;complex processes;complex systems;dynamic environments;resource management;software engineering,,14,,40,,,20120508,13-Mar,,IEEE,IEEE Journals & Magazines,,18
"How Programmers Debug, Revisited: An Information Foraging Theory Perspective",J. Lawrance; C. Bogart; M. Burnett; R. Bellamy; K. Rector; S. D. Fleming,"Wentworth Institute of Technology, Boston",IEEE Transactions on Software Engineering,20130124,2013,39,2,197,215,"Many theories of human debugging rely on complex mental constructs that offer little practical advice to builders of software engineering tools. Although hypotheses are important in debugging, a theory of navigation adds more practical value to our understanding of how programmers debug. Therefore, in this paper, we reconsider how people go about debugging in large collections of source code using a modern programming environment. We present an information foraging theory of debugging that treats programmer navigation during debugging as being analogous to a predator following scent to find prey in the wild. The theory proposes that constructs of scent and topology provide enough information to describe and predict programmer navigation during debugging, without reference to mental states such as hypotheses. We investigate the scope of our theory through an empirical study of 10 professional programmers debugging a real-world open source program. We found that the programmers' verbalizations far more often concerned scent-following than hypotheses. To evaluate the predictiveness of our theory, we created an executable model that predicted programmer navigation behavior more accurately than comparable models that did not consider information scent. Finally, we discuss the implications of our results for enhancing software engineering tools.",0098-5589;00985589,,10.1109/TSE.2010.111,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674060,Information foraging theory;debugging;empirical software engineering;information scent;programmer navigation;software maintenance,Approximation methods;Debugging;Navigation;Predictive models;Programming environments;Topology,cognition;program debugging;public domain software;software maintenance;topology,complex mental constructs;human debugging theories;information foraging theory;information scent constructs;navigation theory;open source code program debugging;programmer navigation behavior prediction;programmer verbalizations;programming environment;software engineering tool enhancement;topology constructs,,32,,54,,,20101223,Feb. 2013,,IEEE,IEEE Journals & Magazines,,18
Identifying Code of Individual Features in Client-Side Web Applications,J. Maras; M. Stula; J. Carlson; I. Crnkovic,"Fac. of Electr. Eng., Mech. Eng., & Naval Archit., Univ. of Split, Split, Croatia",IEEE Transactions on Software Engineering,20131122,2013,39,12,1680,1697,"Web applications are one of today's fastest growing software systems. Structurally, they are composed of two parts: the server side, used for data access and business logic, and the client side, used as a user interface. In recent years, with developers building complex interfaces, the client side is playing an increasingly important role. Unfortunately, the techniques and tools used to support development are not as advanced as in other disciplines. From the user's perspective, the client side offers a number of features that are relatively easy to distinguish. However, the same cannot be said for their implementation details. This makes the understanding, maintenance, and reuse of code difficult. The goal of the work presented in this paper is to improve reusability, maintainability, and performance of client-side web applications by identifying the code that implements a particular feature. We have evaluated the approach based on three different experiments: extracting features, extracting library functionalities, and page optimization. The evaluation shows that the method is able to identify the implementation details of individual features, and that by extracting the identified code, a considerable reduction in code size and increase in performance can be achieved.",0098-5589;00985589,,10.1109/TSE.2013.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6576749,Web applications;code extraction;feature location;program slicing,Browsers;Cascading style sheets;Codes;Feature extraction;HTML;Optimization;Web and internet services,Internet;feature extraction;user interfaces,business logic;client-side Web application maintainability;client-side Web application performance;client-side Web application reusability;data access;feature extraction;individual features code identification;library functionality extraction;page optimization;server side;software systems;user interface,,5,,35,,,20130808,Dec. 2013,,IEEE,IEEE Journals & Magazines,,17
Determining the Cause of a Design Model Inconsistency,A. Reder; A. Egyed,"Johannes Kepler Univ. Linz, Linz, Austria",IEEE Transactions on Software Engineering,20131028,2013,39,11,1531,1548,"When a software engineer finds an inconsistency in a model, then the first question is why? What caused it? Obviously, there must be an error. But where could it be? Or is the design rule erroneous and if yes then which part? The cause of an inconsistency identifies the part of the model or design rule where the error must be. We believe that the visualization of an inconsistency ought to visualize the cause. Understanding the cause is of vital importance before a repair can even be formulated. Indeed, any automation (e.g., code generation, refactoring) has to be considered with caution if it involves model elements that cause inconsistencies. This paper analyzes the basic structure of inconsistent design rules as well as their behavior during validation and presents an algorithm for computing its cause. The approach is fully automated, tool supported, and was evaluated on 14,111 inconsistencies across 29 design models. We found that our approach computes correct causes for inconsistencies, these causes are nearly always a subset of the model elements investigated by the design rules' validation (a naive cause computation approximation), and the computation is very fast (99.8 percent of the causes are computable in <; 100 ms).",0098-5589;00985589,,10.1109/TSE.2013.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6560054,Design tools and techniques;programming environments/construction tools;validation,Computational modeling;Context;Context modeling;Light emitting diodes;Maintenance engineering;Unified modeling language;Visualization,software development management,code generation;design model inconsistency;inconsistent design rules;model elements;refactoring;software engineer,,6,,33,,,20130716,Nov. 2013,,IEEE,IEEE Journals & Magazines,,17
Optimizing Ordered Throughput Using Autonomic Cloud Bursting Schedulers,S. Kailasam; N. Gnanasambandam; J. Dharanipragada; N. Sharma,"Dept. of Comput. Sci. & Eng., Indian Inst. of Technol. Madras, Chennai, India",IEEE Transactions on Software Engineering,20131028,2013,39,11,1564,1581,"Optimizing ordered throughput not only improves the system efficiency but also makes cloud bursting transparent to the user. This is critical from the perspective of user fairness in customer-facing systems, correctness in stream processing systems, and so on. In this paper, we consider optimizing ordered throughput for near real-time, data-intensive, independent computations using cloud bursting. Intercloud computation of data-intensive applications is a challenge due to large data transfer requirements, low intercloud bandwidth, and best-effort traffic on the Internet. The system model we consider is comprised of two processing stages. The first stage uses cloud bursting opportunistically for parallel processing, while the second stage (sequential) expects the output of the first stage to be in the same order as the arrival sequence. We propose three scheduling heuristics as part of an autonomic cloud bursting approach that adapt to changing workload characteristics, variation in bandwidth, and available resources to optimize ordered throughput. We also characterize the operational regimes for cloud bursting as stabilization mode versus acceleration mode, depending on the workload characteristics like the size of data to be transferred for a given compute load. The operational regime characterization helps in deciding how many instances can be optimally utilized in the external cloud.",0098-5589;00985589,,10.1109/TSE.2013.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6520852,Cloud bursting;autonomic;data-intensive;ordered throughput,Cloud computing;Optimization;Scheduling,cloud computing;fault tolerant computing;parallel processing;scheduling,Internet traffic;acceleration mode;arrival sequence;autonomic cloud bursting schedulers;bandwidth variation;customer-facing systems;data size;data-intensive applications;external cloud;intercloud bandwidth;intercloud computation;large data transfer requirements;near real-time data-intensive independent computation;operational regime characterization;ordered throughput optimization;parallel processing;scheduling heuristics;stabilization mode;stream processing system correctness;system efficiency improvement;user fairness;workload characteristics,,6,,41,,,20130527,Nov. 2013,,IEEE,IEEE Journals & Magazines,,17
Early Detection of Collaboration Conflicts and Risks,Y. Brun; R. Holmes; M. D. Ernst; D. Notkin,"University of Massachusetts, Amherst",IEEE Transactions on Software Engineering,20130925,2013,39,10,1358,1375,"Conflicts among developers' inconsistent copies of a shared project arise in collaborative development and can slow progress and decrease quality. Identifying and resolving such conflicts early can help. Identifying situations which may lead to conflicts can prevent some conflicts altogether. By studying nine open-source systems totaling 3.4 million lines of code, we establish that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. Motivated by this finding, we develop a speculative analysis technique that uses previously unexploited information from version control operations to precisely diagnose important classes of conflicts. Then, we design and implement Crystal, a publicly available tool that helps developers identify, manage, and prevent conflicts. Crystal uses speculative analysis to make concrete advice unobtrusively available to developers.",0098-5589;00985589,,10.1109/TSE.2013.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6520859,Collaborative development;Crystal;collaboration conflicts;developer awareness;speculative analysis;version control,Collaboration;Computer science;Control systems;Crystals;History;Open source software;Terminology,configuration management;groupware;program diagnostics;public domain software;software engineering,Crystal;collaboration conflicts;collaborative development;open-source systems;overlapping textual edits;publicly available tool;source code;speculative analysis technique;subsequent build-and-test failures;version control operations,,13,,48,,,20130527,Oct. 2013,,IEEE,IEEE Journals & Magazines,,17
Capsule-Based User Interface Modeling for Large-Scale Applications,D. Milicev; Z. Mijailovic,"University of Belgrade, Belgrade",IEEE Transactions on Software Engineering,20130828,2013,39,9,1190,1207,"We present a novel approach to modeling and implementing user interfaces (UI) of large business applications. The approach is based on the concept of capsule, a profiled structured class from UML which models a simple UI component or a coherent UI fragment of logically and functionally coupled components or other fragments with a clear interface. Consequently, the same modeling concept of capsule with internal structure can be reapplied recursively at successively lower levels of detail within a model, starting from high architectural modeling levels down to the lowest levels of modeling simple UI components. The interface of capsules is defined in terms of pins, while the functional coupling of capsules is specified declaratively by simply wiring their pins. Pins and wires transport messages between capsules, ensuring strict encapsulation. The approach includes a method for formal coupling of capsules' behavior with the underlying object space that provides proper impedance matching between the UI and the business logic while preserving clear separation of concerns between them. We also briefly describe an implementation of a framework that supports the proposed method, including a rich library of ready-to-use capsules, and report on our experience in applying the approach in large-scale industrial systems.",0098-5589;00985589,,10.1109/TSE.2013.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464270,Graphical user interface (GUI);Unified Modeling Language (UML);business applications;data-centric applications;information systems;model-driven development;modeling;software architecture,Buildings;Business;Complexity theory;Couplings;Object oriented modeling;Unified modeling language;User interfaces,Unified Modeling Language;business data processing;user interfaces,UI component;UI fragment;UML;Unified Modeling Language;business application;business logic;capsule concept;capsule interface;capsule-based user interface modeling;functional capsule coupling;object space;profiled structured class;ready-to-use capsule library,,3,,43,,,20130218,Sept. 2013,,IEEE,IEEE Journals & Magazines,,17
Amorphous Slicing of Extended Finite State Machines,K. Androutsopoulos; D. Clark; M. Harman; R. M. Hierons; Z. Li; L. Tratt,"University College London, London",IEEE Transactions on Software Engineering,20130626,2013,39,7,892,909,"Slicing is useful for many software engineering applications and has been widely studied for three decades, but there has been comparatively little work on slicing extended finite state machines (EFSMs). This paper introduces a set of dependence-based EFSM slicing algorithms and an accompanying tool. We demonstrate that our algorithms are suitable for dependence-based slicing. We use our tool to conduct experiments on 10 EFSMs, including benchmarks and industrial EFSMs. Ours is the first empirical study of dependence-based program slicing for EFSMs. Compared to the only previously published dependence-based algorithm, our average slice is smaller 40 percent of the time and larger only 10 percent of the time, with an average slice size of 35 percent for termination insensitive slicing.",0098-5589;00985589,,10.1109/TSE.2012.72,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374192,Slicing;extended finite state machines,Algorithm design and analysis;Approximation algorithms;Automata;Educational institutions;Electronic mail;Software algorithms;Unified modeling language,finite state machines;program slicing;software engineering,amorphous slicing;benchmarks EFSM;dependence-based EFSM slicing algorithm;dependence-based program slicing;extended finite state machine slicing;industrial EFSM;software engineering application;termination insensitive slicing,,4,,62,,,20121204,13-Jul,,IEEE,IEEE Journals & Magazines,,17
The Role of the Tester's Knowledge in Exploratory Software Testing,J. Itkonen; M. V. MÕ_ntylÕ_; C. Lassenius,"Aalto University School of Science, Espoo",IEEE Transactions on Software Engineering,20130429,2013,39,5,707,724,"We present a field study on how testers use knowledge while performing exploratory software testing (ET) in industrial settings. We video recorded 12 testing sessions in four industrial organizations, having our subjects think aloud while performing their usual functional testing work. Using applied grounded theory, we analyzed how the subjects performed tests and what type of knowledge they utilized. We discuss how testers recognize failures based on their personal knowledge without detailed test case descriptions. The knowledge is classified under the categories of domain knowledge, system knowledge, and general software engineering knowledge. We found that testers applied their knowledge either as a test oracle to determine whether a result was correct or not, or for test design, to guide them in selecting objects for test and designing tests. Interestingly, a large number of failures, windfall failures, were found outside the actual focus areas of testing as a result of exploratory investigation. We conclude that the way exploratory testers apply their knowledge for test design and failure recognition differs clearly from the test-case-based paradigm and is one of the explanatory factors of the effectiveness of the exploratory testing approach.",0098-5589;00985589,,10.1109/TSE.2012.55,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298893,Software testing;and V&V;exploratory testing;human factors;methods for SQA;test design;test execution;validation,Context;Knowledge engineering;Observers;Organizations;Software;Software testing,program testing;software engineering,ET;domain knowledge;exploratory software testing;failure recognition;functional testing;general software engineering knowledge;grounded theory;personal knowledge;system knowledge;test design;test oracle;test-case-based paradigm;tester knowledge;windfall failures,,21,,76,,,20120911,13-May,,IEEE,IEEE Journals & Magazines,,17
Reducing Features to Improve Code Change-Based Bug Prediction,S. Shivaji; E. James Whitehead; R. Akella; S. Kim,"University of California, Santa Cruz, Santa Cruz",IEEE Transactions on Software Engineering,20130326,2013,39,4,552,569,"Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.",0098-5589;00985589,,10.1109/TSE.2012.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226427,Reliability;bug prediction;feature selection;machine learning,Computer bugs;Feature extraction;History;Machine learning;Measurement;Software;Support vector machines,belief networks;learning (artificial intelligence);pattern classification;program debugging;support vector machines,SVM classifier;buggy F-measure;classification performance;classifier-based bug prediction;code change-based bug prediction;feature selection technique;machine learned feature reduction;machine learning classifier;naive Bayes classifier;software history;software project;source code file;support vector machine,,43,,53,,,20120626,13-Apr,,IEEE,IEEE Journals & Magazines,,17
Validating Second-Order Mutation at System Level,P. Reales Mateo; M. Polo Usaola; J. L. FernÕçndez AlemÕçn,"University of Castilla-La Mancha, Ciudad Real",IEEE Transactions on Software Engineering,20130326,2013,39,4,570,587,"Mutation has been recognized to be an effective software testing technique. It is based on the insertion of artificial faults in the system under test (SUT) by means of a set of mutation operators. Different operators can mutate each program statement in several ways, which may produce a huge number of mutants. This leads to very high costs for test case execution and result analysis. Several works have approached techniques for cost reduction in mutation testing, like $(n)$-order mutation where each mutant contains $(n)$ artificial faults instead of one. There are two approaches to $(n)$-order mutation: increasing the effectiveness of mutation by searching for good $(n)$-order mutants, and decreasing the costs of mutation testing by reducing the mutants set through the combination of the first-order mutants into $(n)$-order mutants. This paper is focused on the second approach. However, this second use entails a risk: the possibility of leaving undiscovered faults in the SUT, which may distort the perception of the test suite quality. This paper describes an empirical study of different combination strategies to compose second-order mutants at system level as well as a cost-risk analysis of $(n)$-order mutation at system level.",0098-5589;00985589,,10.1109/TSE.2012.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216382,Empirical evaluation;high-order mutation;mutation testing,Algorithm design and analysis;Benchmark testing;Concrete;Educational institutions;Optimization;Software testing,,,,5,,53,,,20120612,13-Apr,,IEEE,IEEE Journals & Magazines,,17
Coordination Breakdowns and Their Impact on Development Productivity and Software Failures,M. Cataldo; J. D. Herbsleb,Robert Bosch LLC,IEEE Transactions on Software Engineering,20130225,2013,39,3,343,360,"The success of software development projects depends on carefully coordinating the effort of many individuals across the multiple stages of the development process. In software engineering, modularization is the traditional technique intended to reduce the interdependencies among modules that constitute a system. Reducing technical dependencies, the theory argues, results in a reduction of work dependencies between teams developing interdependent modules. Although that research stream has been quite influential, it considers a static view of the problem of coordination in engineering activities. Building on a dynamic view of coordination, we studied the relationship between socio-technical congruence and software quality and development productivity. In order to investigate the generality of our findings, our analyses were performed on two large-scale projects from two companies with distinct characteristics in terms of product and process maturity. Our results revealed that the gaps between coordination requirements and the actual coordination activities carried out by the developers significantly increased software failures. Our analyses also showed that higher levels of congruence are associated with improved development productivity. Finally, our results showed the congruence between dependencies and coordinative actions is critical both in mature development settings as well as in novel and dynamic development contexts.",0098-5589;00985589,,10.1109/TSE.2012.32,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6205767,Metrics/measurement;organizational management and coordination;productivity;quality analysis and evaluation,Complexity theory;Context;Organizations;Productivity;Programming;Software quality,software metrics;software quality;software reliability,coordination activity;coordination breakdowns;coordination requirements;development process;development productivity;dynamic development contexts;engineering activity;interdependent modules;large-scale projects;modularization;process maturity;product maturity;socio-technical congruence;software development projects;software engineering;software failures;software quality;technical dependency;work dependency,,30,,78,,,20120529,13-Mar,,IEEE,IEEE Journals & Magazines,,17
Using Dependency Structures for Prioritization of Functional Test Suites,S. e. Z. Haidry; T. Miller,"University of Melbourne, Parkville",IEEE Transactions on Software Engineering,20130124,2013,39,2,258,275,"Test case prioritization is the process of ordering the execution of test cases to achieve a certain goal, such as increasing the rate of fault detection. Increasing the rate of fault detection can provide earlier feedback to system developers, improving fault fixing activity and, ultimately, software delivery. Many existing test case prioritization techniques consider that tests can be run in any order. However, due to functional dependencies that may exist between some test cases-that is, one test case must be executed before another-this is often not the case. In this paper, we present a family of test case prioritization techniques that use the dependency information from a test suite to prioritize that test suite. The nature of the techniques preserves the dependencies in the test ordering. The hypothesis of this work is that dependencies between tests are representative of interactions in the system under test, and executing complex interactions earlier is likely to increase the fault detection rate, compared to arbitrary test orderings. Empirical evaluations on six systems built toward industry use demonstrate that these techniques increase the rate of fault detection compared to the rates achieved by the untreated order, random orders, and test suites ordered using existing ""coarse-grained_Ñù techniques based on function coverage.",0098-5589;00985589,,10.1109/TSE.2012.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189361,Software engineering;test execution;testing and debugging,Complexity theory;Digital signal processing;Fault detection;Schedules;Software;Testing;Weight measurement,program debugging;program testing;software fault tolerance;software process improvement,dependency information;dependency structures;fault detection rate;fault fixing improvement;functional dependencies;functional test suite prioritization;software debugging;software delivery;system developers;system under test;test case execution ordering;test case prioritization technique,,12,,31,,,20120424,Feb. 2013,,IEEE,IEEE Journals & Magazines,,17
Identifying and Summarizing Systematic Code Changes via Rule Inference,M. Kim; D. Notkin; D. Grossman; G. Wilson,"The University of Texas at Austin, Austin",IEEE Transactions on Software Engineering,20121228,2013,39,1,45,62,"Programmers often need to reason about how a program evolved between two or more program versions. Reasoning about program changes is challenging as there is a significant gap between how programmers think about changes and how existing program differencing tools represent such changes. For example, even though modification of a locking protocol is conceptually simple and systematic at a code level, diff extracts scattered text additions and deletions per file. To enable programmers to reason about program differences at a high level, this paper proposes a rule-based program differencing approach that automatically discovers and represents systematic changes as logic rules. To demonstrate the viability of this approach, we instantiated this approach at two different abstraction levels in Java: first at the level of application programming interface (API) names and signatures, and second at the level of code elements (e.g., types, methods, and fields) and structural dependences (e.g., method-calls, field-accesses, and subtyping relationships). The benefit of this approach is demonstrated through its application to several open source projects as well as a focus group study with professional software engineers from a large e-commerce company.",0098-5589;00985589,,10.1109/TSE.2012.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165314,Software evolution;logic-based program representation;program differencing;rule learning,Cloning;Inference algorithms;Libraries;Semantics;Software;Syntactics;Systematics,Java;application program interfaces;electronic commerce;inference mechanisms;reasoning about programs;software engineering,API;Java;application programming interface;large e-commerce company;locking protocol;professional software engineers;reasoning about program;rule inference;rule-based program differencing approach;scattered text additions;structural dependences;systematic code,,13,,50,,,20120306,Jan. 2013,,IEEE,IEEE Journals & Magazines,,17
Supporting Domain Analysis through Mining and Recommending Features from Online Product Listings,N. Hariri; C. Castro-Herrera; M. Mirakhorli; J. Cleland-Huang; B. Mobasher,"Sch. of Comput., DePaul Univ., Chicago, IL, USA",IEEE Transactions on Software Engineering,20131122,2013,39,12,1736,1752,"Domain analysis is a labor-intensive task in which related software systems are analyzed to discover their common and variable parts. Many software projects include extensive domain analysis activities, intended to jumpstart the requirements process through identifying potential features. In this paper, we present a recommender system that is designed to reduce the human effort of performing domain analysis. Our approach relies on data mining techniques to discover common features across products as well as relationships among those features. We use a novel incremental diffusive algorithm to extract features from online product descriptions, and then employ association rule mining and the (k)-nearest neighbor machine learning method to make feature recommendations during the domain analysis process. Our feature mining and feature recommendation algorithms are quantitatively evaluated and the results are presented. Also, the performance of the recommender system is illustrated and evaluated within the context of a case study for an enterprise-level collaborative software suite. The results clearly highlight the benefits and limitations of our approach, as well as the necessary preconditions for its success.",0098-5589;00985589,,10.1109/TSE.2013.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6582404,Domain analysis;association rule mining;clustering;k-nearest neighbor;recommender systems,Algorithm design and analysis;Clustering;Clustering algorithms;Data mining;Domain analysis;Electronic mail;Feature extraction;Nearest neighbor search;Recommender systems,Internet;data mining;groupware;learning (artificial intelligence);pattern classification;recommender systems;software engineering,association rule mining;data mining techniques;domain analysis activity;enterprise-level collaborative software suite;feature extraction;feature mining;feature recommendation algorithms;incremental diffusive algorithm;k-nearest neighbor machine learning method;labor-intensive task;online product descriptions;online product listings;recommender system;software projects;software systems,,28,,52,,,20130816,Dec. 2013,,IEEE,IEEE Journals & Magazines,,16
The Impact of Classifier Configuration and Classifier Combination on Bug Localization,S. W. Thomas; M. Nagappan; D. Blostein; A. E. Hassan,"Queen's University, Kingston",IEEE Transactions on Software Engineering,20130925,2013,39,10,1427,1443,"Bug localization is the task of determining which source code entities are relevant to a bug report. Manual bug localization is labor intensive since developers must consider thousands of source code entities. Current research builds bug localization classifiers, based on information retrieval models, to locate entities that are textually similar to the bug report. Current research, however, does not consider the effect of classifier configuration, i.e., all the parameter values that specify the behavior of a classifier. As such, the effect of each parameter or which parameter values lead to the best performance is unknown. In this paper, we empirically investigate the effectiveness of a large space of classifier configurations, 3,172 in total. Further, we introduce a framework for combining the results of multiple classifier configurations since classifier combination has shown promise in other domains. Through a detailed case study on over 8,000 bug reports from three large-scale projects, we make two main contributions. First, we show that the parameters of a classifier have a significant impact on its performance. Second, we show that combining multiple classifiers--whether those classifiers are hand-picked or randomly chosen relative to intelligently defined subspaces of classifiers--improves the performance of even the best individual classifiers.",0098-5589;00985589,,10.1109/TSE.2013.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6520844,LDA;LSI;Software maintenance;VSM;bug localization;classifier combination;information retrieval,Indexes;Information retrieval;Large scale integration;Matrix decomposition;Measurement;Resource management;Vectors,information retrieval;pattern classification;program debugging,bug localization classifiers;bug report;classifier combination;classifier configuration;classifier parameter;information retrieval models;parameter value;source code entity determination,,23,,62,,,20130527,Oct. 2013,,IEEE,IEEE Journals & Magazines,,16
Name-Based Analysis of Equally Typed Method Arguments,M. Pradel; T. R. Gross,"ETH Zurich, Zurich",IEEE Transactions on Software Engineering,20130725,2013,39,8,1127,1143,"When calling a method that requires multiple arguments, programmers must pass the arguments in the expected order. For statically typed languages, the compiler helps programmers by checking that the type of each argument matches the type of the formal parameter. Unfortunately, types are futile for methods with multiple parameters of the same type. How can a programmer check that equally typed arguments are passed in the correct order? This paper presents two simple, yet effective, static program analyses that detect problems related to the order of equally typed arguments. The key idea is to leverage identifier names to infer the semantics of arguments and their intended positions. The analyses reveal problems that affect the correctness, understandability, and maintainability of a program, such as accidentally reversed arguments and misleading parameter names. Most parts of the analyses are language-agnostic. We evaluate the approach with 24 real-world programs written in Java and C. Our results show the analyses to be effective and efficient. One analysis reveals anomalies in the order of equally typed arguments; it finds 54 relevant problems with a precision of 82 percent. The other analysis warns about misleading parameter names and finds 31 naming bugs with a precision of 39 percent.",0098-5589;00985589,,10.1109/TSE.2013.7,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6419711,Testing and debugging;anomaly detection;documentation;maintenance;method arguments;static program analysis,Access control;Context;Engines;Feature extraction;Java;Program processors;Robustness,C language;Java;program diagnostics,C language;Java language;equally typed method argument;formal parameter;name-based analysis;program correctness;program maintainability;program understandability;static program analysis;statically typed language,,2,,40,,,20130124,Aug. 2013,,IEEE,IEEE Journals & Magazines,,16
Trustrace: Mining Software Repositories to Improve the Accuracy of Requirement Traceability Links,N. Ali; Y. G. GuÕ©hÕ©neuc; G. Antoniol,"&#x00C9;cole Polytechnique de Montr&#x00E9;al, Montr&#x00E9;al",IEEE Transactions on Software Engineering,20130429,2013,39,5,725,741,"Traceability is the only means to ensure that the source code of a system is consistent with its requirements and that all and only the specified requirements have been implemented by developers. During software maintenance and evolution, requirement traceability links become obsolete because developers do not/cannot devote effort to updating them. Yet, recovering these traceability links later is a daunting and costly task for developers. Consequently, the literature has proposed methods, techniques, and tools to recover these traceability links semi-automatically or automatically. Among the proposed techniques, the literature showed that information retrieval (IR) techniques can automatically recover traceability links between free-text requirements and source code. However, IR techniques lack accuracy (precision and recall). In this paper, we show that mining software repositories and combining mined results with IR techniques can improve the accuracy (precision and recall) of IR techniques and we propose Trustrace, a trust--based traceability recovery approach. We apply Trustrace on four medium-size open-source systems to compare the accuracy of its traceability links with those recovered using state-of-the-art IR techniques from the literature, based on the Vector Space Model and Jensen-Shannon model. The results of Trustrace are up to 22.7 percent more precise and have 7.66 percent better recall values than those of the other techniques, on average. We thus show that mining software repositories and combining the mined data with existing results from IR techniques improves the precision and recall of requirement traceability links.",0098-5589;00985589,,10.1109/TSE.2012.71,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341764,Traceability;experts;feature;repositories;requirements;source code;trust-based model,Accuracy;Data mining;Information retrieval;Open source software;Principal component analysis;Software maintenance,data mining;data privacy;information retrieval;software maintenance,IR technique;Jensen-Shannon model;Trustrace approach;information retrieval technique;medium-size open-source system;precision accuracy;recall accuracy;requirement traceability link;software evolution;software maintenance;software repository mining;traceability method;trust-based traceability recovery approach;vector space model,,22,,41,,,20121110,13-May,,IEEE,IEEE Journals & Magazines,,16
A large-scale empirical study of just-in-time quality assurance,Y. Kamei; E. Shihab; B. Adams; A. E. Hassan; A. Mockus; A. Sinha; N. Ubayashi,"Kyushu University, Fukuoka",IEEE Transactions on Software Engineering,20130523,2013,39,6,757,773,"Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (_ÑÒrisky_Ñù) software changes instead of files or packages. We refer to this type of quality assurance activity as _ÑÒJust-In-Time Quality Assurance,_Ñù because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that _ÑÒJust-In-Time Quality Assurance_Ñù may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software.",0098-5589;00985589,,10.1109/TSE.2012.70,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341763,Maintenance;defect prediction;just-in-time prediction;mining software repositories;software metrics,Accuracy;Entropy;Measurement;Object oriented modeling;Predictive models;Quality assurance;Software,program testing;software maintenance;software metrics;software quality,commercial projects;cost reduction;defect prediction models;defect-prone file identification;defect-prone package identification;defect-prone software change identification;just-in-time quality assurance;open source projects;risk model;risky changes;software metrics;software quality assurance activities;software repository mining;software systems;source code inspection;unit testing,,45,1,63,,,20121110,13-Jun,,IEEE,IEEE Journals & Magazines,,16
A Machine Learning Approach to Software Requirements Prioritization,A. Perini; A. Susi; P. Avesani,"Fondazione Bruno Kessler. CIT - IRST, Trento",IEEE Transactions on Software Engineering,20130326,2013,39,4,445,461,"Deciding which, among a set of requirements, are to be considered first and in which order is a strategic process in software development. This task is commonly referred to as requirements prioritization. This paper describes a requirements prioritization method called Case-Based Ranking (CBRank), which combines project's stakeholders preferences with requirements ordering approximations computed through machine learning techniques, bringing promising advantages. First, the human effort to input preference information can be reduced, while preserving the accuracy of the final ranking estimates. Second, domain knowledge encoded as partial order relations defined over the requirement attributes can be exploited, thus supporting an adaptive elicitation process. The techniques CBRank rests on and the associated prioritization process are detailed. Empirical evaluations of properties of CBRank are performed on simulated data and compared with a state-of-the-art prioritization method, providing evidence of the method ability to support the management of the tradeoff between elicitation effort and ranking accuracy and to exploit domain knowledge. A case study on a real software project complements these experimental measurements. Finally, a positioning of CBRank with respect to state-of-the-art requirements prioritization methods is proposed, together with a discussion of benefits and limits of the method.",0098-5589;00985589,,10.1109/TSE.2012.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249686,Requirements management;machine learning;requirements prioritization,Accuracy;Approximation methods;Boosting;Data models;Humans;Software,,,,29,,42,,,20120726,13-Apr,,IEEE,IEEE Journals & Magazines,,16
Ant Colony Optimization for Software Project Scheduling and Staffing with an Event-Based Scheduler,W. N. Chen; J. Zhang,"Sun Yat-sen University, Guangzhou",IEEE Transactions on Software Engineering,20121228,2013,39,1,1,17,"Research into developing effective computer aided techniques for planning software projects is important and challenging for software engineering. Different from projects in other fields, software projects are people-intensive activities and their related resources are mainly human resources. Thus, an adequate model for software project planning has to deal with not only the problem of project task scheduling but also the problem of human resource allocation. But as both of these two problems are difficult, existing models either suffer from a very large search space or have to restrict the flexibility of human resource allocation to simplify the model. To develop a flexible and effective model for software project planning, this paper develops a novel approach with an event-based scheduler (EBS) and an ant colony optimization (ACO) algorithm. The proposed approach represents a plan by a task list and a planned employee allocation matrix. In this way, both the issues of task scheduling and employee allocation can be taken into account. In the EBS, the beginning time of the project, the time when resources are released from finished tasks, and the time when employees join or leave the project are regarded as events. The basic idea of the EBS is to adjust the allocation of employees at events and keep the allocation unchanged at nonevents. With this strategy, the proposed method enables the modeling of resource conflict and task preemption and preserves the flexibility in human resource allocation. To solve the planning problem, an ACO algorithm is further designed. Experimental results on 83 instances demonstrate that the proposed method is very promising.",0098-5589;00985589,,10.1109/TSE.2012.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165315,Software project planning;ant colony optimization (ACO);project scheduling;resource allocation;workload assignment,Humans;Job shop scheduling;Planning;Project management;Resource management;Search problems;Software,ant colony optimisation;human resource management;planning (artificial intelligence);project management;scheduling;software management,ACO;EBS;ant colony optimization algorithm;computer aided techniques;event-based scheduler;human resource allocation problem;planned employee allocation matrix;project task scheduling problem;resource conflict modeling;software engineering;software project planning;software project scheduling;software project staffing;task list;task preemption modeling,,52,,51,,,20120306,Jan. 2013,,IEEE,IEEE Journals & Magazines,,16
On Fault Representativeness of Software Fault Injection,R. Natella; D. Cotroneo; J. A. Duraes; H. S. Madeira,"Federico II University of Naples, Naples",IEEE Transactions on Software Engineering,20121228,2013,39,1,80,96,"The injection of software faults in software components to assess the impact of these faults on other components or on the system as a whole, allowing the evaluation of fault tolerance, is relatively new compared to decades of research on hardware fault injection. This paper presents an extensive experimental study (more than 3.8 million individual experiments in three real systems) to evaluate the representativeness of faults injected by a state-of-the-art approach (G-SWFIT). Results show that a significant share (up to 72 percent) of injected faults cannot be considered representative of residual software faults as they are consistently detected by regression tests, and that the representativeness of injected faults is affected by the fault location within the system, resulting in different distributions of representative/nonrepresentative faults across files and functions. Therefore, we propose a new approach to refine the faultload by removing faults that are not representative of residual software faults. This filtering is essential to assure meaningful results and to reduce the cost (in terms of number of faults) of software fault injection campaigns in complex software. The proposed approach is based on classification algorithms, is fully automatic, and can be used for improving fault representativeness of existing software fault injection approaches.",0098-5589;00985589,,10.1109/TSE.2011.124,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122035,Software fault injection;experimental dependability evaluation;fault-tolerant systems;software reliability,Emulation;Fault location;Fault tolerance;Fault tolerant systems;Hardware;Software;Testing,software fault tolerance,G-SWFIT;classification algorithms;fault location;fault representativeness;fault tolerance;hardware fault injection;nonrepresentative faults;regression tests;software components;software fault injection approaches,,41,,62,,,20120103,Jan. 2013,,IEEE,IEEE Journals & Magazines,,16
Language-Independent and Automated Software Composition: The FeatureHouse Experience,S. Apel; C. KÕ_stner; C. Lengauer,"University of Passau, Passau",IEEE Transactions on Software Engineering,20130213,2013,39,1,63,79,"Superimposition is a composition technique that has been applied successfully in many areas of software development. Although superimposition is a general-purpose concept, it has been (re)invented and implemented individually for various kinds of software artifacts. We unify languages and tools that rely on superimposition by using the language-independent model of feature structure trees (FSTs). On the basis of the FST model, we propose a general approach to the composition of software artifacts written in different languages. Furthermore, we offer a supporting framework and tool chain, called FEATUREHOUSE. We use attribute grammars to automate the integration of additional languages. In particular, we have integrated Java, C#, C, Haskell, Alloy, and JavaCC. A substantial number of case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties that a language must have in order to be ready for superimposition. We discuss perspectives of our approach and demonstrate how we extended FEATUREHOUSE with support for XML languages (in particular, XHTML, XMI/UML, and Ant) and alternative composition approaches (in particular, aspect weaving). Rounding off our previous work, we provide here a holistic view of the FEATUREHOUSE approach based on rich experience with numerous languages and case studies and reflections on several years of research.",0098-5589;00985589,,10.1109/TSE.2011.120,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095570,FeatureHouse;feature structure trees;language independence;software composition;superimposition,Databases;Grammar;Java;Latches;Printers;Software;Unified modeling language,C++ language;Java;Unified Modeling Language;XML;attribute grammars;software tools,Alloy;Ant;C#;FEATUREHOUSE;Haskell;JavaCC;XHTML;XMI/UML;XML language;attribute grammar;automated software composition;feature structure tree;general-purpose concept;integrated Java;language-independent model;language-independent software composition;software artifact composition;software development;superimposition;supporting framework;tool chain,,16,,67,,,20111206,Jan. 2013,,IEEE,IEEE Journals & Magazines,,16
Learning Project Management Decisions: A Case Study with Case-Based Reasoning versus Data Farming,T. Menzies; A. Brady; J. Keung; J. Hihn; S. Williams; O. El-Rawas; P. Green; B. Boehm,"West Virginia Univ., Morgantown, WV, USA",IEEE Transactions on Software Engineering,20131122,2013,39,12,1698,1713,"Background: Given information on just a few prior projects, how do we learn the best and fewest changes for current projects? Aim: To conduct a case study comparing two ways to recommend project changes. 1) Data farmers use Monte Carlo sampling to survey and summarize the space of possible outcomes. 2) Case-based reasoners (CBR) explore the neighborhood around test instances. Method: We applied a state-of-the data farmer (SEESAW) and a CBR tool ()'V2) to software project data. Results: CBR with )'V2 was more effective than SEESAW's data farming for learning best and recommended project changes, effectively reducing runtime, effort, and defects. Further, CBR with )'V2 was comparably easier to build, maintain, and apply in novel domains, especially on noisy data sets. Conclusion: Use CBR tools like )'V2 when data are scarce or noisy or when project data cannot be expressed in the required form of a data farmer. Future Work: This study applied our own CBR tool to several small data sets. Future work could apply other CBR tools and data farmers to other data (perhaps to explore other goals such as, say, minimizing maintenance effort).",0098-5589;00985589,,10.1109/TSE.2013.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6600685,COCOMO;Search-based software engineering;case-based reasoning;data farming,Data models;Mathematical model;Monte Carlo methods;Project management;Search methods;Software engineering,Monte Carlo methods;case-based reasoning;data handling;learning (artificial intelligence);project management;sampling methods;software management,CBR;Monte Carlo sampling;SEESAW;case-based reasoning;data farming;project management decision learning;software project data,,4,,101,,,20130916,Dec. 2013,,IEEE,IEEE Journals & Magazines,,15
Trends in the Quality of Human-Centric Software Engineering Experiments--A Quasi-Experiment,B. Kitchenham; D. I. K. SjÕôberg; T. DybÕ‚; O. P. Brereton; D. Budgen; M. HÕ_st; P. Runeson,"Keele University, Keele",IEEE Transactions on Software Engineering,20130626,2013,39,7,1002,1017,"Context: Several text books and papers published between 2000 and 2002 have attempted to introduce experimental design and statistical methods to software engineers undertaking empirical studies. Objective: This paper investigates whether there has been an increase in the quality of human-centric experimental and quasi-experimental journal papers over the time period 1993 to 2010. Method: Seventy experimental and quasi-experimental papers published in four general software engineering journals in the years 1992-2002 and 2006-2010 were each assessed for quality by three empirical software engineering researchers using two quality assessment methods (a questionnaire-based method and a subjective overall assessment). Regression analysis was used to assess the relationship between paper quality and the year of publication, publication date group (before 2003 and after 2005), source journal, average coauthor experience, citation of statistical text books and papers, and paper length. The results were validated both by removing papers for which the quality score appeared unreliable and using an alternative quality measure. Results: Paper quality was significantly associated with year, citing general statistical texts, and paper length (p <; 0.05). Paper length did not reach significance when quality was measured using an overall subjective assessment. Conclusions: The quality of experimental and quasi-experimental software engineering papers appears to have improved gradually since 1993.",0098-5589;00985589,,10.1109/TSE.2012.76,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374196,Quality evaluation;empirical studies;experimentation;human-centric experiments;software engineering,Correlation;Educational institutions;Guidelines;Humans;Manuals;Materials;Software engineering,citation analysis;design of experiments;publishing;regression analysis;software quality;text analysis,average coauthor experience;experimental design;human-centric experimental journal papers;human-centric software engineering experiments;paper length;paper quality;publication date group;publication year;quality assessment methods;quasi-experimental journal papers;questionnaire-based method;regression analysis;source journal;statistical methods;statistical papers citation;statistical text books citation;subjective overall assessment,,2,,28,,,20121204,13-Jul,,IEEE,IEEE Journals & Magazines,,15
Event Logs for the Analysis of Software Failures: A Rule-Based Approach,M. Cinque; D. Cotroneo; A. Pecchia,"Universit&#x00E1; degli Studi di Napoli Federico II, Naples",IEEE Transactions on Software Engineering,20130523,2013,39,6,806,821,"Event logs have been widely used over the last three decades to analyze the failure behavior of a variety of systems. Nevertheless, the implementation of the logging mechanism lacks a systematic approach and collected logs are often inaccurate at reporting software failures: This is a threat to the validity of log-based failure analysis. This paper analyzes the limitations of current logging mechanisms and proposes a rule-based approach to make logs effective to analyze software failures. The approach leverages artifacts produced at system design time and puts forth a set of rules to formalize the placement of the logging instructions within the source code. The validity of the approach, with respect to traditional logging mechanisms, is shown by means of around 12,500 software fault injection experiments into real-world systems.",0098-5589;00985589,,10.1109/TSE.2012.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320555,Event log;error detection;logging mechanism;rule-based logging;software failures,Analytical models;Failure analysis;Proposals;Software systems;Systematics;Unified modeling language,software fault tolerance,event logs;log-based failure analysis;logging mechanism;rule-based approach;software failures;system design time,,13,,52,,,20121003,13-Jun,,IEEE,IEEE Journals & Magazines,,15
Assessing the Effectiveness of Sequence Diagrams in the Comprehension of Functional Requirements: Results from a Family of Five Experiments,S. AbrahÕ£o; C. Gravino; E. Insfran; G. Scanniello; G. Tortora,"Universitat Polit&#x00E8;cnica de Val&#x00E8;ncia, Val&#x00E8;ncia",IEEE Transactions on Software Engineering,20130225,2013,39,3,327,342,Modeling is a fundamental activity within the requirements engineering process and concerns the construction of abstract descriptions of requirements that are amenable to interpretation and validation. The choice of a modeling technique is critical whenever it is necessary to discuss the interpretation and validation of requirements. This is particularly true in the case of functional requirements and stakeholders with divergent goals and different backgrounds and experience. This paper presents the results of a family of experiments conducted with students and professionals to investigate whether the comprehension of functional requirements is influenced by the use of dynamic models that are represented by means of the UML sequence diagrams. The family contains five experiments performed in different locations and with 112 participants of different abilities and levels of experience with UML. The results show that sequence diagrams improve the comprehension of the modeled functional requirements in the case of high ability and more experienced participants.,0098-5589;00985589,,10.1109/TSE.2012.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6193111,Documentation;requirements specifications;software engineering,Analytical models;Computational modeling;Materials;Object oriented modeling;Software systems;Unified modeling language,Unified Modeling Language;formal specification,UML sequence diagrams;abstract descriptions;effectiveness assessment;family-of-five experiments;functional requirements;functional stakeholders;requirement interpretation;requirement validation;requirements engineering process;requirements specifications;software engineering;unified modeling language,,17,,55,,,20120501,13-Mar,,IEEE,IEEE Journals & Magazines,,15
Automated Behavioral Testing of Refactoring Engines,G. Soares; R. Gheyi; T. Massoni,"Federal University of Campina Grande, Campina Grande",IEEE Transactions on Software Engineering,20130124,2013,39,2,147,162,"Refactoring is a transformation that preserves the external behavior of a program and improves its internal quality. Usually, compilation errors and behavioral changes are avoided by preconditions determined for each refactoring transformation. However, to formally define these preconditions and transfer them to program checks is a rather complex task. In practice, refactoring engine developers commonly implement refactorings in an ad hoc manner since no guidelines are available for evaluating the correctness of refactoring implementations. As a result, even mainstream refactoring engines contain critical bugs. We present a technique to test Java refactoring engines. It automates test input generation by using a Java program generator that exhaustively generates programs for a given scope of Java declarations. The refactoring under test is applied to each generated program. The technique uses SafeRefactor, a tool for detecting behavioral changes, as an oracle to evaluate the correctness of these transformations. Finally, the technique classifies the failing transformations by the kind of behavioral change or compilation error introduced by them. We have evaluated this technique by testing 29 refactorings in Eclipse JDT, NetBeans, and the JastAdd Refactoring Tools. We analyzed 153,444 transformations, and identified 57 bugs related to compilation errors, and 63 bugs related to behavioral changes.",0098-5589;00985589,,10.1109/TSE.2012.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175911,Refactoring;automated testing;program generation,Automatic programming;Computer bugs;Engines;Java;Metals;Testing;Unified modeling language,Java;automatic programming;program testing,JastAdd refactoring tools;Java program generator;Java refactoring engines;SafeRefactor;automated behavioral testing;compilation errors;refactoring engine developers;refactoring transformation,,24,,49,,,20120403,Feb. 2013,,IEEE,IEEE Journals & Magazines,,15
Whole Test Suite Generation,G. Fraser; A. Arcuri,"Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK",IEEE Transactions on Software Engineering,20130124,2013,39,2,276,291,"Not all bugs lead to program crashes, and not always is there a formal specification to check the correctness of a software test's outcome. A common scenario in software testing is therefore that test data are generated, and a tester manually adds test oracles. As this is a difficult task, it is important to produce small yet representative test sets, and this representativeness is typically measured using code coverage. There is, however, a fundamental problem with the common approach of targeting one coverage goal at a time: Coverage goals are not independent, not equally difficult, and sometimes infeasible-the result of test generation is therefore dependent on the order of coverage goals and how many of them are feasible. To overcome this problem, we propose a novel paradigm in which whole test suites are evolved with the aim of covering all coverage goals at the same time while keeping the total size as small as possible. This approach has several advantages, as for example, its effectiveness is not affected by the number of infeasible targets in the code. We have implemented this novel approach in the EvoSuite tool, and compared it to the common approach of addressing one goal at a time. Evaluated on open source libraries and an industrial case study for a total of 1,741 classes, we show that EvoSuite achieved up to 188 times the branch coverage of a traditional approach targeting single branches, with up to 62 percent smaller test suites.",0098-5589;00985589,,10.1109/TSE.2012.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152257,Search-based software engineering;branch coverage;collateral coverage;genetic algorithm;infeasible goal;length,Arrays;Genetic algorithms;Genetic programming;Search problems;Software;Software testing,formal specification;program debugging;program testing,EvoSuite tool;code coverage;formal specification;program crashes;program debugging;software testing;whole test suite generation,,102,1,52,,,20120216,Feb. 2013,,IEEE,IEEE Journals & Magazines,,15
Equality to Equals and Unequals: A Revisit of the Equivalence and Nonequivalence Criteria in Class-Level Testing of Object-Oriented Software,H. Y. Chen; T. H. Tse,"Dept. of Comput. Sci., Jinan Univ., Guangzhou, China",IEEE Transactions on Software Engineering,20131028,2013,39,11,1549,1563,"Algebraic specifications have been used in the testing of object-oriented programs and received much attention since the 1990s. It is generally believed that class-level testing based on algebraic specifications involves two independent aspects: the testing of equivalent and nonequivalent ground terms. Researchers have cited intuitive examples to illustrate the philosophy that even if an implementation satisfies all the requirements specified by the equivalence of ground terms, it may still fail to satisfy some of the requirements specified by the nonequivalence of ground terms. Thus, both the testing of equivalent ground terms and the testing of nonequivalent ground terms have been considered as significant and cannot replace each other. In this paper, we present an innovative finding that, given any canonical specification of a class with proper imports, a complete implementation satisfies all the observationally equivalent ground terms if and only if it satisfies all the observationally nonequivalent ground terms. As a result, these two aspects of software testing cover each other and can therefore replace each other. These findings provide a deeper understanding of software testing based on algebraic specifications, rendering the theory more elegant and complete. We also highlight a couple of important practical implications of our theoretical results.",0098-5589;00985589,,10.1109/TSE.2013.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570471,Software testing;algebraic specification;equivalence criterion;nonequivalence criterion;object-oriented software,Computer science;Context;Observers;Semantics;Software;Software testing,algebraic specification;object-oriented programming;program testing,algebraic specifications;canonical specification;class-level testing;equivalent ground terms;nonequivalence criteria;object-oriented programs;object-oriented software;software testing,,5,,33,,,20130726,Nov. 2013,,IEEE,IEEE Journals & Magazines,,14
Usability through Software Design,L. Carvajal; A. M. Moreno; M. I. SÕçnchez-Segura; A. Seffah,"Fac. de Inf., Univ. Politec. de Madrid, Montegancedo, Spain",IEEE Transactions on Software Engineering,20131028,2013,39,11,1582,1596,"Over the past two decades, the HCI community has proposed specific features that software applications should include to overcome some of the most common usability problems. However, incorporating such usability features into software applications may not be a straightforward process for software developers who have not been trained in usability (i.e., determining when, how, and why usability features should been considered). We have defined a set of usability guidelines for software development to help software engineers incorporate particular usability features into their applications. In this paper, we focus on the software design artifacts provided by the guidelines. We detail the structure of the proposed design artifacts and how they should be used according to the software development process and software architecture used in each application. We have tested our guidelines in an academic setting. Preliminary validation shows that the use of the guidelines reduces development time, improves the quality of the resulting designs, and significantly decreases the perceived complexity of the usability features from the developers' perspective.",0098-5589;00985589,,10.1109/TSE.2013.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523225,Software usability;software design;software design patterns,Communities;Guidelines;Human computer interaction;Unified modeling language;Usability,human computer interaction;object-oriented methods;software architecture;software quality;user interfaces,HCI community;human computer interaction;integral software development quality aspect;software architecture;software design artifacts;software design patterns;software development process;software usability,,9,,24,,,20130604,Nov. 2013,,IEEE,IEEE Journals & Magazines,,14
Model-Based Test Oracle Generation for Automated Unit Testing of Agent Systems,L. Padgham; Z. Zhang; J. Thangarajah; T. Miller,"RMIT University, Melbourne",IEEE Transactions on Software Engineering,20130828,2013,39,9,1230,1244,"Software testing remains the most widely used approach to verification in industry today, consuming between 30-50 percent of the entire development cost. Test input selection for intelligent agents presents a problem due to the very fact that the agents are intended to operate robustly under conditions which developers did not consider and would therefore be unlikely to test. Using methods to automatically generate and execute tests is one way to provide coverage of many conditions without significantly increasing cost. However, one problem using automatic generation and execution of tests is the oracle problem: How can we automatically decide if observed program behavior is correct with respect to its specification? In this paper, we present a model-based oracle generation method for unit testing belief-desire-intention agents. We develop a fault model based on the features of the core units to capture the types of faults that may be encountered and define how to automatically generate a partial, passive oracle from the agent design models. We evaluate both the fault model and the oracle generation by testing 14 agent systems. Over 400 issues were raised, and these were analyzed to ascertain whether they represented genuine faults or were false positives. We found that over 70 percent of issues raised were indicative of problems in either the design or the code. Of the 19 checks performed by our oracle, faults were found by all but 5 of these checks. We also found that 8 out the 11 fault types identified in our fault model exhibited at least one fault. The evaluation indicates that the fault model is a productive conceptualization of the problems to be expected in agent unit testing and that the oracle is able to find a substantial number of such faults with relatively small overhead in terms of false positives.",0098-5589;00985589,,10.1109/TSE.2013.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464272,BDI agents;Test oracles;unit testing,Arrays;Computational modeling;Context;Fault diagnosis;Object oriented modeling;Robustness;Testing,fault diagnosis;multi-agent systems;program testing,agent design models;agent system automated unit testing;belief-desire-intention agents;core units;fault model;intelligent agents;model-based test oracle generation;software testing;test automatic generation;test execution;test input selection,,15,,31,,,20130218,Sept. 2013,,IEEE,IEEE Journals & Magazines,,14
Balancing Privacy and Utility in Cross-Company Defect Prediction,F. Peters; T. Menzies; L. Gong; H. Zhang,"West Virginia University, Morgantown",IEEE Transactions on Software Engineering,20130725,2013,39,8,1054,1068,"Background: Cross-company defect prediction (CCDP) is a field of study where an organization lacking enough local data can use data from other organizations for building defect predictors. To support CCDP, data must be shared. Such shared data must be privatized, but that privatization could severely damage the utility of the data. Aim: To enable effective defect prediction from shared data while preserving privacy. Method: We explore privatization algorithms that maintain class boundaries in a dataset. CLIFF is an instance pruner that deletes irrelevant examples. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. CLIFF+MORPH are tested in a CCDP study among 10 defect datasets from the PROMISE data repository. Results: We find: 1) The CLIFFed+MORPHed algorithms provide more privacy than the state-of-the-art privacy algorithms; 2) in terms of utility measured by defect prediction, we find that CLIFF+MORPH performs significantly better. Conclusions: For the OO defect data studied here, data can be privatized and shared without a significant degradation in utility. To the best of our knowledge, this is the first published result where privatization does not compromise defect prediction.",0098-5589;00985589,,10.1109/TSE.2013.6,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6419712,Privacy;classification;defect prediction,Arrays;Genetic algorithms;Search problems;Sociology;Software;Statistics;Testing,data privacy;program debugging,CCDP;CLIFFed-MORPHed algorithm;OO defect data;PROMISE data repository;class boundaries;cross-company defect prediction;data mutator;defect predictors;privacy balancing;privacy preservation;privatization algorithms;pruner;utility balancing,,24,,58,,,20130124,Aug. 2013,,IEEE,IEEE Journals & Magazines,,14
Monitor-Based Instant Software Refactoring,H. Liu; X. Guo; W. Shao,"Beijing Institute of Technology, Beijing",IEEE Transactions on Software Engineering,20130725,2013,39,8,1112,1126,"Software refactoring is an effective method for improvement of software quality while software external behavior remains unchanged. To facilitate software refactoring, a number of tools have been proposed for code smell detection and/or for automatic or semi-automatic refactoring. However, these tools are passive and human driven, thus making software refactoring dependent on developers' spontaneity. As a result, software engineers with little experience in software refactoring might miss a number of potential refactorings or may conduct refactorings later than expected. Few refactorings might result in poor software quality, and delayed refactorings may incur higher refactoring cost. To this end, we propose a monitor-based instant refactoring framework to drive inexperienced software engineers to conduct more refactorings promptly. Changes in the source code are instantly analyzed by a monitor running in the background. If these changes have the potential to introduce code smells, i.e., signs of potential problems in the code that might require refactorings, the monitor invokes corresponding smell detection tools and warns developers to resolve detected smells promptly. Feedback from developers, i.e., whether detected smells have been acknowledged and resolved, is consequently used to optimize smell detection algorithms. The proposed framework has been implemented, evaluated, and compared with the traditional human-driven refactoring tools. Evaluation results suggest that the proposed framework could drive inexperienced engineers to resolve more code smells (by an increase of 140 percent) promptly. The average lifespan of resolved smells was reduced by 92 percent. Results also suggest that the proposed framework could help developers to avoid similar code smells through timely warnings at the early stages of software development, thus reducing the total number of code smells by 51 percent.",0098-5589;00985589,,10.1109/TSE.2013.4,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6409360,Software refactoring;code smell detection;instant refactoring;monitor,Algorithm design and analysis;Cloning;Detection algorithms;Detectors;Inspection;Monitoring;Software,software maintenance;software quality,code smell detection;monitor-based instant software refactoring;smell detection algorithm;software external behavior;software quality,,11,,50,,,20130110,Aug. 2013,,IEEE,IEEE Journals & Magazines,,14
Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm,N. Mittas; L. Angelis,"Aristotle University of Thessaloniki, Thessaloniki",IEEE Transactions on Software Engineering,20130326,2013,39,4,537,551,"Software Cost Estimation can be described as the process of predicting the most realistic effort required to complete a software project. Due to the strong relationship of accurate effort estimations with many crucial project management activities, the research community has been focused on the development and application of a vast variety of methods and models trying to improve the estimation procedure. From the diversity of methods emerged the need for comparisons to determine the best model. However, the inconsistent results brought to light significant doubts and uncertainty about the appropriateness of the comparison process in experimental studies. Overall, there exist several potential sources of bias that have to be considered in order to reinforce the confidence of experiments. In this paper, we propose a statistical framework based on a multiple comparisons algorithm in order to rank several cost estimation models, identifying those which have significant differences in accuracy, and clustering them in nonoverlapping groups. The proposed framework is applied in a large-scale setup of comparing 11 prediction models over six datasets. The results illustrate the benefits and the significant information obtained through the systematic comparison of alternative methods.",0098-5589;00985589,,10.1109/TSE.2012.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6235961,Cost estimation;management;metrics/measurement;statistical methods,Accuracy;Clustering algorithms;Estimation;Measurement uncertainty;Prediction algorithms;Predictive models;Systematics,,,,35,,54,,,20120710,13-Apr,,IEEE,IEEE Journals & Magazines,,14
A Second Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems,T. Galinac Grbac; P. Runeson; D. Huljeni—_,"Univesity of Rijeka, Rijeka",IEEE Transactions on Software Engineering,20130326,2013,39,4,462,476,"Background: Software engineering is searching for general principles that apply across contexts, for example, to help guide software quality assurance. Fenton and Ohlsson presented such observations on fault distributions, which have been replicated once. Objectives: We aimed to replicate their study again to assess the robustness of the findings in a new environment, five years later. Method: We conducted a literal replication, collecting defect data from five consecutive releases of a large software system in the telecommunications domain, and conducted the same analysis as in the original study. Results: The replication confirms results on unevenly distributed faults over modules, and that fault proneness distributions persist over test phases. Size measures are not useful as predictors of fault proneness, while fault densities are of the same order of magnitude across releases and contexts. Conclusions: This replication confirms that the uneven distribution of defects motivates uneven distribution of quality assurance efforts, although predictors for such distribution of efforts are not sufficiently precise.",0098-5589;00985589,,10.1109/TSE.2012.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6235962,Software fault distributions;empirical research;replication;software metrics,Complexity theory;Context;Measurement;Software;Software engineering;Telecommunications;Testing,,,,9,,43,,,20120710,13-Apr,,IEEE,IEEE Journals & Magazines,,14
Where Should We Fix This Bug? A Two-Phase Recommendation Model,D. Kim; Y. Tao; S. Kim; A. Zeller,"Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Kowloon, China",IEEE Transactions on Software Engineering,20131028,2013,39,11,1597,1610,"To support developers in debugging and locating bugs, we propose a two-phase prediction model that uses bug reports' contents to suggest the files likely to be fixed. In the first phase, our model checks whether the given bug report contains sufficient information for prediction. If so, the model proceeds to predict files to be fixed, based on the content of the bug report. In other words, our two-phase model ""speaks up"" only if it is confident of making a suggestion for the given bug report; otherwise, it remains silent. In the evaluation on the Mozilla ""Firefox"" and ""Core"" packages, the two-phase model was able to make predictions for almost half of all bug reports; on average, 70 percent of these predictions pointed to the correct files. In addition, we compared the two-phase model with three other prediction models: the Usual Suspects, the one-phase model, and BugScout. The two-phase model manifests the best prediction performance.",0098-5589;00985589,,10.1109/TSE.2013.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517844,Bug reports;machine learning;patch file prediction,Computational modeling;Computer bugs;Data mining;Feature extraction;Noise;Predictive models;Software,formal verification;program debugging,BugScout;Core packages;Firefox packages;Mozilla packages;bug report;debugging;speaks up;two-phase model;two-phase prediction model;two-phase recommendation model,,19,,66,,,20130521,Nov. 2013,,IEEE,IEEE Journals & Magazines,,13
Generating Test Cases for Real-Time Systems Based on Symbolic Models,W. L. Andrade; P. D. L. Machado,"Federal University of Campina Grande, Campina Grande",IEEE Transactions on Software Engineering,20130828,2013,39,9,1216,1229,"The state space explosion problem is one of the challenges to be faced by test case generation techniques, particularly when data values need to be enumerated. This problem gets even worse for real-time systems (RTS) that also have time constraints. The usual solution in this context, based on finite state machines or time automata, consists of enumerating data values (restricted to finite domains) while treating time symbolically. In this paper, a symbolic model for conformance testing of real-time systems software named TIOSTS that addresses both data and time symbolically is presented. Moreover, a test case generation process is defined to select more general test cases with variables and parameters that can be instantiated at testing execution time. Generation is based on a combination of symbolic execution and constraint solving for the data part and symbolic analysis for timed aspects. Furthermore, the practical application of the process is investigated through a case study.",0098-5589;00985589,,10.1109/TSE.2013.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475130,Real-time systems and embedded systems;formal methods;symbolic execution;testing strategies,Automata;Clocks;Cost accounting;Data models;Real-time systems;Semantics;Testing,conformance testing;finite state machines;program testing;real-time systems,RTS;TIOSTS;conformance testing;constraint solving;data part;finite state machines;real-time systems;state space explosion problem;symbolic analysis;symbolic execution;symbolic models;test case generation techniques;time automata;time constraints;timed aspects,,6,,38,,,20130306,Sept. 2013,,IEEE,IEEE Journals & Magazines,,13
Active learning and effort estimation: Finding the essential content of software effort estimation data,E. Kocaguneli; T. Menzies; J. Keung; D. Cok; R. Madachy,"West Virginia University, Morgantown",IEEE Transactions on Software Engineering,20130725,2013,39,8,1040,1053,"Background: Do we always need complex methods for software effort estimation (SEE)? Aim: To characterize the essential content of SEE data, i.e., the least number of features and instances required to capture the information within SEE data. If the essential content is very small, then 1) the contained information must be very brief and 2) the value added of complex learning schemes must be minimal. Method: Our QUICK method computes the euclidean distance between rows (instances) and columns (features) of SEE data, then prunes synonyms (similar features) and outliers (distant instances), then assesses the reduced data by comparing predictions from 1) a simple learner using the reduced data and 2) a state-of-the-art learner (CART) using all data. Performance is measured using hold-out experiments and expressed in terms of mean and median MRE, MAR, PRED(25), MBRE, MIBRE, or MMER. Results: For 18 datasets, QUICK pruned 69 to 96 percent of the training data (median = 89 percent). K = 1 nearest neighbor predictions (in the reduced data) performed as well as CART's predictions (using all data). Conclusion: The essential content of some SEE datasets is very small. Complex estimation methods may be overelaborate for such datasets and can be simplified. We offer QUICK as an example of such a simpler SEE method.",0098-5589;00985589,,10.1109/TSE.2012.88,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392173,Software cost estimation;active learning;analogy;k-NN,Complexity theory;Estimation;Euclidean distance;Frequency selective surfaces;Indexes;Labeling;Principal component analysis,data handling;learning (artificial intelligence);software cost estimation;statistical analysis,CART learner;Euclidean distance;K-nearest neighbor prediction;QUICK method;SEE data content;complex learning scheme;mean;median;software effort estimation,,21,,50,,,20121221,Aug. 2013,,IEEE,IEEE Journals & Magazines,,13
"Conservative Bounds for the pfd of a 1-out-of-2 Software-Based System Based on an Assessor's Subjective Probability of ""Not Worse Than Independence""",B. Littlewood; A. Povyakalo,"City Univ., London, UK",IEEE Transactions on Software Engineering,20131122,2013,39,12,1641,1653,"We consider the problem of assessing the reliability of a 1-out-of-2 software-based system, in which failures of the two channels cannot be assumed to be independent with certainty. An informal approach to this problem assesses the channel probabilities of failure on demand (pfds) conservatively, and then multiplies these together in the hope that the conservatism will be sufficient to overcome any possible dependence between the channel failures. Our intention here is to place this kind of reasoning on a formal footing. We introduce a notion of ""not worse than independence""' and assume that an assessor has a prior belief about this, expressed as a probability. We obtain a conservative prior system pfd, and show how a conservative posterior system pfd can be obtained following the observation of a number of demands without system failure. We present some illustrative numerical examples, discuss some of the difficulties involved in this way of reasoning, and suggest some avenues of future research.",0098-5589;00985589,,10.1109/TSE.2013.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564279,1-out-of-2 system;System reliability;dependent failures;software fault tolerance;subjective probability,Cognition;Failure analysis;Fault tolerant systems;Phase frequency detector;Reliability engineering;Software reliability,probability;software reliability,1-out-of-2 software-based system reliability;assessor subjective probability;channel probabilities of failure on demand;conservative bounds;conservative posterior system PFD;formal footing;not worse than independence,,3,,14,,,20130719,Dec. 2013,,IEEE,IEEE Journals & Magazines,,12
Assessing the Cost Effectiveness of Fault Prediction in Acceptance Testing,A. Monden; T. Hayashi; S. Shinoda; K. Shirai; J. Yoshida; M. Barker; K. Matsumoto,"Nara Institute of Science and Technology, Ikoma",IEEE Transactions on Software Engineering,20130925,2013,39,10,1345,1357,"Until now, various techniques for predicting fault-prone modules have been proposed and evaluated in terms of their prediction performance; however, their actual contribution to business objectives such as quality improvement and cost reduction has rarely been assessed. This paper proposes using a simulation model of software testing to assess the cost effectiveness of test effort allocation strategies based on fault prediction results. The simulation model estimates the number of discoverable faults with respect to the given test resources, the resource allocation strategy, a set of modules to be tested, and the fault prediction results. In a case study applying fault prediction of a small system to acceptance testing in the telecommunication industry, results from our simulation model showed that the best strategy was to let the test effort be proportional to ""the number of expected faults in a module Õ‹ log(module size)."" By using this strategy with our best fault prediction model, the test effort could be reduced by 25 percent while still detecting as many faults as were normally discovered in testing, although the company required about 6 percent of the test effort for metrics collection, data cleansing, and modeling. The simulation results also indicate that the lower bound of acceptable prediction accuracy is around 0.78 in terms of an effort-aware measure, Norm(P<sub>opt</sub>). The results indicate that reduction of the test effort can be achieved by fault prediction only if the appropriate test strategy is employed with high enough fault prediction accuracy. Based on these preliminary results, we expect further research to assess their general validity with larger systems.",0098-5589;00985589,,10.1109/TSE.2013.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497441,Complexity measures;fault prediction;quality assurance;resource allocation;simulation,Accuracy;Companies;Measurement;Predictive models;Resource management;Software;Testing,program testing;resource allocation;software cost estimation;software fault tolerance;software metrics,acceptance testing;cost effectiveness assessment;cost reduction;data cleansing;data modeling;effort-aware measure;fault discovery;fault prediction;metrics collection;quality improvement;resource allocation strategy;software testing;telecommunication industry;test effort allocation strategies;test resources,,11,,34,,,20130412,Oct. 2013,,IEEE,IEEE Journals & Magazines,,12
Quantifying the Effect of Code Smells on Maintenance Effort,D. I. K. SjÕôberg; A. Yamashita; B. C. D. Anda; A. Mockus; T. DybÕ‚,"University of Oslo, Oslo",IEEE Transactions on Software Engineering,20130725,2013,39,8,1144,1156,"Context: Code smells are assumed to indicate bad design that leads to less maintainable code. However, this assumption has not been investigated in controlled studies with professional software developers. Aim: This paper investigates the relationship between code smells and maintenance effort. Method: Six developers were hired to perform three maintenance tasks each on four functionally equivalent Java systems originally implemented by different companies. Each developer spent three to four weeks. In total, they modified 298 Java files in the four systems. An Eclipse IDE plug-in measured the exact amount of time a developer spent maintaining each file. Regression analysis was used to explain the effort using file properties, including the number of smells. Result: None of the 12 investigated smells was significantly associated with increased effort after we adjusted for file size and the number of changes; Refused Bequest was significantly associated with decreased effort. File size and the number of changes explained almost all of the modeled variation in effort. Conclusion: The effects of the 12 smells on maintenance effort were limited. To reduce maintenance effort, a focus on reducing code size and the work practices that limit the number of changes may be more beneficial than refactoring code smells.",0098-5589;00985589,,10.1109/TSE.2012.89,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392174,Maintainability;code churn;object-oriented design;product metrics,Context;Electronic mail;Java;Maintenance engineering;Software;Surgery;Time measurement,Java;regression analysis;software maintenance,Eclipse IDE plug-in;Java files;Java systems;code size reduction;code smell effect quantification;code smell refactoring;file properties;file size;maintainable code;maintenance effort;maintenance tasks;refused bequest;regression analysis,,66,,46,,,20121221,Aug. 2013,,IEEE,IEEE Journals & Magazines,,12
Embedding Polychrony into Synchrony,J. Brandt; M. GemÕ_nde; K. Schneider; S. K. Shukla; J. P. Talpin,"University of Kaiserslautern, Kaiserslautern",IEEE Transactions on Software Engineering,20130626,2013,39,7,917,929,"This paper presents an embedding of polychronous programs into synchronous ones. Due to this embedding, it is not only possible to deepen the understanding of these different models of computation, but, more importantly, it is possible to transfer compilation techniques that were developed for synchronous programs to polychronous programs. This transfer is nontrivial because the underlying paradigms differ more than their names suggest: Since synchronous systems react deterministically to given inputs in discrete steps, they are typically used to describe reactive systems with a totally ordered notion of time. In contrast, polychronous system models entail a partially ordered notion of time, and are most suited to interface a system with an asynchronous environment by specifying input/output constraints from which a deterministic controller may eventually be refined and synthesized. As particular examples for the mentioned cross fertilization, we show how a simulator and a verification backend for synchronous programs can be made available to polychronous specifications, which is a first step toward integrating heterogeneous models of computation.",0098-5589;00985589,,10.1109/TSE.2012.85,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381420,Model-driven embedded software;polychronous programming;programming models;synchronous guarded commands;synchronous programming;synchrony hypothesis,Clocks;Computational modeling;Embedded systems;Hardware;Synchronization;Unified modeling language,embedded systems;program compilers;program verification,asynchronous environment;compilation techniques;deterministic controller;heterogeneous model;input-output constraints;polychronous program specification;polychronous system model;reactive systems;synchronous program verification,,4,,42,,,20121213,13-Jul,,IEEE,IEEE Journals & Magazines,,12
Local versus Global Lessons for Defect Prediction and Effort Estimation,T. Menzies; A. Butcher; D. Cok; A. Marcus; L. Layman; F. Shull; B. Turhan; T. Zimmermann,"West Virginia University, Morgantown",IEEE Transactions on Software Engineering,20130523,2013,39,6,822,834,"Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.",0098-5589;00985589,,10.1109/TSE.2012.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363444,Data mining;clustering;defect prediction;effort estimation,Context;Data models;Estimation;Java;Measurement;Software;Telecommunications,automatic test pattern generation;data mining;pattern clustering,PROMISE repository;automated clustering tools;data source;defect dataset;defect prediction;effort estimation;global lessons;learned lesson generated rule;local lessons,,44,,71,,,20121129,13-Jun,,IEEE,IEEE Journals & Magazines,,12
Abstracting runtime heaps for program understanding,M. Marron; C. Sanchez; Z. Su; M. Fahndrich,"Imdea Software Institute, Boadilla del Monte",IEEE Transactions on Software Engineering,20130523,2013,39,6,774,786,"Modern programming environments provide extensive support for inspecting, analyzing, and testing programs based on the algorithmic structure of a program. Unfortunately, support for inspecting and understanding runtime data structures during execution is typically much more limited. This paper provides a general purpose technique for abstracting and summarizing entire runtime heaps. We describe the abstract heap model and the associated algorithms for transforming a concrete heap dump into the corresponding abstract model as well as algorithms for merging, comparing, and computing changes between abstract models. The abstract model is designed to emphasize high-level concepts about heap-based data structures, such as shape and size, as well as relationships between heap structures, such as sharing and connectivity. We demonstrate the utility and computational tractability of the abstract heap model by building a memory profiler. We use this tool to identify, pinpoint, and correct sources of memory bloat for programs from DaCapo.",0098-5589;00985589,,10.1109/TSE.2012.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6331492,Heap structure;memory profiling;program understanding;runtime analysis,Abstracts;Arrays;Computational modeling;Concrete;Runtime;Shape,data structures;merging;program diagnostics;program testing,DaCapo;abstract heap model computational tractability;abstract heap model utility;concrete heap dump;heap structure connectivity;heap structure sharing;high-level concepts;memory bloat;memory profiler;program algorithmic structure;program analysis;program comparison;program computing;program inspection;program merging;program testing;runtime data structure relationships;runtime heap abstracting;runtime heap summarization,,6,,37,,,20121016,13-Jun,,IEEE,IEEE Journals & Magazines,,12
A fluid model for layered queueing networks,M. Tribastone,"Ludwig-Maximilians University of Munich, Munich",IEEE Transactions on Software Engineering,20130523,2013,39,6,744,756,"Layered queueing networks are a useful tool for the performance modeling and prediction of software systems that exhibit complex characteristics such as multiple tiers of service, fork/join interactions, and asynchronous communication. These features generally result in nonproduct form behavior for which particularly efficient approximations based on mean value analysis (MVA) have been devised. This paper reconsiders the accuracy of such techniques by providing an interpretation of layered queueing networks as fluid models. Mediated by an automatic translation into a stochastic process algebra, PEPA, a network is associated with a set of ordinary differential equations (ODEs) whose size is insensitive to the population levels in the system under consideration. A substantial numerical assessment demonstrates that this approach significantly improves the quality of the approximation for typical performance indices such as utilization, throughput, and response time. Furthermore, backed by established theoretical results of asymptotic convergence, the error trend shows monotonic decrease with larger population sizes-a behavior which is found to be in sharp contrast with that of approximate mean value analysis, which instead tends to increase.",0098-5589;00985589,,10.1109/TSE.2012.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6314480,Markov processes;Modeling and prediction;PEPA;mean value analysis;ordinary differential equations;queueing networks,Accuracy;Approximation methods;Servers;Sociology;Statistics;Stochastic processes;Unified modeling language,approximation theory;differential equations;queueing theory;software performance evaluation,MVA;PEPA;approximation quality;asymptotic convergence;automatic translation;fluid models;layered queueing networks;mean value analysis;nonproduct form behavior;numerical assessment;ordinary differential equations;performance indices;performance modeling;performance prediction;software systems;stochastic process algebra,,12,,33,,,20120927,13-Jun,,IEEE,IEEE Journals & Magazines,,12
A Quantitative Approach to Input Generation in Real-Time Testing of Stochastic Systems,L. Carnevali; L. Ridi; E. Vicario,"Universit&#x00E0; degli Studi di Firenze, Firenze",IEEE Transactions on Software Engineering,20130225,2013,39,3,292,304,"In the process of testing of concurrent timed systems, input generation identifies values of temporal parameters that let the Implementation Under Test (IUT) execute selected cases. However, when some parameters are not under control of the driver, test execution may diverge from the selected input and produce an inconclusive behavior. We formulate the problem on the basis of an abstraction of the IUT which we call partially stochastic Time Petri Net (psTPN), where controllable parameters are modeled as nondeterministic values and noncontrollable parameters as random variables with general (GEN) distribution. With reference to this abstraction, we derive the analytical form of the probability that the IUT runs along a selected behavior as a function of choices taken on controllable parameters. In the applicative perspective of real-time testing, this identifies a theoretical upper limit on the probability of a conclusive result, thus providing a means to plan the number of test repetitions that are necessary to guarantee a given probability of test-case coverage. It also provides a constructive technique for an optimal or suboptimal approach to input generation and a way to characterize the probability of conclusive testing under other suboptimal strategies.",0098-5589;00985589,,10.1109/TSE.2012.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226426,Difference Bound Matrix;Real-time testing;Time Petri Nets;input generation;non-Markovian Stochastic Petri Nets;stochastic processes,Automata;Real time systems;Stochastic processes;Testing;Timing;Tin;Vectors,Petri nets;program testing;real-time systems,GEN distribution;IUT;concurrent timed systems;controllable parameters;implementation under test;inconclusive behavior;input generation;nondeterministic values;partially stochastic Time Petri Net;psTPN;quantitative approach;real-time testing;stochastic systems;temporal parameters;test execution,,1,,44,,,20120626,13-Mar,,IEEE,IEEE Journals & Magazines,,12
"Class Schema Evolution for Persistent Object-Oriented Software: Model, Empirical Study, and Automated Support",M. Piccioni; M. Oriol; B. Meyer,"ETH Zurich, Zurich",IEEE Transactions on Software Engineering,20130124,2013,39,2,184,196,"With the wide support for object serialization in object-oriented programming languages, persistent objects have become commonplace and most large object-oriented software systems rely on extensive amounts of persistent data. Such systems also evolve over time. Retrieving previously persisted objects from classes whose schema has changed is, however, difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses these issues through an IDE-integrated approach that handles class schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of potentially corrupt objects. This paper describes a model for class attribute changes, a measure for class evolution robustness, four empirical studies, and the design and implementation of the ESCHER system.",0098-5589;00985589,,10.1109/TSE.2011.123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122034,IDE integration;Versioning;object-oriented class schema evolution;persistence;serialization,Atomic measurements;Databases;Dictionaries;Java;Object oriented modeling;Robustness;Software,object-oriented languages;object-oriented programming;persistent objects,ESCHER system design;ESCHER system implementation;IDE-integrated approach;automatic transformation function generation;class evolution robustness;class schema evolution;object serialization;object-oriented programming languages;object-oriented software systems;persistent object-oriented software;potentially corrupt objects,,3,,43,,,20120103,Feb. 2013,,IEEE,IEEE Journals & Magazines,,12
Using Timed Automata for Modeling Distributed Systems with Clocks: Challenges and Solutions,G. Rodriguez-Navas; J. Proenza,"Universitat de les Illes Balears, Palma de Mallorca",IEEE Transactions on Software Engineering,20130523,2013,39,6,857,868,"The application of model checking for the formal verification of distributed embedded systems requires the adoption of techniques for realistically modeling the temporal behavior of such systems. This paper discusses how to model with timed automata the different types of relationships that may be found among the computer clocks of a distributed system, namely, ideal clocks, drifting clocks, and synchronized clocks. For each kind of relationship, a suitable modeling pattern is thoroughly described and formally verified.",0098-5589;00985589,,10.1109/TSE.2012.73,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374193,Embedded systems;clock synchronization;hybrid automata;model checking;real-time systems;timed automata,Automata;Distributed processing;Embedded systems;Formal verification;Real-time systems,automata theory;distributed processing;embedded systems;formal verification,distributed embedded systems;distributed system computer clocks;drifting clocks;formal verification;ideal clocks;model checking;modeling distributed systems;synchronized clocks;temporal behavior;timed automata,,5,,27,,,20121204,13-Jun,,IEEE,IEEE Journals & Magazines,,11
"Conservative Reasoning about the Probability of Failure on Demand of a 1-out-of-2 Software-Based System in Which One Channel Is ""Possibly Perfect""",B. Littlewood; A. Povyakalo,"Centre for Software Reliability, City Univ. London, London, UK",IEEE Transactions on Software Engineering,20131028,2013,39,11,1521,1530,"In earlier work, [11] (henceforth LR), an analysis was presented of a 1-out-of-2 software-based system in which one channel was _ÑÒpossibly perfect_Ñù. It was shown that, at the aleatory level, the system pfd (probability of failure on demand) could be bounded above by the product of the pfd of channel A and the pnp (probability of nonperfection) of channel B. This result was presented as a way of avoiding the well-known difficulty that for two certainly-fallible channels, failures of the two will be dependent, i.e., the system pfd cannot be expressed simply as a product of the channel pfds. A price paid in this new approach for avoiding the issue of failure dependence is that the result is conservative. Furthermore, a complete analysis requires that account be taken of epistemic uncertainty-here concerning the numeric values of the two parameters pfd<sub>A</sub> and pnp<sub>B</sub>. Unfortunately this introduces a different difficult problem of dependence: estimating the dependence between an assessor's beliefs about the parameters. The work reported here avoids this problem by obtaining results that require only an assessor's marginal beliefs about the individual channels, i.e., they do not require knowledge of the dependence between these beliefs. The price paid is further conservatism in the results.",0098-5589;00985589,,10.1109/TSE.2013.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574864,Software reliability;epistemic uncertainty;fault tolerance;multiversion software;probability of failure;software diversity;software perfection,Cognition;Phase frequency detector;Safety;Software;Software reliability;Uncertainty,belief networks;failure analysis;probability;software reliability;uncertainty handling,1-out-of-2 software-based system;PFD;PNP;assessor marginal belief;certainly fallible channel;conservative reasoning;epistemic uncertainty;probability of failure on demand;probability of nonperfection;software perfection,,3,,15,,,20130805,Nov. 2013,,IEEE,IEEE Journals & Magazines,,9
Coverage Estimation in Model Checking with Bitstate Hashing,S. Ikeda; M. Jibiki; Y. Kuno,"NEC Corporation, Kawasaki",IEEE Transactions on Software Engineering,20130326,2013,39,4,477,486,"Explicit-state model checking which is conducted by state space search has difficulty in exploring satisfactory state space because of its memory requirements. Though bitstate hashing achieves memory efficiency, it cannot guarantee complete verification. Thus, it is desirable to provide a reliability indicator such as a coverage estimate. However, the existing approaches for coverage estimation are not very accurate when a verification run covers a small portion of state space. This mainly stems from the lack of information that reflects characteristics of models. Therefore, we propose coverage estimation methods using a growth curve that approximates an increase in reached states by enlarging a bloom filter. Our approaches improve estimation accuracy by leveraging the statistics from multiple verification runs. Coverage is estimated by fitting the growth curve to these statistics. Experimental results confirm the validity of the proposed growth curve and the applicability of our approaches to practical models. In fact, for practical models, our approaches outperformed the conventional ones when the actual coverage is relatively low.",0098-5589;00985589,,10.1109/TSE.2012.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226428,Coverage estimation;bitstate hashing;model checking,Accuracy;Equations;Estimation;Mathematical model;Probabilistic logic;Reliability;Space exploration,,,,0,,19,,,20120626,13-Apr,,IEEE,IEEE Journals & Magazines,,9
Model-Transformation Design Patterns,K. Lano; S. Kolahdouz-Rahimi,"Department of Informatics, King&#8217;s College London, London WC2R 2LS, United Kingdom",IEEE Transactions on Software Engineering,20141212,2014,40,12,1224,1259,"This paper defines a catalogue of patterns for the specification and design of model transformations, and provides a systematic scheme and classification of these patterns, together with pattern application examples in leading model transformation languages such as ATL, QVT, GrGen.NET, and others. We consider patterns for improving transformation modularization and efficiency and for reducing data storage requirements. We define a metamodel-based formalization of model transformation design patterns, and measurement-based techniques to guide the selection of patterns. We also provide an evaluation of the effectiveness of transformation patterns on a range of different case studies.",0098-5589;00985589,,10.1109/TSE.2014.2354344,EPSRC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891324,Model transformations;design patterns;model-driven development,Analytical models;Complexity theory;Semantics;Software development;Syntactics;Systematics;Unified modeling language,formal specification;object-oriented programming;pattern classification,ATL;GrGen.NET;QVT;data storage;metamodel-based formalization;model transformation languages;model-transformation design patterns;pattern application;pattern classification;systematic scheme;transformation modularization improvement,,13,,72,,,20140904,Dec. 1 2014,,IEEE,IEEE Journals & Magazines,,35
"A General Testability Theory: Classes, Properties, Complexity, and Testing Reductions",I. RodrÕ_guez; L. Llana; P. Rabanal,"Department of Sistemas Inform&#225;ticos y Computaci&#243;n, Universidad Complutense de Madrid, Madrid, Spain",IEEE Transactions on Software Engineering,20140905,2014,40,9,862,894,"In this paper we develop a general framework to reason about testing. The difficulty of testing is assessed in terms of the amount of tests that must be applied to determine whether the system is correct or not. Based on this criterion, five testability classes are presented and related. We also explore conditions that enable and disable finite testability, and their relation to testing hypotheses is studied. We measure how far incomplete test suites are from being complete, which allows us to compare and select better incomplete test suites. The complexity of finding that measure, as well as the complexity of finding minimum complete test suites, is identified. Furthermore, we address the reduction of testing problems to each other, that is, we study how the problem of finding test suites to test systems of some kind can be reduced to the problem of finding test suites for another kind of systems. This enables to export testing methods. In order to illustrate how general notions are applied to specific cases, many typical examples from the formal testing techniques domain are presented.",0098-5589;00985589,,10.1109/TSE.2014.2331690,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839051,Formal testing techniques;general testing frameworks,Abstracts;Complexity theory;Computational modeling;Computer languages;Probabilistic logic;Proposals;Testing,formal languages;formal verification,complexity;finite testability;formal testing techniques domain;general testability theory;incomplete test suites;testability classes;testing reductions,,7,,35,,,20140618,Sept. 1 2014,,IEEE,IEEE Journals & Magazines,,32
REPENT: Analyzing the Nature of Identifier Renamings,V. Arnaoudova; L. M. Eshkevari; M. D. Penta; R. Oliveto; G. Antoniol; Y. G. GuÕ©hÕ©neuc,"Polytech. Montreal, Montreal, QC, Canada",IEEE Transactions on Software Engineering,20140514,2014,40,5,502,532,"Source code lexicon plays a paramount role in software quality: poor lexicon can lead to poor comprehensibility and even increase software fault-proneness. For this reason, renaming a program entity, i.e., altering the entity identifier, is an important activity during software evolution. Developers rename when they feel that the name of an entity is not (anymore) consistent with its functionality, or when such a name may be misleading. A survey that we performed with 71 developers suggests that 39 percent perform renaming from a few times per week to almost every day and that 92 percent of the participants consider that renaming is not straightforward. However, despite the cost that is associated with renaming, renamings are seldom if ever documented-for example, less than 1 percent of the renamings in the five programs that we studied. This explains why participants largely agree on the usefulness of automatically documenting renamings. In this paper we propose REanaming Program ENTities (REPENT), an approach to automatically document-detect and classify-identifier renamings in source code. REPENT detects renamings based on a combination of source code differencing and data flow analyses. Using a set of natural language tools, REPENT classifies renamings into the different dimensions of a taxonomy that we defined. Using the documented renamings, developers will be able to, for example, look up methods that are part of the public API (as they impact client applications), or look for inconsistencies between the name and the implementation of an entity that underwent a high risk renaming (e.g., towards the opposite meaning). We evaluate the accuracy and completeness of REPENT on the evolution history of five open-source Java programs. The study indicates a precision of 88 percent and a recall of 92 percent. In addition, we report an exploratory study investigating and discussing how identifiers are renamed in the five programs, according to our taxonomy.",0098-5589;00985589,,10.1109/TSE.2014.2312942,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776542,Identifier renaming;empirical study;mining software repositories;program comprehension;refactoring,Documentation;Grammar;History;Java;Semantics;Software;Taxonomy,data flow analysis;pattern classification;software fault tolerance;software quality;source code (software),REPENT;data flow analysis;entity identifier;identifier renaming analysis;natural language tools;open-source Java programs;program entity renaming;public API;reanaming program entities;software evolution;software fault-proneness;software quality;source code lexicon;taxonomy dimensions,,14,,58,,,20140320,14-May,,IEEE,IEEE Journals & Magazines,,30
GreenDroid: Automated Diagnosis of Energy Inefficiency for Smartphone Applications,Y. Liu; C. Xu; S. C. Cheung; J. LÕ_,"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong",IEEE Transactions on Software Engineering,20140904,2014,40,9,911,940,"Smartphone applications' energy efficiency is vital, but many Android applications suffer from serious energy inefficiency problems. Locating these problems is labor-intensive and automated diagnosis is highly desirable. However, a key challenge is the lack of a decidable criterion that facilitates automated judgment of such energy problems. Our work aims to address this challenge. We conducted an in-depth study of 173 open-source and 229 commercial Android applications, and observed two common causes of energy problems: missing deactivation of sensors or wake locks, and cost-ineffective use of sensory data. With these findings, wepropose an automated approach to diagnosing energy problems in Android applications. Our approach explores an application's state space by systematically executing the application using Java PathFinder (JPF). It monitors sensor and wake lock operations to detect missing deactivation of sensors and wake locks. It also tracks the transformation and usage of sensory data and judges whether they are effectively utilized by the application using our state-sensitive data utilization metric. In this way, our approach can generate detailed reports with actionable information to assist developers in validating detected energy problems. We built our approach as a tool, GreenDroid, on top of JPF. Technically, we addressed the challenges of generating user interaction events and scheduling event handlers in extending JPF for analyzing Android applications. We evaluated GreenDroid using 13 real-world popular Android applications. GreenDroid completed energy efficiency diagnosis for these applications in a few minutes. It successfully located real energy problems in these applications, and additionally found new unreported energy problems that were later confirmed by developers.",0098-5589;00985589,,10.1109/TSE.2014.2323982,National High-Tech Research & Development Program; National Natural Science Foundation; New Century Excellent Talents in University; Research Grants Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815752,Smartphone application;automated diagnosis;energy inefficiency;green computing;sensory data utilization,Androids;Computer bugs;Google;Green products;Humanoid robots;Open source software;Sensors,Android (operating system);Java;power aware computing;program diagnostics;public domain software;smart phones,GreenDroid;JPF;Java PathFinder;automated diagnosis;automated energy inefficiency diagnosis;commercial Android applications;labor-intensive diagnosis;open-source Android applications;scheduling event handlers;smartphone applications;state-sensitive data utilization metric;user interaction events;wake lock operations,,18,,72,,,20140514,Sept. 1 2014,,IEEE,IEEE Journals & Magazines,,29
Dealing with Traceability in the MDDof Model Transformations,J. M. Vara; V. A. Bollati; ÕÅ. JimÕ©nez; E. Marcos,"Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain",IEEE Transactions on Software Engineering,20140617,2014,40,6,555,583,"Traceability has always been acknowledged as a relevant topic in Software Engineering. However, keeping track of the relationships between the different assets involved in a development process is a complex and tedious task. The fact that the main assets handled in any model-driven engineering project are models and model transformations eases the task. In order to take advantage of this scenario, which has not been appropriately capitalized on by the most widely adopted model transformation languages before, this work presents MeTAGeM-Trace, a methodological and technical proposal with which to support the model-driven development of model transformations that include trace generation. The underlying idea is to start from a high-level specification of the transformation which is subsequently refined into lower-level transformation models in terms of a set of DSLs until the source code that implements the transformation can be generated. Running this transformation produces not only the corresponding target models, but also a trace model between the elements of the source and target models. As part of the proposal, an EMF-based toolkit has been developed to support the development of ATL and ETL model transformations. This toolkit has been empirically validated by conducting a set of case studies following a systematic research methodology.",0098-5589;00985589,,10.1109/TSE.2014.2316132,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784505,Model-driven engineering;model transformations;traceability,Complexity theory;DSL;Data models;Object oriented modeling;Proposals;Software;Software engineering,research and development;software engineering;source code (software),ATL model transformations;DSL;EMF-based toolkit;ETL model transformations;MDD;MeTAGeM-Trace;lower-level transformation models;model transformation languages;model-driven engineering project;software engineering;source code;systematic research methodology;trace generation;traceability,,5,,79,,,20140408,June 1 2014,,IEEE,IEEE Journals & Magazines,,28
Analyzing Critical Decision-Based Processes,C. Damas; B. Lambeau; A. van Lamsweerde,"Dept. of Comput., Univ. Catholique de Louvain, Louvain-La-Neuve, Belgium",IEEE Transactions on Software Engineering,20140502,2014,40,4,338,365,"Decision-based processes are composed of tasks whose application may depend on explicit decisions relying on the state of the process environment. In specific domains such as healthcare, decision-based processes are often complex and critical in terms of timing and resources. The paper presents a variety of tool-supported techniques for analyzing models of such processes. The analyses allow a variety of errors to be detected early and incrementally on partial models, notably: inadequate decisions resulting from inaccurate or outdated information about the environment state; incomplete decisions; non-deterministic task selections; unreachable tasks along process paths; and violations of non-functional process requirements involving time, resources or costs. The proposed techniques are based on different instantiations of the same generic algorithm that propagates decorations iteratively through the process model. This algorithm in particular allows event-based models to be automatically decorated with state-based invariants. A formal language supporting both event-based and state-based specifications is introduced as a process modeling language to enable such analyses. This language mimics the informal flowcharts commonly used by process stakeholders. It extends High-Level Message Sequence Charts with guards on task-related and environment-related variables. The language provides constructs for specifying task compositions, task refinements, decision trees, multi-agent communication scenarios, and time and resource constraints. The proposed techniques are demonstrated on the incremental building and analysis of a complex model of a real protocol for cancer therapy.",0098-5589;00985589,,10.1109/TSE.2014.2312954,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776491,Process modeling;decision errors;domain-specific languages;formal specification;model verification;non-functional requirements;process analysis;safety-critical workflows,Algorithm design and analysis;Analytical models;Blood;Flowcharts;Medical treatment;Semantics;Unified modeling language,cancer;decision trees;formal languages;formal specification;multi-agent systems;patient treatment,cancer therapy;critical decision-based process analysis;decision trees;environment state;environment-related variables;event-based models;event-based specification;formal language;high-level message sequence charts;incomplete decisions;informal flowcharts;multiagent communication scenarios;nondeterministic task selections;nonfunctional process requirement violation;partial models;process modeling language;process paths;resource constraints;state-based invariants;state-based specification;task compositions;task refinements;task-related variables;time constraints;tool-supported techniques;unreachable tasks,,1,,80,,,20140320,14-Apr,,IEEE,IEEE Journals & Magazines,,27
Symbolic Crosschecking of Data-Parallel Floating-Point Code,P. Collingbourne; C. Cadar; P. H. J. Kelly,"Google Inc,",IEEE Transactions on Software Engineering,20140708,2014,40,7,710,737,"We present a symbolic execution-based technique for cross-checking programs accelerated using SIMD or OpenCL against an unaccelerated version, as well as a technique for detecting data races in OpenCL programs. Our techniques are implemented in KLEE-CL, a tool based on the symbolic execution engine KLEE that supports symbolic reasoning on the equivalence between expressions involving both integer and floating-point operations. While the current generation of constraint solvers provide effective support for integer arithmetic, the situation is different for floating-point arithmetic, due to the complexity inherent in such computations. The key insight behind our approach is that floating-point values are only reliably equal if they are essentially built by the same operations. This allows us to use an algorithm based on symbolic expression matching augmented with canonicalisation rules to determine path equivalence. Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation. To support the Intel Streaming SIMD Extension (SSE) instruction set, we lower SSE instructions to equivalent generic vector operations, which in turn are interpreted in terms of primitive integer and floating-point operations. To support OpenCL programs, we symbolically model the OpenCL environment using an OpenCL runtime library targeted to symbolic execution. We detect data races by keeping track of all memory accesses using a memory log, and reporting a race whenever we detect that two accesses conflict. By representing the memory log symbolically, we are also able to detect races associated with symbolically-indexed accesses of memory objects. We used KLEE-CL to prove the bounded equivalence between scalar and data-parallel versions of floating-point programs and find a number - f issues in a variety of open source projects that use SSE and OpenCL, including mismatches between implementations, memory errors, race conditions and a compiler bug.",0098-5589;00985589,,10.1109/TSE.2013.2297120,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698391,Data-parallel code;KLEE-CL;OpenCL;SIMD;floating point;symbolic execution,Computational modeling;Computer architecture;Kernel;Parallel processing;Programming;Semantics;Vectors,data handling;floating point arithmetic;parallel processing;program debugging,KLEE-CL;OpenCL environment;OpenCL programs;SIMD;SSE instruction set;compiler bug;crosschecking programs;data parallel floating point code;equivalent generic vector operations;floating point arithmetic;floating point operations;floating-point operations;floating-point programs;integer arithmetic;integer operations;intel streaming SIMD extension;memory accesses;memory errors;memory log;open source projects;phi node folding transformation;primitive integer operations;race conditions;symbolic crosschecking;symbolic execution based technique;symbolic execution engine;symbolic expression matching;symbolic reasoning,,6,,55,,,20140102,14-Jul,,IEEE,IEEE Journals & Magazines,,27
Test Code Quality and Its Relation to Issue Handling Performance,D. Athanasiou; A. Nugroho; J. Visser; A. Zaidman,"Software Improvement Group, Amstelplein 1, 1096HA Amsterdam, The Netherlands",IEEE Transactions on Software Engineering,20141110,2014,40,11,1100,1125,"Automated testing is a basic principle of agile development. Its benefits include early defect detection, defect causelocalization and removal of fear to apply changes to the code. Therefore, maintaining high quality test code is essential. This study introduces a model that assesses test code quality by combining source code metrics that reflect three main aspects of test codequality: completeness, effectiveness and maintainability. The model is inspired by the Software Quality Model of the SoftwareImprovement Group which aggregates source code metrics into quality ratings based on benchmarking. To validate the model we assess the relation between test code quality, as measured by the model, and issue handling performance. An experiment isconducted in which the test code quality model is applied to <inline-formula><tex-math notation=""LaTeX"">$18$</tex-math><alternatives> <inline-graphic xlink:type=""simple"" xlink:href=""zaidman-ieq1-2342227.gif""/></alternatives></inline-formula> open source systems. The test quality ratings are tested for correlation with issue handling indicators, which are obtained by mining issue repositories. In particular, we study the (1) defect resolution speed, (2) throughput and (3) productivity issue handling metrics. The results reveal a significant positive correlation between test code quality and two out of the three issue handling metrics (throughput and productivity), indicating that good test code quality positively influences issue handling performance.",0098-5589;00985589,,10.1109/TSE.2014.2342227,NWO TestRoots project; RAAK-PRO project EQuA; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6862882,Testing;bugs;defects;measurement;metrics,Benchmark testing;Correlation;Measurement;Productivity;Software;Throughput,,,,16,,90,,,20140723,Nov. 1 2014,,IEEE,IEEE Journals & Magazines,,25
Rate-Based Queueing Simulation Model of Open Source Software Debugging Activities,C. T. Lin; Y. F. Li,"Department of Computer Science and Information Engineering, National Chiayi University, Chiayi, Taiwan",IEEE Transactions on Software Engineering,20141110,2014,40,11,1075,1099,"Open source software (OSS) approach has become increasingly prevalent for software development. As the widespread utilization of OSS, the reliability of OSS products becomes an important issue. By simulating the testing and debugging processes of software life cycle, the rate-based queueing simulation model has shown its feasibility for closed source software (CSS) reliability assessment. However, the debugging activities of OSS projects are different in many ways from those of CSS projects and thus the simulation approach needs to be calibrated for OSS projects. In this paper, we first characterize the debugging activities of OSS projects. Based on this, we propose a new rate-based queueing simulation framework for OSS reliability assessment including the model and the procedures. Then a decision model is developed to determine the optimal version-updating time with respect to two objectives: minimizing the time for version update, and maximizing OSS reliability. To illustrate the proposed framework, three real datasets from Apache and GNOME projects are used. The empirical results indicate that our framework is able to effectively approximate the real scenarios. Moreover, the influences of the core contributor staffing levels are analyzed and the optimal version-updating times are obtained.",0098-5589;00985589,,10.1109/TSE.2014.2354032,"National Science Council, Taiwan; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891380,Queueing theory;bug fixing;bug reporting;multi-attribute utility theory (MAUT);non-homogeneous continuous time Markov chain (NHCTMC);open source software (OSS);optimal version-updating time;rate-based simulation;report judgment,Analytical models;Cascading style sheets;Debugging;Software;Software reliability;Stochastic processes,configuration management;program debugging;program testing;project management;public domain software;queueing theory;software reliability,CSS reliability assessment;OSS approach;OSS products reliability;OSS projects;closed source software;debugging activities;debugging processes;decision model;open source software;optimal version-updating time;rate-based queueing simulation model;software development;software life cycle;testing processes,,2,,50,,,20140904,Nov. 1 2014,,IEEE,IEEE Journals & Magazines,,24
Multi-Objective Quality-Driven Service Selection_ÑÓA Fully Polynomial Time Approximation Scheme,I. Trummer; B. Faltings; W. Binder,"Artificial Intell. Lab., Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland",IEEE Transactions on Software Engineering,20140304,2014,40,2,167,191,"The goal of multi-objective quality-driven service selection (QDSS) is to find service selections for a workflow whose quality-of-service (QoS) values are Pareto-optimal. We consider multiple QoS attributes such as response time, cost, and reliability. A selection is Pareto-optimal if no other selection has better QoS values for some attributes and at least equivalent values for all others. Exact algorithms have been proposed that find all Pareto-optimal selections. They suffer however from exponential complexity. Randomized algorithms scale well but do not offer any formal guarantees on result precision. We present the first approximation scheme for QDSS. It aims at the sweet spot between exact and randomized algorithms: It combines polynomial complexity with formal result precision guarantees. A parameter allows to seamlessly trade result precision against efficiency. We formally analyze complexity and precision guarantees and experimentally compare our algorithm against exact and randomized approaches. Comparing with exact algorithms, our approximation scheme allows to reduce optimization time from hours to seconds. Its approximation error remains below 1.4 percent while randomized algorithms come close to the theoretical maximum.",0098-5589;00985589,,10.1109/TSE.2013.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687160,Quality-driven service selection;approximation algorithms;multi-objective optimization,Approximation algorithms;Approximation methods;Complexity theory;Motion pictures;Optimization;Polynomials;Quality of service,Pareto optimisation;polynomial approximation;quality of service;software quality,Pareto optimal;Pareto optimal selections;QDSS;QoS values;approximation error;exponential complexity;fully polynomial time approximation scheme;least equivalent values;multiobjective quality driven service selection;optimization time;polynomial complexity;quality-of-service;randomized algorithms,,14,,35,,,20131218,Feb. 2014,,IEEE,IEEE Journals & Magazines,,24
Variability in Software Systems_ÑÓA Systematic Literature Review,M. Galster; D. Weyns; D. Tofan; B. Michalik; P. Avgeriou,"Dept. of Comput. Sci. & Software Eng., Univ. of Canterbury, Christchurch, New Zealand",IEEE Transactions on Software Engineering,20140331,2014,40,3,282,306,"Context: Variability (i.e., the ability of software systems or artifacts to be adjusted for different contexts) became a key property of many systems. Objective: We analyze existing research on variability in software systems. We investigate variability handling in major software engineering phases (e.g., requirements engineering, architecting). Method: We performed a systematic literature review. A manual search covered 13 premium software engineering journals and 18 premium conferences, resulting in 15,430 papers searched and 196 papers considered for analysis. To improve reliability and to increase reproducibility, we complemented the manual search with a targeted automated search. Results: Software quality attributes have not received much attention in the context of variability. Variability is studied in all software engineering phases, but testing is underrepresented. Data to motivate the applicability of current approaches are often insufficient; research designs are vaguely described. Conclusions: Based on our findings we propose dimensions of variability in software engineering. This empirically grounded classification provides a step towards a unifying, integrated perspective of variability in software systems, spanning across disparate or loosely coupled research themes in the software engineering community. Finally, we provide recommendations to bridge the gap between research and practice and point to opportunities for future research.",0098-5589;00985589,,10.1109/TSE.2013.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682901,Variability;software engineering;systematic review,Context;Data collection;Decision support systems;Manuals;Software engineering;Software systems;Systematics,program testing;software product lines;software reliability;software reviews,manual search;reproducibility;software engineering community;software engineering journals;software engineering phase;software reliability;software systems;software testing;systematic literature review;targeted automated search;variability handling,,35,,60,,,20131212,14-Mar,,IEEE,IEEE Journals & Magazines,,24
Quality-Aware Service Selection for Service-Based Systems Based on Iterative Multi-Attribute Combinatorial Auction,Q. He; J. Yan; H. Jin; Y. Yang,"Sch. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China",IEEE Transactions on Software Engineering,20140304,2014,40,2,192,215,"The service-oriented paradigm offers support for engineering service-based systems (SBSs) based on service composition where existing services are composed to create new services. The selection of services with the aim to fulfil the quality constraints becomes critical and challenging to the success of SBSs, especially when the quality constraints are stringent. However, none of the existing approaches for quality-aware service composition has sufficiently considered the following two critical issues to increase the success rate of finding a solution: 1) the complementarities between services; and 2) the competition among service providers. This paper proposes a novel approach called combinatorial auction for service selection (CASS) to support effective and efficient service selection for SBSs based on combinatorial auction. In CASS, service providers can bid for combinations of services and apply discounts or premiums to their offers for the multi-dimensional quality of the services. Based on received bids, CASS attempts to find a solution that achieves the SBS owner's optimisation goal while fulfilling all quality constraints for the SBS. When a solution cannot be found based on current bids, the auction iterates so that service providers can improve their bids to increase their chances of winning. This paper systematically describes the auction process and the supporting mechanisms. Experimental results show that by exploiting the complementarities between services and the competition among service providers, CASS significantly outperforms existing quality-aware service selection approaches in finding optimal solutions and guaranteeing system optimality. Meanwhile, the duration and coordination overhead of CASS are kept at satisfactory levels in scenarios on different scales.",0098-5589;00985589,,10.1109/TSE.2013.2297911,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702520,Service-based system;combinatorial auction;integer programming;quality of service;service composition;service selection;web services,Abstracts;Contracts;Optimization;Quality of service;Scattering;Time factors,Web services;combinatorial mathematics;iterative methods;quality of service;service-oriented architecture;software quality,CASS approach;SBS;auction mechanisms;auction process;iterative multiattribute combinatorial auction;optimal solutions;quality constraints;quality-aware service composition;quality-aware service selection;service creation;service providers;service quality;service-based systems;service-oriented paradigm;system optimality,,20,,61,,,20140109,Feb. 2014,,IEEE,IEEE Journals & Magazines,,23
Methodbook: Recommending Move Method Refactorings via Relational Topic Models,G. Bavota; R. Oliveto; M. Gethers; D. Poshyvanyk; A. De Lucia,"University of Sannio, Benevento, Italy",IEEE Transactions on Software Engineering,20140708,2014,40,7,671,694,"During software maintenance and evolution the internal structure of the software system undergoes continuous changes. These modifications drift the source code away from its original design, thus deteriorating its quality, including cohesion and coupling of classes. Several refactoring methods have been proposed to overcome this problem. In this paper we propose a novel technique to identify Move Method refactoring opportunities and remove the Feature Envy bad smell from source code. Our approach, coined as Methodbook, is based on relational topic models (RTM), a probabilistic technique for representing and modeling topics, documents (in our case methods) and known relationships among these. Methodbook uses RTM to analyze both structural and textual information gleaned from software to better support move method refactoring. We evaluated Methodbook in two case studies. The first study has been executed on six software systems to analyze if the move method operations suggested by Methodbook help to improve the design quality of the systems as captured by quality metrics. The second study has been conducted with eighty developers that evaluated the refactoring recommendations produced by Methodbook. The achieved results indicate that Methodbook provides accurate and meaningful recommendations for move method refactoring operations.",0098-5589;00985589,,10.1109/TSE.2013.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684534,Refactoring;empirical studies;relational topic models,Couplings;Educational institutions;Electronic mail;Measurement;Object oriented modeling;Software systems,software maintenance;software metrics;source code (software),Methodbook;modifications drift;quality metrics;recommending move method refactorings;relational topic model;software development;software maintenance;source code,,34,,57,,,20131216,14-Jul,,IEEE,IEEE Journals & Magazines,,23
Reducing Masking Effects in CombinatorialInteraction Testing: A Feedback DrivenAdaptive Approach,C. Yilmaz; E. Dumlu; M. B. Cohen; A. Porter,"Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey",IEEE Transactions on Software Engineering,20140304,2014,40,1,43,66,"The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations. The basic justification for CIT approaches is that they can cost-effectively exercise all system behaviors caused by the settings of t or fewer options. We conjecture, however, that in practice some of these behaviors are not actually tested because of unanticipated masking effects - test case failures that perturb system execution so as to prevent some behaviors from being exercised. While prior research has identified this problem, most solutions require knowing the masking effects a priori. In practice this is impractical, if not impossible. In this work, we reduce the harmful consequences of masking effects. First we define a novel interaction testing criterion, which aims to ensure that each test case has a fair chance to test all valid t-way combinations of option settings. We then introduce a feedback driven adaptive combinatorial testing process (FDA-CIT) to materialize this criterion in practice. At each iteration of FDA-CIT, we detect potential masking effects, heuristically isolate their likely causes (i.e., fault characterization), and then generate new samples that allow previously masked combinations to be tested in configurations that avoid the likely failure causes. The iterations end when the new interaction testing criterion has been satisfied. This paper compares two different fault characterization approaches - an integral part of the proposed approach, and empirically assesses their effectiveness and efficiency in removing masking effects on two widely used open source software systems. It also compares FDA-CIT against error locating arrays, a state of the art approach for detecting and locating failures. Furthermore, the scalability of the proposed approach is evaluated by comparing it with per- ect test scenarios, in which all masking effects are known a priori. Our results suggest that masking effects do exist in practice, and that our approach provides a promising and efficient way to work around them, without requiring that masking effects be known a priori.",0098-5589;00985589,,10.1109/TSE.2013.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654147,Combinatorial testing;adaptive testing;covering arrays;software quality assurance,Adaptive arrays;Educational institutions;Electronic mail;Scalability;Servers;Software systems;Testing,program testing;public domain software;software fault tolerance,CIT approach;FDA-CIT process;combinatorial interaction testing;configuration spaces;covering arrays;error locating arrays;fault characterization approaches;fault detection;fault location;feedback driven adaptive approach;interaction testing criterion;masking effects reduction;open source software systems;perfect test scenarios;potential masking effects detection;software systems;system execution;t-way combinations;test case failures,,8,,44,,,20131104,Jan. 2014,,IEEE,IEEE Journals & Magazines,,23
Supporting Process Model Validation through Natural Language Generation,H. Leopold; J. Mendling; A. Polyvyanyy,"WU Vienna, Austria.",IEEE Transactions on Software Engineering,20140808,2014,40,8,818,840,"The design and development of process-aware information systems is often supported by specifying requirements as business process models. Although this approach is generally accepted as an effective strategy, it remains a fundamental challenge to adequately validate these models given the diverging skill set of domain experts and system analysts. As domain experts often do not feel confident in judging the correctness and completeness of process models that system analysts create, the validation often has to regress to a discourse using natural language. In order to support such a discourse appropriately, so-called verbalization techniques have been defined for different types of conceptual models. However, there is currently no sophisticated technique available that is capable of generating natural-looking text from process models. In this paper, we address this research gap and propose a technique for generating natural language texts from business process models. A comparison with manually created process descriptions demonstrates that the generated texts are superior in terms of completeness, structure, and linguistic complexity. An evaluation with users further demonstrates that the texts are very understandable and effectively allow the reader to infer the process model semantics. Hence, the generated texts represent a useful input for process model validation.",0098-5589;00985589,,10.1109/TSE.2014.2327044,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823180,Business process model validation;natural language text generation;verbalization,Adaptation models;Analytical models;Business;Context;Context modeling;Natural languages;Unified modeling language,information systems;natural language processing,business process models;completeness complexity;linguistic complexity;natural language generation;natural language text generation;natural-looking text generation;process model completeness;process model correctness;process model validation;process-aware information systems;structure complexity;verbalization techniques,,15,,110,,,20140529,Aug. 1 2014,,IEEE,IEEE Journals & Magazines,,22
Automated Fixing of Programs with Contracts,Y. Pei; C. A. Furia; M. Nordio; Y. Wei; B. Meyer; A. Zeller,"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland",IEEE Transactions on Software Engineering,20140514,2014,40,5,427,449,"This paper describes AutoFix, an automatic debugging technique that can fix faults in general-purpose software. To provide high-quality fix suggestions and to enable automation of the whole debugging process, AutoFix relies on the presence of simple specification elements in the form of contracts (such as pre- and postconditions). Using contracts enhances the precision of dynamic analysis techniques for fault detection and localization, and for validating fixes. The only required user input to the AutoFix supporting tool is then a faulty program annotated with contracts; the tool produces a collection of validated fixes for the fault ranked according to an estimate of their suitability. In an extensive experimental evaluation, we applied AutoFix to over 200 faults in four code bases of different maturity and quality (of implementation and of contracts). AutoFix successfully fixed 42 percent of the faults, producing, in the majority of cases, corrections of quality comparable to those competent programmers would write; the used computational resources were modest, with an average time per fix below 20 minutes on commodity hardware. These figures compare favorably to the state of the art in automated program fixing, and demonstrate that the AutoFix approach is successfully applicable to reduce the debugging burden in real-world scenarios.",0098-5589;00985589,,10.1109/TSE.2014.2312918,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776507,Automatic program repair;contracts;dynamic analysis,Automation;Contracts;Debugging;Indexes;Libraries;Software;Software engineering,contracts;fault diagnosis;program debugging;program diagnostics;software tools,AutoFix approach;AutoFix supporting tool;automated program fixing;automatic debugging technique;commodity hardware;computational resources;contracts;debugging process;dynamic analysis techniques;fault detection;fault localization;general-purpose software fault fixing;high-quality fix suggestions,,20,,68,,,20140320,14-May,,IEEE,IEEE Journals & Magazines,,22
Specification and Verification of Normative Texts Using C-O Diagrams,G. DÕ_az; M. E. Cambronero; E. MartÕ_nez; G. Schneider,"Department of Computer Science , University of Castilla-La Mancha, Albacete, Spain",IEEE Transactions on Software Engineering,20140808,2014,40,8,795,817,"C-O diagrams have been introduced as a means to have a more visual representation of normative texts and electronic contracts, where it is possible to represent the obligations, permissions and prohibitions of the different signatories, as well as the penalties resulting from non-fulfillment of their obligations and prohibitions. In such diagrams we are also able to represent absolute and relative timing constraints. In this paper we present a formal semantics for C-O diagrams based on timed automata extended with information regarding the satisfaction and violation of clauses in order to represent different deontic modalities. As a proof of concept, we apply our approach to two different case studies, where the method presented here has successfully identified problems in the specification.",0098-5589;00985589,,10.1109/TSE.2013.54,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657668,C-O diagrams;Normative documents;deontic logic;electronic contracts;formal verification;timed automata;visual models,Automata;Clocks;Contracts;Cost accounting;Formal languages;Semantics;Synchronization,automata theory;formal specification;formal verification;text analysis,C-O diagrams;deontic modalities;electronic contracts;formal semantics;formal specification;formal verification;normative texts;timed automata;timing constraints;visual representation,,3,,33,,,20131107,Aug. 2014,,IEEE,IEEE Journals & Magazines,,22
Construction and Validation of an Instrument for Measuring Programming Skill,G. R. Bergersen; D. I. K. SjÕôberg; T. DybÕ‚,"Department of Informatics, University of Oslo, Oslo, Norway",IEEE Transactions on Software Engineering,20141212,2014,40,12,1163,1184,"Skilled workers are crucial to the success of software development. The current practice in research and industry for assessing programming skills is mostly to use proxy variables of skill, such as education, experience, and multiple-choice knowledge tests. There is as yet no valid and efficient way to measure programming skill. The aim of this research is to develop a valid instrument that measures programming skill by inferring skill directly from the performance on programming tasks. Over two days, 65 professional developers from eight countries solved 19 Java programming tasks. Based on the developers' performance, the Rasch measurement model was used to construct the instrument. The instrument was found to have satisfactory (internal) psychometric properties and correlated with external variables in compliance with theoretical expectations. Such an instrument has many implications for practice, for example, in job recruitment and project allocation.",0098-5589;00985589,,10.1109/TSE.2014.2348997,Research Laboratory and the Research Council of Norway through; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6882243,Skill;empirical software engineering;instrument;measurement;performance;programming,Personnel;Programming profession;Software design;Software development;Software quality,Java;personnel;software development management,Java programming tasks;Rasch measurement model;job recruitment;multiple-choice knowledge tests;professional developers;programming skill measurement;programming tasks;project allocation;proxy variables;psychometric properties;skilled workers;software development,,8,,130,,,20140822,Dec. 1 2014,,IEEE,IEEE Journals & Magazines,,21
On the Effectiveness of Contracts as Test Oracles in the Detection and Diagnosis of Functional Faults in Concurrent Object-Oriented Software,W. Araujo; L. C. Briand; Y. Labiche,"Juniper Networks, 1194&#160;N Mathilda Ave, Sunnyvale, CA 94089",IEEE Transactions on Software Engineering,20141009,2014,40,10,971,992,"Design by contract (DbC) is a software development methodology that focuses on clearly defining the interfaces between components to produce better quality object-oriented software. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. Using Java as the target programming language, we tackle such challenges by augmenting the Java Modelling Language (JML) and modifying the JML compiler (jmlc) to generate runtime assertion checking code to support DbC in concurrent programs. We applied our solution in a carefully designed case study on a highly concurrent industrial software system from the telecommunications domain to assess the effectiveness of contracts as test oracles in detecting and diagnosing functional faults in concurrent software. Based on these results, clear and objective requirements are defined for contracts to be effective test oracles for concurrent programs whilst balancing the effort to design them. Effort is measured indirectly through the contract complexity measure (CCM), a measure we define. Main results include that contracts of a realistic level of completeness and complexity can detect around 76 percent of faults and reduce the diagnosis effort for such faults tenfold. We, therefore, show that DbC can be applied to concurrent software and can be a valuable tool to improve the economics of software engineering.",0098-5589;00985589,,10.1109/TSE.2014.2339829,Juniper Networks; Luxembourg's National Research Fund; NSERC Discovery; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6857355,Concurrent programming;Software Quality/SQAëèMeasurement applied to SQA and V&V;Software/Program VerificationëèProgramming by contract;object-oriented programming,Concurrent computing;Contracts;Interference;Java;Message systems;Programming;Software,Java;concurrency control;contracts;fault diagnosis;object-oriented methods;parallel programming;program compilers;program testing;software quality;software reliability,CCM;DbC;JML compiler;JMLC;Java modelling language;concurrent industrial software system;concurrent programs;concurrent software;contract complexity measure;design by contract;functional fault detection;functional fault diagnosing;object-oriented software quality;runtime assertion checking code;sequential programs;software development methodology;software engineering;target programming language;telecommunication domain;test oracles,,1,,39,,,20140716,Oct. 1 2014,,IEEE,IEEE Journals & Magazines,,21
Predicting Consistency-Maintenance Requirement of Code Clonesat Copy-and-Paste Time,X. Wang; Y. Dang; L. Zhang; D. Zhang; E. Lan; H. Mei,"Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, and with the Department of Computer Science, University of Texas, San Antonio",IEEE Transactions on Software Engineering,20140808,2014,40,8,773,794,"Code clones have always been a double edged sword in software development. On one hand, it is a very convenient way to reuse existing code, and to save coding effort. On the other hand, since developers may need to ensure consistency among cloned code segments, code clones can lead to extra maintenance effort and even bugs. Recently studies on the evolution of code clones show that only some of the code clones experience consistent changes during their evolution history. Therefore, if we can accurately predict whether a code clone will experience consistent changes, we will be able to provide useful recommendations to developers onleveraging the convenience of some code cloning operations, while avoiding other code cloning operations to reduce future consistency maintenance effort. In this paper, we define a code cloning operation as consistency-maintenance-required if its generated code clones experience consistent changes in the software evolution history, and we propose a novel approach that automatically predicts whether a code cloning operation requires consistency maintenance at the time point of performing copy-and-paste operations. Our insight is that whether a code cloning operation requires consistency maintenance may relate to the characteristics of the code to be cloned and the characteristics of its context. Based on a number of attributes extracted from the cloned code and the context of the code cloning operation, we use Bayesian Networks, a machine-learning technique, to predict whether an intended code cloning operation requires consistency maintenance. We evaluated our approach on four subjects-two large-scale Microsoft software projects, and two popular open-source software projects-under two usage scenarios: 1) recommend developers to perform only the cloning operations predicted to be very likely to be consistency-maintenance-free, and 2) recommend developers to perform all cloning operations unless they are predicted very likely to be consiste- cy-maintenance-required. In the first scenario, our approach is able to recommend developers to perform more than 50 percent cloning operations with a precision of at least 94 percent in the four subjects. In the second scenario, our approach is able to avoid 37 to 72 percent consistency-maintenance-required code clones by warning developers on only 13 to 40 percent code clones, in the four subjects.",0098-5589;00985589,,10.1109/TSE.2014.2323972,National 863 Program; National 973 Program; Natural Science Foundation; Science Fund for Creative Research Groups; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815760,Code cloning;consistency maintenance;programming aid,Bayes methods;Cloning;Educational institutions;History;Maintenance engineering;Software;Training,belief networks;learning (artificial intelligence);public domain software;software maintenance,Bayesian networks;Microsoft software projects;code clones;code cloning operations;consistency maintenance effort;consistency-maintenance requirement;copy-and-paste time;machine-learning technique;maintenance effort;open-source software projects;software development,,4,,46,,,20140514,Aug. 1 2014,,IEEE,IEEE Journals & Magazines,,21
Bayesian Networks For Evidence-Based Decision-Making in Software Engineering,A. T. Misirli; A. B. Bener,"Dept. of Inf. Process. Sci., Univ. of Oulu, Oulu, Finland",IEEE Transactions on Software Engineering,20140616,2014,40,6,533,554,"Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making.",0098-5589;00985589,,10.1109/TSE.2014.2321179,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808495,Bayesian networks;Bayesian statistics;Evidence-based decision-making;post-release defects;software metrics;software reliability,Bayes methods;Buildings;Decision making;Medical services;Software;Software engineering;Systematics,belief networks;decision making;software metrics;software reliability,Bayesian networks;SE;associated uncertainty;computational biology;evidence-based decision-making;health care decision-making;hybrid BN node;natural statistical framework;recommendation systems;software engineering;software metrics;software reliability;systematic mapping study,,13,,81,,,20140430,June 1 2014,,IEEE,IEEE Journals & Magazines,,21
Formulating Cost-Effective Monitoring Strategies for Service-Based Systems,Q. He; J. Han; Y. Yang; H. Jin; J. G. Schneider; S. Versteeg,"Sch. of Software & Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia",IEEE Transactions on Software Engineering,20140514,2014,40,5,461,482,"When operating in volatile environments, service-based systems (SBSs) that are dynamically composed from component services must be monitored in order to guarantee timely and successful delivery of outcomes in response to user requests. However, monitoring consumes resources and very often impacts on the quality of the SBSs being monitored. Such resource and system costs need to be considered in formulating monitoring strategies for SBSs. The critical path of a composite SBS, i.e., the execution path in the service composition with the maximum execution time, is of particular importance in cost-effective monitoring as it determines the response time of the entire SBS. In volatile operating environments, the critical path of an SBS is probabilistic, as every execution path can be critical with a certain probability, i.e., its criticality. As such, it is important to estimate the criticalities of different execution paths when deciding which parts of the SBS to monitor. Furthermore, cost-effective monitoring also requires management of the trade-off between the benefit and cost of monitoring. In this paper, we propose CriMon, a novel approach to formulating and evaluating monitoring strategies for SBSs. CriMon first calculates the criticalities of the execution paths and the component services of an SBS and then, based on those criticalities, generates the optimal monitoring strategy considering both the benefit and cost of monitoring. CriMon has two monitoring strategy formulation methods, namely local optimisation and global optimisation. In-lab experimental results demonstrate that the response time of an SBS can be managed cost-effectively through CriMon-based monitoring. The effectiveness and efficiency of the two monitoring strategy formulation methods are also evaluated and compared.",0098-5589;00985589,,10.1109/TSE.2013.48,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642029,QoS;Service-based system;cost of monitoring;criticality;monitoring;response time;value of monitoring;web service,Monitoring;Probabilistic logic;Probability;Quality of service;Runtime;Scattering;Time factors,Web services;service-oriented architecture;system monitoring,CriMon-based monitoring approach;SBSs;Web service;component services;cost-effective monitoring strategy;global optimisation;local optimisation;monitoring strategy formulation methods;service composition;service-based systems;service-oriented computing;system costs,,7,,65,,,20131021,14-May,,IEEE,IEEE Journals & Magazines,,21
Using Traceability Links to Recommend Adaptive Changes for Documentation Evolution,B. Dagenais; M. P. Robillard,"Resulto Inc., Montreal, QC, Canada",IEEE Transactions on Software Engineering,20141110,2014,40,11,1126,1146,"Developer documentation helps developers learn frameworks and libraries, yet developing and maintaining accurate documentation requires considerable effort and resources. Contributors who work on developer documentation often need to manually track all changes in the code, determine which changes are significant enough to document, and then, adapt the documentation. We propose AdDoc, a technique that automatically discovers documentation patterns, i.e., coherent sets of code elements that are documented together, and that reports violations of these patterns as the code and the documentation evolves. We evaluated our approach in a retrospective analysis of four Java open source projects and found that at least 50 percent of all the changes in the documentation were related to existing documentation patterns. Our technique allows contributors to quickly adapt existing documentation, so that they can focus their documentation effort on the new features.",0098-5589;00985589,,10.1109/TSE.2014.2347969,NSERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6878435,Documentation;frameworks;maintainability,Concrete;Documentation;Java;Joining processes;Libraries;Manuals;Sections,Java;data mining;program diagnostics;public domain software;system documentation,AdDoc;Java open source projects;adaptive changes;automatic documentation pattern discovery;code elements;developer documentation;documentation evolution;traceability links,,4,,24,,,20140814,Nov. 1 2014,,IEEE,IEEE Journals & Magazines,,20
Input-Sensitive Profiling,E. Coppa; C. Demetrescu; I. Finocchi,"Department of Computer Science, Sapienza University of Rome, Italy",IEEE Transactions on Software Engineering,20141212,2014,40,12,1185,1205,"In this article we present a building block technique and a toolkit towards automatic discovery of workload-dependentperformance bottlenecks. From one or more runs of a program, our profiler automatically measures how the performance of individual routines scales as a function of the input size, yielding clues to their growth rate. The output of the profiler is, for each executed routine of the program, a set of tuples that aggregate performance costs by input size. The collected profiles can be used to produceperformance plots and derive trend functions by statistical curve fitting techniques. A key feature of our method is the ability toautomatically measure the size of the input given to a generic code fragment: to this aim, we propose an effective metric for estimating the input size of a routine and show how to compute it efficiently. We discuss several examples, showing that our approach can reveal asymptotic bottlenecks that other profilers may fail to detect and can provide useful characterizations of the workload and behavior of individual routines in the context of mainstream applications, yielding several code optimizations as well as algorithmic improvements. To prove the feasibility of our techniques, we implemented a Valgrind tool called aprof and performed an extensive experimentalevaluation on the SPEC CPU2006 benchmarks. Our experiments show that aprof delivers comparable performance to otherprominent Valgrind tools, and can generate informative plots even from single runs on typical workloads for mostalgorithmically-critical routines.",0098-5589;00985589,,10.1109/TSE.2014.2339825,"Italian Ministry of Education, University, and Research (MIUR); ëèAMANDA-Algorithmics for MAssive and Networked DAtaëè; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6858059,Performance profiling;asymptotic analysis;dynamic program analysis;instrumentation,Algorithm design and analysis;Benchmark testing;Context modeling;Market research,curve fitting;program diagnostics;software performance evaluation;software tools;statistical analysis,SPEC CPU2006 benchmarks;Valgrind tools;aprof;automatic workload-dependent performance bottleneck discovery;building block technique;code optimizations;experimental evaluation;generic code fragment;growth rate;input-sensitive profiling;performance plots;program executed routine;statistical curve fitting techniques;tuples,,4,,60,,,20140717,Dec. 1 2014,,IEEE,IEEE Journals & Magazines,,20
A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection,W. Kessentini; M. Kessentini; H. Sahraoui; S. Bechikh; A. Ouni,"Department of Computer Science, University of Montreal, Montreal, Quebec, Canada",IEEE Transactions on Software Engineering,20140904,2014,40,9,841,861,"We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells.",0098-5589;00985589,,10.1109/TSE.2014.2331057,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6835187,Search-based software engineering;code-smells;distributed evolutionary algorithms;software quality,Computational modeling;Detectors;Evolutionary computation;Measurement;Optimization;Sociology;Statistics,evolutionary computation;public domain software;search problems;software engineering;statistical analysis,P-EA approach;code-smells detection;cooperative parallel search-based software engineering approach;distributed optimization problem;open source systems;optimization process;parallel evolutionary algorithms;random search;single population-based approaches;statistical analysis,,15,,57,,,20140616,Sept. 1 2014,,IEEE,IEEE Journals & Magazines,,20
Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines,C. Henard; M. Papadakis; G. Perrouin; J. Klein; P. Heymans; Y. Le Traon,"Interdiscipl. Centre for Security, Reliability & Trust, Univ. of Luxembourg, Luxembourg, Luxembourg",IEEE Transactions on Software Engineering,20140708,2014,40,7,650,670,"Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.",0098-5589;00985589,,10.1109/TSE.2014.2327020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823132,Software product lines;T-wise Interactions;prioritization;search-based approaches;similarity;testing,Arrays;Context;Frequency modulation;Linux;Scalability;Software;Testing,combinatorial mathematics;program testing;software product lines,SPL;combinatorial explosion;configuration process;product configuration generation;product configurations;search based approach;similarity heuristic;software product lines;test configurations,,39,,64,,,20140529,July 1 2014,,IEEE,IEEE Journals & Magazines,,20
An Empirical Analysis of Business Process Execution Language Usage,M. Hertis; M. B. Juric,"Laboratory for Integration of Information Systems, Faculty of Computer and Information Science, University of Ljubljana, Trzaska cesta 25, Ljubljana, Slovenia, and Seltron d.o.o., Trzaska cesta 85&#160;a, Maribor",IEEE Transactions on Software Engineering,20140808,2014,40,8,738,757,"The current state of executable business process languages allows for and demands optimization of design practices and specifications. In this paper, we present the first empirical study that analyses Web Services Business Process Execution Language (WS-BPEL or BPEL) usage and characteristics of real world executable business processes. We have analysed 1,145 BPEL processes by measuring activity usage and process complexity. In addition, we investigated the occurrence of activity usage patterns. The results revealed that the usage frequency of BPEL activities varies and that some activities have a strong co-occurrence. BPEL activities often appear in activity patterns that are repeated in multiple processes. Furthermore, the current process complexity metrics have proved to be inadequate for measuring BPEL process complexity. The empirical results provide fundamental knowledge on how BPEL specification and process design practices can be improved. We propose BPEL design guidelines and BPEL language improvements for the design of more understandable and less complex processes. The results are of interest to business process language designers, business process tool developers, business process designers and developers, and software engineering researchers, and contribute to the general understanding of BPEL and service-oriented architecture.",0098-5589;00985589,,10.1109/TSE.2014.2322618,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6812231,WS-BPEL Analysis;complexity measure;empirical study;process complexity;process comprehension;process patterns;service composition,Business;Complexity theory;Guidelines;Measurement;Semantics;Syntactics;XML,Web Services Business Process Execution Language;service-oriented architecture,BPEL activities;BPEL design guidelines;BPEL language improvements;WS-BPEL;Web services business process execution language;activity usage;design practices;design specifications;empirical analysis;executable business processes;process complexity;service-oriented architecture,,7,,68,,,20140508,Aug. 1 2014,,IEEE,IEEE Journals & Magazines,,19
Improved Evolutionary Algorithm Design for the Project Scheduling Problem Based on Runtime Analysis,L. L. Minku; D. Sudholt; X. Yao,"CERCIA, Univ. of Birmingham, Birmingham, UK",IEEE Transactions on Software Engineering,20140304,2014,40,1,83,102,"Several variants of evolutionary algorithms (EAs) have been applied to solve the project scheduling problem (PSP), yet their performance highly depends on design choices for the EA. It is still unclear how and why different EAs perform differently. We present the first runtime analysis for the PSP, gaining insights into the performance of EAs on the PSP in general, and on specific instance classes that are easy or hard. Our theoretical analysis has practical implications-based on it, we derive an improved EA design. This includes normalizing employees' dedication for different tasks to ensure they are not working overtime; a fitness function that requires fewer pre-defined parameters and provides a clear gradient towards feasible solutions; and an improved representation and mutation operator. Both our theoretical and empirical results show that our design is very effective. Combining the use of normalization to a population gave the best results in our experiments, and normalization was a key component for the practical effectiveness of the new design. Not only does our paper offer a new and effective algorithm for the PSP, it also provides a rigorous theoretical analysis to explain the efficiency of the algorithm, especially for increasingly large projects.",0098-5589;00985589,,10.1109/TSE.2013.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648326,Schedule and organizational issues;evolutionary algorithms;runtime analysis;search-based software engineering;software project management;software project scheduling,Algorithm design and analysis;Resource management;Schedules;Scheduling;Software;Software algorithms;Software engineering,evolutionary computation;project management;scheduling;software development management,PSP;employee dedication;fitness function;improved EA design;improved evolutionary algorithm;mutation operator;population normalization;project scheduling problem;representation operator;runtime analysis;software project scheduling,,10,,45,,,20131025,Jan. 2014,,IEEE,IEEE Journals & Magazines,,19
Overcoming the Equivalent Mutant Problem: A Systematic Literature Review and a Comparative Experiment of Second Order Mutation,L. Madeyski; W. Orzeszyna; R. Torkar; M. JÕ_zala,"Inst. of Inf., Wroclaw Univ. of Technol., Wroclaw, Poland",IEEE Transactions on Software Engineering,20140304,2014,40,1,23,42,"Context. The equivalent mutant problem (EMP) is one of the crucial problems in mutation testing widely studied over decades. Objectives. The objectives are: to present a systematic literature review (SLR) in the field of EMP; to identify, classify and improve the existing, or implement new, methods which try to overcome EMP and evaluate them. Method. We performed SLR based on the search of digital libraries. We implemented four second order mutation (SOM) strategies, in addition to first order mutation (FOM), and compared them from different perspectives. Results. Our SLR identified 17 relevant techniques (in 22 articles) and three categories of techniques: detecting (DEM); suggesting (SEM); and avoiding equivalent mutant generation (AEMG). The experiment indicated that SOM in general and JudyDiffOp strategy in particular provide the best results in the following areas: total number of mutants generated; the association between the type of mutation strategy and whether the generated mutants were equivalent or not; the number of not killed mutants; mutation testing time; time needed for manual classification. Conclusions . The results in the DEM category are still far from perfect. Thus, the SEM and AEMG categories have been developed. The JudyDiffOp algorithm achieved good results in many areas.",0098-5589;00985589,,10.1109/TSE.2013.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613487,Mutation testing;equivalent mutant problem;higher order mutation;second order mutation,Databases;Educational institutions;Informatics;Java;Libraries;Systematics;Testing,digital libraries;program testing,AEMG;DEM;EMP;FOM;JudyDiffOp strategy;SEM;SLR;SOM strategies;avoiding equivalent mutant generation;comparative experiment;detecting;digital libraries;equivalent mutant problem;first order mutation;mutation testing;second order mutation;suggesting;systematic literature review,,25,,79,,,20130927,Jan. 2014,,IEEE,IEEE Journals & Magazines,,19
A Component Model for Model Transformations,J. S. Cuadrado; E. Guerra; J. de Lara,"Department of Computer Science, Universidad Aut&#243;noma de Madrid, Spain",IEEE Transactions on Software Engineering,20141110,2014,40,11,1042,1060,"Model-driven engineering promotes an active use of models to conduct the software development process. In this way, models are used to specify, simulate, verify, test and generate code for the final systems. Model transformations are key enablers for this approach, being used to manipulate instance models of a certain modelling language. However, while other development paradigms make available techniques to increase productivity through reutilization, there are few proposals for the reuse of model transformations across different modelling languages. As a result, transformations have to be developed from scratch even if other similar ones exist. In this paper, we propose a technique for the flexible reutilization of model transformations. Our proposal is based on generic programming for the definition and instantiation of transformation templates, and on component-based development for the encapsulation and composition of transformations. We have designed a component model for model transformations, supported by an implementation currently targeting the Atlas Transformation Language (ATL). To evaluate its reusability potential, we report on a generic transformation component to analyse workflow models through their transformation into Petri nets, which we have reused for eight workflow languages, including UML Activity Diagrams, YAWL and two versions of BPMN.",0098-5589;00985589,,10.1109/TSE.2014.2339852,EU commission with project MONDO; Spanish Ministry of Economy and Competitivity with project Go-Lite; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6858077,Model-driven engineering;component-based development;genericity;model transformation;reusability,Adaptation models;Analytical models;Logic gates;Petri nets;Proposals;Software;Unified modeling language,Petri nets;object-oriented programming;software reusability,ATL;Atlas transformation language;BPMN;Petri nets;UML activity diagrams;YAWL;component model;component-based development;generic programming;generic transformation component;model transformations;model-driven engineering;modelling language instance models;software development process;workflow languages;workflow models,,11,,69,,,20140717,Nov. 1 2014,,IEEE,IEEE Journals & Magazines,,18
GEA: A Goal-Driven Approach toDiscovering Early Aspects,J. Lee; K. H. Hsu,"Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan",IEEE Transactions on Software Engineering,20140616,2014,40,6,584,602,"Aspect-oriented software development has become an important development and maintenance approach to software engineering across requirements, design and implementation phases. However, discovering early aspects from requirements for a better integration of crosscutting concerns into a target system is still not well addressed in the existing works. In this paper, we propose a Goal-driven Early Aspect approach (called GEA) to discovering early aspects by means of a clustering algorithm in which relationships among goals and use cases are utilized to explore similarity degrees of clustering goals, and total interaction degrees are devised to check the validity of the formation of each cluster. Introducing early aspects not only enhances the goal-driven requirements modeling to manage crosscutting concerns, but also provides modularity insights into the analysis and design of software development. Moreover, relationships among goals represented numerically are more informative to discover early aspects and more easily to be processed computationally than qualitative terms. The proposed approach is illustrated by using two problem domains: a meeting scheduler system and a course enrollment system. An experiment is also conducted to evaluate the benefits of the proposed approach with Mann-Whitney U-test to show that the difference between with GEA and without GEA is statistically significant.",0098-5589;00985589,,10.1109/TSE.2014.2322368,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822606,Early aspects;fuzzy logic;goal cluster;goals;goals interaction;use cases,Analytical models;Clustering algorithms;Computer science;Documentation;Educational institutions;Software;Software engineering,aspect-oriented programming;pattern clustering;software maintenance,GEA;Mann-Whitney U-test;aspect-oriented software development;clustering algorithm;course enrollment system;crosscutting concerns;early aspect discovery;goal-driven early aspect approach;goal-driven requirements;maintenance approach;meeting scheduler system;software engineering,,3,,51,,,20140529,June 1 2014,,IEEE,IEEE Journals & Magazines,,18
An Observe-Model-Exercise* Paradigm to Test Event-Driven Systems with Undetermined Input Spaces,B. N. Nguyen; A. M. Memon,"Department of Computer Science, University of Maryland, College Park",IEEE Transactions on Software Engineering,20140331,2014,40,3,216,234,"System testing of software applications with a graphical-user interface (GUI) front-end requires that sequences of GUI events, that sample the application's input space, be generated and executed as test cases on the GUI. However, the context-sensitive behavior of the GUI of most of today's non-trivial software applications makes it practically impossible to fully determine the software's input space. Consequently, GUI testers-both automated and manual-working with undetermined input spaces are, in some sense, blindly navigating the GUI, unknowingly missing allowable event sequences, and failing to realize that the GUI implementation may allow the execution of some disallowed sequences. In this paper, we develop a new paradigm for GUI testing, one that we call Observe-Model-Exercise* (OME*) to tackle the challenges of testing context-sensitive GUIs with undetermined input spaces. Starting with an incomplete model of the GUI's input space, a set of coverage elements to test, and test cases, OME* iteratively observes the existence of new events during execution of the test cases, expands the model of the GUI's input space, computes new coverage elements, and obtains new test cases to exercise the new elements. Our experiment with 8 open-source software subjects, more than 500,000 test cases running for almost 1,100 machine-days, shows that OME* is able to expand the test space on average by 464.11 percent; it detected 34 faults that had never been detected before.",0098-5589;00985589,,10.1109/TSE.2014.2300857,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714448,Test generation;quality concepts;user interfaces,Blogs;Computational modeling;Context;Graphical user interfaces;Layout;Software;Testing,graphical user interfaces;program testing;public domain software,GUI context-sensitive behavior;graphical-user interface front-end;observe-model-exercise paradigm;open-source software subjects;software system testing;test event-driven system;test generation;undetermined input spaces,,10,,44,,,20140116,14-Mar,,IEEE,IEEE Journals & Magazines,,18
Modular Software Model Checking for Distributed Systems,W. Leungwattanakit; C. Artho; M. Hagiya; Y. Tanabe; M. Yamamoto; K. Takahashi,"Dept. of Math. & Inf., Chiba Univ., Chiba, Japan",IEEE Transactions on Software Engineering,20140514,2014,40,5,483,501,"Distributed systems are complex, being usually composed of several subsystems running in parallel. Concurrent execution and inter-process communication in these systems are prone to errors that are difficult to detect by traditional testing, which does not cover every possible program execution. Unlike testing, model checking can detect such faults in a concurrent system by exploring every possible state of the system. However, most model-checking techniques require that a system be described in a modeling language. Although this simplifies verification, faults may be introduced in the implementation. Recently, some model checkers verify program code at runtime but tend to be limited to stand-alone programs. This paper proposes cache-based model checking, which relaxes this limitation to some extent by verifying one process at a time and running other processes in another execution environment. This approach has been implemented as an extension of Java PathFinder, a Java model checker. It is a scalable and promising technique to handle distributed systems. To support a larger class of distributed systems, a checkpointing tool is also integrated into the verification system. Experimental results on various distributed systems show the capability and scalability of cache-based model checking.",0098-5589;00985589,,10.1109/TSE.2013.49,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645368,Software model checking;checkpointing;distributed systems;software verification,Checkpointing;Java;Message systems;Model checking;Scalability;Software;Synchronization,Java;cache storage;checkpointing;concurrency control;distributed processing;program testing;program verification,Java PathFinder extension;Java model checker;cache-based model checking;checkpointing tool;concurrent execution;distributed systems;execution environment;fault detection;interprocess communication;model-checking techniques;modeling language;modular software model checking;parallel subsystems;program code verification;program execution;stand-alone programs,,6,,50,,,20131023,14-May,,IEEE,IEEE Journals & Magazines,,18
How Effectively Does Metamorphic Testing Alleviate the Oracle Problem?,H. Liu; F. C. Kuo; D. Towey; T. Y. Chen,"Australia-India Centre for Autom. Software Eng., RMIT Univ., Melbourne, VIC, Australia",IEEE Transactions on Software Engineering,20140304,2014,40,1,4,22,"In software testing, something which can verify the correctness of test case execution results is called an oracle. The oracle problem occurs when either an oracle does not exist, or exists but is too expensive to be used. Metamorphic testing is a testing approach which uses metamorphic relations, properties of the software under test represented in the form of relations among inputs and outputs of multiple executions, to help verify the correctness of a program. This paper presents new empirical evidence to support this approach, which has been used to alleviate the oracle problem in various applications and to enhance several software analysis and testing techniques. It has been observed that identification of a sufficient number of appropriate metamorphic relations for testing, even by inexperienced testers, was possible with a very small amount of training. Furthermore, the cost-effectiveness of the approach could be enhanced through the use of more diverse metamorphic relations. The empirical studies presented in this paper clearly show that a small number of diverse metamorphic relations, even those identified in an ad hoc manner, had a similar fault-detection capability to a test oracle, and could thus effectively help alleviate the oracle problem.",0098-5589;00985589,,10.1109/TSE.2013.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613484,Software testing;metamorphic relation;metamorphic testing;oracle problem;test oracle,Benchmark testing;Computer crashes;Educational institutions;Software;Software testing;Training,program diagnostics;program testing;software fault tolerance;software quality,fault-detection capability;metamorphic relations;metamorphic testing;oracle problem;program correctness;software analysis;software properties;software testing,,41,,42,,,20130927,Jan. 2014,,IEEE,IEEE Journals & Magazines,,18
Making CEGAR More Efficient in Software Model Checking,C. Tian; Z. Duan; Z. Duan,"ICTT and ISN Laboratory, Xidian University, Xi&#x2019;an, China",IEEE Transactions on Software Engineering,20141212,2014,40,12,1206,1223,"Counter-example guided abstraction refinement (CEGAR) is widely used in software model checking. With an abstract model, the state space is largely reduced, however, a counterexample found in such a model that does not satisfy the desired property may not exist in the concrete model. Therefore, how to check whether a reported counterexample is spurious is a key problem in the abstraction-refinement loop. Next, in the case that a spurious counterexample is found, the abstract model needs to be further refined where an NP-hard state separation problem is often involved. Thus, how to refine the abstract model efficiently has attracted a great attention in the past years. In this paper, by re-analyzing spurious counterexamples, a new formal definition of spurious paths is given. Based on it, efficient algorithms for detecting spurious counterexamples are presented. By the new algorithms, when dealing with infinite counterexamples, the finite prefix to be analyzed will be polynomially shorter than the one dealt with by the existing algorithms. Moreover, in practical terms, the new algorithms can naturally be parallelized that enables multi-core processors contributes more in spurious counterexample checking. In addition, a novel refining approach by adding extra Boolean variables to the abstract model is presented. With this approach, not only the NP-hard state separation problem can be avoided, but also a smaller refined abstract model can be obtained. Experimental results show that the new algorithms perform well in practice.",0098-5589;00985589,,10.1109/TSE.2014.2357442,NSFC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895263,CEGAR;Model checking;abstraction;formal verification;refinement,Benchmark testing;Computational modeling;Model checking;Software design,multiprocessing systems;parallel algorithms;program verification,Boolean variables;CEGAR;NP-hard state separation problem;abstract model;abstraction-refinement loop;counter-example guided abstraction refinement;infinite counterexamples;multicore processors;parallelized algorithms;polynomially-shorter finite prefix;software model checking;spurious counterexample checking;spurious counterexample detection;spurious counterexample reanalysis;spurious paths;state space reduction,,7,,30,,,20140911,Dec. 1 2014,,IEEE,IEEE Journals & Magazines,,17
Requirements Elicitation and Specification Using the Agent Paradigm: The Case Study of an Aircraft Turnaround Simulator,T. Miller; B. Lu; L. Sterling; G. Beydoun; K. Taveter,"Department of Computing and Information Systems, The University of Melbourne, Melbourne, Victoria, Autralia",IEEE Transactions on Software Engineering,20141009,2014,40,10,1007,1024,"In this paper, we describe research results arising from a technology transfer exercise on agent-oriented requirements engineering with an industry partner. We introduce two improvements to the state-of-the-art in agent-oriented requirements engineering, designed to mitigate two problems experienced by ourselves and our industry partner: (1) the lack of systematic methods for agent-oriented requirements elicitation and modelling; and (2) the lack of prescribed deliverables in agent-oriented requirements engineering. We discuss the application of our new approach to an aircraft turnaround simulator built in conjunction with our industry partner, and show how agent-oriented models can be derived and used to construct a complete requirements package. We evaluate this by having three independent people design and implement prototypes of the aircraft turnaround simulator, and comparing the three prototypes. Our evaluation indicates that our approach is effective at delivering correct, complete, and consistent requirements that satisfy the stakeholders, and can be used in a repeatable manner to produce designs and implementations. We discuss lessons learnt from applying this approach.",0098-5589;00985589,,10.1109/TSE.2014.2339827,Australian Research Council Linkage; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6860260,Agent-oriented software engineering;agent-oriented modelling;technology transfer,Aircraft;Analytical models;Atmospheric modeling;Educational institutions;Industries;Object oriented modeling;Software,aerospace computing;aircraft;formal specification;multi-agent systems;object-oriented programming;software agents,agent oriented requirements engineering;agent paradigm;agent-oriented requirement modelling;agent-oriented requirements elicitation;aircraft turnaround simulator;independent people design;industry partner;requirements elicitation;requirements specification;systematic methods,,7,,46,,,20140718,Oct. 1 2014,,IEEE,IEEE Journals & Magazines,,17
An Empirical Study of RefactoringChallenges and Benefits at Microsoft,M. Kim; T. Zimmermann; N. Nagappan,"Department of Electrical and Computer Engineering, University of Texas, Austin",IEEE Transactions on Software Engineering,20140708,2014,40,7,633,649,"It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of semantics-preserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds the top 5 percent of preferentially refactored modules experience higher reduction in the number of inter-module dependencies and several complexity measures but increase size more than the bottom 95 percent. This indicates that measuring the impact of refactoring requires multi-dimensional assessment.",0098-5589;00985589,,10.1109/TSE.2014.2318734,Microsoft SEIF; National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802406,Refactoring;churn;component dependencies;defects;empirical study;software evolution,Complexity theory;Computer bugs;History;Interviews;Size measurement;Software;Software metrics,data analysis;software maintenance;software metrics;software quality,Microsoft;complexity measures;intermodule dependencies;quantitative Windows 7 version history data analysis;refactoring benefits;refactoring challenges;semi-structured interviews;software evolution;software quality improvement;survey,,13,,58,,,20140418,July 1 2014,,IEEE,IEEE Journals & Magazines,,16
Supporting the Combined Selection of Model-Based Testing Techniques,A. C. Dias-Neto; G. H. Travassos,"Institute of Computing, Federal University of Amazonas, Manaus, Brazil",IEEE Transactions on Software Engineering,20141009,2014,40,10,1025,1041,"The technical literature on model-based testing (MBT) offers us several techniques with different characteristics and goals. Contemporary software projects usually need to make use of different software testing techniques. However, a lack of empirical information regarding their scalability and effectiveness is observed. It makes their application difficult in real projects, increasing the technical difficulties to combine two or more MBT techniques for the same software project. In addition, current software testing selection approaches offer limited support for the combined selection of techniques. Therefore, this paper describes the conception and evaluation of an approach aimed at supporting the combined selection of MBT techniques for software projects. It consists of an evidence-based body of knowledge with 219 MBT techniques and their corresponding characteristics and a selection process that provides indicators on the level of adequacy (impact indicator) amongst MBT techniques and software projects characteristics. Results from the data analysis indicate it contributes to improve the effectiveness and efficiency of the selection process when compared to another selection approach available in the technical literature. Aiming at facilitating its use, a computerized infrastructure, evaluated into an industrial context and evolved to implement all the facilities needed to support such selection approach, is presented.",0098-5589;00985589,,10.1109/TSE.2014.2312915,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776497,Software testing;experimental software engineering;model-based testing;recommendation system;software technology selection,Computational modeling;Context;Organizations;Software quality;Software testing,program testing;project management;software development management,MBT techniques;computerized infrastructure;data analysis;impact indicator;model-based testing techniques;software project characteristics;software projects;software testing selection;software testing techniques;technical literature,,4,,39,,,20140320,Oct. 2014,,IEEE,IEEE Journals & Magazines,,16
On the Asymptotic Behavior of Adaptive Testing Strategy for Software Reliability Assessment,J. Lv; B. B. Yin; K. Y. Cai,"Sch. of Autom. Sci. & Electr. Eng., Beihang Univ., Beijing, China",IEEE Transactions on Software Engineering,20140502,2014,40,4,396,412,"In software reliability assessment, one problem of interest is how to minimize the variance of reliability estimator, which is often considered as an optimization goal. The basic idea is that an estimator with lower variance makes the estimates more predictable and accurate. Adaptive Testing (AT) is an online testing strategy, which can be adopted to minimize the variance of software reliability estimator. In order to reduce the computational overhead of decision-making, the implemented AT strategy in practice deviates from its theoretical design that guarantees AT's local optimality. This work aims to investigate the asymptotic behavior of AT to improve its global performance without losing the local optimality. To this end, a new AT strategy named Adaptive Testing with Gradient Descent method (AT-GD) is proposed. Theoretical analysis indicates that AT-GD, a locally optimal testing strategy, converges to the globally optimal solution as the assessment process proceeds. Simulation and experiments are set up to validate AT-GD's effectiveness and efficiency. Besides, sensitivity analysis of AT-GD is also conducted in this study.",0098-5589;00985589,,10.1109/TSE.2014.2310194,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6762895,Adaptive testing;operational profile;software reliability;testing strategy,Aircraft;Global Positioning System;Reliability theory;Software;Software reliability;Testing,decision making;gradient methods;optimisation;program testing;sensitivity analysis;software reliability,AT-GD;adaptive testing strategy;adaptive testing with gradient descent method;asymptotic behavior;computational overhead reduction;decision-making;global performance improvement;locally optimal testing strategy;online testing strategy;optimization goal;sensitivity analysis;software reliability assessment;software reliability estimator,,11,,35,,,20140311,14-Apr,,IEEE,IEEE Journals & Magazines,,16
You Are the Only Possible Oracle: Effective Test Selection for End Users of Interactive Machine Learning Systems,A. Groce; T. Kulesza; C. Zhang; S. Shamasunder; M. Burnett; W. K. Wong; S. Stumpf; S. Das; A. Shinsel; F. Bice; K. McIntosh,"School of Electrical Engineering and Computer Science, Oregon State University, Corvallis",IEEE Transactions on Software Engineering,20140331,2014,40,3,307,323,"How do you test a program when only a single user, with no expertise in software testing, is able to determine if the program is performing correctly? Such programs are common today in the form of machine-learned classifiers. We consider the problem of testing this common kind of machine-generated program when the only oracle is an end user: e.g., only you can determine if your email is properly filed. We present test selection methods that provide very good failure rates even for small test suites, and show that these methods work in both large-scale random experiments using a _ÑÒgold standard_Ñù and in studies with real users. Our methods are inexpensive and largely algorithm-independent. Key to our methods is an exploitation of properties of classifiers that is not possible in traditional software testing. Our results suggest that it is plausible for time-pressured end users to interactively detect failures-even very hard-to-find failures-without wading through a large number of successful (and thus less useful) tests. We additionally show that some methods are able to find the arguably most difficult-to-detect faults of classifiers: cases where machine learning algorithms have high confidence in an incorrect result.",0098-5589;00985589,,10.1109/TSE.2013.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682887,Machine learning;end-user testing;test suite size,Electronic mail;Machine learning algorithms;Software;Software algorithms;Testing;Training;Training data,interactive systems;learning (artificial intelligence);program testing,effective test selection;email;end users;hard-to-find failures;interactive failure detection;interactive machine learning systems;machine generated program;machine learned classifiers;program testing;software testing,,7,,63,,,20131212,14-Mar,,IEEE,IEEE Journals & Magazines,,16
Learning Assumptions for CompositionalVerification of Timed Systems,S. W. Lin; Õ_. AndrÕ©; Y. Liu; J. Sun; J. S. Dong,"Temasek Labs., Nat. Univ. of Singapore, Singapore, Singapore",IEEE Transactions on Software Engineering,20140304,2014,40,2,137,153,"Compositional techniques such as assume-guarantee reasoning (AGR) can help to alleviate the state space explosion problem associated with model checking. However, compositional verification is difficult to be automated, especially for timed systems, because constructing appropriate assumptions for AGR usually requires human creativity and experience. To automate compositional verification of timed systems, we propose a compositional verification framework using a learning algorithm for automatic construction of timed assumptions for AGR. We prove the correctness and termination of the proposed learning-based framework, and experimental results show that our method performs significantly better than traditional monolithic timed model checking.",0098-5589;00985589,,10.1109/TSE.2013.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682903,Automatic assume-guarantee reasoning;model checking;timed systems,Atomic clocks;Cognition;Educational institutions;Explosions;Learning automata;Model checking,formal verification;inference mechanisms;learning (artificial intelligence),AGR techniques;assume-guarantee reasoning techniques;compositional verification framework;learning algorithm;learning assumptions;learning-based framework;monolithic timed model checking;state space explosion problem;timed assumptions;timed systems,,4,,37,,,20131212,Feb. 2014,,IEEE,IEEE Journals & Magazines,,16
An Empirical Methodology to Evaluate Vulnerability Discovery Models,F. Massacci; V. H. Nguyen,"DISI, University of Trento, Trento, TN, Italy",IEEE Transactions on Software Engineering,20141212,2014,40,12,1147,1162,"Vulnerability discovery models (VDMs) operate on known vulnerability data to estimate the total number of vulnerabilities that will be reported after a software is released. VDMs have been proposed by industry and academia, but there has been no systematic independent evaluation by researchers who are not model proponents. Moreover, the traditional evaluation methodology has some issues that biased previous studies in the field. In this work we propose an empirical methodology that systematically evaluates the performance of VDMs along two dimensions (quality and predictability) and addresses all identified issues of the traditional methodology. We conduct an experiment to evaluate most existing VDMs on popular web browsers' vulnerability data. Our comparison shows that the results obtained by the proposed methodology are more informative than those by the traditional methodology. Among evaluated VDMs, the simplest linear model is the most appropriate choice in terms of both quality and predictability for the first 6-12 months since a release date. Otherwise, logistics-based models are better choices.",0098-5589;00985589,,10.1109/TSE.2014.2354037,European Union Seventh Framework Programme; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891367,Software security;empirical evaluation;vulnerability analysis;vulnerability discovery model,Browsers;Computer bugs;Computer security;Data models;Operating systems;Predictive models,online front-ends;security of data;software quality,VDM;Web browser vulnerability data;empirical methodology;logistics-based model;predictability;quality;time 6 month to 12 month;vulnerability discovery model evaluation,,6,,54,,,20140904,Dec. 1 2014,,IEEE,IEEE Journals & Magazines,,15
NLP-KAOS for Systems Goal Elicitation: Smart Metering System Case Study,E. Casagrande; S. Woldeamlak; W. L. Woon; H. H. Zeineldin; D. Svetinovic,"Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE",IEEE Transactions on Software Engineering,20141009,2014,40,10,941,956,"This paper presents a computational method that employs Natural Language Processing (NLP) and text mining techniques to support requirements engineers in extracting and modeling goals from textual documents. We developed a NLP-based goal elicitation approach within the context of KAOS goal-oriented requirements engineering method. The hierarchical relationships among goals are inferred by automatically building taxonomies from extracted goals. We use smart metering system as a case study to investigate the proposed approach. Smart metering system is an important subsystem of the next generation of power systems (smart grids). Goals are extracted by semantically parsing the grammar of goal-related phrases in abstracts of research publications. The results of this case study show that the developed approach is an effective way to model goals for complex systems, and in particular, for the research-intensive complex systems.",0098-5589;00985589,,10.1109/TSE.2014.2339811,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6857327,,Abstracts;Data collection;Data mining;Data models;Natural language processing;Ontologies;Taxonomy,,,,7,,57,,,20140716,Oct. 1 2014,,IEEE,IEEE Journals & Magazines,,15
A Scalable Approach to Exact Model and Commonality Counting for Extended Feature Models,D. Fernandez-Amoros; R. Heradio; J. A. Cerrada; C. Cerrada,"Department of Languages and Computer Systems, Spanish Open University (UNED), Madrid, Spain",IEEE Transactions on Software Engineering,20140904,2014,40,9,895,910,"A software product line is an engineering approach to efficient development of software product portfolios. Key to the success of the approach is to identify the common and variable features of the products and the interdependencies between them, which are usually modeled using feature models. Implicitly, such models also include valuable information that can be used by economic models to estimate the payoffs of a product line. Unfortunately, as product lines grow, analyzing large feature models manually becomes impracticable. This paper proposes an algorithm to compute the total number of products that a feature model represents and, for each feature, the number of products that implement it. The inference of both parameters is helpful to describe the standardization/parameterization balance of a product line, detect scope flaws, assess the product line incremental development, and improve the accuracy of economic models. The paper reports experimental evidence that our algorithm has better runtime performance than existing alternative approaches.",0098-5589;00985589,,10.1109/TSE.2014.2331073,Comunidad de Madrid; Spanish Government; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6835200,Feature models;economic models;formal methods;software product lines,Analytical models;Computational modeling;Economics;Frequency modulation;Headphones;Portfolios;Software,software engineering;software product lines,commonality counting;extended feature models;product line incremental development;software product line;software product portfolio development,,4,,56,,,20140616,Sept. 1 2014,,IEEE,IEEE Journals & Magazines,,15
Static Analysis for Extracting Permission Checks of a Large Scale Framework: The Challenges and Solutions for Analyzing Android,A. Bartel; J. Klein; M. Monperrus; Y. Le Traon,"Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, 4, rue Alphonse Weicker, Luxembourg, Kirchberg",IEEE Transactions on Software Engineering,20140616,2014,40,6,617,632,"A common security architecture is based on the protection of certain resources by permission checks (used e.g., in Android and Blackberry). It has some limitations, for instance, when applications are granted more permissions than they actually need, which facilitates all kinds of malicious usage (e.g., through code injection). The analysis of permission-based framework requires a precise mapping between API methods of the framework and the permissions they require. In this paper, we show that naive static analysis fails miserably when applied with off-the-shelf components on the Android framework. We then present an advanced class-hierarchy and field-sensitive set of analyses to extract this mapping. Those static analyses are capable of analyzing the Android framework. They use novel domain specific optimizations dedicated to Android.",0098-5589;00985589,,10.1109/TSE.2014.2322867,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6813664,Android;Java;Large scale framework;Soot;call-graph;permissions;security;static analysis,Androids;Cameras;Humanoid robots;Java;Security;Servers;Sparks,Android (operating system);optimisation;program diagnostics;security of data,API methods;Android framework;advanced class-hierarchy analysis;common security architecture;field-sensitive set analysis;large scale framework;novel domain specific optimizations;permission checks;permission-based framework;static analysis,,17,,32,,,20140509,June 1 2014,,IEEE,IEEE Journals & Magazines,,15
Detecting Memory Leaks Statically with Full-Sparse Value-Flow Analysis,Y. Sui; D. Ye; J. Xue,"Program. Language & Compilers Group, Univ. of New South Wales, Sydney, NSW, Australia",IEEE Transactions on Software Engineering,20140304,2014,40,2,107,122,"We introduce a static detector, Saber, for detecting memory leaks in C programs. Leveraging recent advances on sparse pointer analysis, Saber is the first to use a full-sparse value-flow analysis for detecting memory leaks statically. Saber tracks the flow of values from allocation to free sites using a sparse value-flow graph (SVFG) that captures def-use chains and value flows via assignments for all memory locations represented by both top-level and address-taken pointers. By exploiting field-, flow- and context-sensitivity during different phases of the analysis, Saber detects memory leaks in a program by solving a graph reachability problem on its SVFG. Saber, which is fully implemented in Open64, is effective at detecting 254 leaks in the 15 SPEC2000 C programs and seven applications, while keeping the false positive rate at 18.3 percent. Saber compares favorably with several static leak detectors in terms of accuracy (leaks and false alarms reported) and scalability (LOC analyzed per second). In particular, compared with Fastcheck (which analyzes allocated objects flowing only into top-level pointers) using the 15 SPEC2000 C programs, Saber detects 44.1 percent more leaks at a slightly higher false positive rate but is only a few times slower.",0098-5589;00985589,,10.1109/TSE.2014.2302311,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720116,Memory Leaks;pointer analysis;sparse value-flow analysis;static analysis,Abstracts;Accuracy;Detectors;Resource management;Scalability;Sensitivity;Standards,C language;program diagnostics;reachability analysis;storage management,Fastcheck;Open64;SPEC2000 C programs;SVFG;Saber static detector;address-taken pointers;context-sensitivity;def-use chains;false positive rate;field-sensitivity;flow-sensitivity;full-sparse value-flow analysis;graph reachability problem;memory leaks detection;memory locations;sparse pointer analysis;sparse value-flow graph;static leak detectors;top-level pointers;value flows,,9,,38,,,20140123,Feb. 2014,,IEEE,IEEE Journals & Magazines,,15
Magiclock: Scalable Detection of Potential Deadlocks in Large-Scale Multithreaded Programs,Y. Cai; W. K. Chan,"Department of Computer Science, City University of Hong Kong, Tat Chee Avenue",IEEE Transactions on Software Engineering,20140331,2014,40,3,266,281,"We present Magiclock, a novel potential deadlock detection technique by analyzing execution traces (containing no deadlock occurrence) of large-scale multithreaded programs. Magiclock iteratively eliminates removable lock dependencies before potential deadlock localization. It divides lock dependencies into thread specific partitions, consolidates equivalent lock dependencies, and searches over the set of lock dependency chains without the need to examine any duplicated permutations of the same lock dependency chains. We validate Magiclock through a suite of real-world, large-scale multithreaded programs. The experimental results show that Magiclock is significantly more scalable and efficient than existing dynamic detectors in analyzing and detecting potential deadlocks in execution traces of large-scale multithreaded programs.",0098-5589;00985589,,10.1109/TSE.2014.2301725,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6718069,Deadlock detection;concurrency;lock order graph;multithreaded programs;scalability,Classification algorithms;Image edge detection;Instruction sets;Message systems;Monitoring;Multicore processing;System recovery,concurrency control;multi-threading;operating systems (computers);system recovery,Magiclock;large-scale multithreaded programs;lock order graph;potential deadlock localization;potential deadlocks scalable detection;scalability,,12,,47,,,20140121,14-Mar,,IEEE,IEEE Journals & Magazines,,15
Governing Software Process Improvementsin Globally Distributed Product Development,N. Ramasubbu,"Joseph M. Katz Graduate School of Business, University of Pittsburgh, 354 Mervis Hall, Pittsburgh",IEEE Transactions on Software Engineering,20140331,2014,40,3,235,250,"Continuous software process improvement (SPI) practices have been extensively prescribed to improve performance of software projects. However, SPI implementation mechanisms have received little scholarly attention, especially in the context of distributed software product development. We took an action research approach to study the SPI journey of a large multinational enterprise that adopted a distributed product development strategy. We describe the interventions and action research cycles enacted over a period of five years in collaboration with the firm, which resulted in a custom SPI framework that catered to both the social and technical needs of the firm's distributed teams. Institutionalizing the process maturity framework got stalled initially because the SPI initiatives were perceived by product line managers as a mechanism for exercising wider controls by the firm's top management. The implementation mechanism was subsequently altered to co-opt product line managers, which contributed to a wider adoption of the SPI framework. Insights that emerge from our analysis of the firm's SPI journey pertain to the integration of the technical and social views of software development, preserving process diversity through the use of a multi-tiered, non-blueprint approach to SPI, the linkage between key process areas and project control modes, and the role of SPI in aiding organizational learning.",0098-5589;00985589,,10.1109/TSE.2013.58,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682900,Software process improvement (SPI);action research;distributed teams;process control;software engineering,Benchmark testing;ISO standards;Process control;Product development;Quality management;Resource management;Software,software management;software process improvement;software product lines,action research approach;custom SPI framework;firm top management;globally distributed software product development strategy;large multinational enterprise;multitiered nonblueprint approach;organizational learning;process maturity framework;product line managers;project control modes;software process improvements;software project performance,,7,,93,,,20131212,14-Mar,,IEEE,IEEE Journals & Magazines,,15
Variability Mining: Consistent Semi-automatic Detection of Product-Line Features,C. KÕ_stner; A. Dreiling; K. Ostermann,"School of Computer Science, Carnegie Mellon University",IEEE Transactions on Software Engineering,20140304,2014,40,1,67,82,"Software product line engineering is an efficient means to generate a set of tailored software products from a common implementation. However, adopting a product-line approach poses a major challenge and significant risks, since typically legacy code must be migrated toward a product line. Our aim is to lower the adoption barrier by providing semi-automatic tool support-called variability mining -to support developers in locating, documenting, and extracting implementations of product-line features from legacy code. Variability mining combines prior work on concern location, reverse engineering, and variability-aware type systems, but is tailored specifically for the use in product lines. Our work pursues three technical goals: (1) we provide a consistency indicator based on a variability-aware type system, (2) we mine features at a fine level of granularity, and (3) we exploit domain knowledge about the relationship between features when available. With a quantitative study, we demonstrate that variability mining can efficiently support developers in locating features.",0098-5589;00985589,,10.1109/TSE.2013.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613490,LEADT;Variability;feature;feature location;mining;reverse engineering;software product line,Companies;Context;Data mining;Educational institutions;Feature extraction;Java;Software,data mining;reverse engineering;software product lines,consistent semi automatic detection;legacy code;product line approach;product line features;reverse engineering;semi automatic tool support;software product line engineering;variability aware type systems;variability mining,,16,,60,,,20130927,Jan. 2014,,IEEE,IEEE Journals & Magazines,,15
Managing Technical Debt in Enterprise Software Packages,N. Ramasubbu; C. F. Kemerer,"Joseph M. Katz Grad. Sch. of Bus., Univ. of Pittsburgh, Pittsburgh, PA, USA",IEEE Transactions on Software Engineering,20140808,2014,40,8,758,772,"We develop an evolutionary model and theory of software technical debt accumulation to facilitate a rigorous and balanced analysis of its benefits and costs in the context of a large commercial enterprise software package. Our theory focuses on the optimization problem involved in managing technical debt, and illustrates the different tradeoff patterns between software quality and customer satisfaction under early and late adopter scenarios at different lifecycle stages of the software package. We empirically verify our theory utilizing a ten year longitudinal data set drawn from 69 customer installations of the software package. We then utilize the empirical results to develop actionable policies for managing technical debt in enterprise software product adoption.",0098-5589;00985589,,10.1109/TSE.2014.2327027,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824267,,Business;Context;Maintenance engineering;Measurement;Software packages;Software quality,,,,9,,57,,,20140602,Aug. 1 2014,,IEEE,IEEE Journals & Magazines,,14
<sc>SymbexNet</sc>: Testing Network Protocol Implementations with Symbolic Execution and Rule-Based Specifications,J. Song; C. Cadar; P. Pietzuch,"Dept. of Comput. & Inf. Security, Sejong Univ., Seoul, South Korea",IEEE Transactions on Software Engineering,20140708,2014,40,7,695,709,"Implementations of network protocols, such as DNS, DHCP and Zeroconf, are prone to flaws, security vulnerabilities and interoperability issues caused by developer mistakes and ambiguous requirements in protocol specifications. Detecting such problems is not easy because (i) many bugs manifest themselves only after prolonged operation; (ii) reasoning about semantic errors requires a machine-readable specification; and (iii) the state space of complex protocol implementations is large. This article presents a novel approach that combines symbolic execution and rule-based specifications to detect various types of flaws in network protocol implementations. The core idea behind our approach is to (1) automatically generate high-coverage test input packets for a network protocol implementation using single- and multi-packet exchange symbolic execution (targeting stateless and stateful protocols, respectively) and then (2) use these packets to detect potential violations of manual rules derived from the protocol specification, and check the interoperability of different implementations of the same network protocol. We present a system based on these techniques, SymbexNet, and evaluate it on multiple implementations of two network protocols: Zeroconf, a service discovery protocol, and DHCP, a network configuration protocol. SymbexNet is able to discover non-trivial bugs as well as interoperability problems, most of which have been confirmed by the developers.",0098-5589;00985589,,10.1109/TSE.2014.2323977,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815719,Symbolic execution;interoperability testing;network security;testing,Computer bugs;Concrete;IP networks;Interoperability;Protocols;Servers;Testing,formal specification;open systems;program debugging;program testing,DHCP;DNS;SymbexNet;Zeroconf;interoperability issues;interoperability problems;machine readable specification;network protocol;protocol implementations;protocol specification;protocol specifications;rule based specifications;security vulnerabilities;semantic errors;symbolic execution;testing network protocol implementations,,4,,50,,,20140514,July 1 2014,,IEEE,IEEE Journals & Magazines,,14
Automatic Summarization of Bug Reports,S. Rastkar; G. C. Murphy; G. Murray,"Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada",IEEE Transactions on Software Engineering,20140502,2014,40,4,366,380,"Software developers access bug reports in a project's bug repository to help with a number of different tasks, including understanding how previous changes have been made and understanding multiple aspects of particular defects. A developer's interaction with existing bug reports often requires perusing a substantial amount of text. In this article, we investigate whether it is possible to summarize bug reports automatically so that developers can perform their tasks by consulting shorter summaries instead of entire bug reports. We investigated whether existing conversation-based automated summarizers are applicable to bug reports and found that the quality of generated summaries is similar to summaries produced for e-mail threads and other conversations. We also trained a summarizer on a bug report corpus. This summarizer produces summaries that are statistically better than summaries produced by existing conversation-based generators. To determine if automatically produced bug report summaries can help a developer with their work, we conducted a task-based evaluation that considered the use of summaries for bug report duplicate detection tasks. We found that summaries helped the study participants save time, that there was no evidence that accuracy degraded when summaries were used and that most participants preferred working with summaries to working with original bug reports.",0098-5589;00985589,,10.1109/TSE.2013.2297712,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6704866,Empirical software engineering;bug report duplicate detection;summarization of software artifacts,Computer bugs;Detectors;Electronic mail;Feature extraction;Natural languages;Software,electronic mail;program debugging;software engineering,automatic summarization;bug report corpus;bug report duplicate detection tasks;bug report summaries;bug reports;bug repository;conversation-based automated summarizers;conversation-based generators;e-mail threads;software developers;task-based evaluation,,24,,41,,,20140109,14-Apr,,IEEE,IEEE Journals & Magazines,,14
Effects of Developer Experience on Learning and Applying Unit Test-Driven Development,R. Latorre,"Univ. Autonoma de Madrid, Madrid, Spain",IEEE Transactions on Software Engineering,20140502,2014,40,4,381,395,"Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.",0098-5589;00985589,,10.1109/TSE.2013.2295827,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690135,Test-driven development;process conformance;programmer productivity;software construction;software engineering process;software quality/SQA;test-first design,Context;Production;Programming profession;Software;Testing;Training,learning (artificial intelligence);program testing;software quality,UTDD;code quality;learning;productivity;software development;unit test-driven development,,8,,44,,,20131220,14-Apr,,IEEE,IEEE Journals & Magazines,,14
iTree: Efficiently Discovering High-Coverage Configurations Using Interaction Trees,C. Song; A. Porter; J. S. Foster,"Fraunhofer USA Center for Experimental Software Engineering, 5825 University Research Court, Suite 1300, College Park",IEEE Transactions on Software Engineering,20140331,2014,40,3,251,265,"Modern software systems are increasingly configurable. While this has many benefits, it also makes some software engineering tasks,such as software testing, much harder. This is because, in theory,unique errors could be hiding in any configuration, and, therefore,every configuration may need to undergo expensive testing. As this is generally infeasible, developers need cost-effective technique for selecting which specific configurations they will test. One popular selection approach is combinatorial interaction testing (CIT), where the developer selects a strength t and then computes a covering array (a set of configurations) in which all t-way combinations of configuration option settings appear at least once. In prior work, we demonstrated several limitations of the CIT approach. In particular, we found that a given system's effective configuration space - the minimal set of configurations needed to achieve a specific goal - could comprise only a tiny subset of the system's full configuration space. We also found that effective configuration space may not be well approximated by t-way covering arrays. Based on these insights we have developed an algorithm called interaction tree discovery (iTree). iTree is an iterative learning algorithm that efficiently searches for a small set of configurations that closely approximates a system's effective configuration space. On each iteration iTree tests the system on a small sample of carefully chosen configurations, monitors the system's behaviors, and then applies machine learning techniques to discover which combinations of option settings are potentially responsible for any newly observed behaviors. This information is used in the next iteration to pick a new sample of configurations that are likely to reveal further new behaviors. In prior work, we presented an initial version of iTree and performed an initial evaluation with promising results. This paper presents an improved iTree algorithm in greater detail. The - ey improvements are based on our use of composite proto-interactions - a construct that improves iTree's ability to correctly learn key configuration option combinations, which in turn significantly improves iTree's running time, without sacrificing effectiveness. Finally, the paper presents a detailed evaluation of the improved iTree algorithm by comparing the coverage it achieves versus that of covering arrays and randomly generated configuration sets, including a significantly expanded scalability evaluation with the ~ 1M-LOC MySQL. Our results strongly suggest that the improved iTree algorithm is highly scalable and can identify a high-coverage test set of configurations more effectively than existing methods.",0098-5589;00985589,,10.1109/TSE.2013.55,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671585,Empirical software engineering;software configurations;software testing and analysis,Algorithm design and analysis;Arrays;Machine learning algorithms;Software algorithms;Software engineering;Software systems;Testing,iterative methods;learning (artificial intelligence);program testing;software engineering,CIT;combinatorial interaction testing;cost-effective technique;efficiently discovering high-coverage configurations;iTree;iTree algorithm;interaction tree discovery;interaction trees;iterative learning algorithm;machine learning techniques;software engineering;software systems;software testing,,2,,39,,,20131120,14-Mar,,IEEE,IEEE Journals & Magazines,,14
Keeping the Development Environment Up to Date_ÑÓA Study of the Situated Practices of Appropriating the Eclipse IDE,S. Draxler; G. Stevens; A. Boden,"Department for Information Systems and New Media, University of Siegen, Siegen, Germany",IEEE Transactions on Software Engineering,20141110,2014,40,11,1061,1074,"Software engineers and developers are surrounded by highly complex software systems. What does it take to cope with these? We introduce a field study that explores the maintenance of the Eclipse Integrated Development Environment by software developers as part of their daily work. The study focuses on appropriation of the Eclipse IDE. We present an empirical view on appropriation as a means to maintain the collective ability to work. We visited seven different organizations and observed and interviewed their members. Each organization was chosen to provide an overall picture of Eclipse use throughout the industry. The results decompose the appropriation of Eclipse by software developers in organizations into four categories: learning, tailoring and discovering, as well as the cross-cutting category: collaboration. The categories are grounded in situations that provoked a need to change as well as in policies adopted for coping with this need. By discussing these categories against the background of Eclipse and its ecosystem, we want to illustrate in what ways appropriation of component- or plugin- based software is nowadays a common and highly complex challenge for Eclipse users, and how the related appropriation practices can be supported by IT systems.",0098-5589;00985589,,10.1109/TSE.2014.2354047,German Federal Ministry for Education and Research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898825,Programmer workbench;deployment;human factors in software design;usage experience,Computer aided software engineering;Context;Ecosystems;Employment;Interviews;Organizations;Software,object-oriented languages;object-oriented programming;software engineering,Eclipse IDE;Eclipse Integrated Development Environment;IT systems;component-based software;development environment;highly complex software systems;plugin-based software;software developers;software engineers,,1,,60,,,20140915,Nov. 1 2014,,IEEE,IEEE Journals & Magazines,,13
Predicting Vulnerable Software Components via Text Mining,R. Scandariato; J. Walden; A. Hovsepyan; W. Joosen,"IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium",IEEE Transactions on Software Engineering,20141009,2014,40,10,993,1006,"This paper presents an approach based on machine learning to predict which components of a software application contain security vulnerabilities. The approach is based on text mining the source code of the components. Namely, each component is characterized as a series of terms contained in its source code, with the associated frequencies. These features are used to forecast whether each component is likely to contain vulnerabilities. In an exploratory validation with 20 Android applications, we discovered that a dependable prediction model can be built. Such model could be useful to prioritize the validation activities, e.g., to identify the components needing special scrutiny.",0098-5589;00985589,,10.1109/TSE.2014.2340398,EU FP7; Research Fund KU Leuven; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6860243,Vulnerabilities;machine learning;prediction model,Androids;Humanoid robots;Measurement;Predictive models;Security;Software;Text mining,data mining;learning (artificial intelligence);program verification;security of data,Android application;machine learning;security vulnerability;source code;text mining;vulnerable software component,,26,,40,,,20140718,Oct. 1 2014,,IEEE,IEEE Journals & Magazines,,13
"On the Accuracy, Efficiency, and Reusability of Automated Test Oracles for Android Devices",Y. D. Lin; J. F. Rojas; E. T. H. Chu; Y. C. Lai,"Department of Computer Science, National Chiao Tung University, University Road, Hsinchu, Taiwan",IEEE Transactions on Software Engineering,20141009,2014,40,10,957,970,"Automated GUI testing consists of simulating user events and validating the changes in the GUI in order to determine if an Android application meets specifications. Traditional record-replay testing tools mainly focus on facilitating the test case writing process but not the replay and verification process. The accuracy of testing tools degrades significantly when the device under test (DUT) is under heavy load. In order to improve the accuracy, our previous work, SPAG, uses event batching and smart wait function to eliminate the uncertainty of the replay process and adopts GUI layout information to verify the testing results. SPAG maintains an accuracy of up to 99.5 percent and outperforms existing methods. In this work, we propose smart phone automated GUI testing tool with camera (SPAG-C), an extension of SPAG, to test an Android hardware device. Our goal is to further reduce the time required to record test cases and increase reusability of the test oracle without compromising test accuracy. In the record stage, SPAG captures screenshots from device's frame buffer and writes verification commands into the test case. Unlike SPAG, SPAG-C captures the screenshots from an external camera instead of frame buffer. In the replay stage, SPAG-C automatically performs image comparison while SPAG simply performs a string comparison to verify the test results. In order to make SPAG-C reusable for different devices and to allow bettersynchronization at the time of capturing images, we develop a new architecture that uses an external camera and Web services to decouple the test oracle. Our experiments show that recording a test case using SPAG-C's automatic verification is as fast as SPAG's but more accurate. Moreover, SPAG-C is 50 to 75 percent faster than SPAG in achieving the same test accuracy. With reusability, SPAG-C reduces the testing time from days to hours for heterogeneous devices.",0098-5589;00985589,,10.1109/TSE.2014.2331982,National Science Council (NSC) and Institute of Information Industry (III) in Taiwan; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840332,Reusable software;test execution;testing tools;user interfaces,Accuracy;Androids;Graphical user interfaces;Humanoid robots;Performance evaluation;Smart phones;Testing,Web services;graphical user interfaces;program testing;smart phones;software reusability,Android application;Android devices;Android hardware device;DUT;GUI layout information;SPAG-C;Web services;automated test oracles;automatic verification;capturing images;device under test;event batching;frame buffer;image comparison;replay process;replay stage;smart phone automated GUI testing tool;smart wait function;test accuracy;test case writing process;traditional record-replay testing tools;user events;verification process;with camera,,6,,35,,,20140619,Oct. 1 2014,,IEEE,IEEE Journals & Magazines,,13
Researcher Bias: The Use of Machine Learning in Software Defect Prediction,M. Shepperd; D. Bowes; T. Hall,"Brunel University, Uxbridge, Middlesex, United Kingdom",IEEE Transactions on Software Engineering,20140616,2014,40,6,603,616,"Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect on predictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build a random effects ANOVA model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.",0098-5589;00985589,,10.1109/TSE.2014.2322358,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824804,Software defect prediction;meta-analysis;researcher bias,Buildings;Correlation;Data models;Measurement;Predictive models;Software;Software engineering,learning (artificial intelligence);object-oriented programming;reverse engineering;software metrics;software performance evaluation;statistical analysis,blind analysis;classifier factor;common response variable;data set factor;defect-prone software component prediction;input metrics factor;intergroup studies;machine learning;model building factors;performance evaluation;random effects ANOVA model;reporting protocol improvement;researcher bias;researcher group factor;reverse engineering;software defect prediction,,40,,53,,,20140603,June 1 2014,,IEEE,IEEE Journals & Magazines,,13
A Model-Driven Methodology for Developing Secure Data-Management Applications,D. Basin; M. Clavel; M. Egea; M. A. G. de Dios; C. Dania,"ETH Zurich, Zu&#x0308;rich, Switzerland",IEEE Transactions on Software Engineering,20140502,2014,40,4,324,337,"We present a novel model-driven methodology for developing secure data-management applications. System developers proceed by modeling three different views of the desired application: its data model, security model, and GUI model. These models formalize respectively the application's data domain, authorization policy, and its graphical interface together with the application's behavior. Afterwards a model-transformation function lifts the policy specified by the security model to the GUI model. This allows a separation of concerns where behavior and security are specified separately, and subsequently combined to generate a security-aware GUI model. Finally, a code generator generates a multi-tier application, along with all support for access control, from the security-aware GUI model. We report on applications built using our approach and the associated tool.",0098-5589;00985589,,10.1109/TSE.2013.2297116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698396,GUI models;Model-driven development;access control;model transformation;model-driven security,Authorization;Data models;Graphical user interfaces;Syntactics;Unified modeling language,authorisation;graphical user interfaces;software engineering,access control;authorization policy;code generator;data model;graphical user intefaces;model-driven methodology;model-transformation function;multitier application;secure data-management applications;security model;security-aware GUI model,,3,,30,,,20140102,14-Apr,,IEEE,IEEE Journals & Magazines,,13
GossipKit: A Unified ComponentFramework for Gossip,F. TaÕËani; S. Lin; G. S. Blair,"IRISA, Univ. of Rennes 1, Rennes, France",IEEE Transactions on Software Engineering,20140304,2014,40,2,123,136,"Although the principles of gossip protocols are relatively easy to grasp, their variety can make their design and evaluation highly time consuming. This problem is compounded by the lack of a unified programming framework for gossip, which means developers cannot easily reuse, compose, or adapt existing solutions to fit their needs, and have limited opportunities to share knowledge and ideas. In this paper, we consider how component frameworks, which have been widely applied to implement middleware solutions, can facilitate the development of gossip-based systems in a way that is both generic and simple. We show how such an approach can maximize code reuse, simplify the implementation of gossip protocols, and facilitate dynamic evolution and redeployment.Also known as _ÑÒepidemic_Ñù protocols.",0098-5589;00985589,,10.1109/TSE.2013.50,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645372,Distributed systems;components;frameworks;protocols,Ad hoc networks;Assembly;Peer-to-peer computing;Programming;Protocols;Software;Wireless sensor networks,distributed processing;object-oriented programming;protocols,GossipKit;code reuse;epidemic protocols;gossip protocols;gossip-based systems;middleware solutions;protocol design;protocol evaluation;unified component framework;unified programming framework,,1,,77,,,20131023,Feb. 2014,,IEEE,IEEE Journals & Magazines,,13
Synthesizing Multithreaded Code from Real-Time Object-Oriented Models via Schedulability-Aware Thread Derivation,S. Kim,"Dept. of Inf. Commun. Eng., Hankuk Univ. of Foreign Studies, Yongin, South Korea",IEEE Transactions on Software Engineering,20140502,2014,40,4,413,426,"One of the major difficulties in developing embedded systems with object-oriented modeling is to translate a designed model into code that satisfies required real-time performance. This paper proposes scenario-based implementation synthesis architecture with timing guarantee (SISAtime) that addresses these difficulties. The problems that SISAtime must solve are: how to synthesize multithreaded-code from a real-time object-oriented model; and how to design supporting development tools and runtime system architecture while ensuring that the scenarios in the system have minimal response times and the code satisfies the given timing constraints with a minimal number of threads. SISAtime provides a new scheduling algorithm which minimizes scenario response times. SISAtime also provides a new thread derivation method that derives tasks and maps tasks to threads while automatically assigning task scheduling attributes. We have fully implemented SISAtime by extending the RoseRT development tool that uses UML 2.0 as a modeling language, and we applied it to an existing industrial private branch exchange system. The performance evaluation results show that the response times, context switches, and the number of threads of the system with SISAtime were reduced by 21.6, 33.2, and 65.2 percent, respectively, compared to the system with the best known existing thread derivation method.",0098-5589;00985589,,10.1109/TSE.2013.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617637,Multitasking;object-oriented design methods;real-time systems and embedded systems;system integration and implementation,Message systems;Object oriented modeling;Ports (Computers);Real-time systems;Time factors;Timing;Unified modeling language,Unified Modeling Language;embedded systems;multi-threading;object-oriented methods;scheduling;software architecture;software performance evaluation,RoseRT development tool;SISAtime;UML 2.0;automatic task scheduling attribute assignment;context switches;development tools;embedded systems;industrial private branch exchange system;modeling language;multithreaded code synthesis;real-time object-oriented models;real-time performance evaluation;runtime system architecture;scenario response time minimization;scenario-based implementation synthesis architecture-with-timing guarantee;schedulability-aware thread derivation;task derivation;task mapping;thread derivation method;timing constraints,,3,,42,,,20131002,14-Apr,,IEEE,IEEE Journals & Magazines,,13
Modeling Human-in-the-Loop Security Analysis and Decision-Making Processes,M. A. Schumann; D. Drusinsky; J. B. Michael; D. Wijesekera,"KEYW Corporation, 7740 Milestone Pkwy, Suite 400, Hanover",IEEE Transactions on Software Engineering,20140304,2014,40,2,154,166,"This paper presents a novel application of computer-assisted formal methods for systematically specifying, documenting, statically and dynamically checking, and maintaining human-centered workflow processes. This approach provides for end-to-end verification and validation of process workflows, which is needed for process workflows that are intended for use in developing and maintaining high-integrity systems. We demonstrate the technical feasibility of our approach by applying it on the development of the US government's process workflow for implementing, certifying, and accrediting cross-domain computer security solutions. Our approach involves identifying human-in-the-loop decision points in the process activities and then modeling these via statechart assertions. We developed techniques to specify and enforce workflow hierarchies, which was a challenge due to the existence of concurrent activities within complex workflow processes. Some of the key advantages of our approach are: it results in development of a model that is executable, supporting both upfront and runtime checking of process-workflow requirements; aids comprehension and communication among stakeholders and process engineers; and provides for incorporating accountability and risk management into the engineering of process workflows.",0098-5589;00985589,,10.1109/TSE.2014.2302433,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727512,Formal methods;information assurance;process modeling;software engineering;statechart assertions;verification and validation,Analytical models;Business;Formal specifications;Object oriented modeling;Runtime;Software;Unified modeling language,decision making;formal specification;formal verification;government data processing;security of data;workflow management software,US government process workflow;United States;accountability;computer-assisted formal methods;cross-domain computer security solutions;decision-making process;end-to-end validation;end-to-end verification;high-integrity systems;human-centered workflow process;human-in-the-loop decision points;human-in-the-loop security analysis;process activities;process documentation;process dynamically checking;process maintenance;process specification;process statically checking;process workflows engineering;risk management;statechart assertions;workflow hierarchies,,1,,44,,,20140128,Feb. 2014,,IEEE,IEEE Journals & Magazines,,12
Conservation of Information: Software_ÑésHidden Clockwork?,L. Hatton,"Fac. of Sci., Eng. & Comput., Kingston Univ., Kingston upon Thames, UK",IEEE Transactions on Software Engineering,20140514,2014,40,5,450,460,"In this paper it is proposed that the Conservation of Hartley-Shannon Information (hereafter contracted to H-S Information) plays the same role in discrete systems as the Conservation of Energy does in physical systems. In particular, using a variational approach, it is shown that the symmetry of scale-invariance, power-laws and the Conservation of H-S Information are intimately related and lead to the prediction that the component sizes of any software system assembled from components made from discrete tokens always asymptote to a scale-free power-law distribution in the unique alphabet of tokens used to construct each component. This is then validated to a very high degree of significance on some 100 million lines of software in seven different programming languages independently of how the software was produced, what it does, who produced it or what stage of maturity it has reached. A further implication of the theory presented here is that the average size of components depends only on their unique alphabet, independently of the package they appear in. This too is demonstrated on the main data set and also on 24 additional Fortran 90 packages.",0098-5589;00985589,,10.1109/TSE.2014.2316158,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784340,Information conservation;component size distribution;power-law;software systems,Bioinformatics;Computer languages;Genetic communication;Genomics;Software systems,information theory;programming languages;software engineering,Fortran;H-S information conservation;Hartley-Shannon information conservation;discrete systems;discrete tokens;energy conservation;physical systems;scale-free power-law distribution;scale-invariance symmetry;software hidden clockwork;software system;variational approach,,3,,30,,,20140408,14-May,,IEEE,IEEE Journals & Magazines,,10
Assessing the Refactorability of Software Clones,N. Tsantalis; D. Mazinanian; G. P. Krishnan,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada",IEEE Transactions on Software Engineering,20151110,2015,41,11,1055,1090,"The presence of duplicated code in software systems is significant and several studies have shown that clones can be potentially harmful with respect to the maintainability and evolution of the source code. Despite the significance of the problem, there is still limited support for eliminating software clones through refactoring, because the unification and merging of duplicated code is a very challenging problem, especially when software clones have gone through several modifications after their initial introduction. In this work, we propose an approach for automatically assessing whether a pair of clones can be safely refactored without changing the behavior of the program. In particular, our approach examines if the differences present between the clones can be safely parameterized without causing any side-effects. The evaluation results have shown that the clones assessed as refactorable by our approach can be indeed refactored without causing any compile errors or test failures. Additionally, the computational cost of the proposed approach is negligible (less than a second) in the vast majority of the examined cases. Finally, we perform a large-scale empirical study on over a million clone pairs detected by four different clone detection tools in nine open-source projects to investigate how refactorability is affected by different clone properties and tool configuration options. Among the highlights of our conclusions, we found that (a) clones in production code tend to be more refactorable than clones in test code, (b) clones with a close relative location (i.e., same method, type, or file) tend to be more refactorable than clones in distant locations (i.e., same hierarchy, or unrelated types), (c) Type-1 clones tend to be more refactorable than the other clone types, and (d) clones with a small size tend to be more refactorable than clones with a larger size.",0098-5589;00985589,,10.1109/TSE.2015.2448531,BusinessëèSoftware Engineering Research Platform; European Union (European Social FundëèESF); Greek national funds through the Operational Program; National Strategic Reference Framework (NSRF); ThalisëèAthens University of Economics; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130676,Clone refactoring;Code duplication;Empirical study;Refactorability assessment;Software clone management;clone refactoring;empirical study;refactorability assessment;software clone management,Arrays;Cloning;Java;Production;Software systems;Space exploration,software maintenance;software management;source code (software),clone detection tools;clone pairs;clone properties;compile errors;computational cost;distant locations;duplicated code;duplicated code merging;duplicated code unification;large-scale empirical;open-source projects;relative location;software clone refactorability assessment;software systems;source code evolution;source code maintainability;test code;test failures;tool configuration;type-1 clones,,9,,63,,,20150622,Nov. 1 2015,,IEEE,IEEE Journals & Magazines,,35
Extending the UML Statecharts Notation to Model Security Aspects,M. El-Attar; H. Luqman; P. KÕçrpÕçti; G. Sindre; A. L. Opdahl,"Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Kingdom of Saudi Arabia",IEEE Transactions on Software Engineering,20150714,2015,41,7,661,690,"Model driven security has become an active area of research during the past decade. While many research works have contributed significantly to this objective by extending popular modeling notations to model security aspects, there has been little modeling support for state-based views of security issues. This paper undertakes a scientific approach to propose a new notational set that extends the UML (Unified Modeling Language) statecharts notation. An online industrial survey was conducted to measure the perceptions of the new notation with respect to its semantic transparency as well as its coverage of modeling state based security aspects. The survey results indicate that the new notation encompasses the set of semantics required in a state based security modeling language and was largely intuitive to use and understand provided very little training. A subject-based empirical evaluation using software engineering professionals was also conducted to evaluate the cognitive effectiveness of the proposed notation. The main finding was that the new notation is cognitively more effective than the original notational set of UML statecharts as it allowed the subjects to read models created using the new notation much quicker.",0098-5589;00985589,,10.1109/TSE.2015.2396526,10.13039/501100004055 - King Fahd University of Petroleum and Minerals; 10.13039/501100004686 - Deanship of Scientific Research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042284,Extended Notation;Industrial Survey;Security Modeling;Statecharts;Subject-Based Experiment;extended notation;industrial survey;security modeling;subject-based experiment,Educational institutions;Object oriented modeling;Proposals;Security;Semantics;Software engineering;Unified modeling language,Unified Modeling Language;security of data;software engineering,UML statecharts notation;Unified Modeling Language statecharts notation;model driven security;modeling state based security aspect coverage;notation cognitive effectiveness;scientific approach;semantic transparency;software engineering professionals;state based security modeling language;subject-based empirical evaluation,,3,,80,,,20150213,July 1 2015,,IEEE,IEEE Journals & Magazines,,29
A Survey on Load Testing of Large-Scale Software Systems,Z. M. Jiang; A. E. Hassan,"Department of Electrical Engineering and Computer ScienceSoftware Construction, AnaLytics and Evaluation (SCALE) Lab, York University, Toronto, ON, Canada",IEEE Transactions on Software Engineering,20151110,2015,41,11,1091,1118,"Many large-scale software systems must service thousands or millions of concurrent requests. These systems must be load tested to ensure that they can function correctly under load (i.e., the rate of the incoming requests). In this paper, we survey the state of load testing research and practice. We compare and contrast current techniques that are used in the three phases of a load test: (1) designing a proper load, (2) executing a load test, and (3) analyzing the results of a load test. This survey will be useful for load testing practitioners and software engineering researchers with interest in the load testing of large-scale software systems.",0098-5589;00985589,,10.1109/TSE.2015.2445340,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123673,Load Testing;Software Quality;Software Testing;Software testing;large-scale software systems;load testing;software quality;survey,Computer bugs;Robustness;Software systems;Stress;Stress measurement;Testing,program testing;software quality,concurrent requests;contrast current techniques;large-scale software systems;load testing;load testing research;software engineering researchers,,8,,196,,,20150615,Nov. 1 2015,,IEEE,IEEE Journals & Magazines,,27
Mining Version Histories for Detecting Code Smells,F. Palomba; G. Bavota; M. D. Penta; R. Oliveto; D. Poshyvanyk; A. De Lucia,"University of Salerno, Fisciano, SA, Italy",IEEE Transactions on Software Engineering,20150513,2015,41,5,462,489,"Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase changeand fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change overtime. In this paper, we propose Historical Information for Smell deTection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system's snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers' perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.",0098-5589;00985589,,10.1109/TSE.2014.2372760,EU; grants; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963448,Code smells;empirical studies;mining software repositories,Accuracy;Association rules;Detectors;Feature extraction;History;Surgery,data mining;program compilers;public domain software,HIST;blob;code analysis;code smell detection;divergent change;feature envy;historical information for smell detection;mining version history;open source project;parallel inheritance;shotgun surgery;single system snapshot,,34,,55,,,20141120,May 1 2015,,IEEE,IEEE Journals & Magazines,,27
Are Slice-Based Cohesion Metrics Actually Useful in Effort-Aware Post-Release Fault-Proneness Prediction? An Empirical Study,Y. Yang; Y. Zhou; H. Lu; L. Chen; Z. Chen; B. Xu; H. Leung; Z. Zhang,"State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China",IEEE Transactions on Software Engineering,20150414,2015,41,4,331,357,"Background. Slice-based cohesion metrics leverage program slices with respect to the output variables of a module to quantify the strength of functional relatedness of the elements within the module. Although slice-based cohesion metrics have been proposed for many years, few empirical studies have been conducted to examine their actual usefulness in predicting fault-proneness. Objective. We aim to provide an in-depth understanding of the ability of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction, i.e. their effectiveness in helping practitioners find post-release faults when taking into account the effort needed to test or inspect the code. Method. We use the most commonly used code and process metrics, including size, structural complexity, Halstead's software science, and code churn metrics, as the baseline metrics. First, we employ principal component analysis to analyze the relationships between slice-based cohesion metrics and the baseline metrics. Then, we use univariate prediction models to investigate the correlations between slice-based cohesion metrics and post-release fault-proneness. Finally, we build multivariate prediction models to examine the effectiveness of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction when used alone or used together with the baseline code and process metrics. Results. Based on open-source software systems, our results show that: 1) slice-based cohesion metrics are not redundant with respect to the baseline code and process metrics; 2) most slice-based cohesion metrics are significantly negatively related to post-release fault-proneness; 3) slice-based cohesion metrics in general do not outperform the baseline metrics when predicting post-release fault-proneness; and 4) when used with the baseline metrics together, however, slice-based cohesion metrics can produce a statistically significant and practically important - mprovement of the effectiveness in effort-aware post-release fault-proneness prediction. Conclusion. Slice-based cohesion metrics are complementary to the most commonly used code and process metrics and are of practical value in the context of effort-aware post-release fault-proneness prediction.",0098-5589;00985589,,10.1109/TSE.2014.2370048,Hong Kong Competitive Earmarked Research; National Key Basic Research and Development Program of China; National Science and Technology Major Project of China; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004377 - PolyU; 10.13039/501100004608 - National Natural Science Foundation of Jiangsu Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6954519,Cohesion;effort-aware;fault-proneness;metrics;prediction;slice-based,Complexity theory;Context;Correlation;Laboratories;Measurement;Predictive models;Software,principal component analysis;public domain software;software metrics,Halstead's software science;baseline code;baseline metrics;code churn metrics;effort aware post-release fault proneness prediction;multivariate prediction models;open source software systems;principal component analysis;process metrics;slice-based cohesion metrics leverage program slices;structural complexity;univariate prediction models,,9,,66,,,20141112,April 1 2015,,IEEE,IEEE Journals & Magazines,,26
Improving Multi-Objective Test Case Selection by Injecting Diversity in Genetic Algorithms,A. Panichella; R. Oliveto; M. D. Penta; A. De Lucia,"Department of Mathematics and Computer Science, University of Salerno, Fisciano, Salerno, Italy",IEEE Transactions on Software Engineering,20150414,2015,41,4,358,383,"A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.",0098-5589;00985589,,10.1109/TSE.2014.2364175,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936894,Empirical Studies;Genetic Algorithms;Orthogonal Design;Regression Testing;Singular Value Decomposition;Test Case Selection;Test case selection;empirical studies;genetic algorithms;orthogonal design;regression testing;singular value decomposition,Genetic algorithms;Genetics;Greedy algorithms;Linear programming;Optimization;Sociology;Testing,genetic algorithms;greedy algorithms;program testing;search problems,DIV-GA;MOGA optimality improvement;diversity-based genetic algorithm;empirical analysis;greedy algorithms;multiobjective genetic algorithms;multiobjective optimization algorithms;multiobjective test case selection improvement;orthogonal design mechanism;orthogonal evolution mechanism;regression testing cost reduction;search process;test case subset prioritization;test case subset selection;test suite subsets,,11,,76,,,20141027,April 1 2015,,IEEE,IEEE Journals & Magazines,,25
Automated Checking of Conformance to Requirements Templates Using Natural Language Processing,C. Arora; M. Sabetzadeh; L. Briand; F. Zimmer,"SnT Centre for Security, Reliability, and Trust, Luxembourg",IEEE Transactions on Software Engineering,20151013,2015,41,10,944,968,"Templates are effective tools for increasing the precision of natural language requirements and for avoiding ambiguities that may arise from the use of unrestricted natural language. When templates are applied, it is important to verify that the requirements are indeed written according to the templates. If done manually, checking conformance to templates is laborious, presenting a particular challenge when the task has to be repeated multiple times in response to changes in the requirements. In this article, using techniques from natural language processing (NLP), we develop an automated approach for checking conformance to templates. Specifically, we present a generalizable method for casting templates into NLP pattern matchers and reflect on our practical experience implementing automated checkers for two well-known templates in the requirements engineering community. We report on the application of our approach to four case studies. Our results indicate that: (1) our approach provides a robust and accurate basis for checking conformance to templates; and (2) the effectiveness of our approach is not compromised even when the requirements glossary terms are unknown. This makes our work particularly relevant to practice, as many industrial requirements documents have incomplete glossaries.",0098-5589;00985589,,10.1109/TSE.2015.2428709,Validation Laboratory and AFR; 10.13039/501100001866 - National Research Fund-Luxembourg; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100933,Case Study Research;Natural Language Processing (NLP);Requirements Templates;Requirements templates;case study research;natural language processing (NLP),Ear;Natural language processing;Pattern matching;Pipelines;Safety;Terminology,formal specification;natural language processing;pattern matching,NLP pattern matcher;conformance automated checking;generalizable method;natural language processing;requirements engineering community;requirements templates,,12,,71,,,20150501,Oct. 1 2015,,IEEE,IEEE Journals & Magazines,,24
Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering,S. L. Lim; P. J. Bentley; N. Kanakam; F. Ishikawa; S. Honiden,"Department of Computer Science, University College, London",IEEE Transactions on Software Engineering,20150107,2015,41,1,40,64,"Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.",0098-5589;00985589,,10.1109/TSE.2014.2360674,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913003,Requirements/specifications;app user behavior;market-driven software engineering;mobile application development;software ecosystems;software product lines;survey research;user requirements,Data mining;Educational institutions;Mobile communication;Smart phones;Software;Software engineering,,,,14,,65,,,20140929,Jan. 1 2015,,IEEE,IEEE Journals & Magazines,,24
Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection,J. Petke; M. B. Cohen; M. Harman; S. Yoo,"Computer Science Department, University College London, London, United Kingdom",IEEE Transactions on Software Engineering,20150915,2015,41,9,901,924,"Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.",0098-5589;00985589,,10.1109/TSE.2015.2421279,"Air Force Office of Scientific Research; CREST: Centre for Research on Evolution, Search and Testing; DAASE; DAASE: Dynamic Adaptive Automated Software Engineering; GISMO: Genetic Improvement of Software for Multiple Objectives; National Science Foundation; UK Engineering and Physical Sciences Research Council (EPSRC); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081752,Combinatorial Interaction Testing;Combinatorial interaction testing;Empirical Studies;Prioritisation;Software Testing;empirical studies;prioritisation;software testing,Fault detection;Flexible printed circuits;Genetic algorithms;Greedy algorithms;Simulated annealing;Testing;Turning,genetic algorithms;greedy algorithms;program testing;simulated annealing;software fault tolerance,CIT test suite generation;SA;combinatorial interaction testing;constraint handling;early fault detection;genetic algorithm;greedy algorithm;pairwise testing;simulated annealing;software system configuration space,,12,,37,,,20150408,Sept. 1 2015,,IEEE,IEEE Journals & Magazines,,23
<sc>Cina</sc>: Suppressing the Detection of Unstable Context Inconsistency,C. Xu; W. Xi; S. C. Cheung; X. Ma; C. Cao; J. Lu,"State Key Laboratory for Novel Software Technology and the Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China",IEEE Transactions on Software Engineering,20150915,2015,41,9,842,865,"Context-aware applications adapt their behavior based on contexts. Contexts can, however, be incorrect. A popular means to build dependable applications is to augment them with a set of constraints to govern the consistency of context values. These constraints are evaluated upon context changes to detect inconsistencies so that they can be timely handled. However, we observe that many context inconsistencies are unstable. They vanish by themselves and do not require handling. Such inconsistencies are detected due to misaligned sensor sampling or improper inconsistency detection scheduling. We call them unstable context inconsistencies (or STINs). STINs should be avoided to prevent unnecessary inconsistency handling and unstable behavioral adaptation to applications. In this article, we study STINs systematically, from examples to theoretical analysis, and present algorithms to suppress their detection. Our key insight is that only certain patterns of context changes can make a consistency constraint subject to the detection of STINs. We derive such patterns and proactively use them to suppress the detection of STINs. We implemented our idea and applied it to real-world applications. Experimental results confirmed its effectiveness in suppressing the detection of numerous STINs with negligible overhead, while preserving the detection of stable context inconsistencies that require inconsistency handling.",0098-5589;00985589,,10.1109/TSE.2015.2418760,National Basic Research 973 Program; 10.13039/100000001 - National Natural Science Foundation; 10.13039/501100002920 - Research Grants Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7078871,Constraint;context inconsistency;impact propagation;instability analysis;pervasive computing,Context;Delays;Finite element analysis;Middleware;Schedules;Sensors,software engineering;ubiquitous computing,CINA;STIN;context-aware applications;unstable context inconsistency detection,,3,,65,,,20150402,Sept. 1 2015,,IEEE,IEEE Journals & Magazines,,23
The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps,G. Bavota; M. Linares-VÕçsquez; C. E. Bernal-CÕçrdenas; M. D. Penta; R. Oliveto; D. Poshyvanyk,"Department of Computer Science, Free University of Bozen-Bolzano, Bolzano, Italy",IEEE Transactions on Software Engineering,20150414,2015,41,4,384,407,"The mobile apps market is one of the fastest growing areas in the information technology. In digging their market share, developers must pay attention to building robust and reliable apps. In fact, users easily get frustrated by repeated failures, crashes, and other bugs; hence, they abandon some apps in favor of their competition. In this paper we investigate how the fault- and change-proneness of APIs used by Android apps relates to their success estimated as the average rating provided by the users to those apps. First, in a study conducted on 5,848 (free) apps, we analyzed how the ratings that an app had received correlated with the fault- and change-proneness of the APIs such app relied upon. After that, we surveyed 45 professional Android developers to assess (i) to what extent developers experienced problems when using APIs, and (ii) how much they felt these problems could be the cause for unfavorable user ratings. The results of our studies indicate that apps having high user ratings use APIs that are less fault- and change-prone than the APIs used by low rated apps. Also, most of the interviewed Android developers observed, in their development experience, a direct relationship between problems experienced with the adopted APIs and the users' ratings that their apps received.",0098-5589;00985589,,10.1109/TSE.2014.2367027,10.13039/100000001 - NSF; 10.13039/501100000780 - European Commission; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945855,API changes;Mining software repositories;android;empirical studies,Androids;Computer bugs;Educational institutions;Electronic mail;History;Humanoid robots;Software,application program interfaces;data mining;mobile computing;program debugging;software fault tolerance;system recovery,API change-proneness;API fault-proneness;Android Apps;information technology;mobile Apps market;software repository mining;user ratings,,38,,70,,,20141104,April 1 2015,,IEEE,IEEE Journals & Magazines,,23
STAR: Stack Trace Based Automatic Crash Reproduction via Symbolic Execution,N. Chen; S. Kim,"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong",IEEE Transactions on Software Engineering,20150210,2015,41,2,198,220,"Software crash reproduction is the necessary first step for debugging. Unfortunately, crash reproduction is often labor intensive. To automate crash reproduction, many techniques have been proposed including record-replay and post-failure-process approaches. Record-replay approaches can reliably replay recorded crashes, but they incur substantial performance overhead to program executions. Alternatively, post-failure-process approaches analyse crashes only after they have occurred. Therefore they do not incur performance overhead. However, existing post-failure-process approaches still cannot reproduce many crashes in practice because of scalability issues and the object creation challenge. This paper proposes an automatic crash reproduction framework using collected crash stack traces. The proposed approach combines an efficient backward symbolic execution and a novel method sequence composition approach to generate unit test cases that can reproduce the original crashes without incurring additional runtime overhead. Our evaluation study shows that our approach successfully exploited 31 (59.6 percent) of 52 crashes in three open source projects. Among these exploitable crashes, 22 (42.3 percent) are useful reproductions of the original crashes that reveal the crash triggering bugs. A comparison study also demonstrates that our approach can effectively outperform existing crash reproduction approaches.",0098-5589;00985589,,10.1109/TSE.2014.2363469,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926857,Crash reproduction;optimization;static analysis;symbolic execution;test case generation,Arrays;Color;Computer crashes;Explosions;Indexes;Optimization;Software,program debugging;program testing;project management;public domain software;system recovery,STAR;backward symbolic execution;crash stack traces;debugging;method sequence composition approach;object creation challenge;open source projects;post-failure-process approach;record-replay approach;scalability issues;software crash reproduction;stack trace based automatic crash reproduction;unit test case generation,,6,,64,,,20141016,Feb. 1 2015,,IEEE,IEEE Journals & Magazines,,22
"Embedding, Evolution, and Validation of Model-Driven Spreadsheets",J. Cunha; J. P. Fernandes; J. Mendes; J. Saraiva,"Universidade Nova de Lisboa, Portugal, and HASLab / INESC TEC, Portugal",IEEE Transactions on Software Engineering,20150311,2015,41,3,241,263,"This paper proposes and validates a model-driven software engineering technique for spreadsheets. The technique that we envision builds on the embedding of spreadsheet models under a widely used spreadsheet system. This means that we enable the creation and evolution of spreadsheet models under a spreadsheet system. More precisely, we embed ClassSheets, a visual language with a syntax similar to the one offered by common spreadsheets, that was created with the aim of specifying spreadsheets. Our embedding allows models and their conforming instances to be developed under the same environment. In practice, this convenient environment enhances evolution steps at the model level while the corresponding instance is automatically co-evolved. Finally, we have designed and conducted an empirical study with human users in order to assess our technique in production environments. The results of this study are promising and suggest that productivity gains are realizable under our model-driven spreadsheet development setting.",0098-5589;00985589,,10.1109/TSE.2014.2361141,European Regional Development; cëèo para a Ciëència e a Tecnologia; 10.13039/100006129 - FCT; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915751,ClassSheets;Embedding;Empirical Validation;Evolution;Models;Spreadsheets;embedding;empirical validation;evolution;models,Atmospheric modeling;Business;Data models;Software;Syntactics;Unified modeling language;Visualization,software development management;spreadsheet programs;visual languages,ClassSheets;model driven software engineering technique;model-driven spreadsheet development;spreadsheet model embedding;spreadsheet model evolution;spreadsheet model validation;spreadsheet system;syntax;visual language,,3,,66,,,20141002,March 1 2015,,IEEE,IEEE Journals & Magazines,,22
The Effect of GoF Design Patterns on Stability: A Case Study,A. Ampatzoglou; A. Chatzigeorgiou; S. Charalampidou; P. Avgeriou,"Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands",IEEE Transactions on Software Engineering,20150811,2015,41,8,781,802,"Stability refers to a software system's resistance to the _ÑÒripple effect_Ñù, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected _ÑÒshielding_Ñù of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern occurrences appear to be the least stable. The results can be used for assessing the benefits and liabilities of the use of patterns and for testing and refactoring prioritization, because less stable classes are expected to require more effort while testing, and urge for refactoring activities that would make them more resistant to change propagation.",0098-5589;00985589,,10.1109/TSE.2015.2414917,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066925,"D.2.2 Design Tools and Techniques;D.2.3.a Object-oriented programming,;D.2.8 Metrics/Measurement;Design Tools and Techniques;Metrics/Measurement;Object-oriented programming",Abstracts;Couplings;Measurement;Open source software;Production facilities;Stability analysis,Java;object-oriented programming;public domain software,GoF design pattern;Java open-source class;change impact analysis;change propagation;coupled pattern occurrence;pattern-participating class;refactoring activity;refactoring prioritization;ripple effect;software system resistance;stability,,10,,53,,,20150324,Aug. 1 2015,,IEEE,IEEE Journals & Magazines,,21
Where Do Configuration Constraints Stem From? An Extraction Approach and an Empirical Study,S. Nadi; T. Berger; C. KÕ_stner; K. Czarnecki,"Department of Computer Science, Technische Universit&#228;t Darmstadt, Darmstadt, Hessen, Germany",IEEE Transactions on Software Engineering,20150811,2015,41,8,820,841,"Highly configurable systems allow users to tailor software to specific needs. Valid combinations of configuration options are often restricted by intricate constraints. Describing options and constraints in a variability model allows reasoning about the supported configurations. To automate creating and verifying such models, we need to identify the origin of such constraints. We propose a static analysis approach, based on two rules, to extract configuration constraints from code. We apply it on four highly configurable systems to evaluate the accuracy of our approach and to determine which constraints are recoverable from the code. We find that our approach is highly accurate (93% and 77% respectively) and that we can recover 28% of existing constraints. We complement our approach with a qualitative study to identify constraint sources, triangulating results from our automatic extraction, manual inspections, and interviews with 27 developers. We find that, apart from low-level implementation dependencies, configuration constraints enforce correct runtime behavior, improve users' configuration experience, and prevent corner cases. While the majority of constraints is extractable from code, our results indicate that creating a complete model requires further substantial domain knowledge and testing. Our results aim at supporting researchers and practitioners working on variability model engineering, evolution, and verification techniques.",0098-5589;00985589,,10.1109/TSE.2015.2415793,ARTEMIS JU; 10.13039/100000001 - NSF; 10.13039/501100000038 - NSERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065312,Reverse-engineering;Variability models;configuration constraints;qualitative studies;reverse-engineering;static analyses,Accuracy;Feature extraction;Interviews;Kernel;Linux;Manuals,configuration management;program diagnostics,configuration combination;configuration constraints;configuration constraints extraction;constraint sources identification;extraction approach;static analysis approach;variability model;variability model engineering;variability model evolution;variability model verification techniques,,9,,80,,,20150323,Aug. 1 2015,,IEEE,IEEE Journals & Magazines,,21
BLISS: Improved Symbolic Execution by Bounded Lazy Initialization with SAT Support,N. Rosner; J. Geldenhuys; N. M. Aguirre; W. Visser; M. F. Frias,"Department of Computer Science, FCEyN, Universidad de Buenos Aires, Buenos Aires, Argentina",IEEE Transactions on Software Engineering,20150714,2015,41,7,639,660,"Lazy Initialization (LI) allows symbolic execution to effectively deal with heap-allocated data structures, thanks to a significant reduction in spurious and redundant symbolic structures. Bounded lazy initialization (BLI) improves on LI by taking advantage of precomputed relational bounds on the interpretation of class fields in order to reduce the number of spurious structures even further. In this paper we present bounded lazy initialization with SAT support (BLISS), a novel technique that refines the search for valid structures during the symbolic execution process. BLISS builds upon BLI, extending it with field bound refinement and satisfiability checks. Field bounds are refined while a symbolic structure is concretized, avoiding cases that, due to the concrete part of the heap and the field bounds, can be deemed redundant. Satisfiability checks on refined symbolic heaps allow us to prune these heaps as soon as they are identified as infeasible, i.e., as soon as it can be confirmed that they cannot be extended to any valid concrete heap. Compared to LI and BLI, BLISS reduces the time required by LI by up to four orders of magnitude for the most complex data structures. Moreover, the number of partially symbolic structures obtained by exploring program paths is reduced by BLISS by over 50 percent, with reductions of over 90 percent in some cases (compared to LI). BLISS uses less memory than LI and BLI, which enables the exploration of states unreachable by previous techniques.",0098-5589;00985589,,10.1109/TSE.2015.2389225,NPRP; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004061,Symbolic PathFinder;Symbolic execution;lazy initialization;tight field bounds,Binary trees;Concrete;Java;Periodic structures;Software,computability;data structures;program verification,BLISS;bounded lazy initialization with SAT support;class fields;field bound refinement;heap-allocated data structures;program path;relational bounds;satisfiability check;symbolic execution;symbolic heap;symbolic structures,,0,,26,,,20150107,July 1 2015,,IEEE,IEEE Journals & Magazines,,21
How Software Designers Interact with Sketches at the Whiteboard,N. Mangano; T. D. LaToza; M. Petre; A. van der Hoek,"Molimur, Mission Viejo, CA",IEEE Transactions on Software Engineering,20150210,2015,41,2,135,156,"Whiteboard sketches play a crucial role in software development, helping to support groups of designers in reasoning about a software design problem at hand. However, little is known about these sketches and how they support design `in the moment', particularly in terms of the relationships among sketches, visual syntactic elements within sketches, and reasoning activities. To address this gap, we analyzed 14 hours of design activity by eight pairs of professional software designers, manually coding over 4000 events capturing the introduction of visual syntactic elements into sketches, focus transitions between sketches, and reasoning activities. Our findings indicate that sketches serve as a rich medium for supporting design conversations. Designers often use general-purpose notations. Designers introduce new syntactic elements to record aspects of the design, or re-purpose sketches as the design develops. Designers constantly shift focus between sketches, using groups of sketches together that contain complementary information. Finally, sketches play an important role in supporting several types of reasoning activities (mental simulation, review of progress, consideration of alternatives). But these activities often leave no trace and rarely lead to sketch creation. We discuss the implications of these and other findings for the practice of software design at the whiteboard and for the creation of new electronic software design sketching tools.",0098-5589;00985589,,10.1109/TSE.2014.2362924,10.13039/100000001 - National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6922572,Interaction styles;systems analysis and design;user-centered design,Cognition;Encoding;Software design;Syntactics;Videos;Visualization,software engineering,reasoning activity;software design;software development;visual syntactic elements;whiteboard sketch,,5,,60,,,20141014,Feb. 1 2015,,IEEE,IEEE Journals & Magazines,,21
Exploiting Model Morphology for Event-Based Testing,F. Belli; M. Beyaz—±t,"Department of Electrical Engineering and Information Technology, University of Paderborn, Paderborn, Germany",IEEE Transactions on Software Engineering,20150210,2015,41,2,113,134,"Model-based testing employs models for testing. Model-based mutation testing (MBMT) additionally involves fault models, called mutants, by applying mutation operators to the original model. A problem encountered with MBMT is the elimination of equivalent mutants and multiple mutants modeling the same faults. Another problem is the need to compare a mutant to the original model for test generation. This paper proposes an event-based approach to MBMT that is not fixed on single events and a single model but rather operates on sequences of events of length k __‚ 1 and invokes a sequence of models that are derived from the original one by varying its morphology based on k. The approach employs formal grammars, related mutation operators, and algorithms to generate test cases, enabling the following: (1) the exclusion of equivalent mutants and multiple mutants; (2) the generation of a test case in linear time to kill a selected mutant without comparing it to the original model; (3) the analysis of morphologically different models enabling the systematic generation of mutants, thereby extending the set of fault models studied in related literature. Three case studies validate the approach and analyze its characteristics in comparison to random testing and another MBMT approach.",0098-5589;00985589,,10.1109/TSE.2014.2360690,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915728,(model) morphology;Model-based mutation testing;grammar-based testing;mutant selection;test generation,Analytical models;Context;Grammar;Morphology;Production;Testing;Unified modeling language,computational complexity;grammars;program testing,MBMT;equivalent mutant;event-based approach;event-based testing;fault models;formal grammars;linear time;model morphology;model-based mutation testing;multiple mutants;mutants;mutation operators;random testing;test generation,,2,,68,,,20141002,Feb. 1 2015,,IEEE,IEEE Journals & Magazines,,21
Replicating and Re-Evaluating the Theory of Relative Defect-Proneness,M. D. Syer; M. Nagappan; B. Adams; A. E. Hassan,"School of Computing, Queen&#8217;s University, Kingston, ON, Canada",IEEE Transactions on Software Engineering,20150210,2015,41,2,176,197,"A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.",0098-5589;00985589,,10.1109/TSE.2014.2361131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914599,Cox Models;Cox models;Defect Modelling;Survival Analysis;Survival analysis;defect modelling,Analytical models;Data models;Hazards;Mathematical model;Measurement;Predictive models;Software,program diagnostics;software quality;software reliability,defect modelling;relative defect-proneness theory;size-defect relationship;software system defects;source code module;survival analysis techniques,,3,,47,,,20141001,Feb. 1 2015,,IEEE,IEEE Journals & Magazines,,21
The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs,C. Le Goues; N. Holtschulte; E. K. Smith; Y. Brun; P. Devanbu; S. Forrest; W. Weimer,"School of Computer Science, Carnegie Mellon University, Pittsburgh, PA",IEEE Transactions on Software Engineering,20151208,2015,41,12,1236,1256,"The field of automated software repair lacks a set of common benchmark problems. Although benchmark sets are used widely throughout computer science, existing benchmarks are not easily adapted to the problem of automatic defect repair, which has several special requirements. Most important of these is the need for benchmark programs with reproducible, important defects and a deterministic method for assessing if those defects have been repaired. This article details the need for a new set of benchmarks, outlines requirements, and then presents two datasets, ManyBugs and IntroClass, consisting between them of 1,183 defects in 15 C programs. Each dataset is designed to support the comparative evaluation of automatic repair algorithms asking a variety of experimental questions. The datasets have empirically defined guarantees of reproducibility and benchmark quality, and each study object is categorized to facilitate qualitative evaluation and comparisons by category of bug or program. The article presents baseline experimental results on both datasets for three existing repair methods, GenProg, AE, and TrpAutoRepair, to reduce the burden on researchers who adopt these datasets for their own comparative evaluations.",0098-5589;00985589,,10.1109/TSE.2015.2454513,10.13039/100000001 - US National Science Foundation (NSF); 10.13039/100000015 - US Department of Energy (DOE); 10.13039/100000181 - AFOSR; 10.13039/100000185 - US Defense Advanced Research Projects Agency (DARPA); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153570,"<sc xmlns:ali=""http://www.niso.org/schemas/ali/1.0/"" xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"">IntroClass</sc>;<sc xmlns:ali=""http://www.niso.org/schemas/ali/1.0/"" xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"">ManyBugs</sc>;Automated program repair;IntroClass;ManyBugs;benchmark;reproducibility;subject defect",Benchmark testing;Computer bugs;Electronic mail;Maintenance engineering;Software systems,C language;benchmark testing;program debugging;software maintenance;software performance evaluation;software quality,C programs;GenProg;IntroClass benchmarks;ManyBugs benchmarks;TrpAutoRepair;automated software repair;automatic defect repair;automatic repair algorithms;benchmark problems;benchmark programs;benchmark quality;benchmark sets;computer science;defects assessment;deterministic method;qualitative evaluation;reproducibility,,20,,80,,,20150709,Dec. 1 2015,,IEEE,IEEE Journals & Magazines,,20
COVERT: Compositional Analysis of Android Inter-App Permission Leakage,H. Bagheri; A. Sadeghi; J. Garcia; S. Malek,"Department of Computer Science, George Mason University, Fairfax, VA",IEEE Transactions on Software Engineering,20150915,2015,41,9,866,886,"Android is the most popular platform for mobile devices. It facilitates sharing of data and services among applications using a rich inter-app communication system. While access to resources can be controlled by the Android permission system, enforcing permissions is not sufficient to prevent security violations, as permissions may be mismanaged, intentionally or unintentionally. Android's enforcement of the permissions is at the level of individual apps, allowing multiple malicious apps to collude and combine their permissions or to trick vulnerable apps to perform actions on their behalf that are beyond their individual privileges. In this paper, we present COVERT, a tool for compositional analysis of Android inter-app vulnerabilities. COVERT's analysis is modular to enable incremental analysis of applications as they are installed, updated, and removed. It statically analyzes the reverse engineered source code of each individual app, and extracts relevant security specifications in a format suitable for formal verification. Given a collection of specifications extracted in this way, a formal analysis engine (e.g., model checker) is then used to verify whether it is safe for a combination of applications-holding certain permissions and potentially interacting with each other-to be installed together. Our experience with using COVERT to examine over 500 real-world apps corroborates its ability to find inter-app vulnerabilities in bundles of some of the most popular apps on the market.",0098-5589;00985589,,10.1109/TSE.2015.2419611,US National Security Agency; 10.13039/100000001 - US National Science Foundation; 10.13039/100000180 - US Department of Homeland Security; 10.13039/100000185 - US Defense Advanced Research Projects Agency; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079508,Android;Formal Verification;Formal verification;Inter-App Vulnerabilities;Inter-App vulnerabilities;Static Analysis;static analysis,Analytical models;Androids;Data mining;Humanoid robots;Metals;Security;Smart phones,Android (operating system);formal specification;formal verification;mobile computing;program diagnostics;security of data,Android inter-app permission leakage;Android inter-app vulnerabilities analysis;Android permission system;COVERT tool;compositional analysis;formal analysis engine;formal verification;incremental analysis;inter-app communication system;mobile devices;security specification;security violation,,19,1,60,,,20150403,Sept. 1 2015,,IEEE,IEEE Journals & Magazines,,20
Forecasting Java Software Evolution Trends Employing Network Models,T. Chaikalis; A. Chatzigeorgiou,"Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece",IEEE Transactions on Software Engineering,20150610,2015,41,6,582,602,"The evolution of networks representing systems in various domains, including social networks, has been extensively studied enabling the development of growth models which govern their behavior over time. The architecture of software systems can also be naturally represented in the form of networks, whose properties change as software evolves. In this paper we attempt to model several aspects of graphs representing object-oriented software systems as they evolve over a number of versions. The goal is to develop a prediction model by considering global phenomena such as preferential attachment, past evolutionary trends such as the tendency of classes to create fewer relations as they age, as well as domain knowledge in terms of principles that have to be followed in object-oriented design. The derived models can provide insight into the future trends of software and potentially form the basis for eliciting improved or novel laws of software evolution. The forecasting power of the proposed model is evaluated against the actual evolution of 10 open-source projects and the achieved accuracy in the prediction of several network and software properties, which reflect the underlying system design, appears to be promising.",0098-5589;00985589,,10.1109/TSE.2014.2381249,European Union (European Social FundëèESF); National Strategic Reference Framework; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985636,Graphs and networks;Objectoriented design methods;Restructuring;Software Architectures;and reengineering;object-oriented design methods;reengineering;restructuring;reverse engineering;software architectures,Analytical models;Forecasting;Market research;Object oriented modeling;Predictive models;Software systems,Java;graph theory;object-oriented programming;social networking (online);software architecture;software maintenance,Java software evolution trend;object-oriented software system;prediction model;preferential attachment;social network;software system architecture,,5,,96,,,20141218,June 1 2015,,IEEE,IEEE Journals & Magazines,,20
"Using Declarative Specification to Improve the Understanding, Extensibility, and Comparison of Model-Inference Algorithms",I. Beschastnikh; Y. Brun; J. Abrahamson; M. D. Ernst; A. Krishnamurthy,"Department of Computer Science, University of British Columbia, Vancouver, BC, Canada",IEEE Transactions on Software Engineering,20150414,2015,41,4,408,428,"It is a staple development practice to log system behavior. Numerous powerful model-inference algorithms have been proposed to aid developers in log analysis and system understanding. Unfortunately, existing algorithms are typically declared procedurally, making them difficult to understand, extend, and compare. This paper presents InvariMint, an approach to specify model-inference algorithms declaratively. We applied the InvariMint declarative approach to two model-inference algorithms. The evaluation results illustrate that InvariMint (1) leads to new fundamental insights and better understanding of existing algorithms, (2) simplifies creation of new algorithms, including hybrids that combine or extend existing algorithms, and (3) makes it easy to compare and contrast previously published algorithms. InvariMint's declarative approach can outperform procedural implementations. For example, on a log of 50,000 events, InvariMint's declarative implementation of the kTails algorithm completes in 12 seconds, while a procedural implementation completes in 18 minutes. We also found that InvariMint's declarative version of the Synoptic algorithm can be over 170 times faster than the procedural implementation.",0098-5589;00985589,,10.1109/TSE.2014.2369047,Microsoft Research via a SEIF; 10.13039/100000001 - NSF; 10.13039/100000185 - DARPA; 10.13039/100006785 - Google; 10.13039/501100000038 - NSERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6951474,API mining;InvariMint;Model inference;declarative specification;inference comparison;inference extensibility;inference understanding;kTails;process mining;specification mining;synoptic,Algorithm design and analysis;Approximation algorithms;Educational institutions;Electronic mail;Inference algorithms;Postal services;Software algorithms,formal specification;inference mechanisms;system monitoring,InvariMint declarative specification approach;kTails algorithm;log system behavior analysis;model inference algorithm specification;synoptic algorithm;system understanding,,9,,42,,,20141110,April 1 2015,,IEEE,IEEE Journals & Magazines,,20
Customizing the Representation Capabilities of Process Models: Understanding the Effects of Perceived Modeling Impediments,B. M. Samuel; L. A. Watkins III; A. Ehle; V. Khatri,"Ivey Business School, Western University 1255 Western Road, London, ON, Canada",IEEE Transactions on Software Engineering,20150107,2015,41,1,19,39,"Process modeling is useful during the analysis and design of systems. Prior research acknowledges both impediments to process modeling that limits its use as well as customizations that can be employed to help improve the creation of process models. However, no research to date has provided a rich examination of the linkages between perceived process modeling impediments and process modeling customizations. In order to help address this gap, we first conceptualized perceived impediments to using process models as a _ÑÒlack of fit_Ñù between process modeling and another factor: (1) the role the process model is intended for; and (2) the task at hand. We conducted a case study at two large health insurance carriers to understand why the lack of fit existed and then show different types of process modeling customizations used to address the lack of fit and found a variety of _ÑÒphysical_Ñù and _ÑÒprocess_Ñù customizations employed to overcome the lack of fits. We generalize our findings into propositions for future research that examine the dynamic interaction between process models and their need to be understood by individuals during systems analysis and design.",0098-5589;00985589,,10.1109/TSE.2014.2354043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898868,Software process models;UML;activity diagrams;requirements/specification stools;requirements/specifications management;requirements/specifications process;use cases,Analytical models;Context;Interviews;Organizations;Software;Unified modeling language,formal specification;software process improvement,dynamic process model interaction;health insurance carriers;lack-of-fit;perceived process modeling impediments;physical customization;process customization;process model creation improvement;process model representation capability customization;process modeling customizations;system analysis;system design,,3,,97,,,20140915,Jan. 1 2015,,IEEE,IEEE Journals & Magazines,,20
Performance Analysis for Object-Oriented Software: A Systematic Mapping,D. Maplesden; E. Tempero; J. Hosking; J. C. Grundy,"Department of Computer Science, University of Auckland, Private Bag 92019, Auckland 1142, New Zealand",IEEE Transactions on Software Engineering,20150714,2015,41,7,691,710,"Performance is a crucial attribute for most software, making performance analysis an important software engineering task. The difficulty is that modern applications are challenging to analyse for performance. Many profiling techniques used in real-world software development struggle to provide useful results when applied to large-scale object-oriented applications. There is a substantial body of research into software performance generally but currently there exists no survey of this research that would help identify approaches useful for object-oriented software. To provide such a review we performed a systematic mapping study of empirical performance analysis approaches that are applicable to object-oriented software. Using keyword searches against leading software engineering research databases and manual searches of relevant venues we identified over 5,000 related articles published since January 2000. From these we systematically selected 253 applicable articles and categorised them according to ten facets that capture the intent, implementation and evaluation of the approaches. Our mapping study results allow us to highlight the main contributions of the existing literature and identify areas where there are interesting opportunities. We also find that, despite the research including approaches specifically aimed at object-oriented software, there are significant challenges in providing actionable feedback on the performance of large-scale object-oriented applications.",0098-5589;00985589,,10.1109/TSE.2015.2396514,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024167,Systematic review;object-oriented;performance;survey,Databases;Mathematical model;Performance analysis;Runtime;Software performance;Systematics,,,,2,,17,,,20150127,July 1 2015,,IEEE,IEEE Journals & Magazines,,19
Evaluating Legal Implementation Readiness Decision-Making,A. K. Massey; P. N. Otto; A. I. AntÕ_n,"Postdoctoral Fellow at the School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA",IEEE Transactions on Software Engineering,20150610,2015,41,6,545,564,"Software systems are increasingly regulated. Software engineers therefore must determine which requirements have met or exceeded their legal obligations and which requirements have not. Requirements that have met or exceeded their legal obligations are legally implementation ready, whereas requirements that have not met or exceeded their legal obligations need further refinement. In this paper, we examine how software engineers make these determinations using a multi-case study with three cases. Each case involves assessment of requirements for an electronic health record system that must comply with the US Health Insurance Portability and Accountability Act (HIPAA) and is measured against the evaluations of HIPAA compliance subject matter experts. Our first case examines how individual graduate-level software engineering students assess whether the requirements met or exceeded their HIPAA obligations. Our second case replicates the findings from our first case using a different set of participants. Our third case examines how graduate-level software engineering students assess requirements using the Wideband Delphi approach to deriving consensus in groups. Our findings suggest that the average graduate-level software engineering student is ill-prepared to write legally compliant software with any confidence and that domain experts are an absolute necessity.",0098-5589;00985589,,10.1109/TSE.2014.2383374,NSF ITR; 10.13039/100000001 - NSF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991569,Legal Implementation Readiness;Legal Requirements;Legal implementation readiness;Regulatory Compliance Software Engineering;Requirements Engineering;legal requirements;regulatory compliance software engineering;requirements engineering,Atmospheric measurements;Decision making;Law;Particle measurements;Software;Software engineering,electronic health records;law;software engineering,HIPAA obligations;US Health Insurance Portability and Accountability Act;electronic health record system;legal implementation readiness decision-making;legal obligations;legally compliant software;requirement assessment;software systems;wideband Delphi approach,,1,,41,,,20141218,June 1 2015,,IEEE,IEEE Journals & Magazines,,19
Automated Synthesis of Mediators to Support Component Interoperability,A. Bennaceur; V. Issarny,"Department of Computing, The Open University, United Kingdom",IEEE Transactions on Software Engineering,20150311,2015,41,3,221,240,"Interoperability is a major concern for the software engineering field, given the increasing need to compose components dynamically and seamlessly. This dynamic composition is often hampered by differences in the interfaces and behaviours of independently-developed components. To address these differences without changing the components, mediators that systematically enforce interoperability between functionally-compatible components by mapping their interfaces and coordinating their behaviours are required. Existing approaches to mediator synthesis assume that an interface mapping is provided which specifies the correspondence between the operations and data of the components at hand. In this paper, we present an approach based on ontology reasoning and constraint programming in order to infer mappings between components' interfaces automatically. These mappings guarantee semantic compatibility between the operations and data of the interfaces. Then, we analyse the behaviours of components in order to synthesise, if possible, a mediator that coordinates the computed mappings so as to make the components interact properly. Our approach is formally-grounded to ensure the correctness of the synthesised mediator. We demonstrate the validity of our approach by implementing the MICS (Mediator synthesis to Connect Components) tool and experimenting it with various real-world case studies.",0098-5589;00985589,,10.1109/TSE.2014.2364844,10.13039/501100000781 - ERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936339,Automated Synthesis;Constraint Programming;Interoperability;Mediators;Protocols;automated synthesis;constraint programming;mediators;protocols,Cognition;Google;Interoperability;Ontologies;Programming;Protocols;Semantics,object-oriented programming;ontologies (artificial intelligence);open systems,MICS;automated mediator synthesis;component interface;component interoperability;constraint programming;dynamic composition;functionally compatible component;interface mapping;mediator synthesis to connect components;ontology reasoning;semantic compatibility,,8,,57,,,20141024,March 1 2015,,IEEE,IEEE Journals & Magazines,,19
Instance Generator and Problem Representation to Improve Object Oriented Code Coverage,A. Sakti; G. Pesant; Y. G. GuÕ©hÕ©neuc,"Department of Computer and Software Engineering, &#201;cole Polytechnique de Montral, Montral, QC, Canada",IEEE Transactions on Software Engineering,20150311,2015,41,3,294,313,"Search-based approaches have been extensively applied to solve the problem of software test-data generation. Yet, test-data generation for object-oriented programming (OOP) is challenging due to the features of OOP, e.g., abstraction, encapsulation, and visibility that prevent direct access to some parts of the source code. To address this problem we present a new automated search-based software test-data generation approach that achieves high code coverage for unit-class testing. We first describe how we structure the test-data generation problem for unit-class testing to generate relevant sequences of method calls. Through a static analysis, we consider only methods or constructors changing the state of the class-under-test or that may reach a test target. Then we introduce a generator of instances of classes that is based on a family of means-of-instantiation including subclasses and external factory methods. It also uses a seeding strategy and a diversification strategy to increase the likelihood to reach a test target. Using a search heuristic to reach all test targets at the same time, we implement our approach in a tool, JTExpert, that we evaluate on more than a hundred Java classes from different open-source libraries. JTExpert gives better results in terms of search time and code coverage than the state of the art, EvoSuite, which uses traditional techniques.",0098-5589;00985589,,10.1109/TSE.2014.2363479,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926828,Automatic Test Data Generation;Automatic test data generation;Diversification Strategy;Java Testing;Java testing;Search Based Software Testing;Seeding Strategy;Unit Class Testing;diversification strategy;search based software testing;seeding strategy;unit class testing,Complexity theory;Generators;Java;Libraries;Production facilities;Search problems;Testing,Java;object-oriented programming;program diagnostics;program testing;public domain software,EvoSuite;JTExpert;Java class evaluation;OOP;abstraction;automated search-based software test-data generation approach;class-under-test;diversification strategy;encapsulation;instance generator;means-of-instantiation;method call sequences;object oriented code coverage;object-oriented programming;open-source libraries;problem representation;search heuristic;search time;search-based approach;seeding strategy;source code;static analysis;unit-class testing;visibility,,18,,53,,,20141016,March 1 2015,,IEEE,IEEE Journals & Magazines,,19
Software Plagiarism Detection with Birthmarks Based on Dynamic Key Instruction Sequences,Z. Tian; Q. Zheng; T. Liu; M. Fan; E. Zhuang; Z. Yang,"Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Department of Computer Science and Technology, Xi&#x2019;an Jiaotong University, Xi&#x2019;an, China",IEEE Transactions on Software Engineering,20151208,2015,41,12,1217,1235,"A software birthmark is a unique characteristic of a program. Thus, comparing the birthmarks between the plaintiff and defendant programs provides an effective approach for software plagiarism detection. However, software birthmark generation faces two main challenges: the absence of source code and various code obfuscation techniques that attempt to hide the characteristics of a program. In this paper, we propose a new type of software birthmark called DYnamic Key Instruction Sequence (DYKIS) that can be extracted from an executable without the need for source code. The plagiarism detection algorithm based on our new birthmarks is resilient to both weak obfuscation techniques such as compiler optimizations and strong obfuscation techniques implemented in tools such as SandMark, Allatori and Upx. We have developed a tool called DYKIS-PD (DYKIS Plagiarism Detection tool) and conducted extensive experiments on large number of binary programs. The tool, the benchmarks and the experimental results are all publicly available.",0098-5589;00985589,,10.1109/TSE.2015.2454508,Fundamental Research Funds for the Central Universities; Key Projects in the National Science and Technology Pillar Program of China; Ministry of Education Innovation Research Team; National Science Foundation of China; 10.13039/100000001 - National Science Foundation (NSF); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153572,Software plagiarism detection;software birthmark,Heuristic algorithms;Licenses;Plagiarism;Software engineering;Watermarking,fraud;program diagnostics;security of data,Allatori;DYKIS plagiarism detection tool;DYKIS-PD;SandMark;Upx;code obfuscation;compiler optimization;dynamic key instruction sequences;software birthmark;software plagiarism detection;source code,,7,,77,,,20150709,Dec. 1 2015,,IEEE,IEEE Journals & Magazines,,18
Automated Oracle Data Selection Support,G. Gay; M. Staats; M. Whalen; M. P. E. Heimdahl,"Department of Computer Science & Engineering, University of South Carolina",IEEE Transactions on Software Engineering,20151110,2015,41,11,1119,1137,"The choice of test oracle-the artifact that determines whether an application under test executes correctly-can significantly impact the effectiveness of the testing process. However, despite the prevalence of tools that support test input selection, little work exists for supporting oracle creation. We propose a method of supporting test oracle creation that automatically selects the oracle data-the set of variables monitored during testing-for expected value test oracles. This approach is based on the use of mutation analysis to rank variables in terms of fault-finding effectiveness, thus automating the selection of the oracle data. Experimental results obtained by employing our method over six industrial systems (while varying test input types and the number of generated mutants) indicate that our method-when paired with test inputs generated either at random or to satisfy specific structural coverage criteria-may be a cost-effective approach for producing small, effective oracle data sets, with fault finding improvements over current industrial best practice of up to 1,435 percent observed (with typical improvements of up to 50 percent).",0098-5589;00985589,,10.1109/TSE.2015.2436920,"Fonds National de la Recherche, Luxembourg; 10.13039/100000001 - NSF; 10.13039/100000001 - US National Science Foundation (NSF); 10.13039/100000104 - NASA; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112189,Oracle Data;Oracle Selection;Test Oracles;Testing;Verification;oracle data;oracle selection;test oracles;verification,Aerospace electronics;Computer crashes;Electronic mail;Monitoring;Software;Testing;Training,program testing;program verification,automated oracle data selection support;mutation analysis;oracle creation;software testing;specific structural coverage criteria;test oracle,,2,,40,,,20150522,Nov. 1 2015,,IEEE,IEEE Journals & Magazines,,18
"Inferring Loop Invariants by Mutation, Dynamic Analysis, and Static Checking",J. P. Galeotti; C. A. Furia; E. May; G. Fraser; A. Zeller,"Saarland University, Saarbr&#x00FC;cken, Germany",IEEE Transactions on Software Engineering,20151013,2015,41,10,1019,1037,"Verifiers that can prove programs correct against their full functional specification require, for programs with loops, additional annotations in the form of loop invariants-properties that hold for every iteration of a loop. We show that significant loop invariant candidates can be generated by systematically mutating postconditions; then, dynamic checking (based on automatically generated tests) weeds out invalid candidates, and static checking selects provably valid ones. We present a framework that automatically applies these techniques to support a program prover, paving the way for fully automatic verification without manually written loop invariants: Applied to 28 methods (including 39 different loops) from various java.util classes (occasionally modified to avoid using Java features not fully supported by the static checker), our DYNAMATE prototype automatically discharged 97 percent of all proof obligations, resulting in automatic complete correctness proofs of 25 out of the 28 methods-outperforming several state-of-the-art tools for fully automatic verification.",0098-5589;00985589,,10.1109/TSE.2015.2431688,EU FP7; 10.13039/501100000781 - ERC; 10.13039/501100000781 - European Research Council; 10.13039/501100004343 - Swiss SNF; 10.13039/501100004963 - European Unionëès Seventh Framework Programme; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105412,Loop invariants;automatic verification;dynamic analysis;functional properties;inference,Arrays;Detectors;Generators;Heuristic algorithms;Instruments;Java;Prototypes,Java;formal specification;program control structures;program testing;program verification;system monitoring,DYNAMATE prototype;Java.util classes;automatic complete correctness proofs;automatic verification;dynamic analysis;functional specification;loop invariant inference;mutation;program prover;static checking;test automatic generation,,2,,80,,,20150511,Oct. 1 2015,,IEEE,IEEE Journals & Magazines,,18
Program Characterization Using Runtime Values and Its Application to Software Plagiarism Detection,Y. C. Jhi; X. Jia; X. Wang; S. Zhu; P. Liu; D. Wu,"Samsung SDS R&D Center, Seoul, Korea",IEEE Transactions on Software Engineering,20150915,2015,41,9,925,943,"Illegal code reuse has become a serious threat to the software community. Identifying similar or identical code fragments becomes much more challenging in code theft cases where plagiarizers can use various automated code transformation or obfuscation techniques to hide stolen code from being detected. Previous works in this field are largely limited in that (i) most of them cannot handle advanced obfuscation techniques, and (ii) the methods based on source code analysis are not practical since the source code of suspicious programs typically cannot be obtained until strong evidences have been collected. Based on the observation that some critical runtime values of a program are hard to be replaced or eliminated by semantics-preserving transformation techniques, we introduce a novel approach to dynamic characterization of executable programs. Leveraging such invariant values, our technique is resilient to various control and data obfuscation techniques. We show how the values can be extracted and refined to expose the critical values and how we can apply this runtime property to help solve problems in software plagiarism detection. We have implemented a prototype with a dynamic taint analyzer atop a generic processor emulator. Our value-based plagiarism detection method (VaPD) uses the longest common subsequence based similarity measuring algorithms to check whether two code fragments belong to the same lineage. We evaluate our proposed method through a set of real-world automated obfuscators. Our experimental results show that the value-based method successfully discriminates 34 plagiarisms obfuscated by SandMark, plagiarisms heavily obfuscated by KlassMaster, programs obfuscated by Thicket, and executables obfuscated by Loco/Diablo.",0098-5589;00985589,,10.1109/TSE.2015.2418777,National High-tech R&D Program of China; 10.13039/100000001 - US National Science Foundation (NSF); 10.13039/100006602 - AFRL; 10.13039/501100001809 - National Natural Science Foundation of China (NSFC); 10.13039/501100002367 - Strategic Priority Research Program of the Chinese Academy of Sciences; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076635,Software plagiarism detection;dynamic code identification;dynamic code identification.,Java;Optimization;Plagiarism;Program processors;Runtime;Semantics,security of data;software engineering;source code (software),Loco/Diablo;SandMark;Thicket;VaPD;generic processor emulator;identical code fragments;illegal code reuse;obfuscation techniques;program characterization;runtime values;semantics-preserving transformation techniques;software community;software plagiarism detection;source code;value-based plagiarism detection method,,3,,59,,,20150401,Sept. 1 2015,,IEEE,IEEE Journals & Magazines,,18
Reducing Feedback Delay of Software Development Tools via Continuous Analysis,K. MuÅªlu; Y. Brun; M. D. Ernst; D. Notkin,"Department of Computer Science & Engineering, University of Washington, Seattle, WA",IEEE Transactions on Software Engineering,20150811,2015,41,8,745,763,"During software development, the sooner a developer learns how code changes affect program analysis results, the more helpful that analysis is. Manually invoking an analysis may interrupt the developer's workflow or cause a delay before the developer learns the implications of the change. A better approach is continuous analysis tools that always provide up-to-date results. We present Codebase Replication, a technique that eases the implementation of continuous analysis tools by converting an existing offline analysis into an IDE-integrated, continuous tool with two desirable properties: isolation and currency. Codebase Replication creates and keeps in sync a copy of the developer's codebase. The analysis runs on the copy codebase without disturbing the developer and without being disturbed by the developer's changes. We developed Solstice, an open-source, publicly-available Eclipse plug-in that implements Codebase Replication. Solstice has less than 2.5 milliseconds overhead for most common developer actions. We used Solstice to implement four Eclipse-integrated continuous analysis tools based on the offline versions of FindBugs, PMD, data race detection, and unit testing. Each conversion required on average 710 LoC and 20 hours of implementation effort. Case studies indicate that Solstice-based continuous analysis tools are intuitive and easy-to-use.",0098-5589;00985589,,10.1109/TSE.2015.2417161,10.13039/100000001 - NSF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069257,Codebase Replication;Continuous analysis;Solstice,Delays;Electronic mail;Interrupters;Servers;Software;Synchronization;Testing,program diagnostics;program testing;software tools,Eclipse-integrated continuous analysis tools;FindBugs;IDE-integrated;PMD;Solstice;codebase replication;data race detection;feedback delay reduction;offline analysis;open-source Eclipse plug-in;program analysis;software development tools;unit testing,,2,,80,,,20150326,Aug. 1 2015,,IEEE,IEEE Journals & Magazines,,18
"Aligning Qualitative, Real-Time, and Probabilistic Property Specification Patterns Using a Structured English Grammar",M. Autili; L. Grunske; M. Lumpe; P. Pelliccione; A. Tang,"Dipartimento di Ingegneria e Scienze dell&#8217;Informazione e Matematica, Universit&#224; dell&#8217;Aquila, Aquila, Italy",IEEE Transactions on Software Engineering,20150714,2015,41,7,620,638,"Formal methods offer an effective means to assert the correctness of software systems through mathematical reasoning. However, the need to formulate system properties in a purely mathematical fashion can create pragmatic barriers to the application of these techniques. For this reason, Dwyer et al. invented property specification patterns which is a system of recurring solutions to deal with the temporal intricacies that would make the construction of reactive systems very hard otherwise. Today, property specification patterns provide general rules that help practitioners to qualify order and occurrence, to quantify time bounds, and to express probabilities of events. Nevertheless, a comprehensive framework combining qualitative, real-time, and probabilistic property specification patterns has remained elusive. The benefits of such a framework are twofold. First, it would remove the distinction between qualitative and quantitative aspects of events; and second, it would provide a structure to systematically discover new property specification patterns. In this paper, we report on such a framework and present a unified catalogue that combines all known plus 40 newly identified or extended patterns. We also offer a natural language front-end to map patterns to a temporal logic of choice. To demonstrate the virtue of this new framework, we applied it to a variety of industrial requirements, and use PSPWizard, a tool specifically developed to work with our unified pattern catalogue, to automatically render concrete instances of property specification patterns to formulae of an underlying temporal logic of choice.",0098-5589;00985589,,10.1109/TSE.2015.2398877,PRESTO; 10.13039/100004807 - DFG; 10.13039/501100000780 - European Commission; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029714,Probabilistic Properties;Real-time Properties;Specification Patterns;Specification patterns;probabilistic properties;real-time properties,Educational institutions;Electronic mail;Grammar;Natural languages;Probabilistic logic;Real-time systems;Software,formal specification;natural language processing;probability;temporal logic,PSPWizard;event probability;event qualitative aspect;event quantitative aspect;formal methods;mathematical reasoning;natural language front-end;occurrence qualification;order qualification;pattern mapping;probabilistic property specification pattern;qualitative property specification pattern;real-time property specification pattern;software system correctness;structured English grammar;temporal intricacies;temporal logic;time bound quantification;unified pattern catalogue,,11,1,47,,,20150203,July 1 2015,,IEEE,IEEE Journals & Magazines,,18
Automatic Detection and Resolution of Lexical Ambiguity in Process Models,F. Pittke; H. Leopold; J. Mendling,"Institute for Information Business, Vienna, WU, Austria",IEEE Transactions on Software Engineering,20150610,2015,41,6,526,544,"System-related engineering tasks are often conducted using process models. In this context, it is essential that these models do not contain structural or terminological inconsistencies. To this end, several automatic analysis techniques have been proposed to support quality assurance. While formal properties of control flow can be checked in an automated fashion, there is a lack of techniques addressing textual quality. More specifically, there is currently no technique available for handling the issue of lexical ambiguity caused by homonyms and synonyms. In this paper, we address this research gap and propose a technique that detects and resolves lexical ambiguities in process models. We evaluate the technique using three process model collections from practice varying in size, domain, and degree of standardization. The evaluation demonstrates that the technique significantly reduces the level of lexical ambiguity and that meaningful candidates are proposed for resolving ambiguity.",0098-5589;00985589,,10.1109/TSE.2015.2396895,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027184,Business Process Models;Identification of Lexical Ambiguity;Identification of lexical ambiguity;Resolution of Lexical Ambiguity;business process models;resolution of lexical ambiguity,Business;Context;Manuals;Natural languages;Object oriented modeling;Unified modeling language;Vectors,computational linguistics;grammars;natural language processing;software engineering,automated fashion;automatic analysis technique;automatic detection;automatic resolution;control flow;homonyms;lexical ambiguity;process model collection;quality assurance;structural inconsistency;synonyms;system-related engineering task;terminological inconsistency;textual quality,,11,,106,,,20150129,June 1 2015,,IEEE,IEEE Journals & Magazines,,18
Safer User Interfaces: A Case Study in Improving Number Entry,H. Thimbleby,"Department of Computer Science, Swansea University, Swansea SA2 0SF, Wales, United Kingdom",IEEE Transactions on Software Engineering,20150714,2015,41,7,711,729,"Numbers are used in critical applications, including finance, healthcare, aviation, and of course in every aspect of computing. User interfaces for number entry in many devices (calculators, spreadsheets, infusion pumps, mobile phones, etc.) have bugs and design defects that induce unnecessary use errors that compromise their dependability. Focusing on Arabic key interfaces, which use digit keys 0-9-ëˆ usually augmented with correction keys, this paper introduces a method for formalising and managing design problems. Since number entry and devices such as calculators have been the subject of extensive user interface research since at least the 1980s, the diverse design defects uncovered imply that user evaluation methodologies are insufficient for critical applications. Likewise, formal methods are not being applied effectively. User interfaces are not trivial and more attention should be paid to their correct design and implementation. The paper includes many recommendations for designing safer number entry user interfaces.",0098-5589;00985589,,10.1109/TSE.2014.2383396,10.13039/501100000266 - Engineering and Physical Sciences Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991548,Error processing;Human Factors in Software Design;Information Interfaces;Representation (HCI);Software/Software Engineering;User Interfaces;User interfaces;human factors in software design;information interfaces and representation (HCI);software/software engineering;user interfaces,Calculators;Computer bugs;Debugging;Pressing;Safety;Software;User interfaces,user interfaces,Arabic key interfaces;correction keys;design problem formalization;design problem management;number entry;safer user interfaces,,8,,47,,,20141218,July 1 2015,,IEEE,IEEE Journals & Magazines,,18
The Oracle Problem in Software Testing: A Survey,E. T. Barr; M. Harman; P. McMinn; M. Shahbaz; S. Yoo,"Department of Computer Science, University College London, Gower Street, London WC2R 2LS, London, United Kingdom",IEEE Transactions on Software Engineering,20150512,2015,41,5,507,525,"Testing involves examining the behaviour of a system in order to discover potential faults. Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the _ÑÒtest oracle problem_Ñù. Test oracle automation is important to remove a current bottleneck that inhibits greater overall test automation. Without test oracle automation, the human has to determine whether observed behaviour is correct. The literature on test oracles has introduced techniques for oracle automation, including modelling, specifications, contract-driven development and metamorphic testing. When none of these is completely adequate, the final source of test oracle information remains the human, who may be aware of informal specifications, expectations, norms and domain specific information that provide informal oracle guidance. All forms of test oracles, even the humble human, involve challenges of reducing cost and increasing benefit. This paper provides a comprehensive survey of current approaches to the test oracle problem and an analysis of trends in this important area of software testing research and practice.",0098-5589;00985589,,10.1109/TSE.2014.2372785,10.13039/501100000266 - Engineering and Physical Sciences Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963470,Automatic testing;Test oracle;Testing formalism;automatic testing;testing formalism,Automation;Licenses;Market research;Probabilistic logic;Reliability;Software testing,formal specification;program testing,contract-driven development;domain specific information;informal oracle guidance;informal specifications;metamorphic testing;oracle automation;software testing practice;software testing research;test oracle information;test oracle problem,,62,,211,,,20141120,May 1 2015,,IEEE,IEEE Journals & Magazines,,18
Quantitative Evaluation of Model-Driven Performance Analysis and Simulation of Component-Based Architectures,F. Brosig; P. Meier; S. Becker; A. Koziolek; H. Koziolek; S. Kounev,"Department of Computer Science, University of W&#252;rzburg, Am Hubland, W&#x00FC;rzburg, Germany",IEEE Transactions on Software Engineering,20150210,2015,41,2,157,175,"During the last decade, researchers have proposed a number of model transformations enabling performance predictions. These transformations map performance-annotated software architecture models into stochastic models solved by analytical means or by simulation. However, so far, a detailed quantitative evaluation of the accuracy and efficiency of different transformations is missing, making it hard to select an adequate transformation for a given context. This paper provides an in-depth comparison and quantitative evaluation of representative model transformations to, e.g., queueing petri nets and layered queueing networks. The semantic gaps between typical source model abstractions and the different analysis techniques are revealed. The accuracy and efficiency of each transformation are evaluated by considering four case studies representing systems of different size and complexity. The presented results and insights gained from the evaluation help software architects and performance engineers to select the appropriate transformation for a given context, thus significantly improving the usability of model transformations for performance prediction.",0098-5589;00985589,,10.1109/TSE.2014.2362755,Collaborative Research Center ëèOn-The-Fly Computingëè; 10.13039/100004807 - DFG; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920061,D.2.10.h Quality analysis and evaluation;D.2.11 Software architectures;D.2.2 Design tools and techniques;Software architectures;design tools and techniques;quality analysis and evaluation,Accuracy;Analytical models;Phase change materials;Predictive models;Software architecture;Stochastic processes;Unified modeling language,object-oriented programming;software architecture;software performance evaluation;stochastic processes,component-based architectures;model-driven performance analysis;performance predictions;quantitative evaluation;representative model transformations;semantic gaps;source model abstractions;stochastic models;transformations map performance-annotated software architecture models,,11,,50,,,20141013,Feb. 1 2015,,IEEE,IEEE Journals & Magazines,,18
Mining Workflow Models fromë_Webë_Applications,M. Schur; A. Roth; A. Zeller,SAP SE,IEEE Transactions on Software Engineering,20151208,2015,41,12,1184,1201,"Modern business applications predominantly rely on web technology, enabling software vendors to efficiently provide them as a service, removing some of the complexity of the traditional release and update process. While this facilitates shorter, more efficient and frequent release cycles, it requires continuous testing. Having insight into application behavior through explicit models can largely support development, testing and maintenance. Model-based testing allows efficient test creation based on a description of the states the application can be in and the transitions between these states. As specifying behavior models that are precise enough to be executable by a test automation tool is a hard task, an alternative is to extract them from running applications. However, mining such models is a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present Process Crawler (ProCrawl), a tool to mine behavior models from web applications that support multi-user workflows. ProCrawl incrementally learns a model by generating program runs and observing the application behavior through the user interface. In our evaluation on several real-world web applications, ProCrawl extracted models that concisely describe the implemented workflows and can be directly used for model-based testing.",0098-5589;00985589,,10.1109/TSE.2015.2461542,10.13039/501100000781 - European Research Council (ERC); 10.13039/501100002347 - German Federal Ministry of Education and Research (BMBF); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169616,Specification mining;dynamic analysis;model-based testing;web system testing,Automation;Browsers;Data mining;Data models;Software engineering;Web services,Internet;data mining;program testing;system monitoring;user interfaces,ProCrawl;Process Crawler;Web applications;Web technology;behavior model;continuous testing;mining workflow model;model-based testing;multiuser workflow;software vendor;test automation tool;test creation;user interface,,1,,47,,,20150728,Dec. 1 2015,,IEEE,IEEE Journals & Magazines,,17
GALE: Geometric Active Learning for Search-Based Software Engineering,J. Krall; T. Menzies; M. Davies,"LoadIQ, NV",IEEE Transactions on Software Engineering,20151013,2015,41,10,1001,1018,"Multi-objective evolutionary algorithms (MOEAs) help software engineers find novel solutions to complex problems. When automatic tools explore too many options, they are slow to use and hard to comprehend. GALE is a near-linear time MOEA that builds a piecewise approximation to the surface of best solutions along the Pareto frontier. For each piece, GALE mutates solutions towards the better end. In numerous case studies, GALE finds comparable solutions to standard methods (NSGA-II, SPEA2) using far fewer evaluations (e.g. 20 evaluations, not 1,000). GALE is recommended when a model is expensive to evaluate, or when some audience needs to browse and understand how an MOEA has made its conclusions.",0098-5589;00985589,,10.1109/TSE.2015.2432024,Qatar/West Virginia University; 10.13039/100000001 - US National Science Foundation (NSF); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105950,Multi-objective optimization;active learning;search based software engineering,Approximation methods;Biological system modeling;Computational modeling;Optimization;Sociology;Software;Standards,Pareto optimisation;approximation theory;computational complexity;evolutionary computation;learning (artificial intelligence);software engineering,GALE;Pareto frontier;geometric active learning;multiobjective evolutionary algorithm;near-linear time MOEA;piecewise approximation;search-based software engineering,,10,,64,,,20150512,Oct. 1 2015,,IEEE,IEEE Journals & Magazines,,17
Who Will Stay in the FLOSS Community? Modeling Participant_Ñés Initial Behavior,M. Zhou; A. Mockus,"School of Electronics Engineering and Computer Science, Peking University and Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China",IEEE Transactions on Software Engineering,20150107,2015,41,1,82,99,"Motivation: To survive and succeed, FLOSS projects need contributors able to accomplish critical project tasks. However, such tasks require extensive project experience of long term contributors (LTCs). Aim: We measure, understand, and predict how the newcomers' involvement and environment in the issue tracking system (ITS) affect their odds of becoming an LTC. Method: ITS data of Mozilla and Gnome, literature, interviews, and online documents were used to design measures of involvement and environment. A logistic regression model was used to explain and predict contributor's odds of becoming an LTC. We also reproduced the results on new data provided by Mozilla. Results: We constructed nine measures of involvement and environment based on events recorded in an ITS. Macro-climate is the overall project environment while micro-climate is person-specific and varies among the participants. Newcomers who are able to get at least one issue reported in the first month to be fixed, doubled their odds of becoming an LTC. The macro-climate with high project popularity and the micro-climate with low attention from peers reduced the odds. The precision of LTC prediction was 38 times higher than for a random predictor. We were able to reproduce the results with new Mozilla data without losing the significance or predictive power of the previously published model. We encountered unexpected changes in some attributes and suggest ways to make analysis of ITS data more reproducible. Conclusions: The findings suggest the importance of initial behaviors and experiences of new participants and outline empirically-based approaches to help the communities with the recruitment of contributors for long-term participation and to help the participants contribute more effectively. To facilitate the reproduction of the study and of the proposed measures in other contexts, we provide the data we retrieved and the scripts we wrote at https://www.passion-lab.org/projects/developerfluency.html.",0098-5589;00985589,,10.1109/TSE.2014.2349496,National Basic Research Program of China; National Hi-Tech Research and Development Program of China; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6880395,Long term contributor;extent of involvement;initial behavior;interaction with environment;issue tracking system;mining software repository;open source software,Atmospheric measurements;Communities;Data mining;Data models;Electronic mail;Particle measurements;Predictive models,behavioural sciences;project management;public domain software,FLOSS community;Free-Libre and/or open source software projects;Gnome;ITS data;LTC;Mozilla data;critical project;issue tracking system;logistic regression model;long term contributors;macroclimate;microclimate;open source software,,13,,47,,,20140819,Jan. 1 2015,,IEEE,IEEE Journals & Magazines,,17
An Eye-Tracking Study of Java Programmers and Application to Source Code Summarization,P. Rodeghero; C. Liu; P. W. McBurney; C. McMillan,"Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN",IEEE Transactions on Software Engineering,20151110,2015,41,11,1038,1054,"Source Code Summarization is an emerging technology for automatically generating brief descriptions of code. Current summarization techniques work by selecting a subset of the statements and keywords from the code, and then including information from those statements and keywords in the summary. The quality of the summary depends heavily on the process of selecting the subset: a high-quality selection would contain the same statements and keywords that a programmer would choose. Unfortunately, little evidence exists about the statements and keywords that programmers view as important when they summarize source code. In this paper, we present an eye-tracking study of 10 professional Java programmers in which the programmers read Java methods and wrote English summaries of those methods. We apply the findings to build a novel summarization tool. Then, we evaluate this tool. Finally, we further analyze the programmers' method summaries to explore specific keyword usage and provide evidence to support the development of source code summarization systems.",0098-5589;00985589,,10.1109/TSE.2015.2442238,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7118751,Source code summaries;program comprehension,Documentation;Java;Navigation;Software;Software engineering;XML,Java;program compilers;source code (software),Java programmer;code generation;eye-tracking study;source code summarization,,4,,75,,,20150605,Nov. 1 2015,,IEEE,IEEE Journals & Magazines,,16
Facilitating Coordination between Software Developers: A Study and Techniques for Timely and Efficient Recommendations,K. Blincoe; G. Valetto; D. Damian,"Software Engineering Global Interaction Lab, Victoria, BC, Canada",IEEE Transactions on Software Engineering,20151013,2015,41,10,969,985,"When software developers fail to coordinate, build failures, duplication of work, schedule slips and software defects can result. However, developers are often unaware of when they need to coordinate, and existing methods and tools that help make developers aware of their coordination needs do not provide timely or efficient recommendations. We describe our techniques to identify timely and efficient coordination recommendations, which we developed and evaluated in a study of coordination needs in the Mylyn software project. We describe how data obtained from tools that capture developer actions within their Integrated Development Environment (IDE) as they occur can be used to timely identify coordination needs; we also describe how properties of tasks coupled with machine learning can focus coordination recommendations to those that are more critical to the developers to reduce information overload and provide more efficient recommendations. We motivate our techniques through developer interviews and report on our quantitative analysis of coordination needs in the Mylyn project. Our results suggest that by leveraging IDE logging facilities, properties of tasks and machine learning techniques awareness tools could make developers aware of critical coordination needs in a timely way. We conclude by discussing implications for software engineering research and tool design.",0098-5589;00985589,,10.1109/TSE.2015.2431680,NECSIS; 10.13039/100000001 - US National Science Foundation (NSF); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105409,Computer-supported cooperative work;Human Factors in Software Design;Management;Metrics/Measurement;Productivity;Programming Teams;human factors in software design;management;metrics/measurement;productivity;programming teams,Accuracy;Correlation;Encoding;Interviews;Manuals;Software;Statistical analysis,groupware;learning (artificial intelligence);programming environments;project management;software tools,IDE logging facilities;Mylyn software project;coordination needs quantitative analysis;coordination recommendation;integrated development environment;machine learning technique awareness tools;software developers;software engineering research;task properties;tool design,,2,,70,,,20150511,Oct. 1 2015,,IEEE,IEEE Journals & Magazines,,16
The Risks of Coverage-Directed Test Case Generation,G. Gay; M. Staats; M. Whalen; M. P. E. Heimdahl,"Department of Computer Science & Engineering, University of South Carolina",IEEE Transactions on Software Engineering,20150811,2015,41,8,803,819,"A number of structural coverage criteria have been proposed to measure the adequacy of testing efforts. In the avionics and other critical systems domains, test suites satisfying structural coverage criteria are mandated by standards. With the advent of powerful automated test generation tools, it is tempting to simply generate test inputs to satisfy these structural coverage criteria. However, while techniques to produce coverage-providing tests are well established, the effectiveness of such approaches in terms of fault detection ability has not been adequately studied. In this work, we evaluate the effectiveness of test suites generated to satisfy four coverage criteria through counterexample-based test generation and a random generation approach-where tests are randomly generated until coverage is achieved-contrasted against purely random test suites of equal size. Our results yield three key conclusions. First, coverage criteria satisfaction alone can be a poor indication of fault finding effectiveness, with inconsistent results between the seven case examples (and random test suites of equal size often providing similar-or even higher-levels of fault finding). Second, the use of structural coverage as a supplement-rather than a target-for test generation can have a positive impact, with random test suites reduced to a coverage-providing subset detecting up to 13.5 percent more faults than test suites generated specifically to achieve coverage. Finally, Observable MC/DC, a criterion designed to account for program structure and the selection of the test oracle, can-in part-address the failings of traditional structural coverage criteria, allowing for the generation of test suites achieving higher levels of fault detection than random test suites of equal size. These observations point to risks inherent in the increase in test automation in critical systems, and the need for more research in how coverage criteria, test generation approaches, the test oracle use- , and system structure jointly influence test effectiveness.",0098-5589;00985589,,10.1109/TSE.2015.2421011,"10.13039/100000001 - NSF; 10.13039/100000104 - NASA; 10.13039/501100001866 - Fonds National de la Recherche, Luxembourg; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081779,Software Testing;Software testing;System Testing;system testing,Aerospace electronics;Fault detection;Measurement;NASA;Software packages;Standards;Testing,program testing;risk management;software fault tolerance,automated test generation tools;counterexample-based test generation;coverage criteria satisfaction;coverage-directed test case generation;critical systems;fault detection;fault finding effectiveness;observable MC/DC;program structure;random generation approach;random test suites;risks;software testing;structural coverage criteria;system structure;test automation;test oracle selection,,15,,56,,,20150408,Aug. 1 2015,,IEEE,IEEE Journals & Magazines,,16
Self-Adapting Reliability in Distributed Software Systems,Y. Brun; J. y. Bang; G. Edwards; N. Medvidovic,"School of Computer Science, University of Massachusetts, Amherst, MA",IEEE Transactions on Software Engineering,20150811,2015,41,8,764,780,"Developing modern distributed software systems is difficult in part because they have little control over the environments in which they execute. For example, hardware and software resources on which these systems rely may fail or become compromised and malicious. Redundancy can help manage such failures and compromises, but when faced with dynamic, unpredictable resources and attackers, the system reliability can still fluctuate greatly. Empowering the system with self-adaptive and self-managing reliability facilities can significantly improve the quality of the software system and reduce reliance on the developer predicting all possible failure conditions. We present iterative redundancy, a novel approach to improving software system reliability by automatically injecting redundancy into the system's deployment. Iterative redundancy self-adapts in three ways: (1) by automatically detecting when the resource reliability drops, (2) by identifying unlucky parts of the computation that happen to deploy on disproportionately many compromised resources, and (3) by not relying on a priori estimates of resource reliability. Further, iterative redundancy is theoretically optimal in its resource use: Given a set of resources, iterative redundancy guarantees to use those resources to produce the most reliable version of that software system possible; likewise, given a desired increase in the system's reliability, iterative redundancy guarantees achieving that reliability using the least resources possible. Iterative redundancy handles even the Byzantine threat model, in which compromised resources collude to attack the system. We evaluate iterative redundancy in three ways. First, we formally prove its self-adaptation, efficiency, and optimality properties. Second, we simulate it at scale using discrete event simulation. Finally, we modify the existing, open-source, volunteer-computing BOINC software system and deploy it on the globally-distributed PlanetLab t- stbed network to empirically evaluate that iterative redundancy is self-adaptive and more efficient than existing techniques.",0098-5589;00985589,,10.1109/TSE.2015.2412134,IARPA; 10.13039/100000001 - National Science Foundation; 10.13039/100000185 - DARPA; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058381,Redundancy;fault-tolerance;iterative redundancy;optimal redundancy;reliability;self-adaptation,Computational modeling;Redundancy;Reliability engineering;Servers;Software reliability;Software systems,discrete event simulation;distributed processing;public domain software;resource allocation;security of data;software quality;software reliability;system recovery,Byzantine threat model;compromise management;compromised resource collusion;discrete event simulation;distributed software systems;dynamic unpredictable resources;failure condition;failure management;globally-distributed PlanetLab testbed network;hardware resources;iterative redundancy;open-source volunteer-computing BOINC software system;optimality property;resource reliability estimate;self-adapting reliability;self-adaptive reliability;self-managing reliability;software resources;software system quality;system reliability,,4,,53,,,20150311,Aug. 1 2015,,IEEE,IEEE Journals & Magazines,,16
Extracting Development Tasks to Navigate Software Documentation,C. Treude; M. P. Robillard; B. Dagenais,"Departamento de Inform&#225;tica e Matem&#225;tica Aplicada, Universidade Federal do Rio Grande do Norte, Natal, RN, Brazil",IEEE Transactions on Software Engineering,20150610,2015,41,6,565,581,"Knowledge management plays a central role in many software development organizations. While much of the important technical knowledge can be captured in documentation, there often exists a gap between the information needs of software developers and the documentation structure. To help developers navigate documentation, we developed a technique for automatically extracting tasks from software documentation by conceptualizing tasks as specific programming actions that have been described in the documentation. More than 70 percent of the tasks we extracted from the documentation of two projects were judged meaningful by at least one of two developers. We present TaskNavigator, a user interface for search queries that suggests tasks extracted with our technique in an auto-complete list along with concepts, code elements, and section headers. We conducted a field study in which six professional developers used TaskNavigator for two weeks as part of their ongoing work. We found search results identified through extracted tasks to be more helpful to developers than those found through concepts, code elements, and section headers. The results indicate that task descriptions can be effectively extracted from software documentation, and that they help bridge the gap between documentation structure and the information needs of software developers.",0098-5589;00985589,,10.1109/TSE.2014.2387172,10.13039/501100000038 - NSERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000568,Auto-Complete;Development Tasks;Natural Language Processing;Navigation;Software Documentation;Software documentation;auto-complete;development tasks;natural language processing;navigation,Data mining;Documentation;Natural language processing;Navigation;Programming;Software;Subscriptions,knowledge management;software engineering;user interfaces,TaskNavigator user interface;documentation structure;information needs;knowledge management;programming action;software developers;software documentation navigation,,11,,59,,,20141231,June 1 2015,,IEEE,IEEE Journals & Magazines,,16
Range Fixes: Interactive Error Resolution for Software Configuration,Y. Xiong; H. Zhang; A. Hubaux; S. She; J. Wang; K. Czarnecki,"School of Electronics Engineering and Computer Science, Institute of Software, Peking University, Beijing, PR China",IEEE Transactions on Software Engineering,20150610,2015,41,6,603,619,"To prevent ill-formed configurations, highly configurable software often allows defining constraints over the available options. As these constraints can be complex, fixing a configuration that violates one or more constraints can be challenging. Although several fix-generation approaches exist, their applicability is limited because (1) they typically generate only one fix or a very long fix list, difficult for the user to identify the desirable fix; and (2) they do not fully support non-Boolean constraints, which contain arithmetic, inequality, and string operators. This paper proposes a novel concept, range fix, for software configuration. A range fix specifies the options to change and the ranges of values for these options. We also design an algorithm that automatically generates range fixes for a violated constraint. We have evaluated our approach with three different strategies for handling constraint interactions, on data from nine open source projects over two configuration platforms. The evaluation shows that our notion of range fix leads to mostly simple yet complete sets of fixes, and our algorithm is able to generate fixes within one second for configuration systems with a few thousands options and constraints.",0098-5589;00985589,,10.1109/TSE.2014.2383381,High-Tech Research and Development Program of China; National Basic Research Program of China; 10.13039/501100000038 - NSERC; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991616,Consistency Management;Consistency management;Error Resolution;Range Fix;Software Configuration;error resolution;range fix;software configuration,Biological system modeling;Concrete;Kernel;Linux;Navigation;Reactive power,constraint handling;software engineering,constraint interaction handling;fix-generation approaches;interactive error resolution;open source projects;range fixes;software configuration,,7,,46,,,20141218,June 1 2015,,IEEE,IEEE Journals & Magazines,,16
Static Fault Localization in Model Transformations,L. BurgueÕ±o; J. Troya; M. Wimmer; A. Vallecillo,"Dept. Lenguajes y Ciencias de la Computaci&#243;n, Universidad de M&#225;laga, Bulevar Louis Pasteur, 35, Malaga, Spain",IEEE Transactions on Software Engineering,20150512,2015,41,5,490,506,"As the complexity of model transformations grows, there is an increasing need to count on methods, mechanisms, and tools for checking their correctness, i.e., the alignment between specifications and implementations. In this paper we present a light-weight and static approach for locating the faulty rules in model transformations, based on matching functions that automatically establish these alignments using the metamodel footprints, i.e., the metamodel elements used. The approach is implemented for the combination of Tracts and ATL, both residing in the Eclipse Modeling Framework, and is supported by the corresponding toolkit. An evaluation discussing the accuracy and the limitations of the approach is also provided. Furthermore, we identify the kinds of transformations which are most suitable for validation with the proposed approach and use mutation techniques to evaluate its effectiveness.",0098-5589;00985589,,10.1109/TSE.2014.2375201,Spanish Project; 10.13039/501100000780 - EC; 10.13039/501100004955 - Austrian Research Promotion Agency; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6967841,Model transformation;model alignment;transformation testing,Analytical models;Complexity theory;Context;Context modeling;Contracts;Navigation;Testing,Unified Modeling Language;fault tolerant computing,Eclipse modeling framework;faulty rules;matching functions;metamodel elements;metamodel footprints;model transformations;mutation techniques;static fault localization,,16,,67,,,20141126,May 1 2015,,IEEE,IEEE Journals & Magazines,,16
The Impact of View Histories on Edit Recommendations,S. Lee; S. Kang; S. Kim; M. Staats,"Department of Computer Science, KAIST, Daejeon 305-701, Republic of Korea, Guseong-dong, Yuseong-gu",IEEE Transactions on Software Engineering,20150311,2015,41,3,314,330,"Recommendation systems are intended to increase developer productivity by recommending files to edit. These systems mine association rules in software revision histories. However, mining coarse-grained rules using only edit histories produces recommendations with low accuracy, and can only produce recommendations after a developer edits a file. In this work, we explore the use of finer-grained association rules, based on the insight that view histories help characterize the contexts of files to edit. To leverage this additional context and fine-grained association rules, we have developed MI, a recommendation system extending ROSE, an existing edit-based recommendation system. We then conducted a comparative simulation of ROSE and MI using the interaction histories stored in the Eclipse Bugzilla system. The simulation demonstrates that MI predicts the files to edit with significantly higher recommendation accuracy than ROSE (about 63 over 35 percent), and makes recommendations earlier, often before developers begin editing. Our results clearly demonstrate the value of considering both views and edits in systems to recommend files to edit, and results in more accurate, earlier, and more flexible recommendations.",0098-5589;00985589,,10.1109/TSE.2014.2362138,"Information Technology Research Center; 10.13039/501100002701 - Ministry of Education; 10.13039/501100003621 - Ministry of Science, ICT & Future Planning; 10.13039/501100003665 - National IT Industry Promotion Agency; 10.13039/501100003725 - National Research Foundation of Korea; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926851,Programming environments/construction tools;association rules;data mining;interactive environments;programmer interaction histories;software maintenance,Accuracy;Association rules;Context;History;Predictive models;Software,data mining;interactive programming;recommender systems,MI;ROSE;association rules mining;coarse grained rules mining;edit histories;edit-based recommendation system;finer grained association rules;programmer interaction histories;software revision histories,,8,,38,,,20141016,March 1 2015,,IEEE,IEEE Journals & Magazines,,16
The Design Space of Bug Fixes and How Developers Navigate It,E. Murphy-Hill; T. Zimmermann; C. Bird; N. Nagappan,"Department of Computer Science, North Carolina State University, Raleigh, NC",IEEE Transactions on Software Engineering,20150107,2015,41,1,65,81,"When software engineers fix bugs, they may have several options as to how to fix those bugs. Which fix they choose has many implications, both for practitioners and researchers: What is the risk of introducing other bugs during the fix? Is the bug fix in the same code that caused the bug? Is the change fixing the cause or just covering a symptom? In this paper, we investigate alternative fixes to bugs and present an empirical study of how engineers make design choices about how to fix bugs. We start with a motivating case study of the Pex4Fun environment. Then, based on qualitative interviews with 40 engineers working on a variety of products, data from six bug triage meetings, and a survey filled out by 326 Microsoft engineers and 37 developers from other companies, we found a number of factors, many of them non-technical, that influence how bugs are fixed, such as how close to release the software is. We also discuss implications for research and practice, including how to make bug prediction and localization more accurate.",0098-5589;00985589,,10.1109/TSE.2014.2357438,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901259,Design concepts;human factors in software design;maintainability,Buildings;Computer bugs;Encoding;Interviews;Navigation;Protocols;Software,program debugging;software engineering,Microsoft engineers;Pex4Fun environment;bug fix design space;bug localization;bug prediction;bug triage meetings;design choices;qualitative interviews;software engineers,,6,,36,,,20140917,Jan. 1 2015,,IEEE,IEEE Journals & Magazines,,16
Integer Parameter Synthesis for Real-Time Systems,A. Jovanovi—_; D. Lime; O. H. Roux,"Ecole Centrale de Nantes - IRCCyN UMR CNRS 6597, Nantes, France",IEEE Transactions on Software Engineering,20150512,2015,41,5,445,461,"We provide a subclass of parametric timed automata (PTA) that we can actually and efficiently analyze, and we argue that it retains most of the practical usefulness of PTA for the modeling of real-time systems. The currently most useful known subclass of PTA, L/U automata, has a strong syntactical restriction for practical purposes, and we show that the associated theoretical results are mixed. We therefore advocate for a different restriction scheme: since in classical timed automata, real-valued clocks are always compared to integers for all practical purposes, we also search for parameter values as bounded integers. We show that the problem of the existence of parameter values such that some TCTL property is satisfied is PSPACE-complete. In such a setting, we can of course synthesize all the values of parameters and we give symbolic algorithms, for reachability and unavoidability properties, to do it efficiently, i.e., without an explicit enumeration. This also has the practical advantage of giving the result as symbolic constraints between the parameters. We finally report on a few experimental results to illustrate the practical usefulness of our approach.",0098-5589;00985589,,10.1109/TSE.2014.2357445,10.13039/501100001665 - ANR; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895298,Timed automata;model-checking;parameters;real-time systems;symbolic algorithms;synthesis,Automata;Clocks;Cost accounting;Delays;Radiation detectors;Real-time systems;Upper bound,automata theory;reachability analysis;real-time systems,PSPACE-complete;PTA;TCTL property;integer parameter synthesis;parametric timed automata;reachability;real-time systems;real-valued clocks;symbolic algorithms;symbolic constraints,,15,,36,,,20140911,May 1 2015,,IEEE,IEEE Journals & Magazines,,16
An Ontology-Based Product Architecture Derivation Approach,H. A. Duran-Limon; C. A. Garcia-Rios; F. E. Castillo-Barrera; R. Capilla,"Department of Information Systems, University of Guadalajara, CUCEA, Mexico",IEEE Transactions on Software Engineering,20151209,2015,41,12,1153,1168,"Software product line (SPL) engineering has proven to improve software quality and shorten development cycles, cost and time. In product line engineering, product derivation is concerned with the realization of the variability at the implementation level. However, the majority of research works focuses on instantiating the variants selected in the final product, while the derivation at the architecture level has been poorly explored. As product line engineers often customize the product architecture by hand during the application engineering phase, the derivation and customization processes of the product line architecture (PLA) might be in some cases error-prone. Consequently, in this research we present an Ontology-based product Architecture Derivation (OntoAD) framework which automates the derivation of product-specific architectures from an SPL architecture. Our solution uses a language-independent model to specify the product line architecture and a model-driven engineering approach for architecture derivation activities. We use an ontology formalism to reason about the automatic generation of model-to-model transformation rules based on the selection of features and we illustrate our approach using a voice over IP motivating example. Finally, we report results about scalability and performance regarding the size of the variability model.",0098-5589;00985589,,10.1109/TSE.2015.2449854,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7134799,Architecture Derivation;Feature Models;Model-Driven Engineering;Ontologies;Product Derivation;Scalability;Software Architecture;Software Product Lines;Software product lines;architecture derivation;feature models;model-driven engineering;ontologies;product derivation;scalability;software architecture,Computer architecture;Ontologies;Scalability;Software architecture;Software product lines;Unified modeling language,ontologies (artificial intelligence);software architecture;software product lines;software quality,OntoAD;PLA;SPL architecture;SPL engineering;architecture derivation activities;features selection;language-independent model;model-driven engineering;model-to-model transformation rules;ontology-based product architecture derivation approach;product customization;product derivation;product line architecture;product-specific architectures;software development cost;software development cycles;software development time;software product line engineering;software quality;voice over IP,,1,,53,,,20150625,Dec. 1 2015,,IEEE,IEEE Journals & Magazines,,15
Guided Mutation Testing for JavaScript Web Applications,S. Mirshokraie; A. Mesbah; K. Pattabiraman,"Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada",IEEE Transactions on Software Engineering,20150512,2015,41,5,429,444,"Mutation testing is an effective test adequacy assessment technique. However, there is a high computational cost in executing the test suite against a potentially large pool of generated mutants. Moreover, there is much effort involved in filtering out equivalent mutants. Prior work has mainly focused on detecting equivalent mutants after the mutation generation phase, which is computationally expensive and has limited efficiency. We propose an algorithm to select variables and branches for mutation as well as a metric, called FunctionRank, to rank functions according to their relative importance from the application's behaviour point of view. We present a technique that leverages static and dynamic analysis to guide the mutation generation process towards parts of the code that are more likely to influence the program's output. Further, we focus on the JavaScript language, and propose a set of mutation operators that are specific to Web applications. We implement our approach in a tool called MUTANDIS. The results of our empirical evaluation show that (1) more than 93 percent of generated mutants are non-equivalent, and (2) more than 75 percent of the surviving non-equivalent mutants are in the top 30 percent of the ranked functions.",0098-5589;00985589,,10.1109/TSE.2014.2371458,10.13039/501100000038 - NSERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960094,JavaScript;Mutation testing;equivalent mutants;guided mutation generation;mutation testing;web applications,Complexity theory;Heuristic algorithms;IEEE Computer Society;Instruments;Measurement;Performance analysis;Testing,Java;program diagnostics;program testing,FunctionRank;JavaScript Web applications;MUTANDIS tool;Web applications;application behaviour;computational cost;dynamic analysis;empirical evaluation;equivalent mutants;function ranking;guided mutation testing;mutation generation phase;mutation operators;nonequivalent mutants;program output;relative function importance;static analysis;test adequacy assessment technique;test suite execution;variable selection,,2,,50,,,20141120,May 1 2015,,IEEE,IEEE Journals & Magazines,,15
A Systematic Study on Explicit-State Non-Zenoness Checking for Timed Automata,T. Wang; J. Sun; X. Wang; Y. Liu; Y. Si; J. S. Dong; X. Yang; X. Li,"College of Computer Science, Zhejiang University, P.R., China",IEEE Transactions on Software Engineering,20150107,2015,41,1,3,18,"Zeno runs, where infinitely many actions occur within finite time, may arise in Timed Automata models. Zeno runs are not feasible in reality and must be pruned during system verification. Thus it is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples during model checking. Existing approaches on non-Zenoness checking include either introducing an additional clock in the Timed Automata models or additional accepting states in the zone graphs. In addition, there are approaches proposed for alternative timed modeling languages, which could be generalized to Timed Automata. In this work, we investigate the problem of non-Zenoness checking in the context of model checking LTL properties, not only evaluating and comparing existing approaches but also proposing a new method. To have a systematic evaluation, we develop a software toolkit to support multiple non-Zenoness checking algorithms. The experimental results show the effectiveness of our newly proposed algorithm, and demonstrate the strengths and weaknesses of different approaches.",0098-5589;00985589,,10.1109/TSE.2014.2359893,National Key Technology Support Program; 10.13039/501100001809 - National Natural Science Foundation Program; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6908008,Timed automata;model checking;non-Zenoness;verification tool,Automata;Clocks;Cost accounting;Educational institutions;Model checking;Safety;Systematics,automata theory;formal verification;graph theory;real-time systems,clocks;explicit-state nonzenoness checking;model checking LTL properties;software toolkit;system verification;systematic evaluation;timed automata models;timed modeling languages;zeno runs;zone graphs,,1,,37,,,20140923,Jan. 1 2015,,IEEE,IEEE Journals & Magazines,,15
Exploring the Relationship between Software Process Adaptive Capability and Organisational Performance,P. Clarke; R. V. O_ÑéConnor; B. Leavy; M. Yilmaz,"School of Computing, Dublin City University, Ireland, and Lero&#8212;The Irish Software Research Centre",IEEE Transactions on Software Engineering,20151208,2015,41,12,1169,1183,"Software development is a complex socio-technical activity, with the result that software development organisations need to establish and maintain robust software development processes. While much debate exists regarding the effectiveness of various software development approaches, no single approach is perfectly suited to all settings and no setting is unchanging. The capability to adapt the software process is therefore essential to sustaining an optimal software process. We designed an exploratory study to concurrently examine software process adaptive capability and organisational performance in 15 software development organisations, finding that companies with greater software process adaptive capability are shown to also experience greater business success. While our exploratory study of the complex relationship between these phenomena is limited in some respects, the findings indicate that software process adaptive capability may be worthy of further integration into software process engineering techniques. Software process adaptive capability may be an important organisational strength when deriving competitive advantage, and those responsible for the creation and evolution of software process models and methodologies may want to focus some of their future efforts in this area.",0098-5589;00985589,,10.1109/TSE.2015.2467388,Irish Software Engineering Research Centre; 10.13039/501100001602 - Science Foundation Ireland; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7214314,Software development;Software engineering;Software engineering process;Software management;software development;software engineering process;software management,IEC Standards;ISO Standards;Software development;Software engineering;Software management,software process improvement,optimal software process;organisational performance;organisational strength;robust software development process;socio-technical activity;software development approach;software development organisation;software process adaptive capability;software process engineering technique,,14,,97,,,20150820,Dec. 1 2015,,IEEE,IEEE Journals & Magazines,,14
Round-Up: Runtime Verification of Quasi Linearizability for Concurrent Data Structures,L. Zhang; A. Chattopadhyay; C. Wang,"Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University (Virginia Tech), Blacksburg, VA",IEEE Transactions on Software Engineering,20151208,2015,41,12,1202,1216,"We propose a new method for runtime checking of a relaxed consistency property called quasi linearizability for concurrent data structures.Quasi linearizability generalizes the standard notion of linearizability by introducing nondeterminism into the parallel computations quantitatively and then exploiting such nondeterminism to improve the runtime performance. However, ensuring the quantitative aspects of this correctness condition in the low-level code of the concurrent data structure implementation is a difficult task.Our runtime verification method is the first fully automated method for checking quasi linearizability in the C/C++ code of concurrent data structures. It guarantees that all the reported quasi linearizability violations manifested by the concurrent executions are real violations. We have implemented our method in a software tool based on the LLVM compiler and a systematic concurrency testing tool called Inspect. Our experimental evaluation shows that the new method is effective in detecting quasi linearizability violations in the source code implementations of concurrent data structures.",0098-5589;00985589,,10.1109/TSE.2015.2467371,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192659,Runtime verification;atomicity;linearizability;partial order reduction;relaxed consistency;serializability;systematic concurrency testing,Concurrent computing;Data structures;History;Legal aspects;Runtime,C++ language;data structures;program compilers;software tools,C/C++ code;Inspect;LLVM compiler;concurrent data structures;concurrent executions;correctness condition;fully automated method;low-level code;nondeterminism;quasi linearizability violations;relaxed consistency property;runtime checking;runtime verification method;software tool;source code;systematic concurrency testing tool,,0,,48,,,20150812,Dec. 1 2015,,IEEE,IEEE Journals & Magazines,,14
On the Composability of Design Patterns,H. Zhu; I. Bayley,"Oxford Brookes University, Oxford, United Kingdom",IEEE Transactions on Software Engineering,20151110,2015,41,11,1138,1152,"In real applications, design patterns are almost always to be found composed with each other. It is crucial that these compositions be validated. This paper examines the notion of validity, and develops a formal method for proving or disproving it, in a context where composition is performed with formally defined operators on formally specified patterns. In particular, for validity, we require that pattern compositions preserve the features, semantics and soundness of the composed patterns. The application of the theory is demonstrated by a formal analysis of overlap-based pattern compositions and a case study of a real pattern-oriented software design.",0098-5589;00985589,,10.1109/TSE.2015.2445341,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123660,Composibility;Design Patterns;Design patterns;Feature preservation;Formal methods;Pattern composition;Semantics preservation;Soundness preservation;composibility;feature preservation;formal methods;pattern composition;semantics preservation;soundness preservation,Cognition;Context;Semantics;Software design;Software systems;Unified modeling language,formal specification;object-oriented methods,design pattern composability;formal analysis method;formally specified patterns;overlap-based pattern compositions;pattern-oriented software design,,4,,70,,,20150615,Nov. 1 2015,,IEEE,IEEE Journals & Magazines,,14
"First, Debug the Test Oracle",X. Guo; M. Zhou; X. Song; M. Gu; J. Sun,"School of Software, Tsinghua University, Beijing, China",IEEE Transactions on Software Engineering,20151013,2015,41,10,986,1000,"Opposing to the oracle assumption, a trustworthy test oracle is not always available in real practice. Since manually written oracles and human judgements are still widely used, testers and programmers are in fact facing a high risk of erroneous test oracles. However, test oracle errors can bring much confusion thus causing extra time consumption in the debugging process. As substantiated by our experiment on the Siemens Test Suite, automatic fault localization algorithms suffer severely from erroneous test oracles, which impede them from reducing debugging time to the full extent. This paper proposes a simple but effective approach to debug the test oracle. Based on the observation that test cases covering similar lines of code usually generate similar results, we are able to identify suspicious test cases that are differently judged by the test oracle from their neighbors. To validate the effectiveness of our approach, experiments are conducted on both the Siemens Test Suite and grep. The results show that averagely over 75 percent of the highlighted test cases are actually test oracle errors. Moreover, performance of fault localization algorithms recovered remarkably with the debugged oracles.",0098-5589;00985589,,10.1109/TSE.2015.2425392,National Key Technologies R&D Program; 10.13039/501100001809 - NSFC Program; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100002858 - Postdoctoral Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091939,Test oracle;debugging;spectrum-based fault localization;test oracle,Accuracy;Algorithm design and analysis;Debugging;Error analysis;Manuals;Measurement;Software,fault diagnosis;program debugging;program testing,Siemens Test Suite;automatic fault localization algorithm;oracle error testing;test oracle debugging,,0,,34,,,20150422,Oct. 1 2015,,IEEE,IEEE Journals & Magazines,,14
An I/O Efficient Approach for Detecting All Accepting Cycles,L. Wu; K. Su; S. Cai; X. Zhang; C. Zhang; S. Wang,"School of Computer Science and Engineering, University of Electronic Science and Technology, Chengdu, China",IEEE Transactions on Software Engineering,20150811,2015,41,8,730,744,"Existing algorithms for I/O Linear Temporal Logic (LTL) model checking usually output a single counterexample for a system which violates the property. However, in real-world applications, such as diagnosis and debugging in software and hardware system designs, people often need to have a set of counterexamples or even all counterexamples. For this purpose, we propose an I/O efficient approach for detecting all accepting cycles, called Detecting All Accepting Cycles (DAAC), where the properties to be verified are in LTL. Different from other algorithms for finding all cycles, DAAC first searches for the accepting strongly connected components (ASCCs), and then finds all accepting cycles of every ASCC, which can avoid searching for a great many paths that are impossible to be extended to accepting cycles. In order to further lower DAAC's I/O complexity and improve its performance, we propose an intersection computation technique and a dynamic path management technique, and exploit a minimal perfect hash function (MPHF). We carry out both complexity and experimental comparisons with the state-of-the-art algorithms including Detect Accepting Cycle (DAC), Maximal Accepting Predecessors (MAP) and Iterative-Deepening Depth-First Search (IDDFS). The comparative results show that our approach is better on the whole in terms of I/O complexity and practical performance, despite the fact that it finds all counterexamples.",0098-5589;00985589,,10.1109/TSE.2015.2411284,China National; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056483,Breath-First Search;Detection of All Accepting Cycles;Model Checking;Model checking;accepting strongly connected component;breath-first search;detection of all accepting cycles;state space explosion,Algorithm design and analysis;Complexity theory;Educational institutions;Heuristic algorithms;Model checking;Software;Software algorithms,formal verification;temporal logic,ASCC;DAAC approach;DAC algorithm;I/O complexity;I/O linear temporal logic;IDDFS algorithm;LTL model checking;MPHF;accepting strongly connected components;detect accepting cycle algorithm;detecting all accepting cycles approach;dynamic path management technique;input-output efficient approach;intersection computation technique;iterative-deepening depth-first search algorithm;maximal accepting predecessors algorithm;minimal perfect hash function,,0,,36,,,20150309,Aug. 1 2015,,IEEE,IEEE Journals & Magazines,,14
Generating Complete Controllable Test Suites for Distributed Testing,R. M. Hierons,"Department of Computer Science, Brunel University, United Kingdom",IEEE Transactions on Software Engineering,20150311,2015,41,3,279,293,"A test suite is m-complete for finite state machine (FSM) M if it distinguishes between M and all faulty FSMs with m states or fewer. While there are several algorithms that generate m-complete test suites, they cannot be directly used in distributed testing since there can be additional controllability and observability problems. Indeed, previous results show that there is no general method for generating an m-complete test suite for distributed testing and so the focus has been on conditions under which this is possible. This paper takes a different approach, which is to generate what we call cm-complete test suites: controllable test suites that distinguish an FSM N with no more than m states from M if this is possible in controllable testing. Thus, under the hypothesis that the system under test has no more than m states, a cm-complete test suite achieves as much as is possible given the restriction that testing should be controllable. We show how the problem of generating a cm-complete test suite can be mapped to the problem of generating an m-complete test suite for a partial FSM. Thus, standard test suite generation methods can be adapted for use in distributed testing.",0098-5589;00985589,,10.1109/TSE.2014.2364035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930767,Software engineering/software/program verification;checking experiment;distributed testing;software engineering/testing and debugging;systems and software;test suite generation,Automata;Computer architecture;Controllability;Observability;Ports (Computers);Protocols;Testing,distributed processing;finite state machines;program debugging;program testing,c<sub>m</sub>-complete test suites;complete controllable test suite generation;controllability problem;controllable testing;distributed testing;faulty FSM;finite state machine;m-complete test suite;observability problem;partial-FSM;standard test suite generation methods;system under test,,6,,49,,,20141020,March 1 2015,,IEEE,IEEE Journals & Magazines,,14
Estimating Computational Requirements in Multi-Threaded Applications,J. F. PÕ©rez; G. Casale; S. Pacheco-Sanchez,"Department of Computing, Imperial College London, United Kingdom",IEEE Transactions on Software Engineering,20150311,2015,41,3,264,278,"Performance models provide effective support for managing quality-of-service (QoS) and costs of enterprise applications. However, expensive high-resolution monitoring would be needed to obtain key model parameters, such as the CPU consumption of individual requests, which are thus more commonly estimated from other measures. However, current estimators are often inaccurate in accounting for scheduling in multi-threaded application servers. To cope with this problem, we propose novel linear regression and maximum likelihood estimators. Our algorithms take as inputs response time and resource queue measurements and return estimates of CPU consumption for individual request types. Results on simulated and real application datasets indicate that our algorithms provide accurate estimates and can scale effectively with the threading levels.",0098-5589;00985589,,10.1109/TSE.2014.2363472,InvestNI/SAP VIRTEX; 10.13039/501100004963 - Seventh Framework Programme; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926798,Application performance management;Demand estimation;Multi-threaded application servers;application performance management;multi-threaded application servers,Computational modeling;Instruction sets;Maximum likelihood estimation;Servers;Time factors;Time measurement,maximum likelihood estimation;multi-threading;quality of service;queueing theory;regression analysis;software performance evaluation;systems analysis,CPU consumption;QoS management;computational requirement estimation;cost management;enterprise applications;expensive high-resolution monitoring;input response time;linear regression;maximum likelihood estimators;multithreaded application server scheduling;performance models;quality-of-service;resource queue measurements,,9,,41,,,20141016,March 1 2015,,IEEE,IEEE Journals & Magazines,,14
Identifying Renaming Opportunities by Expanding Conducted Rename Refactorings,H. Liu; Q. Liu; Y. Liu; Z. Wang,"School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China",IEEE Transactions on Software Engineering,20150915,2015,41,9,887,900,"To facilitate software refactoring, a number of approaches and tools have been proposed to suggest where refactorings should be conducted. However, identification of such refactoring opportunities is usually difficult because it often involves difficult semantic analysis and it is often influenced by many factors besides source code. For example, whether a software entity should be renamed depends on the meaning of its original name (natural language understanding), the semantics of the entity (source code semantics), experience and preference of developers, and culture of companies. As a result, it is difficult to identify renaming opportunities. To this end, in this paper we propose an approach to identify renaming opportunities by expanding conducted renamings. Once a rename refactoring is conducted manually or with tool support, the proposed approach recommends to rename closely related software entities whose names are similar to that of the renamed entity. The rationale is that if an engineer makes a mistake in naming a software entity it is likely for her to make the same mistake in naming similar and closely related software entities. The main advantage of the proposed approach is that it does not involve difficult semantic analysis of source code or complex natural language understanding. Another advantage of this approach is that it is less influenced by subjective factors, e.g., experience and preference of software engineers. The proposed approach has been evaluated on four open-source applications. Our evaluation results show that the proposed approach is accurate in recommending entities to be renamed (average precision 82 percent) and in recommending new names for such entities (average precision 93 percent). Evaluation results also suggest that a substantial percentage (varying from 20 to 23 percent) of rename refactorings are expansible.",0098-5589;00985589,,10.1109/TSE.2015.2427831,Beijing Higher Education Young Elite Teacher Project; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004602 - Program for New Century Excellent Talents in University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7097720,Code Smells;Identification;Refactoring Opportunity;Rename;Software Refactoring;Software refactoring;code smells;identification;refactoring opportunity;rename,Context;Engines;IEEE Potentials;Natural languages;Open source software;Semantics,software maintenance,conducted rename refactorings;renaming opportunities identification;semantic analysis;software entity naming;software refactoring,,1,,36,,,20150429,Sept. 1 2015,,IEEE,IEEE Journals & Magazines,,13
Connecting and Serving the Software Engineering Community,M. B. Dwyer; E. Bodden; B. Fitzgerald; M. Kim; S. Kim; A. J. Ko; E. Mendes; R. Mirandola; A. Moreira; F. Shull; S. Siegel; T. Xie; C. Zhang,,IEEE Transactions on Software Engineering,20160311,2016,42,3,203,280,Presents an editorial discusses the current status and activities supported by this publication.,0098-5589;00985589,,10.1109/TSE.2016.2532379,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7432058,,,,,,,,,,,,March 1 2016,,IEEE,IEEE Journals & Magazines,,77
Evaluating the Effects of Architectural Documentation: A Case Study of a Large Scale Open Source Project,R. Kazman; D. Goldenson; I. Monarch; W. Nichols; G. Valetto,"Software Engineering Institute, Pittsburgh, PA",IEEE Transactions on Software Engineering,20160311,2016,42,3,220,260,"Sustaining large open source development efforts requires recruiting new participants; however, a lack of architectural documentation might inhibit new participants since large amounts of project knowledge are unavailable to newcomers. We present the results of a multitrait, multimethod analysis of the effects of introducing architectural documentation into a substantial open source project-the Hadoop Distributed File System (HDFS). HDFS had only minimal architectural documentation, and we wanted to discover whether the putative benefits of architectural documentation could be observed over time. To do this, we created and publicized an architecture document and then monitored its usage and effects on the project. The results were somewhat ambiguous: by some measures the architecture documentation appeared to effect the project but not by others. Perhaps of equal importance is our discovery that the project maintained, in its Web-accessible JIRA archive of software issues and fixes, enough architectural discussion to support architectural thinking and reasoning. This _ÑÒemergent_Ñù architecture documentation served an important purpose in recording core project members' architectural concerns and resolutions. However, this emergent architecture documentation did not serve all project members equally well; it appears that those on the periphery of the project-newcomers and adopters-still require explicit architecture documentation, as we will show.",0098-5589;00985589,,10.1109/TSE.2015.2465387,Software Engineering Institute; 10.13039/100000005 - Department of Defense; 10.13039/100007063 - Carnegie Mellon University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230299,"Software architecture;documentation;open source software;open source software,;software architecture,",Computer architecture;Documentation;Electronic mail;Measurement;Open source software;Social network services,distributed databases;public domain software;software architecture,HDFS;Hadoop distributed file system;Web-accessible JIRA archive;architectural documentation;architectural reasoning;architectural thinking;large-scale substantial open source project;multitrait-multimethod analysis;project knowledge;project members,,4,,77,,,20150831,March 1 2016,,IEEE,IEEE Journals & Magazines,,40
A Survey on Software Fault Localization,W. E. Wong; R. Gao; Y. Li; R. Abreu; F. Wotawa,"State Key Laboratory of Software Engineering, Wuhan University, Department of Computer Science, University of Texas at Dallas, Richardson, TX",IEEE Transactions on Software Engineering,20160811,2016,42,8,707,740,"Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive - yet equally critical - activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole.",0098-5589;00985589,,10.1109/TSE.2016.2521368,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390282,Software fault localization;execution trace;program debugging;software testing;survey;suspicious code,Complexity theory;Computer bugs;Debugging;Fault diagnosis;Software debugging;Software engineering,program debugging;software reliability,human intervention;program debugging;program fault locations;software developers;software fault localization,,28,,,,,20160125,Aug. 1 2016,,IEEE,IEEE Journals & Magazines,,33
Engineering Adaptive Model-Driven User Interfaces,P. A. Akiki; A. K. Bandara; Y. Yu,"Department of Computer Science, Notre Dame University&#x2014;Louaize, Zouk Mosbeh, Lebanon",IEEE Transactions on Software Engineering,20161208,2016,42,12,1118,1147,"Software applications that are very large-scale, can encompass hundreds of complex user interfaces (UIs). Such applications are commonly sold as feature-bloated off-the-shelf products to be used by people with variable needs in the required features and layout preferences. Although many UI adaptation approaches were proposed, several gaps and limitations including: extensibility and integration in legacy systems, still need to be addressed in the state-of-the-art adaptive UI development systems. This paper presents Role-Based UI Simplification (RBUIS) as a mechanism for increasing usability through adaptive behavior by providing end-users with a minimal feature-set and an optimal layout, based on the context-of-use. RBUIS uses an interpreted runtime model-driven approach based on the Cedar Architecture, and is supported by the integrated development environment (IDE), Cedar Studio. RBUIS was evaluated by integrating it into OFBiz, an open-source ERP system. The integration method was assessed and measured by establishing and applying technical metrics. Afterwards, a usability study was carried out to evaluate whether UIs simplified with RBUIS show an improvement over their initial counterparts. This study leveraged questionnaires, checking task completion times and output quality, and eye-tracking. The results showed that UIs simplified with RBUIS significantly improve end-user efficiency, effectiveness, and perceived usability.",0098-5589;00985589,,10.1109/TSE.2016.2553035,Computing and Communications Department; The Open University; 10.13039/501100000781 - ERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451279,Design tools and techniques;software architectures;support for adaptation;user interfaces,Adaptation models;Adaptive systems;Computer architecture;Usability;User interfaces,ergonomics;programming environments;software architecture;software maintenance;user interfaces,Cedar architecture;IDE;RBUIS;adaptive behavior;engineering adaptive model-driven user interface;integrated development environment;legacy system extensibility;legacy system integration;role-based UI simplification;system usability,,4,,,,,20160412,Dec. 1 2016,,IEEE,IEEE Journals & Magazines,,29
Dynamic Software Project Scheduling through a Proactive-Rescheduling Method,X. Shen; L. L. Minku; R. Bahsoon; X. Yao,"B-DAT & CICAEET, School of Information and Control, Nanjing University of Information Science and Technology, No.219, Ning-Liu Road, Pu-Kou District, Nanjing, P.R. China",IEEE Transactions on Software Engineering,20160714,2016,42,7,658,686,"Software project scheduling in dynamic and uncertain environments is of significant importance to real-world software development. Yet most studies schedule software projects by considering static and deterministic scenarios only, which may cause performance deterioration or even infeasibility when facing disruptions. In order to capture more dynamic features of software project scheduling than the previous work, this paper formulates the project scheduling problem by considering uncertainties and dynamic events that often occur during software project development, and constructs a mathematical model for the resulting multi-objective dynamic project scheduling problem (MODPSP), where the four objectives of project cost, duration, robustness and stability are considered simultaneously under a variety of practical constraints. In order to solve MODPSP appropriately, a multi-objective evolutionary algorithm based proactive-rescheduling method is proposed, which generates a robust schedule predictively and adapts the previous schedule in response to critical dynamic events during the project execution. Extensive experimental results on 21 problem instances, including three instances derived from real-world software projects, show that our novel method is very effective. By introducing the robustness and stability objectives, and incorporating the dynamic optimization strategies specifically designed for MODPSP, our proactive-rescheduling method achieves a very good overall performance in a dynamic environment.",0098-5589;00985589,,10.1109/TSE.2015.2512266,"CERCIA; DAASE: Dynamic Adaptive Automated Software Engineering; Evolutionary Computation for Dynamic Optimization in Network Environments; School of Computer Science, University of Birmingham, United Kingdom; 10.13039/501100000266 - EPSRC; 10.13039/501100001809 - National Natural Science Foundation of China (NSFC); 10.13039/501100004608 - Natural Science Foundation of Jiangsu Province of China; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365465,Schedule and organizational issues;dynamic software project scheduling;mathematical modeling;multi-objective evolutionary algorithms;search-based software engineering,Dynamic scheduling;Job shop scheduling;Robustness;Schedules;Software;Uncertainty,evolutionary computation;project management;scheduling;software development management,MODPSP;critical dynamic events;dynamic environments;dynamic features;dynamic optimization strategies;multiobjective dynamic software project scheduling problem;multiobjective evolutionary algorithm based proactive-rescheduling method;project cost;project duration;project execution;project robustness;project stability;software project development;uncertain environments,,2,,49,,,20151224,July 1 2016,,IEEE,IEEE Journals & Magazines,,28
"Supporting Scope Tracking and Visualization for Very Large-Scale Requirements Engineering-Utilizing FSC+, Decision Patterns, and Atomic Decision Visualizations",K. Wnuk; T. Gorschek; D. Callele; E. A. Karlsson; E. Õ€hlin; B. Regnell,"Software Engineering Research Lab (SERL), Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden",IEEE Transactions on Software Engineering,20160107,2016,42,1,47,74,"Deciding the optimal project scope that fulfills the needs of the most important stakeholders is challenging due to a plethora of aspects that may impact decisions. Large companies that operate in rapidly changing environments experience frequently changing customer needs which force decision makers to continuously adjust the scope of their projects. Change intensity is further fueled by fierce market competition and hard time-to-market deadlines. Staying in control of the changes in thousands of features becomes a major issue as information overload hinders decision makers from rapidly extracting relevant information. This paper presents a visual technique, called Feature Survival Charts+ (FSC+), designed to give a quick and effective overview of the requirements scoping process for Very Large-Scale Requirements Engineering (VLSRE). FSC+ were applied at a large company with thousands of features in the database and supported the transition from plan-driven to a more dynamic and change-tolerant release scope management process. FSC+ provides multiple views, filtering, zooming, state-change intensity views, and support for variable time spans. Moreover, this paper introduces five decision archetypes deduced from the dataset and subsequently analyzed and the atomic decision visualization that shows the frequency of various decisions in the process. The capabilities and usefulness of FSC+, decision patterns (state changes that features undergo) and atomic decision visualizations are evaluated through interviews with practitioners who found utility in all techniques and indicated that their inherent flexibility was necessary to meet the varying needs of the stakeholders.",0098-5589;00985589,,10.1109/TSE.2015.2445347,IKNOWDM project; Knowledge Foundation in Sweden; SCALARE ITEA2 project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123669,- D.2.1Requirements/Specifications D.2.9.d Initiation and scope definition D.2.9 Management;Requirements/specifications;initiation and scope definition;management,Companies;Electronic mail;Planning;Power capacitors;Software;Software engineering;Visualization,data visualisation;formal specification,FSC+;Feature Survival Charts+;VLSRE;atomic decision visualizations;change-tolerant release scope management process;decision patterns;requirements scoping process;scope tracking;very large-scale requirements engineering;very large-scale requirements engineering visualization,,2,,108,,,20150615,Jan. 1 2016,,IEEE,IEEE Journals & Magazines,,27
Enforcing Exception Handling Policies with a Domain-Specific Language,E. A. Barbosa; A. Garcia; M. P. Robillard; B. Jakobus,"OPUS Research Group, Informatics Department, Pontifical Catholic University of Rio de Janeiro, Rua Marqu&#234;s de S&#227;o Vicente, 255-G&#225;vea, Rio de Janeiro, Brazil",IEEE Transactions on Software Engineering,20160610,2016,42,6,559,584,"Current software projects deal with exceptions in implementation and maintenance phases without a clear definition of exception handling policies. We call an exception handling policy the set of design decisions that govern the use of exceptions in a software project. Without an explicit exception handling policy, developers can remain unaware of the originally intended use of exceptions. In this paper, we present Exception Handling Policies Language (EPL), a domain-specific language to specify and verify exception handling policies. The evaluation of EPL was based on a user-centric observational study and case studies. The user-centric study was performed to observe how potential users of the language actually use it. With this study, we could better understand the trade-offs related to different language design decisions based on concrete and well-documented observations and experiences reported by participants. We identified some language characteristics that hindered its use and that motivated new language constructs. In addition, we performed case studies with one open-source project and two industry-strength systems to investigate how specifying and verifying exception handling policies may assist in detecting exception handling problems. The results show that violations of exception handling policies help to indicate potential faults in the exception handling code.",0098-5589;00985589,,10.1109/TSE.2015.2506164,10.13039/501100004586 - Fundaëèëèo Carlos Chagas Filho de Amparo ëè Pesquisa do Estado do Rio de Janeiro (FAPERJ); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348692,Domain-specific language;Exception handling;Exception handling policy;Policy specification;domain-specific language;exception handling policy;policy specification,Java;Robustness;Software reliability;Software systems,exception handling;formal specification;formal verification;programming languages;project management;public domain software;software maintenance,EPL evaluation;domain-specific language;exception handling policies language;exception handling policy specification;exception handling policy verification;implementation phase;industry-strength systems;language design decisions;maintenance phase;open-source project;software projects;user-centric study,,2,,46,,,20151207,June 1 2016,,IEEE,IEEE Journals & Magazines,,25
"A Two-Component Language for Adaptation: Design, Semantics and Program Analysis",P. Degano; G. L. Ferrari; L. Galletta,"Dipartimento di Informatica, Universit&#224; di Pisa, Pisa, Italia",IEEE Transactions on Software Engineering,20160610,2016,42,6,505,529,"Adaptive systems are designed to modify their behaviour in response to changes of their operational environment. We propose a two-component language for adaptive programming, within the Context-Oriented Programming paradigm. It has a declarative constituent for programming the context and a functional one for computing. We equip our language with a dynamic formal semantics. Since wrong adaptation could severely compromise the correct behaviour of applications and violate their properties, we also introduce a two-phase verification mechanism. It is based on a type and effect system that type-checks programs and computes, as an effect, a sound approximation of their behaviour. The effect is exploited at load time to mechanically verify that programs correctly adapt themselves to all possible running environments.",0098-5589;00985589,,10.1109/TSE.2015.2496941,MIUR Prin Project; Universitëè di Pisa PRA project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314969,Adaptive Software;Adaptive software;Context Oriented Programming;Datalog;Formal Methods;Functional Programming;Semantics;Type Systems;Verification;context oriented programming;datalog;formal methods;functional programming;semantics;type systems;verification,Adaptation models;Computer languages;Context;Programming;Semantics;Software;Standards,high level languages;program diagnostics;program verification,adaptive programming;context-oriented programming paradigm;dynamic formal semantics;program analysis;program type-check;two-component language;two-phase verification mechanism,,3,,95,,,20151102,June 1 2016,,IEEE,IEEE Journals & Magazines,,24
Effect of Domain Knowledge on Elicitation Effectiveness: An Internally Replicated Controlled Experiment,A. M. Aranda; O. Dieste; N. Juristo,"Escuela T&#233;cnica Superior de Ingenieros Inform&#225;ticos, Universidad Polit&#233;cnica de Madrid, Campus de Montegancedo, Boadilla del Monte, Spain",IEEE Transactions on Software Engineering,20160512,2016,42,5,427,451,"Context. Requirements elicitation is a highly communicative activity in which human interactions play a critical role. A number of analyst characteristics or skills may influence elicitation process effectiveness. Aim. Study the influence of analyst problem domain knowledge on elicitation effectiveness. Method. We executed a controlled experiment with post-graduate students. The experimental task was to elicit requirements using open interview and consolidate the elicited information immediately afterwards. We used four different problem domains about which students had different levels of knowledge. Two tasks were used in the experiment, whereas the other two were used in an internal replication of the experiment; that is, we repeated the experiment with the same subjects but with different domains. Results. Analyst problem domain knowledge has a small but statistically significant effect on the effectiveness of the requirements elicitation activity. The interviewee has a big positive and significant influence, as does general training in requirements activities and interview experience. Conclusion. During early contacts with the customer, a key factor is the interviewee; however, training in tasks related to requirements elicitation and knowledge of the problem domain helps requirements analysts to be more effective.",0098-5589;00985589,,10.1109/TSE.2015.2494588,Spanish Ministry of Ministry of Economy and Competitiveness; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307191,Controlled experiment;domain knowledge;internal replication;requirements elicitation,Computer science;Interviews;Knowledge engineering;Requirements engineering;Software engineering;Training,software engineering,elicitation effectiveness;internally replicated controlled experiment;problem domain knowledge;requirements elicitation,,1,,56,,,20151026,May 1 2016,,IEEE,IEEE Journals & Magazines,,24
Supporting Self-Adaptation via Quantitative Verification and Sensitivity Analysis at Run Time,A. Filieri; G. Tamburrelli; C. Ghezzi,"Reliable Software Systems Group, University of Stuttgart, Stuttgart, Germany",IEEE Transactions on Software Engineering,20160107,2016,42,1,75,99,"Modern software-intensive systems often interact with an environment whose behavior changes over time, often unpredictably. The occurrence of changes may jeopardize their ability to meet the desired requirements. It is therefore desirable to design software in a way that it can self-adapt to the occurrence of changes with limited, or even without, human intervention. Self-adaptation can be achieved by bringing software models and model checking to run time, to support perpetual automatic reasoning about changes. Once a change is detected, the system itself can predict if requirements violations may occur and enable appropriate counter-actions. However, existing mainstream model checking techniques and tools were not conceived for run-time usage; hence they hardly meet the constraints imposed by on-the-fly analysis in terms of execution time and memory usage. This paper addresses this issue and focuses on perpetual satisfaction of non-functional requirements, such as reliability or energy consumption. Its main contribution is the description of a mathematical framework for run-time efficient probabilistic model checking. Our approach statically generates a set of verification conditions that can be efficiently evaluated at run time as soon as changes occur. The proposed approach also supports sensitivity analysis, which enables reasoning about the effects of changes and can drive effective adaptation strategies.",0098-5589;00985589,,10.1109/TSE.2015.2421318,Programme FP7-PEOPLE-2011-IEF; Project 227977-SMScom; Project 302648-RunMore; 10.13039/501100000780 - European Commission; 10.13039/501100000781 - Programme IDEAS-ERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083754,Costs;Discrete-Time Markov models;Models at Runtime;Non-functional Requirements;Probabilistic Model Checking;Rewards;Self-adaptive Systems;Self-adaptive systems;Software Evolution;Software Reliability;costs;discrete-time Markov models;models at runtime;non-functional requirements;probabilistic model checking;rewards;software evolution;software reliability,Adaptation models;Computational modeling;Markov processes;Model checking;Probabilistic logic;Reliability;Software,probability;program verification;software reliability,execution time;mathematical framework;memory usage;nonfunctional requirements;on-the-fly analysis;perpetual automatic reasoning;perpetual satisfaction;probabilistic model checking;quantitative verification;requirements violation;run time analysis;self-adaptation;sensitivity analysis;software design;software models;software-intensive systems,,17,,91,,,20150409,Jan. 1 2016,,IEEE,IEEE Journals & Magazines,,24
RELAI Testing: A Technique to Assess and Improve Software Reliability,D. Cotroneo; R. Pietrantuono; S. Russo,"Dipartimento di Ingegneria Elettrica e delle Tecnologie dell&#8217;Informazione (DIETI), Universit&#224; di Napoli Federico II, Via Claudio 21, Naples, Italy",IEEE Transactions on Software Engineering,20160512,2016,42,5,452,475,"Testing software to assess or improve reliability presents several practical challenges. Conventional operational testing is a fundamental strategy that simulates the real usage of the system in order to expose failures with the highest occurrence probability. However, practitioners find it unsuitable for assessing/achieving very high reliability levels; also, they do not see the adoption of a _ÑÒreal_Ñù usage profile estimate as a sensible idea, being it a source of non-quantifiable uncertainty. Oppositely, debug testing aims to expose as many failures as possible, but regardless of their impact on runtime reliability. These strategies are used either to assess or to improve reliability, but cannot improve and assess reliability in the same testing session. This article proposes Reliability Assessment and Improvement (RELAI) testing, a new technique thought to improve the delivered reliability by an adaptive testing scheme, while providing, at the same time, a continuous assessment of reliability attained through testing and fault removal. The technique also quantifies the impact of a partial knowledge of the operational profile. RELAI is positively evaluated on four software applications compared, in separate experiments, with techniques conceived either for reliability improvement or for reliability assessment, demonstrating substantial improvements in both cases.",0098-5589;00985589,,10.1109/TSE.2015.2491931,COSMIC; FP7 Marie Curie Industry-Academia Partnerships and Pathways (IAPP); SVEVIA; 10.13039/501100000780 - European Commission; 10.13039/501100003407 - MIUR; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299696,Operational Profile;Operational Testing;Random Testing;Reliability;Sampling;Software Testing;Software testing;operational profile;operational testing;random testing;reliability;sampling,Estimation error;Software;Software reliability;Testing;Uncertainty,probability;program debugging;program testing;software reliability,RELAI testing;adaptive testing scheme;continuous reliability assessment;debug testing;fault removal;nonquantifiable uncertainty;operational testing;reliability assessment and improvement testing;runtime reliability;software applications;software failures;software reliability;software testing,,2,,65,,,20151016,May 1 2016,,IEEE,IEEE Journals & Magazines,,23
Bidirectional Symbolic Analysis for Effective Branch Testing,M. Baluda; G. Denaro; M. PezzÕŒ,"Secure Software Engineering Group, Fraunhofer SIT, Darmstadt, Germany",IEEE Transactions on Software Engineering,20160512,2016,42,5,403,426,"Structural coverage metrics, and in particular branch coverage, are popular approaches to measure the thoroughness of test suites. Unfortunately, the presence of elements that are not executable in the program under test and the difficulty of generating test cases for rare conditions impact on the effectiveness of the coverage obtained with current approaches. In this paper, we propose a new approach that combines symbolic execution and symbolic reachability analysis to improve the effectiveness of branch testing. Our approach embraces the ideal definition of branch coverage as the percentage of executable branches traversed with the test suite, and proposes a new bidirectional symbolic analysis for both testing rare execution conditions and eliminating infeasible branches from the set of test objectives. The approach is centered on a model of the analyzed execution space. The model identifies the frontier between symbolic execution and symbolic reachability analysis, to guide the alternation and the progress of bidirectional analysis towards the coverage targets. The experimental results presented in the paper indicate that the proposed approach can both find test inputs that exercise rare execution conditions that are not identified with state-of-the-art approaches and eliminate many infeasible branches from the coverage measurement. It can thus produce a modified branch coverage metric that indicates the amount of feasible branches covered during testing, and helps team leaders and developers in estimating the amount of not-yet-covered feasible branches. The approach proposed in this paper suffers less than the other approaches from particular cases that may trap the analysis in unbounded loops.",0098-5589;00985589,,10.1109/TSE.2015.2490067,AVATAR; Italian PRIN; 10.13039/501100004343 - SNF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296670,Structural testing;branch coverage;program analysis;symbolic execution;symbolic reachability analysis,Analytical models;Computational modeling;Concrete;Measurement;Reachability analysis;Testing;Valves,program diagnostics;program testing,bidirectional symbolic analysis;branch coverage;branch testing;coverage measurement;structural coverage metrics;symbolic execution;symbolic reachability analysis;test case generation;test objectives;test suite thoroughness measure,,1,,137,,,20151012,May 1 2016,,IEEE,IEEE Journals & Magazines,,23
"Mapping Bug Reports to Relevant Files: A Ranking Model, a Fine-Grained Benchmark, and Feature Evaluation",X. Ye; R. Bunescu; C. Liu,"School of Electrical Engineering and Computer Science, Ohio University, Athens, OH",IEEE Transactions on Software Engineering,20160414,2016,42,4,379,402,"When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and improve productivity. This paper introduces an adaptive ranking approach that leverages project knowledge through functional decomposition of source code, API descriptions of library components, the bug-fixing history, the code change history, and the file dependency graph. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluate the ranking system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the learning-to-rank approach outperforms three recent state-of-the-art methods. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70 percent of the bug reports in the Eclipse Platform and Tomcat projects.",0098-5589;00985589,,10.1109/TSE.2015.2479232,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7270328,Bug reports;learning to rank;software maintenance,Benchmark testing;Collaboration;Computational modeling;Computer bugs;History;Software;Standards,,,,1,,72,,,20150916,April 1 2016,,IEEE,IEEE Journals & Magazines,,23
A Tool-Supported Methodology for Validation and Refinement of Early-Stage Domain Models,M. Autili; A. Bertolino; G. De Angelis; D. D. Ruscio; A. D. Sandro,"Department of Information Engineering Computer Science and Mathematics University of L&#8217;Aquila, Italy",IEEE Transactions on Software Engineering,20160107,2016,42,1,2,25,"Model-driven engineering (MDE) promotes automated model transformations along the entire development process. Guaranteeing the quality of early models is essential for a successful application of MDE techniques and related tool-supported model refinements. Do these models properly reflect the requirements elicited from the owners of the problem domain? Ultimately, this question needs to be asked to the domain experts. The problem is that a gap exists between the respective backgrounds of modeling experts and domain experts. MDE developers cannot show a model to the domain experts and simply ask them whether it is correct with respect to the requirements they had in mind. To facilitate their interaction and make such validation more systematic, we propose a methodology and a tool that derive a set of customizable questionnaires expressed in natural language from each model to be validated. Unexpected answers by domain experts help to identify those portions of the models requiring deeper attention. We illustrate the methodology and the current status of the developed tool MOTHIA, which can handle UML Use Case, Class, and Activity diagrams. We assess MOTHIA effectiveness in reducing the gap between domain and modeling experts, and in detecting modeling faults on the European Project CHOReOS.",0098-5589;00985589,,10.1109/TSE.2015.2449319,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7132782,Domain Modeling;Domain modeling;Early Stage Model;Model Driven Engineering;Model Refinement;Model Validation;Natural Language Questionnaires;Semantic Model Quality;early stage model;model driven engineering;model refinement;model validation;natural language questionnaires;semantic model quality,Biological system modeling;Context;Context modeling;Engines;Load modeling;Semantics;Unified modeling language,Unified Modeling Language;natural language processing,MDE technique;MOTHIA;UML;activity diagram;automated model transformation;early-stage domain model;model-driven engineering;natural language;tool-supported model refinement,,1,,50,,,20150624,Jan. 1 2016,,IEEE,IEEE Journals & Magazines,,23
An Industrial Survey of Safety Evidence Change Impact Analysis Practice,J. L. de la Vara; M. Borg; K. Wnuk; L. Moonen,"Computer Science Department, Carlos III University of Madrid, Avda. de la Universidad 30, 28911 Leganes, Madrid, Spain",IEEE Transactions on Software Engineering,20161208,2016,42,12,1095,1117,"Context. In many application domains, critical systems must comply with safety standards. This involves gathering safety evidence in the form of artefacts such as safety analyses, system specifications, and testing results. These artefacts can evolve during a system's lifecycle, creating a need for change impact analysis to guarantee that system safety and compliance are not jeopardised. Objective. We aim to provide new insights into how safety evidence change impact analysis is addressed in practice. The knowledge about this activity is limited despite the extensive research that has been conducted on change impact analysis and on safety evidence management. Method. We conducted an industrial survey on the circumstances under which safety evidence change impact analysis is addressed, the tool support used, and the challenges faced. Results. We obtained 97 valid responses representing 16 application domains, 28 countries, and 47 safety standards. The respondents had most often performed safety evidence change impact analysis during system development, from system specifications, and fully manually. No commercial change impact analysis tool was reported as used for all artefact types and insufficient tool support was the most frequent challenge. Conclusion. The results suggest that the different artefact types used as safety evidence co-evolve. In addition, the evolution of safety cases should probably be better managed, the level of automation in safety evidence change impact analysis is low, and the state of the practice can benefit from over 20 improvement areas.",0098-5589;00985589,,10.1109/TSE.2016.2553032,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450627,Safety-critical system;change impact analysis;safety evidence;state of the practice;survey research,Best practices;Certification;Industries;Market research;Safety;Standards,safety-critical software;software standards,SECIA;industrial survey;safety evidence change impact analysis;safety standard;safety-critical system,,9,,,,,20160411,Dec. 1 2016,,IEEE,IEEE Journals & Magazines,,22
"Light-Weight, Inter-Procedural and Callback-Aware Resource Leak Detection for Android Apps",T. Wu; J. Liu; Z. Xu; C. Guo; Y. Zhang; J. Yan; J. Zhang,State Key Laboratory of Computer ScienceInstitute of SoftwareChinese Academy of Sciences,IEEE Transactions on Software Engineering,20161110,2016,42,11,1054,1076,"Android devices include many embedded resources such as Camera, Media Player and Sensors. These resources require programmers to explicitly request and release them. Missing release operations might cause serious problems such as performance degradation or system crash. This kind of defects is called resource leak. Despite a large body of existing works on testing and analyzing Android apps, there still remain several challenging problems. In this work, we present Relda2, a light-weight and precise static resource leak detection tool. We first systematically collected a resource table, which includes the resources that the Android reference requires developers release manually. Based on this table, we designed a general approach to automatically detect resource leaks. To make a more precise inter-procedural analysis, we construct a Function Call Graph for each Android application, which handles function calls of user-defined methods and the callbacks invoked by the Android framework at the same time. To evaluate Relda2's effectiveness and practical applicability, we downloaded 103 apps from popular app stores and an open source community, and found 67 real resource leaks, which we have confirmed manually.",0098-5589;00985589,,10.1109/TSE.2016.2547385,National Basic Research (973); 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442579,Android apps;byte-code analysis;inter-procedural analysis;resource leak;static analysis,Androids;Computer bugs;Humanoid robots;Java;Leak detection;Smart phones;Testing,Android (operating system);embedded systems;graph theory;program diagnostics;resource allocation,Android application;Relda2;callback-aware resource leak detection tool;embedded resources;function call graph;interprocedural analysis;static analysis,,8,,,,,20160328,Nov. 1 2016,,IEEE,IEEE Journals & Magazines,,22
Probabilistic Interface Automata,E. Pavese; V. Braberman; S. Uchitel,"Departamento de Computaci&#x00F3;n, Universidad de Buenos Aires",IEEE Transactions on Software Engineering,20160915,2016,42,9,843,865,"System specifications have long been expressed through automata-based languages, which allow for compositional construction of complex models and enable automated verification techniques such as model checking. Automata-based verification has been extensively used in the analysis of systems, where they are able to provide yes/no answers to queries regarding their temporal properties. Probabilistic modelling and checking aim at enriching this binary, qualitative information with quantitative information, more suitable to approaches such as reliability engineering. Compositional construction of software specifications reduces the specification effort, allowing the engineer to focus on specifying individual component behaviour to then analyse the composite system behaviour. Compositional construction also reduces the validation effort, since the validity of the composite specification should be dependent on the validity of the components. These component models are smaller and thus easier to validate. Compositional construction poses additional challenges in a probabilistic setting. Numerical annotations of probabilistically independent events must be contrasted against estimations or measurements, taking care of not compounding this quantification with exogenous factors, in particular the behaviour of other system components. Thus, the validity of compositionally constructed system specifications requires that the validated probabilistic behaviour of each component continues to be preserved in the composite system. However, existing probabilistic automata-based formalisms do not support specification of non-deterministic and probabilistic component behaviour which, when observed through logics such as pCTL, is preserved in the composite system. In this paper we present a probabilistic extension to Interface Automata which preserves pCTL properties under probabilistic fairness by ensuring a probabilistic branching simulation between component and composite automata. T- e extension not only supports probabilistic behaviour but also allows for weaker prerequisites to interfacing composition, that supports delayed synchronisation that may be required because of internal component behaviour. These results are equally applicable as an extension to non-probabilistic Interface Automata.",0098-5589;00985589,,10.1109/TSE.2016.2527000,ANPCyT PICT; MEALS; UBACYT; 10.13039/501100002923 - CONICET PIP; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401103,Behaviour models;interface automata;model checking;probability,Automata;Computational modeling;Interconnected systems;Model checking;Probabilistic logic;Semantics;Synchronization,formal specification;probabilistic automata;program verification,automata-based languages;automata-based verification;automated verification techniques;binary qualitative information;complex models;component automata;component behaviour;composite automata;composite system behaviour;compositional construction;compositionally constructed system specifications;exogenous factors;internal component behaviour;model checking;nondeterministic behaviour;nonprobabilistic interface automata;numerical annotations;pCTL;probabilistic branching simulation;probabilistic checking;probabilistic component behaviour;probabilistic fairness;probabilistic interface automata;probabilistic modelling;probabilistically independent events;quantitative information;reliability engineering;software specifications;system components;system specifications;temporal properties,,,,,,,20160208,Sept. 1 2016,,IEEE,IEEE Journals & Magazines,,22
Model Checking Software with First Order Logic Specifications Using AIG Solvers,M. A. Noureddine; F. A. Zaraket,"Department of Computer Science, University of Illinois at Urbana Champaign, IL",IEEE Transactions on Software Engineering,20160811,2016,42,8,741,763,"Static verification techniques leverage Boolean formula satisfiability solvers such as SAT and SMT solvers that operate on conjunctive normal form and first order logic formulae, respectively, to validate programs. They force bounds on variable ranges and execution time and translate the program and its specifications into a Boolean formula. They are limited to programs of relatively low complexity for the following reasons. (1) A small increase in the bounds can cause a large increase in the size of the translated formula. (2) Boolean satisfiability solvers are restricted to using optimizations that apply at the level of the formula. Finally, (3) the Boolean formulae often need to be regenerated with higher bounds to ensure the correctness of the translation. We present a method that uses And-Inverter-Graph (AIG) sequential circuits, and AIG synthesis and verification frameworks to validate programs. An AIG is a Boolean formula with memory elements, logically complete negated conjunction gates, and a hierarchical structure. Encoding the validation problem of a program as an AIG (1) typically provides a more succinct representation than a Boolean formulae encoding with no memory elements, (2) preserves the high-level structure of the program, and (3) enables the use of a number of powerful automated analysis techniques that have no counterparts for other Boolean formulae such as CNF. Our method takes an imperative program with a first order logic specification consisting of a precondition and a postcondition pair, and a bound on the program variable ranges, and produces an AIG with a designated output that is <inline-formula><tex-math notation=""LaTeX"">${true}$</tex-math><alternatives> <inline-graphic xlink:type=""simple"" xlink:href=""zaraket-ieq1-2520468.gif""/></alternatives></inline-formula> when the program violates the specification. Our method uses AIG synthesis reduction techniques to reduce the AIG, and then uses AIG verification techniques to check the satisfi- bility of the designated output. The results show that our method can validate designs that are not possible with other state of the art techniques, and with bounds that are an order of magnitude larger.",0098-5589;00985589,,10.1109/TSE.2016.2520468,American University of Beirut; University Research Board; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7389426,Boolean satisfiability solvers;Hoare triplet;Software verification;static analysis,Encoding;Interpolation;Model checking;Optimization;Radiation detectors;Sequential circuits;Software,,,,,,,,,20160121,Aug. 1 2016,,IEEE,IEEE Journals & Magazines,,22
A Multi-Objective Technique to Prioritize Test Cases,A. Marchetto; M. M. Islam; W. Asghar; A. Susi; G. Scanniello,independent researchers,IEEE Transactions on Software Engineering,20161013,2016,42,10,918,940,"While performing regression testing, an appropriate choice for test case ordering allows the tester to early discover faults in source code. To this end, test case prioritization techniques can be used. Several existing test case prioritization techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test case prioritization technique that determines the ordering of test cases that maximize the number of discovered faults that are both technical and business critical. In other words, our new technique aims at both early discovering faults and reducing the execution cost of test cases. To this end, we automatically recover links among software artifacts (i.e., requirements specifications, test cases, and source code) and apply a metric-based approach to automatically identify critical and fault-prone portions of software artifacts, thus becoming able to give them more importance during test case prioritization. We experimentally evaluated our technique on 21 Java applications. The obtained results support our hypotheses on efficiency and effectiveness of our new technique and on the use of automatic artifacts analysis and weighting in test case prioritization.",0098-5589;00985589,,10.1109/TSE.2015.2510633,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362042,Regression testing;requirements;test case prioritization;testing,Business;Electronic mail;Fault diagnosis;Optimization;Software;Software engineering;Testing,formal specification;formal verification;program testing;regression analysis;software fault tolerance;software metrics;source code (software);systems analysis,metric-based approach;multiobjective technique;regression testing;requirements specification;software artifact;source code fault;test case prioritization,,1,,,,,20151222,Oct. 1 2016,,IEEE,IEEE Journals & Magazines,,22
An Approach to Checking Consistency between UML Class Model and Its Java Implementation,H. M. Chavez; W. Shen; R. B. France; B. A. Mechling; G. Li,"Computer Science, Kalamazoo, MI",IEEE Transactions on Software Engineering,20160414,2016,42,4,322,344,"Model Driven Engineering (MDE) aims to expedite the software development process by providing support for transforming models to running systems. Many modeling tools provide forward engineering features, which automatically translate a model into a skeletal program that developers must complete. Inconsistencies between a design model and its implementation, however, can arise, particularly when a final implementation is developed dependently on the code from which it was generated. Manually checking that an implementation conforms to its model is a daunting task. Thus, an MDE tool that developers can use to check that implementations conform to their models can significantly improve a developer's productivity. This paper presents a model-based approach for testing whether or not an implementation satisfies the constraints imposed by its design model. Our model-based testing approach aims to efficiently reduce the test input space while supporting branch coverage criteria. To evaluate the approach's ability to uncover inconsistencies, we developed a prototypical tool and applied it to the Eclipse UML2 projects. We were able to uncover inconsistencies between the models and their implementations using the tool.",0098-5589;00985589,,10.1109/TSE.2015.2488645,Open Project of Shanghai Key Lab. of Trustworthy Computing; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294689,Class Diagrams;Class diagrams;Java;Model checking;UML;model checking,Java;Object oriented modeling;Semantics;Software;Testing;Unified modeling language,,,,1,,43,,,20151008,April 1 2016,,IEEE,IEEE Journals & Magazines,,22
HYDRA: Massively Compositional Model for Cross-Project Defect Prediction,X. Xia; D. Lo; S. J. Pan; N. Nagappan; X. Wang,"College of Computer Science and Technology, Zhejiang University Hangzhou, Zhejiang, China",IEEE Transactions on Software Engineering,20161013,2016,42,10,977,998,"Most software defect prediction approaches are trained and applied on data from the same project. However, often a new project does not have enough training data. Cross-project defect prediction, which uses data from other projects to predict defects in a particular project, provides a new perspective to defect prediction. In this work, we propose a HYbrid moDel Reconstruction Approach (HYDRA) for cross-project defect prediction, which includes two phases: genetic algorithm (GA) phase and ensemble learning (EL) phase. These two phases create a massive composition of classifiers. To examine the benefits of HYDRA, we perform experiments on 29 datasets from the PROMISE repository which contains a total of 11,196 instances (i.e., Java classes) labeled as defective or clean. We experiment with logistic regression as the underlying classification algorithm of HYDRA. We compare our approach with the most recently proposed cross-project defect prediction approaches: TCA+ by Nam et al., Peters filter by Peters et al., GP by Liu et al., MO by Canfora et al., and CODEP by Panichella et al. Our results show that HYDRA achieves an average F1-score of 0.544. On average, across the 29 datasets, these results correspond to an improvement in the F1-scores of 26.22 , 34.99, 47.43, 28.61, and 30.14 percent over TCA+, Peters filter, GP, MO, and CODEP, respectively. In addition, HYDRA on average can discover 33 percent of all bugs if developers inspect the top 20 percent lines of code, which improves the best baseline approach (TCA+) by 44.41 percent. We also find that HYDRA improves the F1-score of Zero-R which predict all the instances to be defective by 5.42 percent, but improves Zero-R by 58.65 percent when inspecting the top 20 percent lines of code. In practice, Zero-R can be hard to use since it simply predicts all of the instances to be defective, and thus developers have to inspect all of the instances to find the defective ones. Moreover, we notice the improvement of HYDRA ov- r other baseline approaches in terms of F1-score and when inspecting the top 20 percent lines of code are substantial, and in most cases the improvements are significant and have large effect sizes across the 29 datasets.",0098-5589;00985589,,10.1109/TSE.2016.2543218,Ministry of Science and Technology of China; National Basic Research Program of China; National Key Technology R&D Program; 10.13039/501100001809 - NSFC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435328,Cross-project defect prediction;ensemble learning;genetic algorithm;transfer learning,Architecture;Buildings;Data models;Genetic algorithms;Measurement;Predictive models;Training,genetic algorithms;learning (artificial intelligence);pattern classification;regression analysis;software fault tolerance,EL phase;GA phase;HYDRA model;PROMISE repository;classification algorithm;cross-project defect prediction;genetic algorithm phase;hybrid model reconstruction approach;logistic regression;massively compositional model;phase and ensemble learning phase;software defect prediction approach,,13,,,,,20160317,Oct. 1 2016,,IEEE,IEEE Journals & Magazines,,21
Parallel Performance Problems on Shared-Memory Multicore Systems: Taxonomy and Observation,R. Atachiants; G. Doherty; D. Gregg,"Trinity College Dublin, Ireland",IEEE Transactions on Software Engineering,20160811,2016,42,8,764,785,"The shift towards multicore processing has led to a much wider population of developers being faced with the challenge of exploiting parallel cores to improve software performance. Debugging and optimizing parallel programs is a complex and demanding task. Tools which support development of parallel programs should provide salient information to allow programmers of multicore systems to diagnose and distinguish performance problems. Appropriate design of such tools requires a systematic analysis of the problems which might be identified, and the information used to diagnose them. Building on the literature, we put forward a potential taxonomy of parallel performance problems, and an observational model which links measurable performance data to these problems. We present a validation of this model carried out with parallel programming experts, identifying areas of agreement and disagreement. This is accompanied with a survey of the prevalence of these problems in software development. From this we can identify contentious areas worthy of further exploration, as well as those with high prevalence and strong agreement, which are natural candidates for initial moves towards better tool support.",0098-5589;00985589,,10.1109/TSE.2016.2519346,Irish Software Research Centre; 10.13039/501100001602 - Science Foundation Ireland; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7386691,"Parallel programming, multicore, multi-threaded, optimization, performance problems, performance analysis, diagnosis, debugging, taxonomy.",Computers;Context;Hardware;Multicore processing;Parallel programming;Software;Taxonomy,parallel programming;performance evaluation;program debugging;shared memory systems;software tools,multicore processing;parallel performance problems;parallel program debugging;parallel program optimization;shared-memory multicore systems;software performance;tool support,,1,,,,,20160119,Aug. 1 2016,,IEEE,IEEE Journals & Magazines,,21
Developer Micro Interaction Metrics for Software Defect Prediction,T. Lee; J. Nam; D. Han; S. Kim; H. Peter In,"Korea University, Seoul, South Korea",IEEE Transactions on Software Engineering,20161110,2016,42,11,1015,1035,"To facilitate software quality assurance, defect prediction metrics, such as source code metrics, change churns, and the number of previous defects, have been actively studied. Despite the common understanding that developer behavioral interaction patterns can affect software quality, these widely used defect prediction metrics do not consider developer behavior. We therefore propose micro interaction metrics (MIMs), which are metrics that leverage developer interaction information. The developer interactions, such as file editing and browsing events in task sessions, are captured and stored as information by Mylyn, an Eclipse plug-in. Our experimental evaluation demonstrates that MIMs significantly improve overall defect prediction accuracy when combined with existing software measures, perform well in a cost-effective manner, and provide intuitive feedback that enables developers to recognize their own inefficient behaviors during software development.",0098-5589;00985589,,10.1109/TSE.2016.2550458,"Next-Generation Information Computing Development Program; 10.13039/100007431 - National Research Foundation of Korea; 10.13039/501100004085 - Ministry of Education, Science and Technology; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447797,Defect prediction;Mylyn;developer interaction;software metrics;software quality,Complexity theory;Quality assurance;Software metrics;Software quality,software maintenance;software metrics;software quality,Eclipse plug-in;MIM metric;Mylyn plug-in;defect prediction metrics;developer behavioral interaction patterns;developer interaction information;developer microinteraction metrics;software defect prediction;software development;software quality assurance,,1,,,,,20160405,Nov. 1 2016,,IEEE,IEEE Journals & Magazines,,20
Metamorphic Testing for Software Quality Assessment: A Study of Search Engines,Z. Q. Zhou; S. Xiang; T. Y. Chen,"School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia",IEEE Transactions on Software Engineering,20160311,2016,42,3,264,284,"Metamorphic testing is a testing technique that can be used to verify the functional correctness of software in the absence of an ideal oracle. This paper extends metamorphic testing into a user-oriented approach to software verification, validation, and quality assessment, and conducts large scale empirical studies with four major web search engines: Google, Bing, Chinese Bing, and Baidu. These search engines are very difficult to test and assess using conventional approaches owing to the lack of an objective and generally recognized oracle. The results are useful for both search engine developers and users, and demonstrate that our approach can effectively alleviate the oracle problem and challenges surrounding a lack of specifications when verifying, validating, and evaluating large and complex software systems.",0098-5589;00985589,,10.1109/TSE.2015.2478001,10.13039/501100000923 - Australian Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7254235,Software quality;lack of system specification;metamorphic testing;oracle problem;quality assessment;search engine;user-oriented testing;validation;verification,Google;Search engines;Software algorithms;Software quality;Testing;Web pages,Internet;program testing;program verification;search engines;software quality;user interfaces,Baidu;Chinese Bing;Google;Web search engines;metamorphic testing;software quality assessment;software validation;software verification;user-oriented approach,,14,,53,,,20150910,March 1 2016,,IEEE,IEEE Journals & Magazines,,20
Seer: A Lightweight Online Failure Prediction Approach,B. Ozcelik; C. Yilmaz,freelance software developer,IEEE Transactions on Software Engineering,20160107,2016,42,1,26,46,"Online failure prediction approaches aim to predict the manifestation of failures at runtime before the failures actually occur. Existing approaches generally refrain themselves from collecting internal execution data, which can further improve the prediction quality. One reason behind this general trend is the runtime overhead incurred by the measurement instruments that collect the data. Since these approaches are targeted at deployed software systems, excessive runtime overhead is generally undesirable. In this work we conjecture that large cost reductions in collecting internal execution data for online failure prediction may derive from pushing the substantial parts of the data collection work onto the hardware. To test this hypothesis, we present a lightweight online failure prediction approach, called Seer, in which most of the data collection work is performed by fast hardware performance counters. The hardware-collected data is augmented with further data collected by a minimal amount of software instrumentation that is added to the systems software. In our empirical evaluations conducted on three open source projects, Seer performed significantly better than other related approaches in predicting the manifestation of failures.",0098-5589;00985589,,10.1109/TSE.2015.2442577,European Community Framework Programme; Marie Curie International Reintegration; 10.13039/501100004410 - Scientific and Technological Research Council of Turkey; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120143,Online failure prediction;hardware performance counters;software quality assurance;software reliability,Hardware;Indexes;Instruments;Predictive models;Radiation detectors;Runtime;Software,data handling;public domain software;quality assurance;software cost estimation;software quality;software reliability;system recovery,Seer;cost reduction;fast hardware performance counters;internal execution data collection;lightweight online failure prediction;measurement instruments;open source projects;prediction quality improvement;runtime overhead;software instrumentation;software quality assurance;software reliability;software systems,,1,,60,,,20150609,Jan. 1 2016,,IEEE,IEEE Journals & Magazines,,20
A Survey on Metamorphic Testing,S. Segura; G. Fraser; A. B. Sanchez; A. Ruiz-CortÕ©s,"Department of Computer Languages and Systems, Universidad de Sevilla, Spain",IEEE Transactions on Software Engineering,20160915,2016,42,9,805,824,"A test oracle determines whether a test execution reveals a fault, often by comparing the observed program output to the expected output. This is not always practical, for example when a program's input-output relation is complex and difficult to capture formally. Metamorphic testing provides an alternative, where correctness is not determined by checking an individual concrete output, but by applying a transformation to a test input and observing how the program output _ÑÒmorphs_Ñù into a different one as a result. Since the introduction of such metamorphic relations in 1998, many contributions on metamorphic testing have been made, and the technique has seen successful applications in a variety of domains, ranging from web services to computer graphics. This article provides a comprehensive survey on metamorphic testing: It summarises the research results and application areas, and analyses common practice in empirical studies of metamorphic testing as well as the main open challenges.",0098-5589;00985589,,10.1109/TSE.2016.2532875,Andalusian Government projects THEOS; BELI; CICYT projects TAPAS; COPAS; Spanish Government; 10.13039/501100002924 - European Commission (FEDER); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422146,Metamorphic testing;oracle problem;survey,Concrete;Distance measurement;Google;Libraries;Search engines;Testing;Web services,program testing,metamorphic relations;metamorphic testing;test execution;test oracle,,22,,,,,20160229,Sept. 1 2016,,IEEE,IEEE Journals & Magazines,,19
Software Reliability Analysis Using Weakest Preconditions in Linear Assignment Programs,H. Luo; X. Liu; X. Chen; T. Long; R. Jiang,"Department of Measurement and Control Engineering, School of Manufacturing Science and Engineering, Sichuan University, Chengdu, P.R.China",IEEE Transactions on Software Engineering,20160915,2016,42,9,866,885,"Weakest preconditions derived from triple axiomatic semantics have been widely used to prove the correctness of programs. They can also be applied to evaluate the reliability of software. However, deducing a weakest precondition, as well as determining its propagation path, encounters challenges such as unknown constraint conditions, symbol computation and means of representation. To address these challenges, in this paper, we utilize the disjunctive normal form of if-else branch structure to capture reasonable propagation paths of the weakest precondition. Meanwhile, by removing the sequential dependencies, we demonstrate how to get the weakest precondition of loop-structure by leveraging program function. Moreover, we extensively explore three modeling characteristics (i.e., path extension, innermost connection and condition leap) for deducing the weakest precondition of structured programs. Finally, taking the definition of program node and storage structure of weakest precondition as bases, we design a serial of modeling algorithms. Based on symbol computation and recursive call technology with Depth-First Search (DFS), our algorithms can not only be used to deduce the weakest precondition, but also to capture the propagate path of the weakest precondition. Experiments illustrate the efficacy and effectiveness of our proposed models and designed deductive algorithms.",0098-5589;00985589,,10.1109/TSE.2016.2521379,Research Foundation of Young Teachers in Sichuan University of P.R. China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398131,Weakest precondition;cell-structure;condition leap;innermost connection;node;path extension,Algorithm design and analysis;Cognition;Computational modeling;Computer bugs;Semantics;Software reliability,linear programming;search problems;software reliability,DFS;condition leap characteristic;deductive algorithms;depth-first search;if-else branch structure;innermost connection characteristic;linear assignment program;loop-structure precondition;path extension characteristic;program correctness;propagation path;recursive call technology;software reliability analysis;symbol computation;triple axiomatic semantics;weakest preconditions,,,,,,,20160203,Sept. 1 2016,,IEEE,IEEE Journals & Magazines,,19
Variability Analysis of Requirements: Considering Behavioral Differences and Reflecting Stakeholders_Ñé Perspectives,N. Itzik; I. Reinhartz-Berger; Y. Wand,"Department of Information Systems, University of Haifa, Haifa, Israel",IEEE Transactions on Software Engineering,20160714,2016,42,7,687,706,"Adoption of Software Product Line Engineering (SPLE) to support systematic reuse of software-related artifacts within product families is challenging, time-consuming and error-prone. Analyzing the variability of existing artifacts needs to reflect different perspectives and preferences of stakeholders in order to facilitate decisions in SPLE adoption. Considering that requirements drive many development methods and activities, we introduce an approach to analyze variability of behaviors as presented in functional requirements. The approach, called semantic and ontological variability analysis (SOVA), uses ontological and semantic considerations to automatically analyze differences between initial states (preconditions), external events (triggers) that act on the system, and final states (post-conditions) of behaviors. The approach generates feature diagrams typically used in SPLE to model variability. Those diagrams are organized according to perspective profiles, reflecting the needs and preferences of the potential stakeholders for given tasks. We conducted an empirical study to examine the usefulness of the approach by comparing it to an existing tool which is mainly based on a latent semantic analysis measurement. SOVA appears to create outputs that are more comprehensible in significantly shorter times. These results demonstrate SOVA's potential to allow for flexible, behavior-oriented variability analysis.",0098-5589;00985589,,10.1109/TSE.2015.2512599,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7366597,Software product line engineering;feature diagrams;ontology;requirements specifications;variability analysis,Feature extraction;Semantics;Software;Software product lines;Stakeholders;Systematics,formal specification;ontologies (artificial intelligence);software product lines,SOVA;SPLE adoption;behavior-oriented variability analysis;feature diagrams;latent semantic analysis measurement;model variability;requirements variability analysis;semantic-and-ontological variability analysis;software product line engineering,,4,,53,,,20151225,July 1 2016,,IEEE,IEEE Journals & Magazines,,19
Software Development in Startup Companies: The Greenfield Startup Model,C. Giardino; N. Paternoster; M. Unterkalmsteiner; T. Gorschek; P. Abrahamsson,"Faculty of Computer Science, Free University of Bolzano/Bozen, Dominikanerplatz 3, Italy",IEEE Transactions on Software Engineering,20160610,2016,42,6,585,604,"Software startups are newly created companies with no operating history and oriented towards producing cutting-edge products. However, despite the increasing importance of startups in the economy, few scientific studies attempt to address software engineering issues, especially for early-stage startups. If anything, startups need engineering practices of the same level or better than those of larger companies, as their time and resources are more scarce, and one failed project can put them out of business. In this study we aim to improve understanding of the software development strategies employed by startups. We performed this state-of-practice investigation using a grounded theory approach. We packaged the results in the Greenfield Startup Model (GSM), which explains the priority of startups to release the product as quickly as possible. This strategy allows startups to verify product and market fit, and to adjust the product trajectory according to early collected user feedback. The need to shorten time-to-market, by speeding up the development through low-precision engineering activities, is counterbalanced by the need to restructure the product before targeting further growth. The resulting implications of the GSM outline challenges and gaps, pointing out opportunities for future research to develop and validate engineering practices in the startup context.",0098-5589;00985589,,10.1109/TSE.2015.2509970,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360225,Grounded Theory;Software Development;Software development;Startups;grounded theory;startups,Companies;Context;GSM;History;Software;Software engineering,software development management,GSM;Greenfield startup model;engineering practices;grounded theory approach;software development strategies;software engineering issues;software startups;state-of-practice investigation;time-to-market;user feedback,,7,,119,,,20151217,June 1 2016,,IEEE,IEEE Journals & Magazines,,19
A Multi-Site Joint Replication of a Design Patterns Experiment Using Moderator Variables to Generalize across Contexts,J. L. Krein; L. Prechelt; N. Juristo; A. Nanthaamornphong; J. C. Carver; S. Vegas; C. D. Knutson; K. D. Seppi; D. L. Eggett,"Department of Computer Science, Provo, UT",IEEE Transactions on Software Engineering,20160414,2016,42,4,302,321,"<bold>Context.</bold> Several empirical studies have explored the benefits of software design patterns, but their collective results are highly inconsistent. Resolving the inconsistencies requires investigating moderators_ÑÓi.e., variables that cause an effect to differ across contexts. <bold>Objectives.</bold> Replicate a design patterns experiment at multiple sites and identify sufficient moderators to generalize the results across prior studies. <bold>Methods.</bold> We perform a close replication of an experiment investigating the impact (in terms of time and quality) of design patterns (Decorator and Abstract Factory) on software maintenance. The experiment was replicated once previously, with divergent results. We execute our replication at four universities_ÑÓspanning two continents and three countries_ÑÓusing a new method for performing distributed replications based on closely coordinated, small-scale instances (_ÑÒjoint replication_Ñù). We perform two analyses: 1)ë_a <italic> post-hoc</italic> analysis of moderators, based on frequentist and Bayesian statistics; 2)ë_an <italic>a priori </italic> analysis of the original hypotheses, based on frequentist statistics. <bold>Results.</bold> The main effect differs across the previous instances of the experiment and across the sites in our distributed replication. Our analysis of moderators (including developer experience and pattern knowledge) resolves the differences sufficiently to allow for cross-context (and cross-study) conclusions. The final conclusions represent 126 participants from five universities and 12 software companies, spanning two continents and at least four countries. <bold>Conclusions.</bold> The Decorator pattern is found to be preferable to a simpler solution during maintenance, as long as the developer has at least some prior knowledge of the pattern. For A- stract Factory, the simpler solution is found to be mostly equivalent to the pattern solution. Abstract Factory is shown to require a higher level of knowledge and/or experience than Decorator for the pattern to be beneficial.",0098-5589;00985589,,10.1109/TSE.2015.2488625,Spanish Ministry of Economy and Competitiveness; 10.13039/100004673 - LLC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294706,Design patterns;controlled experiment;joint replication;moderator variables;multi-site;software maintenance,Context modeling;Design methodology;Production facilities;Training,,,,1,,42,,,20151008,April 1 2016,,IEEE,IEEE Journals & Magazines,,19
A Lightweight System for Detecting and Tolerating Concurrency Bugs,M. Zhang; Y. Wu; S. Lu; S. Qi; J. Ren; W. Zheng,"Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China",IEEE Transactions on Software Engineering,20161013,2016,42,10,899,917,"Along with the prevalence of multi-threaded programs, concurrency bugs have become one of the most important sources of software bugs. Even worse, due to the non-deterministic nature of concurrency bugs, these bugs are both difficult to detect and fix even after the detection. As a result, it is highly desired to develop an all-around approach that is able to not only detect them during the testing phase but also tolerate undetected bugs during production runs. However, existing bug-detecting and bug-tolerating tools are usually either <italic>1)</italic> constrained in types of bugs they can handle or <italic>2)</italic> requiring specific hardware supports for achieving an acceptable overhead. In this paper, we present a novel program invariant, name Anticipating Invariant (<sc>Ai</sc>), that can detect most types of concurrency bugs. More importantly, <sc>Ai</sc> can be used to anticipate many concurrency bugs before any irreversible changes have been made. Thus it enables us to develop a software-only system that is able to forestall failures with a simple thread stalling technique, which does not rely on execution roll-back and hence has good performance. Experiments with 35 real-world concurrency bugs demonstrate that <sc>Ai</sc> is capable of detecting and tolerating many important types of concurrency bugs, including both atomicity and order violations. It has also exposed two new bugs (confirmed by developers) that were never reported before in the literature. Performance evaluation with 6 representative parallel programs shows that <sc>Ai</sc> incurs negligible overhead (<inline-formula> <tex-math notation=""LaTeX"">$ < 1\%$</tex-math><alternatives> <inline-graphic xlink:type=""simple"" xlink:href=""wu-ieq1-2531666.gif""/></alternatives></inline-formula>) for many nontrivial desktop and server applications.",0098-5589;00985589,,10.1109/TSE.2016.2531666,Chinese Special Project of Science and Technology; National Basic Research (973) Program of China; National High-Tech R&D (863) Program of China; 10.13039/100000001 - US National Science Foundation; 10.13039/501100001809 - Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412768,Concurrency bugs;bug tolerating;software reliability,Artificial intelligence;Computer bugs;Concurrent computing;Hardware;Testing;Turning,,,,1,,,,,20160218,Oct. 1 2016,,IEEE,IEEE Journals & Magazines,,18
The Role of Ethnographic Studies in Empirical Software Engineering,H. Sharp; Y. Dittrich; C. R. B. de Souza,"Open University, Walton Hall, Milton Keynes, UK",IEEE Transactions on Software Engineering,20160811,2016,42,8,786,804,"Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.",0098-5589;00985589,,10.1109/TSE.2016.2519887,10.13039/501100003593 - CNPq; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387744,Design tools and techniques;computer-supported collaborative work;human factors in software design;software engineering process,Computer science;Context;Electronic mail;Guidelines;Sociology;Software;Software engineering,cultural aspects;human factors;software process improvement,empirical software engineering;ethnography;human aspects;social aspects;sociotechnological realities;software development practice;software engineering tools;software process development,,5,,,,,20160120,Aug. 1 2016,,IEEE,IEEE Journals & Magazines,,18
Coverage-Aware Test Database Reduction,J. Tuya; C. d. l. Riva; M. J. SuÕçrez-Cabal; R. Blanco,"Dpto. Inform&#x00E1;tica, University of Oviedo, Campus Universitario de Gij&#x00F3;n, Gij&#x00F3;n, Spain",IEEE Transactions on Software Engineering,20161013,2016,42,10,941,959,"Functional testing of applications that process the information stored in databases often requires a careful design of the test database. The larger the test database, the more difficult it is to develop and maintain tests as well as to load and reset the test data. This paper presents an approach to reduce a database with respect to a set of SQL queries and a coverage criterion. The reduction procedures search the rows in the initial database that contribute to the coverage in order to find a representative subset that satisfies the same coverage as the initial database. The approach is automated and efficiently executed against large databases and complex queries. The evaluation is carried out over two real life applications and a well-known database benchmark. The results show a very large degree of reduction as well as scalability in relation to the size of the initial database and the time needed to perform the reduction.",0098-5589;00985589,,10.1109/TSE.2016.2519032,ERDF; Principality of Asturias; Spanish Ministry of Economy and Competitiveness; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7384760,Test database reduction;test coverage of code;test design;testing tools,Benchmark testing;Databases;Fault detection;Minimization;Production;Scalability,SQL;database management systems;design;information storage;program testing;software tools,SQL queries;coverage criterion;coverage-aware test database reduction;database benchmark;functional testing;information storage;test design;testing tools,,2,,,,,20160118,Oct. 1 2016,,IEEE,IEEE Journals & Magazines,,18
Test Case Prioritization Using Lexicographical Ordering,S. Eghbali; L. Tahvildari,"Department of Electrical and Computer Engineering, 200 University Ave West, University of Waterloo, Waterloo, Ontario",IEEE Transactions on Software Engineering,20161208,2016,42,12,1178,1195,"Test case prioritization aims at ordering test cases to increase the rate of fault detection, which quantifies how fast faults are detected during the testing phase. A common approach for test case prioritization is to use the information of previously executed test cases, such as coverage information, resulting in an iterative (greedy) prioritization algorithm. Current research in this area validates the fact that using coverage information can improve the rate of fault detection in prioritization algorithms. The performance of such iterative prioritization schemes degrade as the number of ties encountered in prioritization steps increases. In this paper, using the notion of lexicographical ordering, we propose a new heuristic for breaking ties in coverage based techniques. Performance of the proposed technique in terms of the rate of fault detection is empirically evaluated using a wide range of programs. Results indicate that the proposed technique can resolve ties and in turn noticeably increases the rate of fault detection.",0098-5589;00985589,,10.1109/TSE.2016.2550441,10.13039/501100000038 - Natural Sciences and Engineering Research Council of Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456343,Regression testing;lexicographical ordering;test case prioritization,Fault detection;Fault diagnosis;Feature extraction;Regression analysis;Software testing,fault diagnosis;greedy algorithms;iterative methods;program testing;regression analysis,coverage information;fault detection;iterative greedy prioritization algorithm;lexicographical ordering;regression testing;test case prioritization,,1,,,,,20160421,Dec. 1 2016,,IEEE,IEEE Journals & Magazines,,17
Enhanced Code Conversion Approach for the Integrated Cross-Platform Mobile Development (ICPMD),W. S. El-Kassas; B. A. Abdullah; A. H. Yousef; A. M. Wahba,"Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt",IEEE Transactions on Software Engineering,20161110,2016,42,11,1036,1053,"Mobile development companies aim to maximize the return on investments by making their mobile applications (Apps) available on different mobile platforms. Consequently, the same App is developed several times; each time the developer uses the programming languages and development tools of a specific platform. Therefore, there is a need to have cross-platform mobile applications development solutions that enable the developers to develop the App once and run it everywhere. The Integrated Cross-Platform Mobile Applications Development (ICPMD) solution is one of the attempts that enables the developers to use the most popular programming languages like Java for Android and C# for Windows Phone 8 (WP8). ICPMD is used to transform both the source code and user interface to another language to generate full Apps on the target platform. This paper extends ICPMD by proposing a new code conversion approach based on XSLT and Regular Expressions to ease the conversion process. In addition, it provides the assessment method to compare the ICPMD efficiency with competing approaches. Several Apps are converted from WP8 to Android and vice versa. The ICPMD evaluation results show reasonable improvement over commercial cross-platform mobile development tools (Titanium and Xamarin).",0098-5589;00985589,,10.1109/TSE.2016.2543223,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442177,"Cross-platform mobile development;ICPMD, source code patterns;code conversion;code reuse;generated apps",Application programming interfaces;Java;Mobile communication;Runtime;Smart phones;Titanium,Android (operating system);mobile computing;programming languages;smart phones;software tools;source code (software);user interfaces,ICPMD;code conversion;development tool;integrated cross-platform mobile development;mobile application development;mobile platform;programming language;smart phone;source code;user interface,,,,,,,20160325,Nov. 1 2016,,IEEE,IEEE Journals & Magazines,,17
Dynamic Testing for Deadlocks via Constraints,Y. Cai; Q. Lu,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Software Engineering,20160915,2016,42,9,825,842,"Existing deadlock detectors are either not scalable or may report false positives when suggesting cycles as potential deadlocks. Additionally, they may not effectively trigger deadlocks and handle false positives. We propose a technique called ConLock<sup>+</sup>, which firstly analyzes each cycle and its corresponding execution to identify a set of scheduling constraints that are necessary conditions to trigger the corresponding deadlock. The ConLock<sup>+</sup> technique then performs a second run to enforce the set of constraints, which will trigger a deadlock if the cycle is a real one. Or if not, ConLock<sup>+</sup> reports a steering failure for that cycle and also identifies other similar cycles which would also produce steering failures. For each confirmed deadlock, ConLock<sup>+</sup> performs a static analysis to identify conflicting memory access that would also contribute to the occurrence of the deadlock. This analysis is helpful to enable developers to understand and fix deadlocks. ConLock<sup>+</sup> has been validated on a suite of real-world programs with 16 real deadlocks. The results show that across all 811 cycles, ConLock<sup>+</sup> confirmed all of the 16 deadlocks with a probability of __‚80 percent. For the remaining cycles, ConLock<sup>+</sup> reported steering failures and also identified that five deadlocks also involved conflicting memory accesses.",0098-5589;00985589,,10.1109/TSE.2016.2537335,National Basic Research (973) Program of China; 10.13039/501100001809 - National Science Foundation of China (NSFC); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423814,Deadlock triggering;constraint;reliability;scheduling;should-happen-before relation;verification,Detectors;Instruction sets;Probabilistic logic;Schedules;Synchronization;System recovery;Testing,concurrency control;program diagnostics;program testing;scheduling,ConLock+;deadlock detectors;dynamic testing;memory access;real-world programs;scheduling constraints;static analysis;steering failure,,1,,,,,20160302,Sept. 1 2016,,IEEE,IEEE Journals & Magazines,,17
A Game-Theoretic Foundation for the Maximum Software Resilience against Dense Errors,C. H. Huang; D. A. Peled; S. Schewe; F. Wang,"Graduate Institute of Electronic Engineering, National Taiwan University, Taiwan, ROC",IEEE Transactions on Software Engineering,20160714,2016,42,7,605,622,"Safety-critical systems need to maintain their functionality in the presence of multiple errors caused by component failures or disastrous environment events. We propose a game-theoretic foundation for synthesizing control strategies that maximize the resilience of a software system in defense against a realistic error model. The new control objective of such a game is called <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives> <inline-graphic xlink:type=""simple"" xlink:href=""wang-ieq1-2510001.gif""/></alternatives></inline-formula>-resilience. In order to be <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives> <inline-graphic xlink:type=""simple"" xlink:href=""wang-ieq2-2510001.gif""/></alternatives></inline-formula>-resilient, a system needs to rapidly recover from infinitely many waves of a small number of up to <inline-formula><tex-math notation=""LaTeX"">$k$ </tex-math><alternatives><inline-graphic xlink:type=""simple"" xlink:href=""wang-ieq3-2510001.gif""/></alternatives></inline-formula> close errors provided that the blocks of up to <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives> <inline-graphic xlink:type=""simple"" xlink:href=""wang-ieq4-2510001.gif""/></alternatives></inline-formula> errors are separated by short time intervals, which can be used by the system to recover. We first argue why we believe this to be the right level of abstraction for safety critical systems when local faults are few and far between. We then show how the analysis of <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives> <inline-graphic xlink:type=""simple"" xlink:href=""wang-ieq5-2510001.gif""/></alternatives></inline-formula>-resilience problems can be formulated as a model-checking problem of a mild extension to the alternating-time <inline-formula> <tex-math notation=""LaTeX"">$\mu$</tex-math><alternatives><inline-graphic xlink:type=""simple"" xlink:href=""wang-ieq6-2510001.gif""/> </alternatives></inline-formula>-calcu- us (AMC). The witness for <inline-formula><tex-math notation=""LaTeX"">$k$ </tex-math><alternatives><inline-graphic xlink:type=""simple"" xlink:href=""wang-ieq7-2510001.gif""/></alternatives></inline-formula> resilience, which can be provided by the model checker, can be used for providing control strategies that are optimal with respect to resilience. We show that the computational complexity of constructing such optimal control strategies is low and demonstrate the feasibility of our approach through an implementation and experimental results.",0098-5589;00985589,,10.1109/TSE.2015.2510001,Efficient Synthesis Method of Control for Concurrent Systems; Research Center for Information Technology Innovation (CITI); 10.13039/100007225 - MOST; 10.13039/501100000266 - Engineering and Physical Science Research Council (EPSRC); 10.13039/501100003977 - ISF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360234,Fault tolerance;complexity;formal verification;game;model-checking;resilience;strategy,Computer science;Game theory;Games;Resilience;Safety;Software systems,,,,2,,44,,,20151217,July 1 2016,,IEEE,IEEE Journals & Magazines,,17
Designing Autonomic Management Systems by Using Reactive Control Techniques,N. Berthier; Õ_. Rutten; N. De Palma; S. M. K. Gueye,"ERODS team, University of Grenoble LIG B&#226;t. C, 220 rue de la Chimie, St Martin d&#x0027;H&#x00E8;res, France",IEEE Transactions on Software Engineering,20160714,2016,42,7,640,657,"The ever growing complexity of software systems has led to the emergence of automated solutions for their management. The software assigned to this work is usually called an Autonomic Management System (AMS). It is ordinarily designed as a composition of several managers, which are pieces of software evaluating the dynamics of the system under management through measurements (e.g., workload, memory usage), taking decisions, and acting upon it so that it stays in a set of acceptable operating states. However, careless combination of managers may lead to inconsistencies in the taken decisions, and classical approaches dealing with these coordination problems often rely on intricate and ad hoc solutions. To tackle this problem, we take a global view and underscore that AMSs are intrinsically reactive, as they react to flows of monitoring data by emitting flows of reconfiguration actions. Therefore we propose a new approach for the design of AMSs, based on synchronous programming and discrete controller synthesis techniques. They provide us with high-level languages for modeling the system to manage, as well as means for statically guaranteeing the absence of logical coordination problems. Hence, they suit our main contribution, which is to obtain guarantees at design time about the absence of logical inconsistencies in the taken decisions. We detail our approach, illustrate it by designing an AMS for a realistic multi-tier application, and evaluate its practicality with an implementation.",0098-5589;00985589,,10.1109/TSE.2015.2510004,MINALOGIC; 10.13039/501100001665 - ANR INFRA; 10.13039/501100001665 - French ANR project Ctrl-Green; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360217,Autonomic computing;coordination;discrete control;reactive programming,Actuators;Automata;Programming;Sensor systems;Software,software management;software performance evaluation,AMS;ad hoc solutions;autonomic management systems;discrete controller synthesis techniques;emitting flows;logical coordination problems;monitoring data flow;reactive control techniques;realistic multitier application;software evaluation;software measurements;software systems;synchronous programming,,1,,47,,,20151217,July 1 2016,,IEEE,IEEE Journals & Magazines,,17
Black-Box String Test Case Generation through a Multi-Objective Optimization,A. Shahbazi; J. Miller,"Department of Electrical and Computer Engineering, Edmonton, AB, Canada",IEEE Transactions on Software Engineering,20160414,2016,42,4,361,378,"String test cases are required by many real-world applications to identify defects and security risks. Random Testing (RT) is a low cost and easy to implement testing approach to generate strings. However, its effectiveness is not satisfactory. In this research, black-box string test case generation methods are investigated. Two objective functions are introduced to produce effective test cases. The diversity of the test cases is the first objective, where it can be measured through string distance functions. The second objective is guiding the string length distribution into a Benford distribution based on the hypothesis that the population of strings is right-skewed within its range. When both objectives are applied via a multi-objective optimization algorithm, superior string test sets are produced. An empirical study is performed with several real-world programs indicating that the generated string test cases outperform test cases generated by other methods.",0098-5589;00985589,,10.1109/TSE.2015.2487958,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293669,Adaptive random testing;automated test case generation;black-box testing;mutation;random testing;software testing;string distance;string test cases,Biological cells;Genetic algorithms;Power capacitors;Sociology;Statistics;Subspace constraints;Testing,optimisation;program testing;security of data,Benford distribution;RT;black-box string test case generation;multiobjective optimization algorithm;objective functions;random testing;security risks;string distance functions,,7,,76,,,20151007,April 1 2016,,IEEE,IEEE Journals & Magazines,,17
Exploring Mobile End User Development: Existing Use and Design Factors,A. Namoun; A. Daskalopoulou; N. Mehandjiev; Z. Xun,"Islamic University of Madinah, Medina, Saudi Arabia",IEEE Transactions on Software Engineering,20161013,2016,42,10,960,976,"Mobile devices are everywhere, and the scope of their use is growing from simple calling and texting through Internet browsing to more technical activities such as creating message processing filters and connecting different apps. However, building tools which provide effective support for such advanced technical use of mobile devices by non-programmers (mobile end user development or mEUD) requires thorough understanding of user needs and motivations, including factors which can impact user intentions regarding mEUD activities. We propose a model linking these mEUD factors with mobile users' attitudes towards, and intent of doing mEUD, and discuss a number of implications for supporting mEUD. Our research process is user-centered, and we formulate a number of hypotheses by fusing results from an exploratory survey which gathers facts about mEUD motivations and activities, and from a focus group study, which delivers deeper understanding of particular mEUD practices and issues. We then test the hypothesized relationships through a follow-up enquiry mixing quantitative and qualitative techniques, leading to the creation of a preliminary mEUD model. Altogether we have involved 275 mobile users in our research. Our contribution links seven mEUD factors with mEUD intentions and attitudes, and highlights a number of implications for mEUD support.",0098-5589;00985589,,10.1109/TSE.2016.2532873,Manchester Business School; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416245,Human factors in software design;mobile environments;models and principles;requirements/specifications,Context;Electronic mail;Games;Mashups;Mobile communication;Mobile handsets,human factors;mobile computing;software engineering;user centred design,mEUD;mobile device;mobile end user development;software development;user attitude;user-centered design,,1,,,,,20160223,Oct. 1 2016,,IEEE,IEEE Journals & Magazines,,16
Asymptotic Perturbation Bounds for Probabilistic Model Checking with Empirically Determined Probability Parameters,G. Su; Y. Feng; T. Chen; D. S. Rosenblum,"Department of Computer Science, School of Computing, National University of Singapore",IEEE Transactions on Software Engineering,20160714,2016,42,7,623,639,"Probabilistic model checking is a verification technique that has been the focus of intensive research for over a decade. One important issue with probabilistic model checking, which is crucial for its practical significance but is overlooked by the state-of-the-art largely, is the potential discrepancy between a stochastic model and the real-world system it represents when the model is built from statistical data. In the worst case, a tiny but nontrivial change to some model quantities might lead to misleading or even invalid verification results. To address this issue, in this paper, we present a mathematical characterization of the consequences of model perturbations on the verification distance. The formal model that we adopt is a parametric variant of discrete-time Markov chains equipped with a vector norm to measure the perturbation. Our main technical contributions include a closed-form formulation of asymptotic perturbation bounds, and computational methods for two arguably most useful forms of those bounds, namely linear bounds and quadratic bounds. We focus on verification of reachability properties but also address automata-based verification of omega-regular properties. We present the results of a selection of case studies that demonstrate that asymptotic perturbation bounds can accurately estimate maximum variations of verification results induced by model perturbations.",0098-5589;00985589,,10.1109/TSE.2015.2508444,Singapore Ministry of Education; State Key Laboratory of Novel Software Technology at Nanjing University; 10.13039/100006296 - CAS/SAFEA; 10.13039/501100000923 - Australian Research Council; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7355393,Asymptotic perturbation bound;discrete-time Markov chain;numerical iteration;optimization;parametric Markov chain;perturbation analysis;probabilistic model checking;quadratic programming,Computational modeling;Markov processes;Mathematical model;Model checking;Perturbation methods;Probabilistic logic,Markov processes;automata theory;formal verification;linear programming;probability;quadratic programming;reachability analysis,asymptotic perturbation bounds;automata-based verification;closed-form formulation;computational methods;empirically determined probability parameters;formal model;linear bounds;mathematical characterization;maximum verification variation estimation;model perturbations;omega-regular properties;parametric discrete-time Markov chains;perturbation measure;probabilistic model checking;quadratic bounds;reachability property verification;real-world system;statistical data;stochastic model;vector norm;verification distance;verification technique,,3,,53,,,20151217,July 1 2016,,IEEE,IEEE Journals & Magazines,,16
Verifying Synchronization for Atomicity Violation Fixing,Q. Shi; J. Huang; Z. Chen; B. Xu,"State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China",IEEE Transactions on Software Engineering,20160311,2016,42,3,280,296,"Atomicity is a fundamental property to guarantee the isolation of a work unit (i.e., a sequence of related events in a thread) from concurrent threads. However, ensuring atomicity is often very challenging due to complex thread interactions. We present an approach to help developers verify whether such work units, which have triggered bugs due to certain violations of atomicity, are sufficiently synchronized or not by locks introduced for fixing the bugs. A key feature of our approach is that it combines the fortes of both bug-driven and change-aware techniques, which enables it to effectively verify synchronizations by testing only a minimal set of suspicious atomicity violations without any knowledge on the to-be-isolated work units, thus being more efficient and practical than other approaches. Besides, unlike existing approaches, our approach effectively utilizes all the inferred execution traces even they may not be completely feasible, such that the verification algorithm can converge much faster. We demonstrate via extensive evaluation that our approach is much more effective and efficient than the state-of-the-arts. Besides, we show that although there have existed sound automatic fixing techniques for atomicity violations, our approach is still necessary and useful for quality assurance of concurrent programs, because the assumption behind our approach is much weaker. We have also investigated one of the largest bug databases and found that insufficient synchronizations are common and difficult to be found in software development.",0098-5589;00985589,,10.1109/TSE.2015.2477820,National Basic Research Program of China; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7254228,Atomicity violations;dynamic trace analysis;fix;insufficient synchronization;maximal sound verification,Computer bugs;Instruction sets;Java;Optimization;Runtime;Schedules;Synchronization,program debugging;software quality;synchronisation,atomicity violation fixing;automatic fixing techniques;bug databases;bug-driven techniques;change-aware techniques;complex thread interactions;concurrent programs;concurrent threads;extensive evaluation;quality assurance;software development;suspicious atomicity violations;synchronization verification;to-be-isolated work units;triggered bugs;verification algorithm;work unit isolation,,1,,48,,,20150910,March 1 2016,,IEEE,IEEE Journals & Magazines,,16
G<sc>o</sc>P<sc>rime</sc>: A Fully Decentralized Middleware for Utility-Aware Service Assembly,M. Caporuscio; V. Grassi; M. Marzolla; R. Mirandola,"Department of Computer Science, Linnaeus University, V&#x00E4;xj&#x00F6;, Sweden",IEEE Transactions on Software Engineering,20160211,2016,42,2,136,152,"Modern applications, e.g., for pervasive computing scenarios, are increasingly reliant on systems built from multiple distributed components, which must be suitably composed to meet some specified functional and non-functional requirements. A key challenge is how to efficiently and effectively manage such complex systems. The use of self-management capabilities has been suggested as a possible way to address this challenge. To cope with the scalability and robustness issues of large distributed systems, self-management should ideally be architected in a decentralized way, where the overall system behavior emerges from local decisions and interactions. Within this context, we propose GOPRIME, a fully decentralized middleware solution for the adaptive self-assembly of distributed services. The GOPRIME goal is to build and maintain an assembly of services that, besides functional requirements, fulfils also global quality-of-service and structural requirements. The key aspect of GOPRIME is the use of a gossip protocol to achieve decentralized information dissemination and decision making. To show the validity of our approach, we present results from the experimentation of a prototype implementation of GOPRIME in a mobile health application, and an extensive set of simulation experiments that assess the effectiveness of GOPRIME in terms of scalability, robustness and convergence speed.",0098-5589;00985589,,10.1109/TSE.2015.2476797,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243346,Service-oriented architecture;gossip protocol;pervasive computing;quality of service;runtime adaptation,Assembly;Compounds;Middleware;Peer-to-peer computing;Quality of service;Robustness;Scalability,distributed processing;middleware;ubiquitous computing,GOPRIME;decentralized information dissemination;decision making;distributed components;fully decentralized middleware;gossip protocol;mobile health application;pervasive computing;self-management capabilities;utility-aware service assembly,,4,,30,,,20150904,Feb. 1 2016,,IEEE,IEEE Journals & Magazines,,16
<italic>SITAR</italic>: GUI Test Script Repair,Z. Gao; Z. Chen; Y. Zou; A. M. Memon,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",IEEE Transactions on Software Engineering,20160211,2016,42,2,170,186,"System testing of a GUI-based application requires that test cases, consisting of sequences of user actions/events, be executed and the software's output be verified. To enable automated re-testing, such test cases are increasingly being coded as low-level test scripts, to be replayed automatically using test harnesses. Whenever the GUI changes-widgets get moved around, windows get merged-some scripts become unusable because they no longer encode valid input sequences. Moreover, because the software's output may have changed, their test oracles-assertions and checkpoints-encoded in the scripts may no longer correctly check the intended GUI objects. We present ScrIpT repAireR (SITAR), a technique to automatically repair unusable low-level test scripts. SITAR uses reverse engineering techniques to create an abstract test for each script, maps it to an annotated event-flow graph (EFG), uses repairing transformations and human input to repair the test, and synthesizes a new _ÑÒrepaired_Ñù test script. During this process, SITAR also repairs the reference to the GUI objects used in the checkpoints yielding a final test script that can be executed automatically to validate the revised software. SITAR amortizes the cost of human intervention across multiple scripts by accumulating the human knowledge as annotations on the EFG. An experiment using QTP test scripts suggests that SITAR is effective in that 41-89 percent unusable test scripts were repaired. Annotations significantly reduced human cost after 20 percent test scripts had been repaired.",0098-5589;00985589,,10.1109/TSE.2015.2454510,National Basic Research Program of China; US National Science Foundation; 10.13039/501100001809 - NSFC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7214294,GUI test script;GUI testing;human knowledge accumulation;test script repair,Automation;Computational modeling;Electronic mail;Graphical user interfaces;Maintenance engineering;Software;Testing,graph theory;graphical user interfaces;program testing;software maintenance,EFG;GUI-based application;SITAR;automated retesting;event-flow graph;graphical user interface;software validation;system testing;test script repairer,,2,,32,,,20150820,Feb. 1 2016,,IEEE,IEEE Journals & Magazines,,16
Probabilistic Model Checking of Regenerative Concurrent Systems,M. Paolieri; A. HorvÕçth; E. Vicario,"Department of Information Engineering, Universit&#224; di Firenze, Firenze, Italy",IEEE Transactions on Software Engineering,20160211,2016,42,2,153,169,"We consider the problem of verifying quantitative reachability properties in stochastic models of concurrent activities with generally distributed durations. Models are specified as stochastic time Petri nets and checked against Boolean combinations of interval until operators imposing bounds on the probability that the marking process will satisfy a goal condition at some time in the interval [ï±, ï_] after an execution that never violates a safety property. The proposed solution is based on the analysis of regeneration points in model executions: a regeneration is encountered after a discrete event if the future evolution depends only on the current marking and not on its previous history, thus satisfying the Markov property. We analyze systems in which multiple generally distributed timers can be started or stopped independently, but regeneration points are always encountered with probability 1 after a bounded number of discrete events. Leveraging the properties of regeneration points in probability spaces of execution paths, we show that the problem can be reduced to a set of Volterra integral equations, and we provide algorithms to compute their parameters through the enumeration of finite sequences of stochastic state classes encoding the joint probability density function (PDF) of generally distributed timers after each discrete event. The computation of symbolic PDFs is limited to discrete events before the first regeneration, and the repetitive structure of the stochastic process is exploited also before the lower bound ï±, providing crucial benefits for large time bounds. A case study is presented through the probabilistic formulation of Fischer's mutual exclusion protocol, a well-known real-time verification benchmark.",0098-5589;00985589,,10.1109/TSE.2015.2468717,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202875,Markov Regenerative Process;Markov Renewal Theory;Markov regenerative process;Markov renewal theory;Probabilistic Model Checking;Probabilistic model checking;Reachability;Stochastic Petri Net;reachability;stochastic Petri net,Computational modeling;Markov processes;Numerical models;Petri nets;Probabilistic logic;Probability density function,Markov processes;Petri nets;Volterra equations;concurrency control;program verification,Boolean combinations;Fischer's mutual exclusion protocol;Markov property;Volterra integral equations;concurrent activities;discrete event;distributed timers;joint probability density function;model executions;probabilistic model checking;quantitative reachability properties;real-time verification benchmark;regeneration point analysis;regenerative concurrent systems;stochastic models;stochastic process;stochastic state classes;stochastic time Petri nets;symbolic PDFs,,5,,47,,,20150814,Feb. 1 2016,,IEEE,IEEE Journals & Magazines,,16
Automatic Source Code Summarization of Context for Java Methods,P. W. McBurney; C. McMillan,"College of Computer Science and Engineering, University Notre Dame, Notre Dame, IN",IEEE Transactions on Software Engineering,20160211,2016,42,2,103,119,"Source code summarization is the task of creating readable summaries that describe the functionality of software. Source code summarization is a critical component of documentation generation, for example as Javadocs formed from short paragraphs attached to each method in a Java program. At present, a majority of source code summarization is manual, in that the paragraphs are written by human experts. However, new automated technologies are becoming feasible. These automated techniques have been shown to be effective in select situations, though a key weakness is that they do not explain the source code's context. That is, they can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a source code summarization technique that writes English descriptions of Java methods by analyzing how those methods are invoked. We then performed two user studies to evaluate our approach. First, we compared our generated summaries to summaries written manually by experts. Then, we compared our summaries to summaries written by a state-of-the-art automatic summarization tool. We found that while our approach does not reach the quality of human-written summaries, we do improve over the state-of-the-art summarization tool in several dimensions by a statistically-significant margin.",0098-5589;00985589,,10.1109/TSE.2015.2465386,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181703,Source code summarization;automatic documentation;program comprehension,Context;Documentation;Generators;Java;Natural languages;Software;XML,Java;object-oriented methods,Java methods;Java program;Javadocs;automatic summarization tool;documentation generation;human-written summaries;software functionality;source code summarization technique;user studies,,7,,51,,,20150806,Feb. 1 2016,,IEEE,IEEE Journals & Magazines,,16
Inner Source in Platform-Based Product Engineering,D. Riehle; M. Capraro; D. Kips; L. Horn,"Computer Science Department, Friedrich-Alexander University Erlangen-N&#x00FC;rnberg, Erlangen, Germany",IEEE Transactions on Software Engineering,20161208,2016,42,12,1162,1177,"Inner source is an approach to collaboration across intra-organizational boundaries for the creation of shared reusable assets. Prior project reports on inner source suggest improved code reuse and better knowledge sharing. Using a multiple-case case study research approach, we analyze the problems that three major software development organizations were facing in their product line engineering efforts. We find that a root cause, the separation of product units as profit centers from a platform organization as a cost center, leads to delayed deliveries, increased defect rates, and redundant software components. All three organizations assume that inner source can help solve these problems. The article analyzes the expectations that these companies were having towards inner source and the problems they were experiencing in its adoption. Finally, the article presents our conclusions on how these organizations should adapt their existing engineering efforts.",0098-5589;00985589,,10.1109/TSE.2016.2554553,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452676,Inner source;case study research;open collaboration;open source;platform-based product engineering;product families;product line engineering,Best practices;Collaboration;Open source software;Product design;Software product lines,asset management;public domain software;software product lines,inner source;platform-based product line engineering;product unit separation;profit center;shared reusable asset creation;software development organization,,,,,,,20160414,Dec. 1 2016,,IEEE,IEEE Journals & Magazines,,15
Composite Constant Propagation and its Application to Android Program Analysis,D. Octeau; D. Luchaup; S. Jha; P. McDaniel,"Department of Computer Sciences, University of Wisconsin, Madison, WI",IEEE Transactions on Software Engineering,20161110,2016,42,11,999,1014,"Many program analyses require statically inferring the possible values of composite types. However, current approaches either do not account for correlations between object fields or do so in an ad hoc manner. In this paper, we introduce the problem of composite constant propagation. We develop the first generic solver that infers all possible values of complex objects in an interprocedural, flow and context-sensitive manner, taking field correlations into account. Composite constant propagation problems are specified using COAL, a declarative language. We apply our COAL solver to the problem of inferring Android Inter-Component Communication (ICC) values, which is required to understand how the components of Android applications interact. Using COAL, we model ICC objects in Android more thoroughly than the state-of-the-art. We compute ICC values for 489 applications from the Google Play store. The ICC values we infer are substantially more precise than previous work. The analysis is efficient, taking two minutes per application on average. While this work can be used as the basis for many whole-program analyses of Android applications, the COAL solver can also be used to infer the values of composite objects in many other contexts.",0098-5589;00985589,,10.1109/TSE.2016.2550446,10.13039/100000001 - National Science Foundation; 10.13039/100006785 - Google; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447806,Android application analysis;Composite constant;ICC;constant propagation;inter-component communication,Androids;Coal;Context;Correlation;Humanoid robots;Object oriented modeling;Receivers,Android (operating system);mobile computing;program diagnostics,Android applications;Android inter-component communication value;Android program analysis;COAL declarative language;Google Play Store;ICC values;composite constant propagation;whole-program analysis,,,,,,,20160405,Nov. 1 2016,,IEEE,IEEE Journals & Magazines,,15
To Be Optimal or Not in Test-Case Prioritization,D. Hao; L. Zhang; L. Zang; Y. Wang; X. Wu; T. Xie,"Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China",IEEE Transactions on Software Engineering,20160512,2016,42,5,490,505,"Software testing aims to assure the quality of software under test. To improve the efficiency of software testing, especially regression testing, test-case prioritization is proposed to schedule the execution order of test cases in software testing. Among various test-case prioritization techniques, the simple additional coverage-based technique, which is a greedy strategy, achieves surprisingly competitive empirical results. To investigate how much difference there is between the order produced by the additional technique and the optimal order in terms of coverage, we conduct a study on various empirical properties of optimal coverage-based test-case prioritization. To enable us to achieve the optimal order in acceptable time for our object programs, we formulate optimal coverage-based test-case prioritization as an integer linear programming (ILP) problem. Then we conduct an empirical study for comparing the optimal technique with the simple additional coverage-based technique. From this empirical study, the optimal technique can only slightly outperform the additional coverage-based technique with no statistically significant difference in terms of coverage, and the latter significantly outperforms the former in terms of either fault detection or execution time. As the optimal technique schedules the execution order of test cases based on their structural coverage rather than detected faults, we further implement the ideal optimal test-case prioritization technique, which schedules the execution order of test cases based on their detected faults. Taking this ideal technique as the upper bound of test-case prioritization, we conduct another empirical study for comparing the optimal technique and the simple additional technique with this ideal technique. From this empirical study, both the optimal technique and the additional technique significantly outperform the ideal technique in terms of coverage, but the latter significantly outperforms the former two techniq- es in terms of fault detection. Our findings indicate that researchers may need take cautions in pursuing the optimal techniques in test-case prioritization with intermediate goals.",0098-5589;00985589,,10.1109/TSE.2015.2496939,National 973 Program of China; 10.13039/100000001 - National Science Foundation; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314957,Empirical Study;Greedy Algorithm;Integer Linear Programming;Test-Case Prioritization;Test-case prioritization;empirical study;greedy algorithm;integer linear programming,Fault detection;Integer linear programming;Measurement;Schedules;Software;Software testing,integer programming;linear programming;program testing,ILP problem;integer linear programming;optimal coverage-based test-case prioritization;regression testing;simple additional coverage-based technique;software quality;software testing;test-case prioritization techniques,,4,,47,,,20151102,May 1 2016,,IEEE,IEEE Journals & Magazines,,15
A Probabilistic Analysis of the Efficiency of Automated Software Testing,M. BÕ_hme; S. Paul,"Software Engineering Chair, Germany",IEEE Transactions on Software Engineering,20160414,2016,42,4,345,360,"We study the relative efficiencies of the random and systematic approaches to automated software testing. Using a simple but realistic set of assumptions, we propose a general model for software testing and define sampling strategies for random (R) and systematic (S<sub>0</sub>) testing, where each sampling is associated with a sampling cost: 1 and c units of time, respectively. The two most important goals of software testing are: (i) achieving in minimal time a given degree of confidence x in a program's correctness and (ii) discovering a maximal number of errors within a given time bound nÍ_. For both (i) and (ii), we show that there exists a bound on c beyond which R performs better than S<sub>0</sub> on the average. Moreover for (i), this bound depends asymptotically only on x. We also show that the efficiency of R can be fitted to the exponential curve. Using these results we design a hybrid strategy H that starts with R and switches to S<sub>0</sub> when S<sub>0</sub> is expected to discover more errors per unit time. In our experiments we find that H performs similarly or better than the most efficient of both and that S<sub>0</sub> may need to be significantly faster than our bounds suggest to retain efficiency over R.",0098-5589;00985589,,10.1109/TSE.2015.2487274,SPECMATE; 10.13039/501100000781 - ERC; 10.13039/501100001459 - Singapore's Ministry of Education; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7289448,Efficient Testing;Error-based Partitioning;Partition Testing;Partition testing;Random Testing;Testing Theory;efficient testing;error-based partitioning;random testing;testing theory,Color;Input variables;Random variables;Software engineering;Software testing;Systematics,probability;program testing,automated software testing;exponential curve;hybrid strategy;probabilistic analysis;random testing;sampling strategies;systematic testing,,4,,37,,,20151005,April 1 2016,,IEEE,IEEE Journals & Magazines,,15
Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study,N. MellegÕ‚rd; A. Ferwerda; K. Lind; R. Heldal; M. R. V. Chaudron,"Electromobility Group at the Research Institute Viktoria Swedish ICT, Gothenburg, Sweden",IEEE Transactions on Software Engineering,20160311,2016,42,3,245,260,"Domain-specific modelling (DSM) is a modern software development technology that aims at enhancing productivity. One of the claimed advantages of DSM is increased maintainability of software. However, current empirical evidence supporting this claim is lacking. In this paper, we contribute evidence from a case study conducted at a software development company. We study how the introduction of DSM affected the maintenance of a legacy system. We collected data about the maintenance phase of a system that was initially developed using manual programming, but which was gradually replaced by DSM development. We performed statistical analyses of the relation between the use of DSM and the time needed to resolve defects, the defect density, and the phase in which defects were detected. The results show that after introducing DSM the defect density is lower, that defects are found earlier, but resolving defects takes longer. Other observed benefits are that the number of developers and the number of person-hours needed for maintaining the system decreased, and the portability to new platforms increased. Our findings are useful for organizations that consider introducing DSM and would like to know which benefits can be realized in software maintenance.",0098-5589;00985589,,10.1109/TSE.2015.2479221,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7270333,Empirical investigation;maintenance measurement;process measurement;productivity;software maintenance,Business;DSL;Maintenance engineering;Productivity;Software maintenance;Unified modeling language,software maintenance;statistical analysis,DSM;domain-specific modelling;legacy system;manual programming;software development company;software development technology;software maintainability;software maintenance;statistical analysis,,1,,42,,,20150916,March 1 2016,,IEEE,IEEE Journals & Magazines,,15
"Detecting, Tracing, and Monitoring Architectural Tactics in Code",M. Mirakhorli; J. Cleland-Huang,"Department of Software Engineering, Rochester, NY",IEEE Transactions on Software Engineering,20160311,2016,42,3,205,220,"Software architectures are often constructed through a series of design decisions. In particular, architectural tactics are selected to satisfy specific quality concerns such as reliability, performance, and security. However, the knowledge of these tactical decisions is often lost, resulting in a gradual degradation of architectural quality as developers modify the code without fully understanding the underlying architectural decisions. In this paper we present a machine learning approach for discovering and visualizing architectural tactics in code, mapping these code segments to tactic traceability patterns, and monitoring sensitive areas of the code for modification events in order to provide users with up-to-date information about underlying architectural concerns. Our approach utilizes a customized classifier which is trained using code extracted from fifty performance-centric and safety-critical open source software systems. Its performance is compared against seven off-the-shelf classifiers. In a controlled experiment all classifiers performed well; however our tactic detector outperformed the other classifiers when used within the larger context of the Hadoop Distributed File System. We further demonstrate the viability of our approach for using the automatically detected tactics to generate viable and informative messages in a simulation of maintenance events mined from Hadoop's change management system.",0098-5589;00985589,,10.1109/TSE.2015.2479217,Research Experience for Undergraduates; 10.13039/100000001 - US National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7270338,Architecture;tactics;traceability;traceability information models,Authentication;Biomedical monitoring;Detectors;Heart beat;Monitoring;Reliability,learning (artificial intelligence);pattern classification;program diagnostics;public domain software;software architecture;system monitoring,Hadoop Distributed File System;Hadoops change management system;code architectural tactics detection;code architectural tactics monitoring;code architectural tactics tracing;code architectural tactics visualization;machine learning;off-the-shelf classifiers;performance-centric software systems;safety-critical open source software systems;tactic traceability patterns,,8,,73,,,20150916,March 1 2016,,IEEE,IEEE Journals & Magazines,,15
Using Reduced Execution Flow Graph to Identify Library Functions in Binary Code,J. Qiu; X. Su; P. Ma,"School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China",IEEE Transactions on Software Engineering,20160211,2016,42,2,187,202,"Discontinuity and polymorphism of a library function create two challenges for library function identification, which is a key technique in reverse engineering. A new hybrid representation of dependence graph and control flow graph called Execution Flow Graph (EFG) is introduced to describe the semantics of binary code. Library function identification turns to be a subgraph isomorphism testing problem since the EFG of a library function instance is isomorphic to the sub-EFG of this library function. Subgraph isomorphism detection is time-consuming. Thus, we introduce a new representation called Reduced Execution Flow Graph (REFG) based on EFG to speed up the isomorphism testing. We have proved that EFGs are subgraph isomorphic as long as their corresponding REFGs are subgraph isomorphic. The high efficiency of the REFG approach in subgraph isomorphism detection comes from fewer nodes and edges in REFGs and new lossless filters for excluding the unmatched subgraphs before detection. Experimental results show that precisions of both the EFG and REFG approaches are higher than the state-of-the-art tool and the REFG approach sharply decreases the processing time of the EFG approach with consistent precision and recall.",0098-5589;00985589,,10.1109/TSE.2015.2470241,10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7210204,Reverse engineering;inline function;library function identification;static analysis;subgraph isomorphism and graph mining,Binary codes;Flow graphs;Joining processes;Libraries;Registers;Reverse engineering;Testing,binary codes;graph theory;reverse engineering;software libraries,REFG;binary code;control flow graph;dependence graph;hybrid representation;library function identification;library function instance;lossless filters;reduced execution flow graph;reverse engineering;subgraph isomorphism detection;subgraph isomorphism testing problem,,2,,29,,,20150819,Feb. 1 2016,,IEEE,IEEE Journals & Magazines,,15
Crossover Designs in Software Engineering Experiments: Benefits and Perils,S. Vegas; C. Apa; N. Juristo,"Escuela T&#233;cnica Superior de Ingenieros Inform&#225;ticos, Universidad Polit&#233;cnica de Madrid, Boadilla del Monte, Madrid, Spain",IEEE Transactions on Software Engineering,20160211,2016,42,2,120,135,"In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats.",0098-5589;00985589,,10.1109/TSE.2015.2467378,Spanish Ministry of Economy and Competitiveness research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192651,Data analysis;Experimental software engineering;carryover;controlled experiment;crossover design;crossover designs;data analysis,Animals;Atmospheric measurements;Information processing;Particle measurements;Psychology;Software engineering;US Government agencies,software engineering,SE research;crossover design;crossover experiment;software engineering experiments,,4,,38,,,20150812,Feb. 1 2016,,IEEE,IEEE Journals & Magazines,,15
Parallel Algorithms for Testing Finite State Machines:Generating UIO Sequences,R. M. Hierons; U. C. TÕ_rker,"Department of Computer Science, Brunel University London, United Kingdom",IEEE Transactions on Software Engineering,20161110,2016,42,11,1077,1091,This paper describes an efficient parallel algorithm that uses many-core GPUs for automatically deriving Unique Input Output sequences (UIOs) from Finite State Machines. The proposed algorithm uses the global scope of the GPU's global memory through coalesced memory access and minimises the transfer between CPU and GPU memory. The results of experiments indicate that the proposed method yields considerably better results compared to a single core UIO construction algorithm. Our algorithm is scalable and when multiple GPUs are added into the system the approach can handle FSMs whose size is larger than the memory available on a single GPU.,0098-5589;00985589,,10.1109/TSE.2016.2539964,10.13039/100007065 - NVIDIA; 10.13039/501100004410 - Scientific and Technological Research Council of Turkey; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429774,Software engineering/software/program verification;finite state machine;general purpose graphics processing units;software engineering/test design;software engineering/testing and debugging;unique input output sequence generation,Algorithm design and analysis;Automata;Automation;Graphics processing units;Software algorithms;Testing,finite state machines;graphics processing units;input-output programs;multiprocessing systems;parallel algorithms;program testing,UIO sequence generation;coalesced memory access;finite state machine testing;graphics processing unit;many-core GPU;memory transfer;parallel algorithms;single core UIO construction algorithm;unique input output sequences,,1,,,,,20160309,Nov. 1 2016,,IEEE,IEEE Journals & Magazines,,14
Dynamic and Automatic Feedback-Based Threshold Adaptation for Code Smell Detection,H. Liu; Q. Liu; Z. Niu; Y. Liu,"School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China",IEEE Transactions on Software Engineering,20160610,2016,42,6,544,558,"Most code smell detection tools expose thresholds to engineers for customization because code smell detection is essentially subjective and application specific. Another reason why engineers should customize these thresholds is that they have different working schedules and different requirements on software quality. They have their own unique need on precision and recall in smell detection. This unique need should be fulfilled by adjusting thresholds of smell detection tools. However, it is difficult for software engineers, especially inexperienced ones, to adjust often contradicting and related thresholds manually. One of the possible reasons is that engineers do not know the exact quantitative relation between threshold values and performance, e.g., precision. In this paper, we propose an approach to adapting thresholds automatically and dynamically. Engineers set a target precision manually according to their working schedules and quality requirements. With feedback from engineers, the proposed approach then automatically searches for a threshold setting to maximize recall while having precision close to the target precision. The proposed approach has been evaluated on open-source applications. Evaluation results suggest that the proposed approach is effective.",0098-5589;00985589,,10.1109/TSE.2015.2503740,Beijing Higher Education Young Elite Teacher Project; Ministry of Science and Technology of China; National Strategic Basic Research Program (ëè973ëè Program); The 111 Project of Beijing Institute of Technology; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004602 - Program for New Century Excellent Talents in University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7337457,Code Smells;Feedback Control;Smell Identification;Software Refactoring;Software refactoring;code smells;feedback control;smell identification,Algorithm design and analysis;Cloning;Detection algorithms;Genetic algorithms;Measurement;Schedules;Software,software maintenance;software quality,code smell detection;feedback-based threshold adaptation;open-source applications;software engineering;software quality,,1,,56,,,20151125,June 1 2016,,IEEE,IEEE Journals & Magazines,,14
Finding and Evaluating the Performance Impact of Redundant Data Access for Applications that are Developed Using Object-Relational Mapping Frameworks,T. H. Chen; W. Shang; Z. M. Jiang; A. E. Hassan; M. Nasser; P. Flora,"Software Analysis and Intelligence Lab, School of Computing, Queen&#x0027;s University, Kingston, ON, Canada",IEEE Transactions on Software Engineering,20161208,2016,42,12,1148,1161,"Developers usually leverage Object-Relational Mapping (ORM) to abstract complex database accesses for large-scale systems. However, since ORM frameworks operate at a lower-level (i.e., data access), ORM frameworks do not know how the data will be used when returned from database management systems (DBMSs). Therefore, ORM cannot provide an optimal data retrieval approach for all applications, which may result in accessing redundant data and significantly affect system performance. Although ORM frameworks provide ways to resolve redundant data problems, due to the complexity of modern systems, developers may not be able to locate such problems in the code; hence, may not proactively resolve the problems. In this paper, we propose an automated approach, which we implement as a Java framework, to locate redundant data problems. We apply our framework on one enterprise and two open source systems. We find that redundant data problems exist in 87 percent of the exercised transactions. Due to the large number of detected redundant data problems, we propose an automated approach to assess the impact and prioritize the resolution efforts. Our performance assessment result shows that by resolving the redundant data problems, the system response time for the studied systems can be improved by an average of 17 percent.",0098-5589;00985589,,10.1109/TSE.2016.2553039,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451264,Performance;database;object-relational mapping (ORM);program analysis,Complexity theory;Computer bugs;Databases;Java;Object tracking;System performance;Time factors,Java;database management systems;program diagnostics;public domain software;software performance evaluation,DBMS;Java framework;ORM framework;database abstraction;database management system;object-relational mapping framework;open source system;performance impact evaluation;program analysis;redundant data access,,2,,,,,20160412,Dec. 1 2016,,IEEE,IEEE Journals & Magazines,,13
Automatically Recommending Peer Reviewers in Modern Code Review,M. B. Zanjani; H. Kagdi; C. Bird,"Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, Kansas",IEEE Transactions on Software Engineering,20160610,2016,42,6,530,543,"Code review is an important part of the software development process. Recently, many open source projects have begun practicing code review through _ÑÒmodern_Ñù tools such as GitHub pull-requests and Gerrit. Many commercial software companies use similar tools for code review internally. These tools enable the owner of a source code change to request individuals to participate in the review, i.e., reviewers. However, this task comes with a challenge. Prior work has shown that the benefits of code review are dependent upon the expertise of the reviewers involved. Thus, a common problem faced by authors of source code changes is that of identifying the best reviewers for their source code change. To address this problem, we present an approach, namely cHRev, to automatically recommend reviewers who are best suited to participate in a given review, based on their historical contributions as demonstrated in their prior reviews. We evaluate the effectiveness of cHRev on three open source systems as well as a commercial codebase at Microsoft and compare it to the state of the art in reviewer recommendation. We show that by leveraging the specific information in previously completed reviews (i.e.,quantification of review comments and their recency), we are able to improve dramatically on the performance of prior approaches, which (limitedly) operate on generic review information (i.e., reviewers of similar source code file and path names) or source coderepository data. We also present the insights into why our approach cHRev outperforms the existing approaches.",0098-5589;00985589,,10.1109/TSE.2015.2500238,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328331,Gerrit;Modern code review;code change;reviewer recommendation,Androids;Birds;Electronic mail;History;Humanoid robots;Inspection;Software,software engineering;software reviews,Gerrit tool;GitHub pull-requests tool;cHRev approach;code review;commercial codebase;open source projects;peer reviewer recommendation;software development process;source code change,,3,,55,,,20151112,June 1 2016,,IEEE,IEEE Journals & Magazines,,13
Targeted Scrum: Applying Mission Command to Agile Software Development,D. P. Harvie; A. Agah,"Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS",IEEE Transactions on Software Engineering,20160512,2016,42,5,476,489,"Software engineering and mission command are two separate but similar fields, as both are instances of complex problem solving in environments with ever changing requirements. Our research hypothesis is that modifications to agile software development based on inspirations from mission command can improve the software engineering process in terms of planning, prioritizing, and communication of software requirements and progress, as well as improving the overall software product. Targeted Scrum is a modification of Traditional Scrum based on three inspirations from Mission Command: End State, Line of Effort, and Targeting. These inspirations have led to the introduction of the Product Design Meeting and modifications of some current Scrum meetings and artifacts. We tested our research hypothesis using a semester-long undergraduate level software engineering class. Students developed two software projects, one using Traditional Scrum and the other using Targeted Scrum. We then assessed how well both methodologies assisted the software development teams in planning and developing the software architecture, prioritizing requirements, and communicating progress. We also evaluated the software product produced by both methodologies. We found that Targeted Scrum did better in assisting the software development teams in the planning and prioritization of the requirements. However, Targeted Scrum had a negligible effect on improving the software development teams external and internal communications. Finally, Targeted Scrum did not have an impact on the product quality by the top performing and worst performing teams. Targeted Scrum did assist the product quality of the teams in the middle of the performance spectrum.",0098-5589;00985589,,10.1109/TSE.2015.2489654,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296686,"Agile;Empirical Software Engineering;Line of Effort;Product Design Meeting;Scrum;Scrum, Mission Command;agile;empirical software engineering;line of effort;mission command;product design meeting",Force;Planning;Product design;Scrum (Software development);Software;Software engineering,software architecture;software prototyping,agile software development;end state;line of effort;mission command;performance spectrum;product design meeting;product quality;software architecture;software engineering;software product improvement;software projects;software requirements;targeted Scrum;targeting;traditional Scrum,,2,,34,,,20151012,May 1 2016,,IEEE,IEEE Journals & Magazines,,13
The Role of Method Chains and Comments in Software Readability and Comprehension_ÑÓAn Experiment,J. BÕ_rstler; B. Paech,"Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden",IEEE Transactions on Software Engineering,20160915,2016,42,9,886,898,"Software readability and comprehension are important factors in software maintenance. There is a large body of research on software measurement, but the actual factors that make software easier to read or easier to comprehend are not well understood. In the present study, we investigate the role of method chains and code comments in software readability and comprehension. Our analysis comprises data from 104 students with varying programming experience. Readability and comprehension were measured by perceived readability, reading time and performance on a simple cloze test. Regarding perceived readability, our results show statistically significant differences between comment variants, but not between method chain variants. Regarding comprehension, there are no significant differences between method chain or comment variants. Student groups with low and high experience, respectively, show significant differences in perceived readability and performance on the cloze tests. Our results do not show any significant relationships between perceived readability and the other measures taken in the present study. Perceived readability might therefore be insufficient as the sole measure of software readability or comprehension. We also did not find any statistically significant relationships between size and perceived readability, reading time and comprehension.",0098-5589;00985589,,10.1109/TSE.2016.2527791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404062,Software readability;comments;experiment;method chains;software comprehension;software measurement,Complexity theory;Guidelines;Object oriented modeling;Programming;Software;Software engineering;Software measurement,software maintenance;software metrics,cloze tests;code comments;method chains;software comprehension;software maintenance;software measurement;software readability,,1,,,,,20160211,Sept. 1 2016,,IEEE,IEEE Journals & Magazines,,12
A Taxonomy and Qualitative Comparison of Program Analysis Techniques for Security Assessment of Android Software,A. Sadeghi; H. Bagheri; J. Garcia; S. Malek,"School of Information and Computer Sciences, University of California, Irvine, CA",IEEE Transactions on Software Engineering,20170613,2017,43,6,492,530,"In parallel with the meteoric rise of mobile software, we are witnessing an alarming escalation in the number and sophistication of the security threats targeted at mobile platforms, particularly Android, as the dominant platform. While existing research has made significant progress towards detection and mitigation of Android security, gaps and challenges remain. This paper contributes a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area. We have carefully followed the systematic literature review process, and analyzed the results of more than 300 research papers, resulting in the most comprehensive and elaborate investigation of the literature in this area of research. The systematic analysis of the research literature has revealed patterns, trends, and gaps in the existing literature, and underlined key challenges and opportunities that will shape the focus of future research efforts.",0098-5589;00985589,,10.1109/TSE.2016.2615307,10.13039/100000001 - National Science Foundation; 10.13039/100000180 - Department of Homeland Security; 10.13039/100000181 - Air Force Office of Scientific Research; 10.13039/100000183 - Army Research Office; 10.13039/100000185 - Defense Advanced Research Projects Agency; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583740,Taxonomy and survey;android platform;program analysis;security assessment,Androids;Humanoid robots;Malware;Mobile communication;Security;Systematics;Taxonomy,Android (operating system);mobile computing;program diagnostics;security of data,Android security;Android software;dominant platform;mobile platforms;mobile software;program analysis;security assessment;security threats;taxonomy,,,,,,,20161005,June 1 2017,,IEEE,IEEE Journals & Magazines,,38
A Survey of App Store Analysis for Software Engineering,W. Martin; F. Sarro; Y. Jia; Y. Zhang; M. Harman,"Department of Computer Science, University College London, London, United Kingdom",IEEE Transactions on Software Engineering,20170915,2017,43,9,817,847,"App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.",0098-5589;00985589,,10.1109/TSE.2016.2630689,EPRSC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765038,API;App store;analysis;ecosystem;feature;mining;release planning;requirements engineering;reviews;security,Ecosystems;Google;Market research;Mobile communication;Security;Software;Software engineering,program testing;software engineering,app store analysis;release planning;requirements engineering;software deployment;software design;software development;software engineering;software repositories;software security;software testing,,4,,,,,20161202,Sept. 1 2017,,IEEE,IEEE Journals & Magazines,,30
Static Analysis of Model Transformations,J. S. Cuadrado; E. Guerra; J. de Lara,"Department of Languages and Systems, Universidad de Murcia, Murcia, Spain",IEEE Transactions on Software Engineering,20170915,2017,43,9,868,897,"Model transformations are central to Model-Driven Engineering (MDE), where they are used to transform models between different languages; to refactor and simulate models; or to generate code from models. Thus, given their prominent role in MDE, practical methods helping in detecting errors in transformations and automate their verification are needed. In this paper, we present a method for the static analysis of ATL model transformations. The method aims at discovering typing and rule errors, like unresolved bindings, uninitialized features or rule conflicts. It relies on static analysis and type inference, and uses constraint solving to assert whether a source model triggering the execution of a given problematic statement can possibly exist. Our method is supported by a tool that integrates seamlessly with the ATL development environment. To evaluate the usefulness of our method, we have used it to analyse a public repository of ATL transformations. The high number of errors discovered shows that static analysis of ATL transformations is needed in practice. Moreover, we have measured the precision and recall of the method by considering a synthetic set of transformations obtained by mutation techniques, and comparing with random testing. The experiment shows good overall results in terms of false positives and negatives.",0098-5589;00985589,,10.1109/TSE.2016.2635137,R&D programme of the Madrid Region; 10.13039/501100003329 - Spanish MINECO; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765073,ATL;Model-driven engineering;model finders;model transformation;static analysis;verification and testing,Analytical models;Computational modeling;Manuals;Model driven engineering;Testing;Transforms;Unified modeling language,constraint handling;error detection;formal verification;inference mechanisms;program diagnostics;random functions,ATL model transformations;MDE;constraint solving;error detection;model-driven engineering;mutation techniques;public repository analysis;random testing;rule conflicts;rule errors;static analysis;type inference;typing errors;uninitialized features;unresolved bindings;verification automation,,1,,,,,20161202,Sept. 1 2017,,IEEE,IEEE Journals & Magazines,,29
Automated Extraction and Clustering of Requirements Glossary Terms,C. Arora; M. Sabetzadeh; L. Briand; F. Zimmer,"SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg",IEEE Transactions on Software Engineering,20171013,2017,43,10,918,945,"A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it clusters the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need for any user-specified parameters. To evaluate our approach, we report on three industrial case studies, as part of which we also examine the perceptions of the involved subject matter experts about the usefulness of our approach. Our evaluation notably suggests that: (1) Over requirements documents, our approach is more accurate than major generic term extraction tools. Specifically, in our case studies, our approach leads to gains of 20 percent or more in terms of recall when compared to existing tools, while at the same time either improving precision or leaving it virtually unchanged. And, (2) the experts involved in our case studies find the clusters generated by our approach useful as an aid for glossary construction.",0098-5589;00985589,,10.1109/TSE.2016.2635134,European Union&#8217;s Horizon 2020 research and innovation program; 10.13039/501100000781 - European Research Council; 10.13039/501100001866 - Luxembourg&#8217;s National Research Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765062,Requirements glossaries;case study research;clustering;natural language processing;term extraction,Clustering algorithms;Monitoring;Natural languages;Pipelines;Servers;Software;Terminology,natural language processing;pattern clustering;text analysis,automated approach;candidate glossary terms;clustering algorithm;generic term extraction tools;glossary construction;mathematically-based procedure;natural language requirements documents;requirements glossary terms;software requirements document;technical terms;user-specified parameters,,,,,,,20161202,Oct. 1 2017,,IEEE,IEEE Journals & Magazines,,27
When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away),M. Tufano; F. Palomba; G. Bavota; R. Oliveto; M. D. Penta; A. De Lucia; D. Poshyvanyk,"College of William and Mary, Williamsburg, VA",IEEE Transactions on Software Engineering,20171110,2017,43,11,1063,1088,"Technical debt is a metaphor introduced by Cunningham to indicate _ÑÒnot quite right code which we postpone making it right_Ñù. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80 percent of smells survive in the system. Also, among the 20 percent of removed instances, only 9 percent are removed as a direct consequence of refactoring operations.",0098-5589;00985589,,10.1109/TSE.2017.2653105,10.13039/100000001 - NSF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7817894,Code smells;empirical study;mining software repositories,Androids;Ecosystems;History;Humanoid robots;Maintenance engineering;Software systems,data mining;public domain software;software maintenance;software quality;source code (software),code comprehensibility;code maintainability;code quality;code smells;commits mining;open source projects;smell instances;smell-introducing commits;technical debt,,1,,,,,20170116,Nov. 1 2017,,IEEE,IEEE Journals & Magazines,,25
Supporting Change Impact Analysis Using a Recommendation System: An Industrial Case Study in a Safety-Critical Context,M. Borg; K. Wnuk; B. Regnell; P. Runeson,"SICS Swedish ICT AB, Ideon Science Park, Building Beta 2, Scheelev&#x00E4;gen 17, Lund, Sweden",IEEE Transactions on Software Engineering,20170714,2017,43,7,675,700,"Change Impact Analysis (CIA) during software evolution of safety-critical systems is a labor-intensive task. Several authors have proposed tool support for CIA, but very few tools were evaluated in industry. We present a case study on ImpRec, a recommendation System for Software Engineering (RSSE), tailored for CIA at a process automation company. ImpRec builds on assisted tracing, using information retrieval solutions and mining software repositories to recommend development artifacts, potentially impacted when resolving incoming issue reports. In contrast to the majority of tools for automated CIA, ImpRec explicitly targets development artifacts that are not source code. We evaluate ImpRec in a two-phase study. First, we measure the correctness of ImpRec's recommendations by a simulation based on 12 years' worth of issue reports in the company. Second, we assess the utility of working with ImpRec by deploying the RSSE in two development teams on different continents. The results suggest that ImpRec presents about 40 percent of the true impact among the top-10 recommendations. Furthermore, user log analysis indicates that ImpRec can support CIA in industry, and developers acknowledge the value of ImpRec in interviews. In conclusion, our findings show the potential of reusing traceability associated with developers' past activities in an RSSE.",0098-5589;00985589,,10.1109/TSE.2016.2620458,Embedded Applications Software Engineering; ORION; 10.13039/100003077 - Knowledge Foundation; 10.13039/501100003252 - Lund University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7637029,Case;maintenance management;software and system safety;tracing,Automation;Context;Industries;Software engineering;Software systems;Unified modeling language,program diagnostics;recommender systems;safety-critical software;software engineering,CIA;ImpRec;RSSE;change impact analysis;information retrieval solution;recommendation system for software engineering;safety-critical context;safety-critical system;software evolution;software repository mining,,1,,,,,20161024,July 1 2017,,IEEE,IEEE Journals & Magazines,,25
A Feature-Based Classification of Model Repair Approaches,N. Macedo; T. Jorge; A. Cunha,High-Assurance Software Laboratory (HASLab)INESC TEC,IEEE Transactions on Software Engineering,20170714,2017,43,7,615,640,"Consistency management, the ability to detect, diagnose and handle inconsistencies, is crucial during the development process in Model-driven Engineering (MDE). As the popularity and application scenarios of MDE expanded, a variety of different techniques were proposed to address these tasks in specific contexts. Of the various stages of consistency management, this work focuses on inconsistency handling in MDE, particularly in model repair techniques. This paper proposes a feature-based classification system for model repair techniques, based on an systematic literature review of the area. We expect this work to assist developers and researchers from different disciplines in comparing their work under a unifying framework, and aid MDE practitioners in selecting suitable model repair approaches.",0098-5589;00985589,,10.1109/TSE.2016.2620145,North Portugal Regional Operational Programme; 10.13039/501100008530 - European Regional Development Fund (ERDF); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7605502,"Model-driven engineering, consistency management, inconsistency handling, model repair",Context;Feature extraction;Maintenance engineering;Software engineering;Systematics;Taxonomy;Unified modeling language,pattern classification;software maintenance,MDE;consistency management;feature-based classification system;model repair approach;model-driven engineering,,,,,,,20161021,July 1 2017,,IEEE,IEEE Journals & Magazines,,25
CACheck: Detecting and Repairing Cell Arrays in Spreadsheets,W. Dou; C. Xu; S. C. Cheung; J. Wei,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Software Engineering,20170314,2017,43,3,226,251,"Spreadsheets are widely used by end users for numerical computation in their business. Spreadsheet cells whose computation is subject to the same semantics are often clustered in a row or column as a cell array. When a spreadsheet evolves, the cells in a cell array can degenerate due to ad hoc modifications. Such degenerated cell arrays no longer keep cells prescribing the same computational semantics, and are said to exhibit ambiguous computation smells. We propose CACheck, a novel technique that automatically detects and repairs smelly cell arrays by recovering their intended computational semantics. Our empirical study on the EUSES and Enron corpora finds that such smelly cell arrays are common. Our study also suggests that CACheck is useful for detecting and repairing real spreadsheet problems caused by smelly cell arrays. Compared with our previous work AmCheck, CACheck detects smelly cell arrays with higher precision and recall rate.",0098-5589;00985589,,10.1109/TSE.2016.2584059,Collaborative Innovation Center of Novel Software Technology and Industrialization of China; General Research Fund; National Key Research and Development Plan; 10.13039/501100001809 - National Natural Science Foundation; 10.13039/501100002920 - Research Grants Council; 10.13039/501100004826 - Beijing Natural Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498607,Spreadsheet;ambiguous computation smell;cell array,Business;Computer science;Electronic mail;Maintenance engineering;Nonhomogeneous media;Semantics;Software,software engineering;spreadsheet programs,CACheck;EUSES corpora;Enron corpora;ad hoc modifications;numerical computation;smelly cell arrays;spreadsheets,,2,,,,,20160623,March 1 2017,,IEEE,IEEE Journals & Magazines,,25
Improving Automated Bug Triaging with Specialized Topic Model,X. Xia; D. Lo; Y. Ding; J. M. Al-Kofahi; T. N. Nguyen; X. Wang,"College of Computer Science and Technology, Zhejiang University, Hangzhou, China",IEEE Transactions on Software Engineering,20170313,2017,43,3,272,297,"Bug triaging refers to the process of assigning a bug to the most appropriate developer to fix. It becomes more and more difficult and complicated as the size of software and the number of developers increase. In this paper, we propose a new framework for bug triaging, which maps the words in the bug reports (i.e., the term space) to their corresponding topics (i.e., the topic space). We propose a specialized topic modeling algorithm named <italic> multi-feature topic model (MTM)</italic> which extends Latent Dirichlet Allocation (LDA) for bug triaging. <italic>MTM </italic> considers product and component information of bug reports to map the term space to the topic space. Finally, we propose an incremental learning method named <italic>TopicMiner</italic> which considers the topic distribution of a new bug report to assign an appropriate fixer based on the affinity of the fixer to the topics. We pair <italic> TopicMiner</italic> with MTM (<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math> <alternatives><inline-graphic xlink:href=""xia-ieq1-2576454.gif""/></alternatives></inline-formula></italic>). We have evaluated our solution on 5 large bug report datasets including GCC, OpenOffice, Mozilla, Netbeans, and Eclipse containing a total of 227,278 bug reports. We show that <italic>TopicMiner<inline-formula><tex-math notation=""LaTeX""> $^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq2-2576454.gif""/></alternatives></inline-formula> </italic> can achieve top-1 and top-5 prediction accuracies of 0.4831-0.6868, and 0.7686-0.9084, respectively. We also compare <italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives> <inline-graphic xlink:href=""xia-ieq3-2576454.gif""/></alternatives></inline-formula></italic> with Bugzie, LDA-KL, SVM-LDA, LDA-Activity, and Yang etë_al.'s approach. The results show that <italic>TopicMiner<inline-formula> <tex-math notatio- =""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq4-2576454.gif""/> </alternatives></inline-formula></italic> on average improves top-1 and top-5 prediction accuracies of Bugzie by 128.48 and 53.22 percent, LDA-KL by 262.91 and 105.97 percent, SVM-LDA by 205.89 and 110.48 percent, LDA-Activity by 377.60 and 176.32 percent, and Yang etë_al.'s approach by 59.88 and 13.70 percent, respectively.",0098-5589;00985589,,10.1109/TSE.2016.2576454,Ministry of Science and Technology of China; National Basic Research Program of China (the 973 Program); National Key Technology R&D Program; 10.13039/501100001809 - NSFC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484672,Developer;bug triaging;feature information;topic model,Computer bugs;Indexes;Learning systems;Resource management;Software;Software algorithms;Support vector machines,,,,4,,,,,20160607,March 1 2017,,IEEE,IEEE Journals & Magazines,,25
Automated Steering of Model-Based Test Oracles to Admit Real Program Behaviors,G. Gay; S. Rayadurgam; M. P. E. Heimdahl,"Department of Computer Science & Engineering, University of South Carolina, Columbia, SC",IEEE Transactions on Software Engineering,20170612,2017,43,6,531,555,"The test oracle-a judge of the correctness of the system under test (SUT)-is a major component of the testing process. Specifying test oracles is challenging for some domains, such as real-time embedded systems, where small changes in timing or sensory input may cause large behavioral differences. Models of such systems, often built for analysis and simulation, are appealing for reuse as test oracles. These models, however, typically represent an idealized system, abstracting away certain issues such as non-deterministic timing behavior and sensor noise. Thus, even with the same inputs, the model's behavior may fail to match an acceptable behavior of the SUT, leading to many false positives reported by the test oracle. We propose an automated steering framework that can adjust the behavior of the model to better match the behavior of the SUT to reduce the rate of false positives. This model steering is limited by a set of constraints (defining the differences in behavior that are acceptable) and is based on a search process attempting to minimize a dissimilarity metric. This framework allows non-deterministic, but bounded, behavioral differences, while preventing future mismatches by guiding the oracle-within limits-to match the execution of the SUT. Results show that steering significantly increases SUT-oracle conformance with minimal masking of real faults and, thus, has significant potential for reducing false positives and, consequently, testing and debugging costs while improving the quality of the testing process.",0098-5589;00985589,,10.1109/TSE.2016.2615311,10.13039/100000001 - US National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583721,Software testing;model-based development;model-based testing;test oracles;verification,Analytical models;Computational modeling;Delays;Hardware;Pacemakers;Software;Testing,program debugging;program testing;program verification,SUT correctness;automated steering;debugging costs;fault masking;model steering;model-based test oracles;real program behaviors;search process;system under test correctness;testing process quality,,,,,,,20161005,June 1 2017,,IEEE,IEEE Journals & Magazines,,24
Model Transformation Modularization as a Many-Objective Optimization Problem,M. Fleck; J. Troya; M. Kessentini; M. Wimmer; B. Alkhazi,"TU Wien, Wien, Austria",IEEE Transactions on Software Engineering,20171110,2017,43,11,1009,1032,"Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical anal- sis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.",0098-5589;00985589,,10.1109/TSE.2017.2654255,Andalusian Government project COPAS; CICYT project BELI; Ford Alliance Program; SEBASE; Spanish Government; 10.13039/100002427 - Ford Motor Company; 10.13039/501100000780 - European Commission (FEDER); 10.13039/501100003413 - BMWFW; 10.13039/501100006012 - Christian Doppler Forschungsgesellschaft; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820199,ATL;MDE;Model transformation;NSGA-III;SBSE;modularization,Adaptation models;Algorithm design and analysis;Computer bugs;Measurement;Object oriented modeling;Software engineering;Unified modeling language,genetic algorithms;program debugging;search problems;software maintenance;software quality,ATL transformations;NSGA-III;automated search-based approach;bug fixing;concrete transformation language;higher-order transformations;in-place transformation engine;maintenance activities;many-objective optimization problem;metamodels version;model transformation modularization;model transformation programs;model transformations modularization;monolithic artifacts;statistical analysis;transformation languages;transformation rules,,,,,,,20170117,Nov. 1 2017,,IEEE,IEEE Journals & Magazines,,23
GK-Tail+ An Efficient Approach to Learn Software Models,L. Mariani; M. PezzÕŒ; M. Santoro,"Department of Informatics, Systems and Communication, University of Milano Bicocca, Milano, Italy",IEEE Transactions on Software Engineering,20170811,2017,43,8,715,738,"Inferring models of program behavior from execution samples can provide useful information about a system, also in the increasingly common case of systems that evolve and adapt in their lifetime, and without requiring large developers' effort. Techniques for learning models of program behavior from execution traces shall address conflicting challenges of recall, specificity and performance: They shall generate models that comprehensively represent the system behavior (recall) while limiting the amount of illegal behaviors that may be erroneously accepted by the model (specificity), and should infer the models within a reasonable time budget to process industrial scale systems (performance). In our early work, we designed GK-tail, an approach that can infer guarded finite state machines that model the behavior of object-oriented programs in terms of sequences of method calls and constraints on the parameter values. GK-tail addresses well two of the three main challenges, since it infers guarded finite state machines with a high level of recall and specificity, but presents severe limitations in terms of performance that reduce its scalability. In this paper, we present GK-tail+, a new approach to infer guarded finite state machines from execution traces of object-oriented programs. GK-tail+ proposes a new set of inference criteria that represent the core element of the inference process: It largely reduces the inference time of GK-tail while producing guarded finite state machines with a comparable level of recall and specificity. Thus, GK-tail+ advances the preliminary results of GK-tail by addressing all the three main challenges of learning models of program behavior from execution traces.",0098-5589;00985589,,10.1109/TSE.2016.2623623,ASysT: Automatic System Testing; Swiss National Foundation; The H2020 Learn project; 10.13039/501100000781 - ERC; 10.13039/501100000781 - ERC Consolidator Grant 2014 program; 10.13039/501100004343 - SNF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728088,Dynamic model learning;guarded finite state machines;software models;specification mining;state based models,Adaptation models;Analytical models;Limiting;Merging;Object oriented modeling;Software systems,finite state machines;learning (artificial intelligence);object-oriented programming,GK-Tail+;execution traces;guarded finite state machines;illegal behaviors;industrial scale systems;inference criteria;inference process;inference time;inferring models;learning models;object-oriented programs;program behavior;reasonable time budget;software models;system behavior,,,,,,,20161101,Aug. 1 2017,,IEEE,IEEE Journals & Magazines,,23
Online Reliability Prediction via Motifs-Based Dynamic Bayesian Networks for Service-Oriented Systems,H. Wang; L. Wang; Q. Yu; Z. Zheng; A. Bouguettaya; M. R. Lyu,"School of Computer Science and Engineering and Key Laboratory of Computer Network and Information Integration, Southeast University, SIPAILOU 2, Nanjing, China",IEEE Transactions on Software Engineering,20170612,2017,43,6,556,579,"A service-oriented System of Systems (SoS) considers a system as a service and constructs a robust and value-added SoS by outsourcing external component systems through service composition techniques. Online reliability prediction for the component systems for the purpose of assuring the overall Quality of Service (QoS) is often a major challenge in coping with a loosely coupled SoS operating under dynamic and uncertain running environments. It is also a prerequisite for guaranteeing runtime QoS of a SoS through optimal service selection for reliable system construction. We propose a novel online reliability time series prediction approach for the component systems in a service-oriented SoS. We utilize Probabilistic Graphical Models (PGMs) to yield near-future, time series predictions. We assess the approach via invocation records collected from widely used real Web services. Experimental results have confirmed the effectiveness of the approach.",0098-5589;00985589,,10.1109/TSE.2016.2615615,Novel Software Technology and Industrialization and Wireless Communications Technology; 10.13039/501100000923 - Australian Research Councilëèëèëès; 10.13039/501100001809 - NSFC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585067,Online reliability prediction;service-oriented computing;system of systems;time series,Quality of service;Software reliability;Throughput;Time factors;Time series analysis;Web services,belief networks;service-oriented architecture,PGM;QoS;component systems;invocation records;motifs-based dynamic Bayesian networks;online reliability prediction;online reliability time series prediction;optimal service selection;probabilistic graphical models;quality of service;real Web services;reliable system construction;service composition;service-oriented SoS;service-oriented system of systems;time series predictions;uncertain running environments,,,,,,,20161006,June 1 2017,,IEEE,IEEE Journals & Magazines,,23
Test Oracle Strategies for Model-Based Testing,N. Li; J. Offutt,"Research and Development Division, Medidata Solutions, New York, NY",IEEE Transactions on Software Engineering,20170414,2017,43,4,372,395,"Testers use model-based testing to design abstract tests from models of the system's behavior. Testers instantiate the abstract tests into concrete tests with test input values and test oracles that check the results. Given the same test inputs, more elaborate test oracles have the potential to reveal more failures, but may also be more costly. This research investigates the ability for test oracles to reveal failures. We define ten new test oracle strategies that vary in amount and frequency of program state checked. We empirically compared them with two baseline test oracle strategies. The paper presents several main findings. (1) Test oracles must check more than runtime exceptions because checking exceptions alone is not effective at revealing failures. (2) Test oracles do not need to check the entire output state because checking partial states reveals nearly as many failures as checking entire states. (3) Test oracles do not need to check program states multiple times because checking states less frequently is as effective as checking states more frequently. In general, when state machine diagrams are used to generate tests, checking state invariants is a reasonably effective low cost approach to creating test oracles.",0098-5589;00985589,,10.1109/TSE.2016.2597136,10.13039/100006369 - George Mason University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529115,RIPR model;Test oracle;model-based testing;subsumption;test automation;test oracle strategy,Concrete;Context;Observability;Software;System testing;Unified modeling language,data flow analysis;diagrams;finite state machines;program testing;software fault tolerance,abstract test design;failure revelation;model-based testing;partial state checking;runtime exception;state machine diagram;test oracle strategy,,2,,,,,20160802,April 1 2017,,IEEE,IEEE Journals & Magazines,,23
Timed Automata Modeling and Verification for Publish-Subscribe Structures Using Distributed Resources,V. Valero; G. DÕ_az; M. E. Cambronero,"Department of Computer Science, University of Castilla-La Mancha, Albacete, Spain",IEEE Transactions on Software Engineering,20170109,2017,43,1,76,99,"In this paper we present a Timed Automata model for the Publish/Subscribe paradigm in the context of Web Service Compositions with distributed resources, on the basis of an algebraic language inspired by the WSRF standard constructions. This framework allows a set of participants in a Web Service composition to interact with one another and also to manage a collection of distributed resources. The model includes operations for clients to publish, discover and subscribe to resources, so as to be notified when the resource property values fulfill certain conditions (topic-based subscription). Simulation and model-checking techniques can therefore be applied to the obtained network of timed automata, in order to check whether certain properties of interest are satisfied. A specific case study is finally presented to illustrate the model and the verification of the relevant properties on the obtained timed automata model.",0098-5589;00985589,,10.1109/TSE.2016.2560842,Spanish Government; 10.13039/501100002924 - FEDER; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463051,Publish/subscribe;formal modeling;model checking;timed automata;verification,Automata;Clocks;Context modeling;Probabilistic logic;Semantics;Unified modeling language;Web services,Web services;algebra;automata theory;formal verification;message passing;middleware;resource allocation,WSRF standard constructions;Web service compositions;algebraic language;model checking;publish-subscribe paradigm;publish-subscribe structures;resource distribution;resource property values;timed automata modeling;timed automata verification;topic-based subscription,,1,,,,,20160429,Jan. 1 2017,,IEEE,IEEE Journals & Magazines,,23
Self-Adaptive and Online QoS Modeling for Cloud-Based Software Services,T. Chen; R. Bahsoon,"CERCIA, School of Computer Science, University of Birmingham, Birmingham, United Kingdom",IEEE Transactions on Software Engineering,20170512,2017,43,5,453,475,"In the presence of scale, dynamism, uncertainty and elasticity, cloud software engineers faces several challenges when modeling Quality of Service (QoS) for cloud-based software services. These challenges can be best managed through self-adaptivity because engineers' intervention is difficult, if not impossible, given the dynamic and uncertain QoS sensitivity to the environment and control knobs in the cloud. This is especially true for the shared infrastructure of cloud, where unexpected interference can be caused by co-located software services running on the same virtual machine; and co-hosted virtual machines within the same physical machine. In this paper, we describe the related challenges and present a fully dynamic, self-adaptive and online QoS modeling approach, which grounds on sound information theory and machine learning algorithms, to create QoS model that is capable to predict the QoS value as output over time by using the information on environmental conditions, control knobs and interference as inputs. In particular, we report on in-depth analysis on the correlations of selected inputs to the accuracy of QoS model in cloud. To dynamically selects inputs to the models at runtime and tune accuracy, we design self-adaptive hybrid dual-learners that partition the possible inputs space into two sub-spaces, each of which applies different symmetric uncertainty based selection techniques; the results of sub-spaces are then combined. Subsequently, we propose the use of adaptive multi-learners for building the model. These learners simultaneously allow several learning algorithms to model the QoS function, permitting the capability for dynamically selecting the best model for prediction on the fly. We experimentally evaluate our models in the cloud environment using RUBiS benchmark and realistic FIFA 98 workload. The results show that our approach is more accurate and effective than state-of-the-art modelings.",0098-5589;00985589,,10.1109/TSE.2016.2608826,DAASE: Dynamic Adaptive Automated Software Engineering; The PhD scholarship from the School of Computer Science; 10.13039/501100000266 - EPSRC Grant; 10.13039/501100000855 - University of Birmingham; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572219,Software quality;cloud computing;machine learning;performance modeling;search-based software engineering;self-adaptive systems,Adaptation models;Cloud computing;Interference;Quality of service;Sensitivity;Uncertainty,cloud computing;learning (artificial intelligence);quality of service;virtual machines,FIFA 98 workload;QoS function;RUBiS benchmark;cloud environment;cloud infrastructure;cloud-based software services;colocated software service;control knobs;environmental conditions;fully dynamic self-adaptive online QoS modeling;information theory;input space partitioning;machine learning algorithm;quality of service;self-adaptive hybrid dual-learners;self-adaptivity;symmetric uncertainty based selection technique;uncertain QoS sensitivity;virtual machine,,,,,,,20160920,May 1 2017,,IEEE,IEEE Journals & Magazines,,22
An Enhanced Bailout Protocol for Mixed Criticality Embedded Software,I. Bate; A. Burns; R. I. Davis,"Department of Computer Science, University of York, York, United Kingdom",IEEE Transactions on Software Engineering,20170414,2017,43,4,298,320,"To move mixed criticality research into industrial practice requires models whose run-time behaviour is acceptable to systems engineers. Certain aspects of current models, such as abandoning lower criticality tasks when certain situations arise, do not give the robustness required in application domains such as the automotive and aerospace industries. In this paper a new bailout protocol is developed that still guarantees high criticality software but minimises the negative impact on lower criticality software via a timely return to normal operation. We show how the bailout protocol can be integrated with existing techniques, utilising both offline slack and online gain-time to further improve performance. Static analysis is provided for schedulability guarantees, while scenario-based evaluation via simulation is used to explore the effectiveness of the protocol.",0098-5589;00985589,,10.1109/TSE.2016.2592907,ESPRC; EU FP7 IP PROXIMA; 10.13039/100005185 - MCC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516652,Real-time systems;fixed priority scheduling;mixed criticality;mode changes,Analytical models;Job shop scheduling;Protocols;Safety;Software;Software engineering;Standards,embedded systems;safety-critical software;scheduling,enhanced bailout protocol;high criticality software;lower criticality software;mixed criticality embedded software;online gain-time;scenario-based evaluation;schedulability guarantees,,1,,,,,20160719,April 1 2017,,IEEE,IEEE Journals & Magazines,,22
ARENA: An Approach for the Automated Generation of Release Notes,L. Moreno; G. Bavota; M. D. Penta; R. Oliveto; A. Marcus; G. Canfora,"University of Texas at Dallas, Richardson, TX",IEEE Transactions on Software Engineering,20170213,2017,43,2,106,127,"Release notes document corrections, enhancements, and, in general, changes that were implemented in a new release of a software project. They are usually created manually and may include hundreds of different items, such as descriptions of new features, bug fixes, structural changes, new or deprecated APIs, and changes to software licenses. Thus, producing them can be a time-consuming and daunting task. This paper describes ARENA (Automatic RElease Notes generAtor), an approach for the automatic generation of release notes. ARENA extracts changes from the source code, summarizes them, and integrates them with information from versioning systems and issue trackers. ARENA was designed based on the manual analysis of 990 existing release notes. In order to evaluate the quality of the release notes automatically generated by ARENA, we performed four empirical studies involving a total of 56 participants (48 professional developers and eight students). The obtained results indicate that the generated release notes are very good approximations of the ones manually produced by developers and often include important information that is missing in the manually created release notes.",0098-5589;00985589,,10.1109/TSE.2016.2591536,Markos project; 10.13039/100000001 - National Science Foundation; 10.13039/501100000780 - European Commission; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7513412,Release notes;software documentation;software evolution,Computer bugs;Documentation;Feature extraction;Libraries;Licenses;Open source software,application program interfaces;program debugging;software engineering;source code (software);system documentation,API;ARENA;automated release note generation;bug fixes;new features;release note document corrections;release note document enhancements;software licenses;software project;source code change extraction;source code change summarization;structural changes,,1,,,,,20160714,Feb. 1 2017,,IEEE,IEEE Journals & Magazines,,21
Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs,J. Xuan; M. Martinez; F. DeMarco; M. ClÕ©ment; S. L. Marcote; T. Durieux; D. Le Berre; M. Monperrus,"State Key Lab of Software Engineering, School of Computer, Wuhan University, Wuhan, China",IEEE Transactions on Software Engineering,20170109,2017,43,1,34,55,"We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes.",0098-5589;00985589,,10.1109/TSE.2016.2560811,INRIA Internship program; INRIA postdoctoral research fellowship; National Natural Science Foundation of China; Young Talent Development Program of the China Computer Federation; 10.13039/501100004794 - CNRS delegation program; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463060,Automatic repair;SMT;fault localization;patch generation,Computer bugs;Encoding;Indexes;Java;Maintenance engineering;Open source software;Runtime,Java;computability;object-oriented programming;program debugging;public domain software;software fault tolerance;software maintenance,Apache Commons Lang;Apache Commons Math;Java programs;Nopol;SMT problem;angelic fix localization;automatic conditional statement bug repairing;buggy IF conditions;buggy program;code patch;conditional expression;objected-oriented features;open-source projects;patch generation;runtime trace collection;satisfiability modulo theory problem;test execution,,10,,,,,20160429,Jan. 1 2017,,IEEE,IEEE Journals & Magazines,,21
Semantics-Based Obfuscation-Resilient Binary Code Similarity Comparison with Applications to Software and Algorithm Plagiarism Detection,L. Luo; J. Ming; D. Wu; P. Liu; S. Zhu,"College of Information Sciences and Technology, Pennsylvania State University, University Park, PA",IEEE Transactions on Software Engineering,20171208,2017,43,12,1157,1177,"Existing code similarity comparison methods, whether source or binary code based, are mostly not resilient to obfuscations. Identifying similar or identical code fragments among programs is very important in some applications. For example, one application is to detect illegal code reuse. In the code theft cases, emerging obfuscation techniques have made automated detection increasingly difficult. Another application is to identify cryptographic algorithms which are widely employed by modern malware to circumvent detection, hide network communications, and protect payloads among other purposes. Due to diverse coding styles and high programming flexibility, different implementation of the same algorithm may appear very distinct, causing automatic detection to be very hard, let alone code obfuscations are sometimes applied. In this paper, we propose a binary-oriented, obfuscation-resilient binary code similarity comparison method based on a new concept, longest common subsequence of semantically equivalent basic blocks , which combines rigorous program semantics with longest common subsequence based fuzzy matching. We model the semantics of a basic block by a set of symbolic formulas representing the input-output relations of the block. This way, the semantic equivalence (and similarity) of two blocks can be checked by a theorem prover. We then model the semantic similarity of two paths using the longest common subsequence with basic blocks as elements. This novel combination has resulted in strong resiliency to code obfuscation. We have developed a prototype. The experimental results show that our method can be applied to software plagiarism and algorithm detection, and is effective and practical to analyze real-world software.",0098-5589;00985589,,10.1109/TSE.2017.2655046,10.13039/100000001 - US National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823022,Software plagiarism detection;algorithm detection;binary code similarity comparison;constraint solving;obfuscation;symbolic execution,Binary codes;Computational modeling;Plagiarism;Semantics;Software algorithms;Software development;Syntactics,fuzzy set theory;invasive software;program diagnostics;software engineering;theorem proving,automatic detection;code obfuscations;code similarity comparison methods;code theft cases;cryptographic algorithms;diverse coding styles;identical code fragments;illegal code reuse;longest common subsequence;obfuscation techniques;obfuscation-resilient binary code similarity comparison method;rigorous program semantics;semantic similarity;semantically equivalent basic blocks,,1,,,,,20170118,Dec. 1 2017,,IEEE,IEEE Journals & Magazines,,20
Identifying Extract Method Refactoring Opportunities Based on Functional Relevance,S. Charalampidou; A. Ampatzoglou; A. Chatzigeorgiou; A. Gkortzis; P. Avgeriou,"Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands",IEEE Transactions on Software Engineering,20171013,2017,43,10,954,974,"`Extract Method' is considered one of the most frequently applied and beneficial refactorings, since the corresponding Long Method smell is among the most common and persistent ones. Although Long Method is conceptually related to the implementation of diverse functionalities within a method, until now, this relationship has not been utilized while identifying refactoring opportunities. In this paper we introduce an approach (accompanied by a tool) that aims at identifying source code chunks that collaborate to provide a specific functionality, and propose their extraction as separate methods. The accuracy of the proposed approach has been empirically validated both in an industrial and an open-source setting. In the former case, the approach was capable of identifying functionally related statements within two industrial long methods (approx. 500 LoC each), with a recall rate of 93 percent. In the latter case, based on a comparative study on open-source data, our approach ranks better compared to two well-known techniques of the literature. To assist software engineers in the prioritization of the suggested refactoring opportunities the approach ranks them based on an estimate of their fitness for extraction. The provided ranking has been validated in both settings and proved to be strongly correlated with experts' opinion.",0098-5589;00985589,,10.1109/TSE.2016.2645572,ITEA2; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801138,Design tools and techniques;metrics/measurement;object-oriented programming,Computer science;Data mining;Mathematics;Measurement;Open source software;Syntactics,public domain software;software maintenance,Extract Method;Long Method smell;beneficial refactorings;diverse functionalities;functional relevance;functionally related statements;industrial long methods;open-source data;open-source setting;refactoring opportunities;source code chunks;specific functionality,,,,,,,20161228,Oct. 1 2017,,IEEE,IEEE Journals & Magazines,,20
Imprecise Matching of Requirements Specifications for Software Services Using Fuzzy Logic,M. C. Platenius; A. Shaker; M. Becker; E. HÕ_llermeier; W. SchÕ_fer,"Software Engineering Group, Heinz Nixdorf Institute, Paderborn University, Germany",IEEE Transactions on Software Engineering,20170811,2017,43,8,739,759,"Today, software components are provided by global markets in the form of services. In order to optimally satisfy service requesters and service providers, adequate techniques for automatic service matching are needed. However, a requester's requirements may be vague and the information available about a provided service may be incomplete. As a consequence, fuzziness is induced into the matching procedure. The contribution of this paper is the development of a systematic matching procedure that leverages concepts and techniques from fuzzy logic and possibility theory based on our formal distinction between different sources and types of fuzziness in the context of service matching. In contrast to existing methods, our approach is able to deal with imprecision and incompleteness in service specifications and to inform users about the extent of induced fuzziness in order to improve the user's decision-making. We demonstrate our approach on the example of specifications for service reputation based on ratings given by previous users. Our evaluation based on real service ratings shows the utility and applicability of our approach.",0098-5589;00985589,,10.1109/TSE.2016.2632115,Collaborative Research Center ëèëèëèOn-The-Fly Computingëèëèëè; 10.13039/100004807 - German Research Foundation (DFG); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755807,Service selection;decision making;fuzzy logic;non-functional properties;requirements specifications;service matching;uncertainty,Context;Decision making;Fuzzy logic;Security;Software;Software engineering;Uncertainty,decision making;fuzzy logic;fuzzy set theory;pattern matching;software engineering,automatic service matching;fuzzy logic;possibility theory;requirement specification imprecise matching;software components;software services;user decision-making,,,,,,,20161123,Aug. 1 2017,,IEEE,IEEE Journals & Magazines,,20
Preventing Defects: The Impact of Requirements Traceability Completeness on Software Quality,P. Rempel; P. MÕ_der,"Software Engineering for Safety-Critical Systems Group, Technische Universit&#x00E4;t Ilmenau, Ilmenau, Germany",IEEE Transactions on Software Engineering,20170811,2017,43,8,777,797,"Requirements traceability has long been recognized as an important quality of a well-engineered system. Among stakeholders, traceability is often unpopular due to the unclear benefits. In fact, little evidence exists regarding the expected traceability benefits. There is a need for empirical work that studies the effect of traceability. In this paper, we focus on the four main requirements implementation supporting activities that utilize traceability. For each activity, we propose generalized traceability completeness measures. In a defined process, we selected 24 medium to large-scale open-source projects. For each software project, we quantified the degree to which a studied development activity was enabled by existing traceability with the proposed measures. We analyzed that data in a multi-level Poisson regression analysis. We found that the degree of traceability completeness for three of the studied activities significantly affects software quality, which we quantified as defect rate. Our results provide for the first time empirical evidence that more complete traceability decreases the expected defect rate in the developed software. The strong impact of traceability completeness on the defect rate suggests that traceability is of great practical value for any kind of software development project, even if traceability is not mandated by a standard or regulation.",0098-5589;00985589,,10.1109/TSE.2016.2622264,10.13039/501100002347 - German Ministry of Education and Research (BMBF); 10.13039/501100004403 - Thëèëèringer Aufbaubank (TAB); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723818,Requirements traceability;bugs;change impact analysis;defects;empirical validation;error proneness;regression analysis;requirements satisfaction analysis;software quality;source code justification analysis;traceability completeness;traceability metrics,Context;Software engineering;Software quality;Software systems;Stakeholders;Standards,public domain software;regression analysis;software quality;stochastic processes,generalized traceability completeness measurement;large-scale open-source projects;multilevel Poisson regression analysis;requirement traceability completeness impact;software development project;software project;software quality,,,,,,,20161027,Aug. 1 2017,,IEEE,IEEE Journals & Magazines,,20
Model-Based Self-Aware Performance and Resource Management Using the Descartes Modeling Language,N. Huber; F. Brosig; S. Spinner; S. Kounev; M. BÕ_hr,"Department of Computer Science, Chair of Software Engineering, University of W&#x00FC;rzburg, W&#x00FC;rzburg, Germany",IEEE Transactions on Software Engineering,20170512,2017,43,5,432,452,"Modern IT systems have increasingly distributed and dynamic architectures providing flexibility to adapt to changes in the environment and thus enabling higher resource efficiency. However, these benefits come at the cost of higher system complexity and dynamics. Thus, engineering systems that manage their end-to-end application performance and resource efficiency in an autonomic manner is a challenge. In this article, we present a holistic model-based approach for self-aware performance and resource management leveraging the Descartes Modeling Language (DML), an architecture-level modeling language for online performance and resource management. We propose a novel online performance prediction process that dynamically tailors the model solving depending on the requirements regarding accuracy and overhead. Using these prediction capabilities, we implement a generic model-based control loop for proactive system adaptation. We evaluate our model-based approach in the context of two representative case studies showing that with the proposed methods, significant resource efficiency gains can be achieved while maintaining performance requirements. These results represent the first end-to-end validation of our approach, demonstrating its potential for self-aware performance and resource management in the context of modern IT systems and infrastructures.",0098-5589;00985589,,10.1109/TSE.2016.2613863,10.13039/100004807 - Deutsche Forschungsgemeinschaft (DFG); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577879,Autonomic;adaptation;efficiency;model-based;modeling language;performance;self-aware,Adaptation models;Computer architecture;Dynamic scheduling;Predictive models;Resource management;Software;Unified modeling language,software architecture;software performance evaluation,DML;Descartes modeling language;IT systems;architecture-level modeling language;distributed architectures;dynamic architectures;end-to-end application performance;engineering systems;generic model-based control loop;holistic model-based approach;model-based self-aware performance;online performance prediction process;proactive system adaptation;resource efficiency;resource efficiency gains;resource management;system complexity,,3,,,,,20160927,May 1 2017,,IEEE,IEEE Journals & Magazines,,20
DECAF: A Platform-Neutral Whole-System Dynamic Binary Analysis Platform,A. Henderson; L. K. Yan; X. Hu; A. Prakash; H. Yin; S. McCamant,"Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY",IEEE Transactions on Software Engineering,20170213,2017,43,2,164,184,"Dynamic binary analysis is a prevalent and indispensable technique in program analysis. While several dynamic binary analysis tools and frameworks have been proposed, all suffer from one or more of: prohibitive performance degradation, a semantic gap between the analysis code and the program being analyzed, architecture/OS specificity, being user-mode only, and lacking APIs. We present DECAF, a virtual machine based, multi-target, whole-system dynamic binary analysis framework built on top of QEMU. DECAF provides Just-In-Time Virtual Machine Introspection and a plugin architecture with a simple-to-use event-driven programming interface. DECAF implements a new instruction-level taint tracking engine at bit granularity, which exercises fine control over the QEMU Tiny Code Generator (TCG) intermediate representation to accomplish on-the-fly optimizations while ensuring that the taint propagation is sound and highly precise. We perform a formal analysis of DECAF's taint propagation rules to verify that most instructions introduce neither false positives nor false negatives. We also present three platform-neutral plugins-Instruction Tracer, Keylogger Detector, and API Tracer, to demonstrate the ease of use and effectiveness of DECAF in writing cross-platform and system-wide analysis tools. Implementation of DECAF consists of 9,550 lines of C++ code and 10,270 lines of C code and we evaluate DECAF using CPU2006 SPEC benchmarks and show average overhead of 605 percent for system wide tainting and 12 percent for VMI.",0098-5589;00985589,,10.1109/TSE.2016.2589242,10.13039/100000001 - US National Science Foundation; 10.13039/100005139 - McAfee Inc.; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506264,Dynamic binary analysis;dynamic taint analysis;virtual machine introspection,Computer architecture;Context;Instruments;Kernel;Registers;Semantics;Virtual machining,C++ language;application program interfaces;program compilers;program diagnostics;software architecture;software performance evaluation;software portability;software tools;source code (software);virtual machines,API tracer;C code;C++ code;CPU2006 SPEC benchmarks;DECAF;QEMU;QEMU tiny code generator;TCG;architecture-OS specificity;code analysis;cross-platform analysis tools;dynamic binary analysis tools;event-driven programming interface;formal analysis;instruction tracer;instruction-level taint tracking engine;just-in-time virtual machine introspection;keylogger detector;on-the-fly optimizations;platform-neutral plugins;platform-neutral whole-system dynamic binary analysis platform;plug-in architecture;program analysis;prohibitive performance degradation;system wide tainting;system- wide analysis tools;taint propagation;virtual machine based multitarget whole-system dynamic binary analysis,,,,,,,20160707,Feb. 1 2017,,IEEE,IEEE Journals & Magazines,,20
Adaptive Multi-Objective Evolutionary Algorithms for Overtime Planning in Software Projects,F. Sarro; F. Ferrucci; M. Harman; A. Manna; J. Ren,"University College London, CREST Centre, London, United Kingdom",IEEE Transactions on Software Engineering,20171013,2017,43,10,898,917,"Software engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. Recently, we introduced a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. This approach was empirically evaluated on six real world software projects and compared against state-of-the-art evolutionary approaches and currently used overtime strategies. The results showed that our proposal comfortably outperformed all the benchmarks considered. This paper extends our previous work by investigating adaptive multi-objective approaches to meta-heuristic operator selection, thereby extending and (as the results show) improving algorithmic performance. We also extended our empirical study to include two new real world software projects, thereby enhancing the scientific evidence for the technical performance claims made in the paper. Our new results, over all eight projects studied, showed that our adaptive algorithm outperforms the considered state of the art multi-objective approaches in 93 percent of the experiments (with large effect size). The results also confirm that our approach significantly outperforms current overtime planning practices in 100 percent of the experiments (with large effect size).",0098-5589;00985589,,10.1109/TSE.2017.2650914,Microsoft Azure Research; 10.13039/501100000266 - EPSRC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814340,NSGAII;Software engineering;hyperheuristic;management;multi-objective evolutionary algorithms;overtime;planning;project scheduling;search-based software engineering,Adaptive algorithms;Planning;Project management;Search problems;Software;Software engineering;Standards,DP management;decision support systems;evolutionary computation;project management;software development management;software quality,adaptive multiobjective approaches;adaptive multiobjective evolutionary algorithms;evolutionary approaches;meta-heuristic operator selection;multiobjective decision support approach;overtime planning practices;overtime strategies;project risks;software engineers;software projects,,,,,,,20170111,Oct. 1 2017,,IEEE,IEEE Journals & Magazines,,19
Software Numerical Instability Detection and Diagnosis by Combining Stochastic and Infinite-Precision Testing,E. Tang; X. Zhang; N. T. MÕ_ller; Z. Chen; X. Li,"State Key Laboratory for Novel Software Technology and Software Institute of Nanjing University, Jiangsu, China",IEEE Transactions on Software Engineering,20171013,2017,43,10,975,994,"Numerical instability is a well-known problem that may cause serious runtime failures. This paper discusses the reason of instability in software development process, and presents a toolchain that not only detects the potential instability in software, but also diagnoses the reason for such instability. We classify the reason of instability into two categories. When it is introduced by software requirements, we call the instability caused by problem . In this case, it cannot be avoided by improving software development, but requires inspecting the requirements, especially the underlying mathematical properties. Otherwise, we call the instability caused by practice. We design our toolchain as four loosely-coupled tools, which combine stochastic arithmetic with infinite-precision testing. Each tool in our toolchain can be configured with different strategies according to the properties of the analyzed software. We evaluate our toolchain on subjects from literature. The results show that it effectively detects and separates the instabilities caused by problems from others. We also conduct an evaluation on the latest version of GNU Scientific Library, and the toolchain finds a few real bugs in the well-maintained and widely deployed numerical library. With the help of our toolchain, we report the details and fixing advices to the GSL buglist.",0098-5589;00985589,,10.1109/TSE.2016.2642956,NSF Award; National Basic Research Program of China 973 Program; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792694,Numerical analysis;infinite-precision arithmetic;software testing;stochastic arithmetic,Algorithm design and analysis;Computer bugs;Libraries;Software;Software algorithms;Software testing,fault diagnosis;formal specification;mathematics computing;numerical stability;program debugging;program testing;software quality;software reliability;software tools;stochastic processes,GNU scientific library;GSL buglist;infinite-precision testing;loosely-coupled tools;mathematical properties;numerical library;potential instability;runtime failures;software development process;software numerical instability detection;software numerical instability diagnosis;software requirements;stochastic arithmetic;toolchain,,,,,,,20161221,Oct. 1 2017,,IEEE,IEEE Journals & Magazines,,19
Reporting Usability Defects: A Systematic Literature Review,N. S. M. Yusop; J. Grundy; R. Vasa,"Faculty of Computer and Mathematical Science, Universiti Teknologi MARA, Malaysia",IEEE Transactions on Software Engineering,20170915,2017,43,9,848,867,"Usability defects can be found either by formal usability evaluation methods or indirectly during system testing or usage. No matter how they are discovered, these defects must be tracked and reported. However, empirical studies indicate that usability defects are often not clearly and fully described. This study aims to identify the state of the art in reporting of usability defects in the software engineering and usability engineering literature. We conducted a systematic literature review of usability defect reporting drawing from both the usability and software engineering literature from January 2000 until March 2016. As a result, a total of 57 studies were identified, in which we classified the studies into three categories: reporting usability defect information, analysing usability defect data and key challenges. Out of these, 20 were software engineering studies and 37 were usability studies. The results of this systematic literature review show that usability defect reporting processes suffer from a number of limitations, including: mixed data, inconsistency of terms and values of usability defect data, and insufficient attributes to classify usability defects. We make a number of recommendations to improve usability defect reporting and management in software engineering.",0098-5589;00985589,,10.1109/TSE.2016.2638427,"Ministry of Higher Education Malaysia, Universiti Teknologi MARA (UiTM); 10.13039/100000163 - ARC Discovery projects scheme, the Deakin Software Technology Innovation Lab; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779159,Systematic review;test management;usability defect reporting;usability testing;user interface,Bibliographies;Human computer interaction;Software engineering;Systematics;Testing;Usability,formal verification;pattern classification;product design;program testing,formal usability evaluation;mixed data;software engineering;system testing;usability defect classification;usability defect data;usability defect reporting processes,,,,,,,20161209,Sept. 1 2017,,IEEE,IEEE Journals & Magazines,,19
How Social and Communication Channels Shape and Challenge a Participatory Culture in Software Development,M. A. Storey; A. Zagalsky; F. F. Filho; L. Singer; D. M. German,"University of Victoria, Victoria, BC, Canada",IEEE Transactions on Software Engineering,20170213,2017,43,2,185,204,"Software developers use many different communication tools and channels in their work. The diversity of these tools has dramatically increased over the past decade and developers now have access to a wide range of socially enabled communication channels and social media to support their activities. The availability of such social tools is leading to a participatory culture of software development, where developers want to engage with, learn from, and co-create software with other developers. However, the interplay of these social channels, as well as the opportunities and challenges they may create when used together within this participatory development culture are not yet well understood. In this paper, we report on a large-scale survey conducted with 1,449 GitHub users. We discuss the channels these developers find essential to their work and gain an understanding of the challenges they face using them. Our findings lay the empirical foundation for providing recommendations to developers and tool designers on how to use and improve tools for software developers.",0098-5589;00985589,,10.1109/TSE.2016.2584053,10.13039/501100000038 - Natural Sciences and Engineering Research Council of Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498605,CSCW;Participatory culture;communication;social media;software engineering,Collaboration;Communication channels;Electronic mail;Face;Knowledge engineering;Media;Software,cultural aspects;professional communication;social networking (online);software development management;team working,GitHub users;communication tools;participatory culture;social channels;social media;social tools;socially enabled communication channels;software developers;software development,,2,,,,,20160623,Feb. 1 2017,,IEEE,IEEE Journals & Magazines,,19
Dependence Guided Symbolic Execution,H. Wang; T. Liu; X. Guan; C. Shen; Q. Zheng; Z. Yang,"Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi&#x0027;an Jiaotong University, Xi'an, China",IEEE Transactions on Software Engineering,20170313,2017,43,3,252,271,"Symbolic execution is a powerful technique for systematically exploring the paths of a program and generating the corresponding test inputs. However, its practical usage is often limited by the <italic>path explosion</italic> problem, that is, the number of explored paths usually grows exponentially with the increase of program size. In this paper, we argue that for the purpose of fault detection it is not necessary to systematically explore the paths, and propose a new symbolic execution approach to mitigate the path explosion problem by predicting and eliminating the redundant paths based on symbolic value. Our approach can achieve the equivalent fault detection capability as traditional symbolic execution without exhaustive path exploration. In addition, we develop a practical implementation called Dependence Guided Symbolic Execution (DGSE) to soundly approximate our approach. Through exploiting program dependence, DGSE can predict and eliminate the redundant paths at a reasonable computational cost. Our empirical study shows that the redundant paths are abundant and widespread in a program. Compared with traditional symbolic execution, DGSE only explores 6.96 to 96.57 percent of the paths and achieves a speedup of 1.02<inline-formula> <tex-math notation=""LaTeX"">$\times$</tex-math><alternatives><inline-graphic xlink:href=""liu-ieq1-2584063.gif""/> </alternatives></inline-formula> to 49.56<inline-formula><tex-math notation=""LaTeX"">$\times$</tex-math><alternatives> <inline-graphic xlink:href=""liu-ieq2-2584063.gif""/></alternatives></inline-formula>. We have released our tool and the benchmarks used to evaluate DGSE<inline-formula><tex-math notation=""LaTeX"">$^\ast$</tex-math><alternatives> <inline-graphic xlink:href=""liu-ieq3-2584063.gif""/></alternatives></inline-formula>.",0098-5589;00985589,,10.1109/TSE.2016.2584063,Fundamental Research Funds for the Central Universities; Ministry of Education Innovation Research Team; National Research Program of China; 10.13039/100000001 - US National Science Foundation; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004806 - Fok Ying-Tong Education Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497518,Symbolic execution;path coverage;program dependence,Benchmark testing;Computational efficiency;Electronic mail;Explosions;Fault detection;Input variables,,,,4,,,,,20160622,March 1 2017,,IEEE,IEEE Journals & Magazines,,19
Process Aspects and Social Dynamics of Contemporary Code Review: Insights from Open Source Development and Industrial Practice at Microsoft,A. Bosu; J. C. Carver; C. Bird; J. Orbeck; C. Chockley,"Department of Computer Science, Southern Illinois University, Carbondale, IL",IEEE Transactions on Software Engineering,20170109,2017,43,1,56,75,"Many open source and commercial developers practice contemporary code review, a lightweight, informal, tool-based code review process. To better understand this process and its benefits, we gathered information about code review practices via surveys of open source software developers and developers from Microsoft. The results of our analysis suggest that developers spend approximately 10-15 percent of their time in code reviews, with the amount of effort increasing with experience. Developers consider code review important, stating that in addition to finding defects, code reviews offer other benefits, including knowledge sharing, community building, and maintaining code quality. The quality of the code submitted for review helps reviewers form impressions about their teammates, which can influence future collaborations. We found a large amount of similarity between the Microsoft and OSS respondents. One interesting difference is that while OSS respondents view code review as an important method of impression formation, Microsoft respondents found knowledge dissemination to be more important. Finally, we found little difference between distributed and co-located Microsoft teams. Our findings identify the following key areas that warrant focused research: 1) exploring the non-technical benefits of code reviews, 2) helping developers in articulating review comments, and 3) assisting reviewers' program comprehension during code reviews.",0098-5589;00985589,,10.1109/TSE.2016.2576451,10.13039/100000001 - US National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484733,Code review;OSS;commercial projects;open source;peer impressions;survey,Collaboration;Context;Human factors;Inspection;Instruments;Measurement;Organizations,public domain software;software engineering;software management;software reviews;team working,Microsoft teams;OSS;code quality;contemporary code review;knowledge dissemination;open source software developers,,,,,,,20160607,Jan. 1 2017,,IEEE,IEEE Journals & Magazines,,19
On the Positive Effect of Reactive Programming on Software Comprehension: An Empirical Study,G. Salvaneschi; S. Proksch; S. Amann; S. Nadi; M. Mezini,"Department of Computer Science, Reactive Systems Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany",IEEE Transactions on Software Engineering,20171208,2017,43,12,1125,1143,"Starting from the first investigations with strictly functional languages, reactive programming has been proposed as the programming paradigm for reactive applications. Over the years, researchers have enriched reactive languages with more powerful abstractions, embedded these abstractions into mainstream languages-including object-oriented languages-and applied reactive programming to several domains, such as GUIs, animations, Web applications, robotics, and sensor networks. However, an important assumption behind this line of research is that, beside other claimed advantages, reactive programming makes a wide class of otherwise cumbersome applications more comprehensible. This claim has never been evaluated. In this paper, we present the first empirical study that evaluates the effect of reactive programming on comprehension. The study involves 127 subjects and compares reactive programming to the traditional object-oriented style with the Observer design pattern. Our findings show that program comprehension is significantly enhanced by the reactive-programming paradigm-a result that suggests to further develop research in this field.",0098-5589;00985589,,10.1109/TSE.2017.2655524,German Federal Ministry of Education and Research; 10.13039/501100000781 - European Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827078,Reactive programming;controlled experiment;empirical study;software comprehension,Programming;Robot sensing systems;Runtime;Software development,functional languages;object-oriented languages;object-oriented programming,functional languages;object-oriented languages;observer design pattern;program comprehension;reactive languages;reactive programming;software comprehension,,,,,,,20170119,Dec. 1 2017,,IEEE,IEEE Journals & Magazines,,18
Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt,E. d. S. Maldonado; E. Shihab; N. Tsantalis,"Department of Computer Science and Software Engineering, Data-Driven Analysis of Software (DAS) Lab, Concordia University, Montreal, QC, Canada",IEEE Transactions on Software Engineering,20171110,2017,43,11,1044,1062,"The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-admitted technical debt), and that the most common types of self-admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-admitted technical debt, significantly outperforming the current state-of-the-art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset.",0098-5589;00985589,,10.1109/TSE.2017.2654244,10.13039/501100000038 - Natural Sciences and Engineering Research Council of Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820211,Technical debt;empirical study;natural language processing;source code comments,Entropy;Java;Manuals;Natural language processing;Software;Structured Query Language;Unified modeling language,Java;SQL;computer crime;natural language processing;project management;public domain software;software maintenance;software management;software quality,NLP;Natural Language Processing;design debt;open source projects;requirement debt;self-admitted technical debt detection;source code comment classification;source code quality,,2,,,,,20170117,Nov. 1 2017,,IEEE,IEEE Journals & Magazines,,18
A Qualitative Study of Application-Level Caching,J. Mertz; I. Nunes,"Instituto de Inform&#x00E1;tica, Universidade Federal do Rio Grande do Sul, Porto Alegre-RS, Brazil",IEEE Transactions on Software Engineering,20170915,2017,43,9,798,816,"Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users' demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on specific details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications.",0098-5589;00985589,,10.1109/TSE.2016.2633992,BRA; 10.13039/501100002322 - CAPES; 10.13039/501100003593 - CNPq; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762909,Application-level caching;guideline;pattern;qualitative study;web application,Databases;Guidelines;HTML;Maintenance engineering;Scalability;Servers;Software,Internet;cache storage;project management;software management,Internet-based services;Web applications;application-level caching;caching logic;software projects,,,,,,,20161201,Sept. 1 2017,,IEEE,IEEE Journals & Magazines,,18
Approaches to Co-Evolution of Metamodels and Models: A Survey,R. Hebig; D. E. Khelladi; R. Bendraou,"Computer Science and Engineering G&#x00F6;teborg, Chalmers University of Technology, G&#x00F6;teborg, Sweden",IEEE Transactions on Software Engineering,20170512,2017,43,5,396,414,"Modeling languages, just as all software artifacts, evolve. This poses the risk that legacy models of a company get lost, when they become incompatible with the new language version. To address this risk, a multitude of approaches for metamodel-model co-evolution were proposed in the last 10 years. However, the high number of solutions makes it difficult for practitioners to choose an appropriate approach. In this paper, we present a survey on 31 approaches to support metamodel-model co-evolution. We introduce a taxonomy of solution techniques and classify the existing approaches. To support researchers, we discuss the state of the art, in order to better identify open issues. Furthermore, we use the results to provide a decision support for practitioners, who aim to adopt solutions from research.",0098-5589;00985589,,10.1109/TSE.2016.2610424,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7569018,Survey;design notations and documentation;metamodels;models;software engineering,Atmospheric modeling;Biological system modeling;Companies;Libraries;Productivity;Taxonomy;Unified modeling language,software engineering,coevolution approaches;decision support;metamodel-model coevolution;solution technique taxonomy,,2,,,,,20160915,May 1 2017,,IEEE,IEEE Journals & Magazines,,18
An Improved SDA Based Defect Prediction Framework for Both Within-Project and Cross-Project Class-Imbalance Problems,X. Y. Jing; F. Wu; X. Dong; B. Xu,"State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China",IEEE Transactions on Software Engineering,20170414,2017,43,4,321,339,"Background. Solving the class-imbalance problem of within-project software defect prediction (SDP) is an important research topic. Although some class-imbalance learning methods have been presented, there exists room for improvement. For cross-project SDP, we found that the class-imbalanced source usually leads to misclassification of defective instances. However, only one work has paid attention to this cross-project class-imbalance problem. Objective. We aim to provide effective solutions for both within-project and cross-project class-imbalance problems. Method. Subclass discriminant analysis (SDA), an effective feature learning method, is introduced to solve the problems. It can learn features with more powerful classification ability from original metrics. For within-project prediction, we improve SDA for achieving balanced subclasses and propose the improved SDA (ISDA) approach. For cross-project prediction, we employ the semi-supervised transfer component analysis (SSTCA) method to make the distributions of source and target data consistent, and propose the SSTCA+ISDA prediction approach. Results. Extensive experiments on four widely used datasets indicate that: 1) ISDA-based solution performs better than other state-of-the-art methods for within-project class-imbalance problem; 2) SSTCA+ISDA proposed for cross-project class-imbalance problem significantly outperforms related methods. Conclusion. Within-project and cross-project class-imbalance problems greatly affect prediction performance, and we provide a unified and effective prediction framework for both problems.",0098-5589;00985589,,10.1109/TSE.2016.2597849,National Nature Science Foundation of China; Research Project of NJUPT; The Chinese 973 Program; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530877,ISDA based defect prediction framework;Software defect prediction (SDP);cross-project class-imbalance;improved subclass discriminant analysis (ISDA);within-project class-imbalance,Learning systems;Measurement;Predictive models;Software;Software engineering;Support vector machines,learning (artificial intelligence);software engineering,SDA based defect prediction;SDP;SSTCA;class-imbalance learning;cross-project class-imbalance problem;semisupervised transfer component analysis;software defect prediction;subclass discriminant analysis;within-project class-imbalance problem,,2,,,,,20160803,April 1 2017,,IEEE,IEEE Journals & Magazines,,18
Efficient Dynamic Updates of Distributed Components Through Version Consistency,L. Baresi; C. Ghezzi; X. Ma; V. P. L. Manna,"Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy",IEEE Transactions on Software Engineering,20170414,2017,43,4,340,358,"Modern component-based distributed software systems are increasingly required to offer non-stop service and thus their updates must be carried out at runtime. Different authors have already proposed solutions for the safe management of dynamic updates. Our contribution aims at improving their efficiency without compromising safety. We propose a new criterion, called version consistency, which defines when a dynamic update can be safely and efficiently applied to the components that execute distributed transactions. Version consistency ensures that distributed transactions be served as if they were operated on a single coherent version of the system despite possible concurrent updates. The paper presents a distributed algorithm for checking version consistency efficiently, formalizes the proposed approach by means of a graph transformation system, and verifies its correctness through model checking. The paper also presents ConUp, a novel prototype framework that supports the approach and offers a viable, concrete solution for the use of version consistency. Both the approach and ConUp are evaluated on a significant third-party application. Obtained results witness the benefits of the proposed solution with respect to both timeliness and disruption.",0098-5589;00985589,,10.1109/TSE.2016.2592913,EEB-Edifici a zero consumo energetico in distretti urbani intelligenti; Italian Technology Cluster For Smart Communities; Telecom Italia; The 973 Program of China; 10.13039/501100001809 - NSFC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516718,Component-based distributed system;dynamic update;version-consistency,Concrete;Model checking;Portals;Runtime;Safety;Software systems,distributed programming;formal verification;graph theory,distributed components;distributed software systems;dynamic update;graph transformation system;version consistency,,,,,,,20160719,April 1 2017,,IEEE,IEEE Journals & Magazines,,18
Automated Synthesis and Dynamic Analysis of Tradeoff Spaces for Object-Relational Mapping,H. Bagheri; C. Tang; K. Sullivan,"Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE",IEEE Transactions on Software Engineering,20170213,2017,43,2,145,163,"Producing software systems that achieve acceptable tradeoffs among multiple non-functional properties remains a significant engineering problem. We propose an approach to solving this problem that combines synthesis of spaces of design alternatives from logical specifications and dynamic analysis of each point in the resulting spaces. We hypothesize that this approach has potential to help engineers understand important tradeoffs among dynamically measurable properties of system components at meaningful scales within reach of existing synthesis tools. To test this hypothesis, we developed tools to enable, and we conducted, a set of experiments in the domain of relational databases for object-oriented data models. For each of several data models, we used our approach to empirically test the accuracy of a published suite of metrics to predict tradeoffs based on the static schema structure alone. The results show that exhaustive synthesis and analysis provides a superior view of the tradeoff spaces for such designs. This work creates a path forward toward systems that achieve significantly better tradeoffs among important system properties.",0098-5589;00985589,,10.1109/TSE.2016.2587646,10.13039/100000001 - National Science Foundation; 10.13039/100000001 - US National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506009,ORM;Specification-driven synthesis;dynamic analysis;relational logic;static analysis;tradespace analysis,Data models;Load modeling;Measurement;Object oriented modeling;Relational databases;Semantics,data models;formal specification;object-oriented methods;program diagnostics,automated tradeoff space synthesis;dynamic analysis;dynamic tradeoff space analysis;dynamically measurable properties;logical specifications;nonfunctional properties;object-oriented data models;object-relational mapping;relational databases;software systems;static schema structure;system properties,,,,,,,20160707,Feb. 1 2017,,IEEE,IEEE Journals & Magazines,,18
Automating Live Update for Generic Server Programs,C. Giuffrida; C. Iorgulescu; G. Tamburrelli; A. S. Tanenbaum,"Vrije Universiteit Amsterdam, De Boelelaan, Amsterdam, Netherlands",IEEE Transactions on Software Engineering,20170314,2017,43,3,207,225,"The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents <italic>Mutable Checkpoint-Restart</italic> (<italic>MCR</italic>), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the running version to safely reach a quiescent state and then allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (<italic>Apache httpd</italic>, <italic>nginx</italic>, <italic>OpenSSH</italic> and <italic>vsftpd</italic>) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible performance overhead (2 percent on average) and moderate memory overhead (3.9<inline-formula><tex-math notation=""LaTeX"">$\times$ </tex-math><alternatives><inline-graphic xlink:href=""giuffrida-ieq1-2584066.gif""/></alternatives></inline-formula> on average, without optimizations).",0098-5589;00985589,,10.1109/TSE.2016.2584066,10.13039/501100000781 - ERC; 10.13039/501100000781 - European Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497481,DSU;Live update;checkpoint-restart;garbage collection;quiescence detection;record-replay,Buildings;Convergence;Data structures;Manuals;Servers;Software;System recovery,,,,,,,,,20160622,March 1 2017,,IEEE,IEEE Journals & Magazines,,18
A Dissection of the Test-Driven Development Process: Does It Really Matter to Test-First or to Test-Last?,D. Fucci; H. Erdogmus; B. Turhan; M. Oivo; N. Juristo,"Department of Information Processing Science, University of Oulu, Oulu, Finland",IEEE Transactions on Software Engineering,20170714,2017,43,7,597,614,"Background: Test-driven development (TDD) is a technique that repeats short coding cycles interleaved with testing. The developer first writes a unit test for the desired functionality, followed by the necessary production code, and refactors the code. Many empirical studies neglect unique process characteristics related to TDD iterative nature. Aim: We formulate four process characteristic: sequencing, granularity, uniformity, and refactoring effort. We investigate how these characteristics impact quality and productivity in TDD and related variations. Method: We analyzed 82 data points collected from 39 professionals, each capturing the process used while performing a specific development task. We built regression models to assess the impact of process characteristics on quality and productivity. Quality was measured by functional correctness. Result: Quality and productivity improvements were primarily positively associated with the granularity and uniformity. Sequencing, the order in which test and production code are written, had no important influence. Refactoring effort was negatively associated with both outcomes. We explain the unexpected negative correlation with quality by possible prevalence of mixed refactoring. Conclusion: The claimed benefits of TDD may not be due to its distinctive test-first dynamic, but rather due to the fact that TDD-like processes encourage fine-grained, steady steps that improve focus and flow.",0098-5589;00985589,,10.1109/TSE.2016.2616877,FiDiPro; 10.13039/501100002341 - Academy of Finland; 10.13039/501100003406 - TEKES; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592412,Test-driven development;empirical investigation;external quality;process dimensions;productivity,Companies;Conferences;Context;Productivity;Sequential analysis;Testing,program testing;regression analysis;software engineering,TDD iterative nature;TDD technique;granularity characteristic;refactoring effort characteristic;regression model;sequencing characteristic;test-driven development process;uniformity characteristic,,,,,,,20161018,July 1 2017,,IEEE,IEEE Journals & Magazines,,17
An Empirical Comparison of Model Validation Techniques for Defect Prediction Models,C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto,"Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan",IEEE Transactions on Software Engineering,20170109,2017,43,1,1,18,"Defect prediction models help software quality assurance teams to allocate their limited resources to the most defect-prone modules. Model validation techniques, such as <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math> <alternatives><inline-graphic xlink:href=""tantithamthavorn-ieq1-2584050.gif""/></alternatives></inline-formula>-fold cross-validation, use historical data to estimate how well a model will perform in the future. However, little is known about how accurate the estimates of model validation techniques tend to be. In this paper, we investigate the bias and variance of model validation techniques in the domain of defect prediction. Analysis of 101 public defect datasets suggests that 77 percent of them are highly susceptible to producing unstable results_Ñ_ - selecting an appropriate model validation technique is a critical experimental design choice. Based on an analysis of 256 studies in the defect prediction literature, we select the 12 most commonly adopted model validation techniques for evaluation. Through a case study of 18 systems, we find that single-repetition holdout validation tends to produce estimates with 46-229 percent more bias and 53-863 percent more variance than the top-ranked model validation techniques. On the other hand, out-of-sample bootstrap validation yields the best balance between the bias and variance of estimates in the context of our study. Therefore, we recommend that future defect prediction studies avoid single-repetition holdout validation, and instead, use out-of-sample bootstrap validation.",0098-5589;00985589,,10.1109/TSE.2016.2584050,Advancing Strategic International Networks to Accelerate the Circulation of Talented Researchers; Interdisciplinary Global Networks for Accelerating Theory and Practice in Software Ecosystem; 10.13039/501100000038 - Natural Sciences and Engineering Research Council of Canadaëè(NSERC); 10.13039/501100000646 - JSPS; 10.13039/501100000646 - JSPS Fellows; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497471,Defect prediction models;bootstrap validation;cross validation;holdout validation;model validation techniques,Analytical models;Context;Context modeling;Data models;Logistics;Predictive models;Software,,,,4,,,,,20160622,Jan. 1 2017,,IEEE,IEEE Journals & Magazines,,17
Locating Software Faults Based on Minimum Debugging Frontier Set,F. Li; Z. Li; W. Huo; X. Feng,"State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R.China",IEEE Transactions on Software Engineering,20170811,2017,43,8,760,776,"In this article, we propose a novel state-based fault-localization approach. Given an observed failure that is reproducible under the same program input, this new approach uses two main techniques to reduce the state exploration cost. Firstly, the execution trace to be analyzed for the observed failure is successively narrowed by making the set of trace points in each step a cut of the dynamic dependence graph. Such a cut divides the remaining trace into two parts and, based on the sparse symbolic exploration outcome, one part is removed from further exploration. This process continues until reaching where the fault is determined to be. Second, the cut in each step is chosen such that the union of the program states from the members of the cut is of the minimum size among all candidate cuts. The set of statement instances in the chosen cut is called a minimum debugging frontier set (MDFS). To evaluate our approach, we apply it to 16 real bugs from real world programs and compare our fault reports with those generated by state-of-the-art approaches. Results show that the MDFS approach obtains high quality fault reports for these test cases with considerably higher efficiency than previous approaches.",0098-5589;00985589,,10.1109/TSE.2016.2632122,National High Technology Research and Development Program of China; 10.13039/100000001 - National Science Foundation of United States; 10.13039/501100001809 - National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755837,Fault localization;dynamic dependence graph;minimum debugging frontier set;sparse symbolic exploration,Computer aided software engineering;Computer architecture;Computer bugs;Computers;Debugging;Indexes;Software,graph theory;program debugging;software fault tolerance,MDFS;dynamic dependence graph;execution trace;minimum debugging frontier set;program bugs;software faults location;sparse symbolic exploration;state exploration cost;state-based fault-localization;trace points,,,,,,,20161123,Aug. 1 2017,,IEEE,IEEE Journals & Magazines,,16
Keyword Search for Building Service-Based Systems,Q. He; R. Zhou; X. Zhang; Y. Wang; D. Ye; F. Chen; J. C. Grundy; Y. Yang,"State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China",IEEE Transactions on Software Engineering,20170714,2017,43,7,658,674,"With the fast growth of applications of service-oriented architecture (SOA) in software engineering, there has been a rapid increase in demand for building service-based systems (SBSs) by composing existing Web services. Finding appropriate component services to compose is a key step in the SBS engineering process. Existing approaches require that system engineers have detailed knowledge of SOA techniques which is often too demanding. To address this issue, we propose Keyword Search for Service-based Systems (KS3), a novel approach that integrates and automates the system planning, service discovery and service selection operations for building SBSs based on keyword search. KS3 assists system engineers without detailed knowledge of SOA techniques in searching for component services to build SBSs by typing a few keywords that represent the tasks of the SBSs with quality constraints and optimisation goals for system quality, e.g., reliability, throughput and cost. KS3 offers a new paradigm for SBS engineering that can significantly save the time and effort during the system engineering process. We conducted large-scale experiments using two real-world Web service datasets to demonstrate the practicality, effectiveness and efficiency of KS3.",0098-5589;00985589,,10.1109/TSE.2016.2624293,10.13039/501100000923 - Australian Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7731135,Service-based system;cloud computing;keyword search;quality of service;service composition;web service,Buildings;Data models;Keyword search;Libraries;Planning;Service-oriented architecture,Web services;service-oriented architecture,KS3;SBS engineering process;SOA techniques;Web services;keyword search;service-based system building;service-oriented architecture;software engineering,,,,,,,20161102,July 1 2017,,IEEE,IEEE Journals & Magazines,,16
A Framework for Evaluating the Results of the SZZ Approach for Identifying Bug-Introducing Changes,D. A. da Costa; S. McIntosh; W. Shang; U. Kulesza; R. Coelho; A. E. Hassan,"Department of Informatics and Applied Mathematics (DIMAp), Federal University of Rio Grande do Norte, Natal-RN, Brazil",IEEE Transactions on Software Engineering,20170714,2017,43,7,641,657,"The approach proposed by Silwerski, Zimmermann, and Zeller (SZZ) for identifying bug-introducing changes is at the foundation of several research areas within the software engineering discipline. Despite the foundational role of SZZ, little effort has been made to evaluate its results. Such an evaluation is a challenging task because the ground truth is not readily available. By acknowledging such challenges, we propose a framework to evaluate the results of alternative SZZ implementations. The framework evaluates the following criteria: (1) the earliest bug appearance, (2) the future impact of changes, and (3) the realism of bug introduction. We use the proposed framework to evaluate five SZZ implementations using data from ten open source projects. We find that previously proposed improvements to SZZ tend to inflate the number of incorrectly identified bug-introducing changes. We also find that a single bug-introducing change may be blamed for introducing hundreds of future bugs. Furthermore, we find that SZZ implementations report that at least 46 percent of the bugs are caused by bug-introducing changes that are years apart from one another. Such results suggest that current SZZ implementations still lack mechanisms to accurately identify bug-introducing changes. Our proposed framework provides a systematic mean for evaluating the data that is generated by a given SZZ implementation.",0098-5589;00985589,,10.1109/TSE.2016.2616306,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588121,SZZ;bug detection;evaluation framework;software repository mining,Computer bugs;Electronic mail;History;Manuals;Software;Software engineering;Systematics,program debugging,SZZ approach;Silwerski-Zimmermann-Zeller approach;bug-introducing change identification;data evaluation;ground truth;open source projects,,,,,,,20161011,July 1 2017,,IEEE,IEEE Journals & Magazines,,16
The Value of Exact Analysis in Requirements Selection,L. Li; M. Harman; F. Wu; Y. Zhang,"Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom",IEEE Transactions on Software Engineering,20170612,2017,43,6,580,596,"Uncertainty is characterised by incomplete understanding. It is inevitable in the early phase of requirements engineering, and can lead to unsound requirement decisions. Inappropriate requirement choices may result in products that fail to satisfy stakeholders' needs, and might cause loss of revenue. To overcome uncertainty, requirements engineering decision support needs uncertainty management. In this research, we develop a decision support framework METRO for the Next Release Problem (NRP) to manage algorithmic uncertainty and requirements uncertainty. An exact NRP solver (NSGDP) lies at the heart of METRO. NSGDP's exactness eliminates interference caused by approximate existing NRP solvers. We apply NSGDP to three NRP instances, derived from a real world NRP instance, RALIC, and compare with NSGA-II, a widely-used approximate (inexact) technique. We find the randomness of NSGA-II results in decision makers missing up to 99.95 percent of the optimal solutions and obtaining up to 36.48 percent inexact requirement selection decisions. The chance of getting an inexact decision using existing approximate approaches is negatively correlated with the implementation cost of a requirement (Spearman r up to -0.72). Compared to the inexact existing approach, NSGDP saves 15.21 percent lost revenue, on average, for the RALIC dataset.",0098-5589;00985589,,10.1109/TSE.2016.2615100,DAASE; 10.13039/501100000266 - EPSRC; 10.13039/501100004543 - China Scholarship Council (CSC); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582553,Software engineering;exact multi-objective optimisation;next release problem;simulation optimisation,Optimization;Robustness;Software;Software algorithms;Software engineering;Stakeholders;Uncertainty,decision support systems;formal specification,METRO;NRP solvers;NSGDP;RALIC dataset;algorithmic uncertainty management;exact NRP solver;inexact requirement selection decisions;next-release problem;optimal solutions;requirements engineering decision support;requirements uncertainty management,,,,,,,20161004,June 1 2017,,IEEE,IEEE Journals & Magazines,,16
A System for Profiling and Monitoring Database Access Patterns by Application Programs for Anomaly Detection,L. Bossi; E. Bertino; S. R. Hussain,"Department of Computer Science, Purdue University, West Lafayette, IN",IEEE Transactions on Software Engineering,20170512,2017,43,5,415,431,"Database Management Systems (DBMSs) provide access control mechanisms that allow database administrators (DBAs) to grant application programs access privileges to databases. Though such mechanisms are powerful, in practice finer-grained access control mechanism tailored to the semantics of the data stored in the DMBS is required as a first class defense mechanism against smart attackers. Hence, custom written applications which access databases implement an additional layer of access control. Therefore, securing a database alone is not enough for such applications, as attackers aiming at stealing data can take advantage of vulnerabilities in the privileged applications and make these applications to issue malicious database queries. An access control mechanism can only prevent application programs from accessing the data to which the programs are not authorized, but it is unable to prevent misuse of the data to which application programs are authorized for access. Hence, we need a mechanism able to detect malicious behavior resulting from previously authorized applications. In this paper, we present the architecture of an anomaly detection mechanism, DetAnom, that aims to solve such problem. Our approach is based the analysis and profiling of the application in order to create a succinct representation of its interaction with the database. Such a profile keeps a signature for every submitted query and also the corresponding constraints that the application program must satisfy to submit the query. Later, in the detection phase, whenever the application issues a query, a module captures the query before it reaches the database and verifies the corresponding signature and constraints against the current context of the application. If there is a mismatch, the query is marked as anomalous. The main advantage of our anomaly detection mechanism is that, in order to build the application profiles, we need neither any previous knowledge of application vulnerabilities nor a- y example of possible attacks. As a result, our mechanism is able to protect the data from attacks tailored to database applications such as code modification attacks, SQL injections, and also from other data-centric attacks as well. We have implemented our mechanism with a software testing technique called concolic testing and the PostgreSQL DBMS. Experimental results show that our profiling technique is close to accurate, requires acceptable amount of time, and the detection mechanism incurs low runtime overhead.",0098-5589;00985589,,10.1109/TSE.2016.2598336,Cyber Security Division; Homeland Security Advanced Research Projects Agency; Northrop Grumman Systems Corporation; 10.13039/100000180 - Department of Homeland Security (DHS); 10.13039/100008287 - Science and Technology Directorate; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7534833,Database;SQL injection;anomaly detection;application profile;insider attacks,Access control;Databases;Engines;Software;Software testing,authorisation;database management systems;digital signatures;program diagnostics;program testing;query processing;software architecture,DBA;DBMS;DetAnom;PostgreSQL;access control mechanism;anomaly detection mechanism architecture;application programs;concolic testing;database access patterns monitoring;database access patterns profiling;database administrator;database management systems;defense mechanism;malicious behavior detection;query submission;signature;smart attackers;software testing technique,,1,,,,,20160805,May 1 2017,,IEEE,IEEE Journals & Magazines,,16
A Study of Causes and Consequences of Client-Side JavaScript Bugs,F. S. Ocariza; K. Bajaj; K. Pattabiraman; A. Mesbah,"Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada",IEEE Transactions on Software Engineering,20170213,2017,43,2,128,144,"Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.",0098-5589;00985589,,10.1109/TSE.2016.2586066,10.13039/100002418 - Intel Corporation; 10.13039/501100000038 - Natural Sciences and Engineering Research Council of Canada (NSERC); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501855,Document Object Model (DOM);Faults;JavaScript;bug reports;empirical study,Cascading style sheets;Computer bugs;Data mining;HTML;Market research;Reliability;Servers,Internet;Java;client-server systems;program debugging;software fault tolerance,DOM-related JavaScript faults;JavaScript code;JavaScript programmers;Web application components;bug repositories;client-server communications;client-side JavaScript bugs;document object model;information extraction;static analysis tools;user-interactivity,,1,,,,,20160629,Feb. 1 2017,,IEEE,IEEE Journals & Magazines,,16
"The Work Life of Developers: Activities, Switches and Perceived Productivity",A. N. Meyer; L. E. Barton; G. C. Murphy; T. Zimmermann; T. Fritz,"University of Zurich, Z&#x00FC;rich, Switzerland",IEEE Transactions on Software Engineering,20171208,2017,43,12,1178,1193,"Many software development organizations strive to enhance the productivity of their developers. All too often, efforts aimed at improving developer productivity are undertaken without knowledge about how developers spend their time at work and how it influences their own perception of productivity. To fill in this gap, we deployed a monitoring application at 20 computers of professional software developers from four companies for an average of 11 full work day in situ. Corroborating earlier findings, we found that developers spend their time on a wide variety of activities and switch regularly between them, resulting in highly fragmented work. Our findings extend beyond existing research in that we correlate developers' work habits with perceived productivity and also show productivity is a personal matter. Although productivity is personal, developers can be roughly grouped into morning, low-at-lunch and afternoon people. A stepwise linear regression per participant revealed that more user input is most often associated with a positive, and emails, planned meetings and work unrelated websites with a negative perception of productivity. We discuss opportunities of our findings, the potential to predict high and low productivity and suggest design approaches to create better tool support for planning developers' work day and improving their personal productivity.",0098-5589;00985589,,10.1109/TSE.2017.2656886,10.13039/501100000038 - NSERC; 10.13039/501100002670 - ABB; 10.13039/501100004343 - SNF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829407,Productivity;developer activity;human factors;interruptions;user studies;work fragmentation,Encoding;Human factors;Monitoring;Productivity;Software development,regression analysis;software development management;software engineering,developer productivity;highly fragmented work;perceived productivity;personal productivity;professional software developers;software development organizations;work unrelated websites,,,,,,,20170123,Dec. 1 2017,,IEEE,IEEE Journals & Magazines,,15
The Use of Summation to Aggregate Software Metrics Hinders the Performance of Defect Prediction Models,F. Zhang; A. E. Hassan; S. McIntosh; Y. Zou,"School of Computing, Queen&#x0027;s University, Kingston, ON, Canada",IEEE Transactions on Software Engineering,20170512,2017,43,5,476,491,"Defect prediction models help software organizations to anticipate where defects will appear in the future. When training a defect prediction model, historical defect data is often mined from a Version Control System (VCS, e.g., Subversion), which records software changes at the file-level. Software metrics, on the other hand, are often calculated at the class- or method-level (e.g., McCabe's Cyclomatic Complexity). To address the disagreement in granularity, the class- and method-level software metrics are aggregated to file-level, often using summation (i.e., McCabe of a file is the sum of the McCabe of all methods within the file). A recent study shows that summation significantly inflates the correlation between lines of code (Sloc) and cyclomatic complexity (Cc) in Java projects. While there are many other aggregation schemes (e.g., central tendency, dispersion), they have remained unexplored in the scope of defect prediction. In this study, we set out to investigate how different aggregation schemes impact defect prediction models. Through an analysis of 11 aggregation schemes using data collected from 255 open source projects, we find that: (1) aggregation schemes can significantly alter correlations among metrics, as well as the correlations between metrics and the defect count; (2) when constructing models to predict defect proneness, applying only the summation scheme (i.e., the most commonly used aggregation scheme in the literature) only achieves the best performance (the best among the 12 studied configurations) in 11 percent of the studied projects, while applying all of the studied aggregation schemes achieves the best performance in 40 percent of the studied projects; (3) when constructing models to predict defect rank or count, either applying only the summation or applying all of the studied aggregation schemes achieves similar performance, with both achieving the closest to the best performance more often than the other s- udied aggregation schemes; and (4) when constructing models for effort-aware defect prediction, the mean or median aggregation schemes yield performance values that are significantly closer to the best performance than any of the other studied aggregation schemes. Broadly speaking, the performance of defect prediction models are often underestimated due to our community's tendency to only use the summation aggregation scheme. Given the potential benefit of applying additional aggregation schemes, we advise that future defect prediction models should explore a variety of aggregation schemes.",0098-5589;00985589,,10.1109/TSE.2016.2599161,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7539677,Defect prediction;aggregation scheme;software metrics,Correlation;Data models;Indexes;Predictive models;Software;Software metrics,Java;data aggregation;data mining;public domain software;software metrics,Cc;Java projects;McCabe cyclomatic complexity;Sloc;class-level software metrics;defect prediction models;effort-aware defect prediction;granularity disagreement;historical defect data mining;lines of code;method-level software metrics;open source projects;software changes recording;software metrics aggregation;software organizations;summation;version control system,,1,,,,,20160810,May 1 2017,,IEEE,IEEE Journals & Magazines,,15
Autofolding for Source Code Summarization,J. Fowkes; P. Chanthirasegaran; R. Ranca; M. Allamanis; M. Lapata; C. Sutton,"School of Informatics, University of Edinburgh, Edinburgh, UK",IEEE Transactions on Software Engineering,20171208,2017,43,12,1095,1109,"Developers spend much of their time reading and browsing source code, raising new opportunities for summarization methods. Indeed, modern code editors provide code folding, which allows one to selectively hide blocks of code. However this is impractical to use as folding decisions must be made manually or based on simple rules. We introduce the autofolding problem, which is to automatically create a code summary by folding less informative code regions. We present a novel solution by formulating the problem as a sequence of AST folding decisions, leveraging a scoped topic model for code tokens. On an annotated set of popular open source projects, we show that our summarizer outperforms simpler baselines, yielding a 28 percent error reduction. Furthermore, we find through a case study that our summarizer is strongly preferred by experienced developers. More broadly, we hope this work will aid program comprehension by turning code folding into a usable and valuable tool.",0098-5589;00985589,,10.1109/TSE.2017.2664836,10.13039/501100000266 - Engineering and Physical Sciences Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843666,"Source code summarization, program comprehension, topic modelling",Complexity theory;Feature extraction;Natural languages;Software development;Source coding,public domain software;source code (software),AST folding decisions;autofolding problem;code folding;code tokens;informative code regions;modern code editors;open source projects;source code summarization,,,,,,,20170206,Dec. 1 2017,,IEEE,IEEE Journals & Magazines,,14
AutoSense: A Framework for Automated Sensitivity Analysis of Program Data,B. Nongpoh; R. Ray; S. Dutta; A. Banerjee,"Department of Computer Science & Engineering, National Institute of Technology Meghalaya, Shillong, India",IEEE Transactions on Software Engineering,20171208,2017,43,12,1110,1124,"In recent times, approximate computing is being increasingly adopted across the computing stack, from algorithms to computing hardware, to gain energy and performance efficiency by trading accuracy within acceptable limits. Approximation aware programming languages have been proposed where programmers can annotate data with type qualifiers (e.g., precise and approx) to denote its reliability. However, programmers need to judiciously annotate so that the accuracy loss remains within the desired limits. This can be non-trivial for large applications where error resilient and non-resilient program data may not be easily identifiable. Mis-annotation of even one data as error resilient/insensitive may result in an unacceptable output. In this paper, we present AutoSense, a framework to automatically classify resilient (insensitive) program data versus the sensitive ones with probabilistic reliability guarantee. AutoSense implements a combination of dynamic and static analysis methods for data sensitivity analysis. The dynamic analysis is based on statistical hypothesis testing, while the static analysis is based on classical data flow analysis. Experimental results compare our automated data classification with reported manual annotations on popular benchmarks used in approximate computing literature. AutoSense achieves promising reliability results compared to manual annotations and earlier methods, as evident from the experimental results.",0098-5589;00985589,,10.1109/TSE.2017.2654251,National Institute of Technology Meghalaya and Visvesvaraya Ph.D. Scheme; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820185,Approximate computing;hypothesis testing;sensitivity analysis;sequential probability ratio test,Approximate computing;Probabilistic logic;Quality of service;Sensitivity analysis;Sequential analysis,data flow analysis;pattern classification;probability;program diagnostics;program verification;sensitivity analysis;statistical analysis,AutoSense;accuracy loss;approximate computing literature;approximation aware programming languages;automated data classification;automated sensitivity analysis;classical data flow analysis;computing hardware;computing stack;data sensitivity analysis;dynamic analysis;energy efficiency;manual annotations;performance efficiency;probabilistic reliability;resilient program data;static analysis;trading accuracy,,,,,,,20170117,Dec. 1 2017,,IEEE,IEEE Journals & Magazines,,14
Interaction Models and Automated Control under Partial Observable Environments,D. Ciolek; V. Braberman; N. D_ÑéIppolito; N. Piterman; S. Uchitel,"Departamento de Computaci&#x00F3;n, Universidad de Buenos Aires, Argentina",IEEE Transactions on Software Engineering,20170109,2017,43,1,19,33,"The problem of automatically constructing a software component such that when executed in a given environment satisfies a goal, is recurrent in software engineering. Controller synthesis is a field which fits into this vision. In this paper we study controller synthesis for partially observable LTS models. We exploit the link between partially observable control and non-determinism and show that, unlike fully observable LTS or Kripke structure control problems, in this setting the existence of a solution depends on the interaction model between the controller-to-be and its environment. We identify two interaction models, namely Interface Automata and Weak Interface Automata, define appropriate control problems and describe synthesis algorithms for each of them.",0098-5589;00985589,,10.1109/TSE.2016.2564959,ANPCYT PICT; CONICET PIP; MEALS; PBM-FIMBSE; UBACYT; 10.13039/501100000781 - ERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7466810,LTS;controller synthesis;imperfect-information games,Automata;Context;Maintenance engineering;Observability;Servers;Uncertainty,automata theory;software engineering,automated control;controller synthesis;interaction models;labelled transition systems;partially observable LTS models;partially observable control;software engineering;weak interface automata,,,,,,,20160509,Jan. 1 2017,,IEEE,IEEE Journals & Magazines,,14
Language Inclusion Checking of Timed Automata with Non-Zenoness,X. Wang; J. Sun; T. Wang; S. Qin,"College of Computer Science, Zhejiang University, Hangzhou, P.R. China",IEEE Transactions on Software Engineering,20171110,2017,43,11,995,1008,"Given a timed automaton P modeling an implementation and a timed automaton S as a specification, the problem of language inclusion checking is to decide whether the language of P is a subset of that of S. It is known to be undecidable. The problem gets more complicated if non-Zenoness is taken into consideration. A run is Zeno if it permits infinitely many actions within finite time. Otherwise it is non-Zeno. Zeno runs might present in both P and S. It is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples of language inclusion checking. In this work, we propose a zone-based semi-algorithm for language inclusion checking with non-Zenoness. It is further improved with simulation reduction based on LU-simulation. Though our approach is not guaranteed to terminate, we show that it does in many cases through empirical study. Our approach has been incorporated into the PAT model checker, and applied to multiple systems to show its usefulness.",0098-5589;00985589,,10.1109/TSE.2017.2653778,10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100007040 - Singapore University of Technology and Design; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7819478,Timed automata;language inclusion;non-Zenoness,Analytical models;Automata;Clocks;Real-time systems;Safety;Semantics;Sun,automata theory;formal specification;formal verification,LU-simulation;PAT model checker;finite time;language inclusion checking;nonZenoness;simulation reduction;specification;timed automata;timed automaton modeling;undecidability;zone-based semi-algorithm,,,,,,,20170116,Nov. 1 2017,,IEEE,IEEE Journals & Magazines,,13
Automatic Contract Insertion with CCBot,S. A. Carr; F. Logozzo; M. Payer,"Purdue University, West Lafayette, IN",IEEE Transactions on Software Engineering,20170811,2017,43,8,701,714,"Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.",0098-5589;00985589,,10.1109/TSE.2016.2625248,10.13039/100000001 - NSF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7736073,Contract-based verification;assertions;automated patching;class invariants,C# languages;Computer bugs;Contracts;Instruments;Reactive power;Runtime;Semantics,C# language;program compilers;program diagnostics;program verification;software fault tolerance,CCBot;CodeContracts Bot;automatic code transformation;automatic contract insertion;contract-based static analysis;contract-based verification;fault detection;file paths;null pointer dereferences;object invariants;open-source C# implementation;out-of-bounds array accesses;static analysis tools;static contract checker,,,,,,,20161104,Aug. 1 2017,,IEEE,IEEE Journals & Magazines,,13
Reasoning About Identifier Spaces: How to Make Chord Correct,P. Zave,"AT&T Laboratories&#x2014;Research, Bedminster, NJ",IEEE Transactions on Software Engineering,20171208,2017,43,12,1144,1156,"The Chord distributed hash table (DHT) is well-known and often used to implement peer-to-peer systems. Chord peers find other peers, and access their data, through a ring-shaped pointer structure in a large identifier space. Despite claims of proven correctness, i.e., eventual reachability, previous work has shown that the Chord ring-maintenance protocol is not correct under its original operating assumptions. Previous work has not, however, discovered whether Chord could be made correct under the same assumptions. The contribution of this paper is to provide the first specification of correct operations and initialization for Chord, an inductive invariant that is necessary and sufficient to support a proof of correctness, and two independent proofs of correctness. One proof is informal and intuitive, and applies to networks of any size. The other proof is based on a formal model in Alloy, and uses fully automated analysis to prove the assertions for networks of bounded size. The two proofs complement each other in several important ways.",0098-5589;00985589,,10.1109/TSE.2017.2655056,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823003,Computers and information processing;distributed computing;formal verification;peer-to-peer computing;software engineering,Analytical models;Distributed processing;Formal verification;Information processing;Peer-to-peer computing;Structural rings,peer-to-peer computing;protocols;telecommunication network topology,Chord peers;Chord ring-maintenance protocol;hash table;identifier space;peer-to-peer systems;ring-shaped pointer structure,,,,,,,20170118,Dec. 1 2017,,IEEE,IEEE Journals & Magazines,,12
Mining Sequences of Developer Interactions in Visual Studio for Usage Smells,K. Damevski; D. C. Shepherd; J. Schneider; L. Pollock,"Department of Computer Science, Virginia Commonwealth University, Richmond, VA",IEEE Transactions on Software Engineering,20170414,2017,43,4,359,371,"In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional.",0098-5589;00985589,,10.1109/TSE.2016.2592905,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516714,IDE usage data;data mining;pattern mining;usability analysis,Data analysis;Data mining;Debugging;Navigation;Software engineering;Usability;Visualization,data mining;program debugging;software engineering,IDE interactions;active file navigation;code search;developer interactions sequences mining;frequent IDE usage pattern mining;large-scale dataset mining;software development activities;time-ordered sequences identifies;usage smells extraction;visual studio,,,,,,,20160719,April 1 2017,,IEEE,IEEE Journals & Magazines,,12
Testing from Partial Finite State Machines without Harmonised Traces,R. M. Hierons,"Department of Computer Science, Brunel University London, Uxbridge, United Kingdom",IEEE Transactions on Software Engineering,20171110,2017,43,11,1033,1043,"This paper concerns the problem of testing from a partial, possibly non-deterministic, finite state machine (FSM) S. Two notions of correctness (quasi-reduction and quasi-equivalence) have previously been defined for partial FSMs but these, and the corresponding test generation techniques, only apply to FSMs that have harmonised traces. We show how quasi-reduction and quasi-equivalence can be generalised to all partial FSMs. We also consider the problem of generating an m-complete test suite from a partial FSM S: a test suite that is guaranteed to determine correctness as long as the system under test has no more than m states. We prove that we can complete S to form a completely-specified non-deterministic FSM S' such that any m-complete test suite generated from S' can be converted into an m-complete test suite for S. We also show that there is a correspondence between test suites that are reduced for S and S' and also that are minimal for S and S'.",0098-5589;00985589,,10.1109/TSE.2017.2652457,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815407,Software engineering/software/program verification;checking experiment;partial finite state machine;software engineering/testing and debugging;systems and software,Automata;Debugging;Fault detection;Indexes;Redundancy;Software;Testing,finite state machines;formal specification;program testing,finite state machine;harmonised traces;m-complete test suite;nondeterministic FSM;partial FSMs;partial finite state machines;test generation techniques,,,,,,,20170116,Nov. 1 2017,,IEEE,IEEE Journals & Magazines,,10
Automated Test Case Generation as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets,A. Panichella; F. M. Kifetew; P. Tonella,"SnT, University of Luxembourg, Luxembourg, Esch-sur-Alzette, Luxembourg",IEEE Transactions on Software Engineering,20180212,2018,44,2,122,158,"The test case generation is intrinsically a multi-objective problem, since the goal is covering multiple test targets (e.g., branches). Existing search-based approaches either consider one target at a time or aggregate all targets into a single fitness function (whole-suite approach). Multi and many-objective optimisation algorithms (MOAs) have never been applied to this problem, because existing algorithms do not scale to the number of coverage objectives that are typically found in real-world software. In addition, the final goal for MOAs is to find alternative trade-off solutions in the objective space, while in test generation the interesting solutions are only those test cases covering one or more uncovered targets. In this paper, we present Dynamic Many-Objective Sorting Algorithm (DynaMOSA), a novel many-objective solver specifically designed to address the test case generation problem in the context of coverage testing. DynaMOSA extends our previous many-objective technique Many-Objective Sorting Algorithm (MOSA) with dynamic selection of the coverage targets based on the control dependency hierarchy. Such extension makes the approach more effective and efficient in case of limited search budget. We carried out an empirical study on 346 Java classes using three coverage criteria (i.e., statement, branch, and strong mutation coverage) to assess the performance of DynaMOSA with respect to the whole-suite approach (WS), its archive-based variant (WSA) and MOSA. The results show that DynaMOSA outperforms WSA in 28 percent of the classes for branch coverage (+8 percent more coverage on average) and in 27 percent of the classes for mutation coverage (+11 percent more killed mutants on average). It outperforms WS in 51 percent of the classes for statement coverage, leading to +11 percent more coverage on average. Moreover, DynaMOSA outperforms its predecessor MOSA for all the three coverage criteria in 19 percent of the classes with +8 percent more code coverage o- average.",0098-5589;00985589,,10.1109/TSE.2017.2663435,National Research Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840029,Evolutionary testing;automatic test case generation;many-objective optimisation,Algorithm design and analysis;Genetic algorithms;Heuristic algorithms;Optimization;Software algorithms;Sorting;Testing,optimisation;program testing;search problems;sorting,DynaMOSA;MOAs;Many-Objective Sorting Algorithm;Many-Objective optimisation problem;automated test case generation;branch coverage;dynamic target selection;many-objective optimisation algorithms;many-objective solver;multiobjective problem;multiple test targets;search-based approaches;test input data;test sequence,,1,,,,,20170202,Feb. 1 2018,,IEEE,IEEE Journals & Magazines,,36
Automatic Software Refactoring via Weighted Clustering in Method-Level Networks,Y. Wang; H. Yu; Z. Zhu; W. Zhang; Y. Zhao,"Software College, Northeastern University, Shenyang, China",IEEE Transactions on Software Engineering,20180313,2018,44,3,202,236,"In this study, we describe a system-level multiple refactoring algorithm, which can identify the move method, move field, and extract class refactoring opportunities automatically according to the principle of _ÑÒhigh cohesion and low coupling._Ñù The algorithm works by merging and splitting related classes to obtain the optimal functionality distribution from the system-level. Furthermore, we present a weighted clustering algorithm for regrouping the entities in a system based on merged method-level networks. Using a series of preprocessing steps and preconditions, the _ÑÒbad smells_Ñù introduced by cohesion and coupling problems can be removed from both the non-inheritance and inheritance hierarchies without changing the code behaviors. We rank the refactoring suggestions based on the anticipated benefits that they bring to the system. Based on comparisons with related research and assessing the refactoring results using quality metrics and empirical evaluation, we show that the proposed approach performs well in different systems and is beneficial from the perspective of the original developers. Finally, an open source tool is implemented to support the proposed approach.",0098-5589;00985589,,10.1109/TSE.2017.2679752,"MOE research center for online education, China; National Natural Science Foundation of China; Ph.D. Start-up Foundation of Liaoning Province, China; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874207,Clustering analysis;cohesion;complex network;coupling;software refactoring,Clustering algorithms;Couplings;Measurement;Partitioning algorithms;Software algorithms;Software systems,network theory (graphs);pattern clustering;software maintenance;software quality,automatic software refactoring;class refactoring opportunities;coupling problems;high cohesion;inheritance hierarchies;low coupling;merged method-level networks;merging classes;open source tool;optimal functionality distribution;preprocessing steps;refactoring results;refactoring suggestions;splitting related classes;system-level multiple refactoring algorithm;weighted clustering algorithm,,,,,,,20170308,March 1 2018,,IEEE,IEEE Journals & Magazines,,34
Hybrid Program Dependence Approximation for Effective Dynamic Impact Prediction,H. Cai,"School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA",IEEE Transactions on Software Engineering,20180416,2018,44,4,334,364,"Impact analysis determines the effects that program entities of interest, or changes to them, may have on the rest of the program for software measurement, maintenance, and evolution tasks. Dynamic impact analysis could be one major approach to impact analysis that computes smaller impact setsthan static alternatives for concrete sets of executions. However, existing dynamic approaches often produce impact sets that are too large to be useful, hindering their adoption in practice. To address this problem, we propose to exploit static program dependencies to drastically prune false-positive impacts that are not exercised by the set of executions utilized by the analysis, via hybrid dependence approximation. Further, we present a novel dynamic impact analysis called Diver which leverages both the information provided by the dependence graph and method-execution events to identify runtimemethod-level dependencies, hence dynamic impact sets, much more precisely without reducing safety and at acceptable costs. We evaluate Diver on ten Java subjects of various sizes and application domains against both arbitrary queries covering entire programs and practical queries based on changes actually committed by developers to actively evolving software repositories. Our extensive empirical studies show that Diver can significantly improve the precision of impact prediction, with 100-186 percent increase, with respect to a representative existing alternative thus provide a far more effective option for dynamic impact prediction. Following a similar rationale to Diver, we further developed and evaluated an online dynamic impact analysis called DiverOnline which produces impact sets immediately upon the termination of program execution. Our results show that compared to the offline approach, for the same precision, the online approach can reduce the time by 50 percent on average for answering all possible queries in the given program at once albeit at the price of possibly signific- nt increase in runtime overhead. For users interested in one specific query only, the online approach may compute the impact set for that query during runtime without much slowing down normal program operation. Further, the online analysis, which does not incur any space cost beyond the static-analysis phase, may be favored against the offline approach when trace storage and/or related file-system resource consumption becomes a serious challenge or even stopper for adopting dynamic impact prediction. Therefore, the online and offline analysis together offer complementary options to practitioners accommodating varied application/task scenarios and diverse budget constraints.",0098-5589;00985589,,10.1109/TSE.2017.2692783,ONR; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7895205,Static program dependence;dynamic analysis;efficiency;impact prediction;online impact analysis;precision,Concrete;Java;Maintenance engineering;Performance analysis;Runtime;Software;Software measurement,Java;graph theory;program diagnostics;software maintenance;system monitoring,Diver;DiverOnline;Java;dependence graph;dynamic impact prediction;false-positive impacts;hybrid program dependence approximation;offline analysis;online dynamic impact analysis;program execution;runtime method-level dependencies;software evolution;software maintenance;software measurement;software repositories;static program dependencies;static-analysis phase,,,,,,,20170412,April 1 2018,,IEEE,IEEE Journals & Magazines,,30
Test Case Generation for Boolean Expressions by Cell Covering,L. Yu; W. T. Tsai,"School of Software and Microelectronics in Peking University, Beijing, China",IEEE Transactions on Software Engineering,20180110,2018,44,1,70,99,"This paper characterizes Boolean expression faults as changes of the topological structures in terms of shrinking and/or expanding regions in K-map. A cell-covering is a set of cells (test cases) in K-map to cover the fault regions such that faults guarantee to be detected. Minimizing cell covering can be formulated as an Integer Linear Programming (ILP) problem. By analyzing the structures of the constraint coefficient matrix, the original problem can be decomposed into sub-programs that can be solved instead of the original problem, and this significantly reduces the time needed for ILP execution. An efficient approximate algorithm with a tight theoretical bound is used to address those complex Boolean expressions by corresponding the cell-covering problem to the set-covering problem. The optimal approach and the approximate approach are combined into a hybrid process to identify test cases based on the fraction analysis on the ILP relaxation. The proposed approach is evaluated by three sets of Boolean expressions and the results are compared with three leading approaches with respect to test sizes, time consumption and fault detection capabilities. For most Boolean expressions encountered, the proposed approach obtains optimal solutions quickly, and produces near-optimal solutions rapidly for those rare and complex expressions.",0098-5589;00985589,,10.1109/TSE.2017.2669184,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855791,Boolean expression testing;approximate algorithms;cell-covering problem;fault characterization,Algorithm design and analysis;Approximation algorithms;Fault detection;Optimization;Periodic structures;Software;Testing,Boolean functions;approximation theory;computational complexity;fault diagnosis;integer programming;linear programming;set theory,Boolean expression faults;cell covering minimization;cell-covering;complex Boolean expressions;complex expressions;fault detection capabilities;integer linear programming problem;set-covering problem;test case generation;topological structures,,,,,,,20170214,Jan. 1 2018,,IEEE,IEEE Journals & Magazines,,29
Complete and Interpretable Conformance Checking of Business Processes,L. GarcÕ_a-BaÕ±uelos; N. R. T. P. van Beest; M. Dumas; M. L. Rosa; W. Mertens,"University of Tartu, Tartu, Estonia",IEEE Transactions on Software Engineering,20180313,2018,44,3,262,290,"This article presents a method for checking the conformance between an event log capturing the actual execution of a business process, and a model capturing its expected or normative execution. Given a process model and an event log, the method returns a set of statements in natural language describing the behavior allowed by the model but not observed in the log and vice versa. The method relies on a unified representation of process models and event logs based on a well-known model of concurrency, namely event structures. Specifically, the problem of conformance checking is approached by converting the event log into an event structure, converting the process model into another event structure, and aligning the two event structures via an error-correcting synchronized product. Each difference detected in the synchronized product is then verbalized as a natural language statement. An empirical evaluation shows that the proposed method can handle real datasets and produces more concise and higher-level difference descriptions than state-of-the-art conformance checking methods. In a survey designed according to the technology acceptance model, practitioners showed a preference towards the proposed method with respect to a state-of-the-art baseline.",0098-5589;00985589,,10.1109/TSE.2017.2668418,Australian Research Council Discovery; Estonian Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852436,Process mining;conformance checking;event log;event structure;process model,Business;Computational modeling;Context modeling;Data mining;Natural languages;Software systems;Synchronization,business data processing;data mining;natural languages,business process expected execution;business process normative execution;concurrency model;conformance checking;error-correcting synchronized product;event log;event structure;natural language statement;process model;technology acceptance model,,,,,,,20170213,March 1 2018,,IEEE,IEEE Journals & Magazines,,28
Understanding Diverse Usage Patterns from Large-Scale Appstore-Service Profiles,X. Liu; H. Li; X. Lu; T. Xie; Q. Mei; F. Feng; H. Mei,"Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, Beijing, China",IEEE Transactions on Software Engineering,20180416,2018,44,4,384,411,"The prevalence of smart mobile devices has promoted the popularity of mobile applications (a.k.a. apps). Supporting mobility has become a promising trend in software engineering research. This article presents an empirical study of behavioral service profiles collected from millions of users whose devices are deployed with Wandoujia, a leading Android app-store service in China. The dataset of Wandoujia service profiles consists of two kinds of user behavioral data from using 0.28 million free Android apps, including (1) app management activities (i.e., downloading, updating, and uninstalling apps) from over 17 million unique users and (2) app network usage from over 6 million unique users. We explore multiple aspects of such behavioral data and present patterns of app usage. Based on the findings as well as derived knowledge, we also suggest some new open opportunities and challenges that can be explored by the research community, including app development, deployment, delivery, revenue, etc.",0098-5589;00985589,,10.1109/TSE.2017.2685387,MCubed; National Basic Research Program (973) of China; National Science Foundation; Natural Science Foundation of China; University of Michigan; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883939,Mobile apps;app store;user behavior analysis,Androids;Biological system modeling;Electronic mail;Humanoid robots;Mobile communication;Software;Software engineering,Android (operating system);mobile computing;public domain software;software engineering,Wandoujia service profiles;app development;app management activities;app network usage;app usage;behavioral service profiles;diverse usage patterns;free Android apps;large-scale appstore-service profiles;leading Android app-store service;mobile applications;smart mobile devices;software engineering research;user behavioral data,,,,,,,20170321,April 1 2018,,IEEE,IEEE Journals & Magazines,,27
Detecting Trivial Mutant Equivalences via Compiler Optimisations,M. Kintis; M. Papadakis; Y. Jia; N. Malevris; Y. Le Traon; M. Harman,"Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Esch-sur-Alzette 4365, Luxembourg",IEEE Transactions on Software Engineering,20180416,2018,44,4,308,333,"Mutation testing realises the idea of fault-based testing, i.e., using artificial defects to guide the testing process. It is used to evaluate the adequacy of test suites and to guide test case generation. It is a potentially powerful form of testing, but it is well-known that its effectiveness is inhibited by the presence of equivalent mutants. We recently studied Trivial Compiler Equivalence (TCE) as a simple, fast and readily applicable technique for identifying equivalent mutants for C programs. In the present work, we augment our findings with further results for the Java programming language. TCE can remove a large portion of all mutants because they are determined to be either equivalent or duplicates of other mutants. In particular, TCE equivalent mutants account for 7.4 and 5.7 percent of all C and Java mutants, while duplicated mutants account for a further 21 percent of all C mutants and 5.4 percent Java mutants, on average. With respect to a benchmark ground truth suite (of known equivalent mutants), approximately 30 percent (for C) and 54 percent (for Java) are TCE equivalent. It is unsurprising that results differ between languages, since mutation characteristics are language-dependent. In the case of Java, our new results suggest that TCE may be particularly effective, finding almost half of all equivalent mutants.",0098-5589;00985589,,10.1109/TSE.2017.2684805,"Centre for Research on Evolution Search and Testing (CREST); EPSRC project; Microsoft Azure; National Research Fund, Luxembourg; Research Centre of Athens University of Economics and Business (RC/AUEB); UCL; UK EPSRC projects; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7882714,Mutation testing;compiler optimisation;duplicated mutants;equivalent mutants,Electronic mail;Java;Optimization;Program processors;Syntactics;Testing,Java;program compilers;program testing,Java mutants;Java programming language;TCE duplicated mutants;TCE equivalent mutants;compiler equivalence;fault-based testing;mutation testing;test case generation;trivial mutant equivalences detection,,,,,,,20170320,April 1 2018,,IEEE,IEEE Journals & Magazines,,25
Empirical Evaluation of the Impact of Object-Oriented Code Refactoring on Quality Attributes: A Systematic Literature Review,J. Al Dallal; A. Abdin,"Department of Information Science, Kuwait University, Safat, Kuwait",IEEE Transactions on Software Engineering,20180108,2018,44,1,44,69,"Software refactoring is a maintenance task that addresses code restructuring to improve its quality. Many studies have addressed the impact of different refactoring scenarios on software quality. This study presents a systematic literature review that aggregates, summarizes, and discusses the results of 76 relevant primary studies (PSs) concerning the impact of refactoring on several internal and external quality attributes. The included PSs were selected using inclusion and exclusion criteria applied to relevant articles published before the end of 2015. We analyzed the PSs based on a set of classification criteria, including software quality attributes and measures, refactoring scenarios, evaluation approaches, datasets, and impact results. We followed the vote-counting approach to determine the level of consistency among the PS reported results concerning the relationship between refactoring and software quality. The results indicated that different refactoring scenarios sometimes have opposite impacts on different quality attributes. Therefore, it is false that refactoring always improves all software quality aspects. The vote-counting study provided a clear view of the impacts of some individual refactoring scenarios on some internal quality attributes such as cohesion, coupling, complexity, inheritance, and size, but failed to identify their impacts on external and other internal quality attributes due to insufficient findings.",0098-5589;00985589,,10.1109/TSE.2017.2658573,; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833023,quality attribute;quality measure;refactoring scenario;systematic literature review,Bibliographies;Libraries;Object oriented modeling;Software quality;Systematics;Unified modeling language,software maintenance;software metrics;software quality,76 relevant primary studies;PSs;classification criteria;code restructuring;different quality attributes;different refactoring scenarios;evaluation approaches;external quality;individual refactoring scenarios;internal quality;maintenance task;opposite impacts;software quality aspects;software quality attributes;software refactoring;systematic literature review;vote-counting approach;vote-counting study,,,,,,,20170125,Jan. 1 2018,,IEEE,IEEE Journals & Magazines,,25
"Choosing Component Origins for Software Intensive Systems: In-House, COTS, OSS or Outsourcing?_ÑÓA Case Survey",K. Petersen; D. Badampudi; S. M. A. Shah; K. Wnuk; T. Gorschek; E. Papatheocharous; J. Axelsson; S. Sentilles; I. Crnkovic; A. Cicchetti,"Department of Software Engineering, Blekinge Institute of Technology, Campus Gr&#x00E4;svik, Karlskrona, Sweden",IEEE Transactions on Software Engineering,20180313,2018,44,3,237,261,"The choice of which software component to use influences the success of a software system. Only a few empirical studies investigate how the choice of components is conducted in industrial practice. This is important to understand to tailor research solutions to the needs of the industry. Existing studies focus on the choice for off-the-shelf (OTS) components. It is, however, also important to understand the implications of the choice of alternative component sourcing options (CSOs), such as outsourcing versus the use of OTS. Previous research has shown that the choice has major implications on the development process as well as on the ability to evolve the system. The objective of this study is to explore how decision making took place in industry to choose among CSOs. Overall, 22 industrial cases have been studied through a case survey. The results show that the solutions specifically for CSO decisions are deterministic and based on optimization approaches. The non-deterministic solutions proposed for architectural group decision making appear to suit the CSO decision making in industry better. Interestingly, the final decision was perceived negatively in nine cases and positively in seven cases, while in the remaining cases it was perceived as neither positive nor negative.",0098-5589;00985589,,10.1109/TSE.2017.2677909,ORION project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870688,COTS;Decision making;OSS;in-house;outsourcing,Companies;Computer architecture;Decision making;Industries;Outsourcing;Software,decision making;object-oriented programming;outsourcing;software architecture,COTS;CSO decision making;CSO decisions;CSOs;OSS;OTS;Outsourcing;alternative component sourcing options;architectural group decision;architectural group decision making;component origins;nondeterministic solutions;off-the-shelf components;software component;software intensive systems;software system;tailor research solutions,,1,,,,,20170303,March 1 2018,,IEEE,IEEE Journals & Magazines,,24
Detecting Overly Strong Preconditions in Refactoring Engines,M. Mongiovi; R. Gheyi; G. Soares; M. Ribeiro; P. Borba; L. Teixeira,"Department of Computing and Systems, Federal University of Campina Grande, Campina Grande, PB, Brazil",IEEE Transactions on Software Engineering,20180514,2018,44,5,429,452,"Refactoring engines may have overly strong preconditions preventing developers from applying useful transformations. We find that 32 percent of the Eclipse and JRRT test suites are concerned with detecting overly strong preconditions. In general, developers manually write test cases, which is costly and error prone. Our previous technique detects overly strong preconditions using differential testing. However, it needs at least two refactoring engines. In this work, we propose a technique to detect overly strong preconditions in refactoring engines without needing reference implementations. We automatically generate programs and attempt to refactor them. For each rejected transformation, we attempt to apply it again after disabling the preconditions that lead the refactoring engine to reject the transformation. If it applies a behavior preserving transformation, we consider the disabled preconditions overly strong. We evaluate 10 refactorings of Eclipse and JRRT by generating 154,040 programs. We find 15 overly strong preconditions in Eclipse and 15 in JRRT. Our technique detects 11 bugs that our previous technique cannot detect while missing 5 bugs. We evaluate the technique by replacing the programs generated by JDolly with the input programs of Eclipse and JRRT test suites. Our technique detects 14 overly strong preconditions in Eclipse and 4 in JRRT.",0098-5589;00985589,,10.1109/TSE.2017.2693982,CAPES; CNPq; DEVASSES; FACEPE; FAPEAL PPGs; National Institute of Science and Technology for Software Engineering (INES); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898404,Refactoring;automated testing;overly strong preconditions;program generation,Computer bugs;Databases;Electronic mail;Engines;Java;Testing;Usability,C language;Java;automatic programming;object-oriented programming;program debugging;program testing;software maintenance,Eclipse;JDolly;JRRT test suites;differential testing;overly strong preconditions;refactoring engine;rejected transformation,,,,,,,20170412,May 1 2018,,IEEE,IEEE Journals & Magazines,,23
Predicting Delivery Capability in Iterative Software Development,M. Choetkiertikul; H. K. Dam; T. Tran; A. Ghose; J. Grundy,"Faculty of Engineering and Information Sciences, School of Computing and Information Technology, University of Wollongong, NSW, Australia",IEEE Transactions on Software Engineering,20180612,2018,44,6,551,573,"Iterative software development has become widely practiced in industry. Since modern software projects require fast, incremental delivery for every iteration of software development, it is essential to monitor the execution of an iteration, and foresee a capability to deliver quality products as the iteration progresses. This paper presents a novel, data-driven approach to providing automated support for project managers and other decision makers in predicting delivery capability for an ongoing iteration. Our approach leverages a history of project iterations and associated issues, and in particular, we extract characteristics of previous iterations and their issues in the form of features. In addition, our approach characterizes an iteration using a novel combination of techniques including feature aggregation statistics, automatic feature learning using the Bag-of-Words approach, and graph-based complexity measures. An extensive evaluation of the technique on five large open source projects demonstrates that our predictive models outperform three common baseline methods in Normalized Mean Absolute Error and are highly accurate in predicting the outcome of an ongoing iteration.",0098-5589;00985589,,10.1109/TSE.2017.2693989,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898472,Mining software engineering repositories;empirical software engineering;iterative software development,Agile software development;Complexity theory;Data mining;Feature extraction;Iterative methods;Predictive models;Software,,,,1,,,,,20170412,June 1 2018,,IEEE,IEEE Journals & Magazines,,22
Measuring the Impact of Code Dependencies on Software Architecture Recovery Techniques,T. Lutellier; D. Chollak; J. Garcia; L. Tan; D. Rayside; N. Medvidovi—_; R. Kroeger,"University of Waterloo, Waterloo, ON, Canada",IEEE Transactions on Software Engineering,20180212,2018,44,2,159,181,"Many techniques have been proposed to automatically recover software architectures from software implementations. A thorough comparison among the recovery techniques is needed to understand their effectiveness and applicability. This study improves on previous studies in two ways. First, we study the impact of leveraging accurate symbol dependencies on the accuracy of architecture recovery techniques. In addition, we evaluate other factors of the input dependencies such as the level of granularity and the dynamic-bindings graph construction. Second, we recovered the architecture of a large system, Chromium, that was not available previously. Obtaining the ground-truth architecture of Chromium involved two years of collaboration with its developers. As part of this work, we developed a new submodule-based technique to recover preliminary versions of ground-truth architectures. The results of our evaluation of nine architecture recovery techniques and their variants suggest that (1) using accurate symbol dependencies has a major influence on recovery quality, and (2) more accurate recovery techniques are needed. Our results show that some of the studied architecture recovery techniques scale to very large systems, whereas others do not.",0098-5589;00985589,,10.1109/TSE.2017.2671865,"Google Faculty Research Award; Infosys Technologies, Ltd.; Natural Sciences and Engineering Research Council of Canada; Ontario Ministry of Research and Innovation; U.S. National Science Foundation; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859416,Software architecture;empirical software engineering;maintenance and evolution;program comprehension,Chromium;Computer architecture;Heuristic algorithms;Manuals;Software;Software algorithms;Software architecture,software architecture;software quality;system recovery,accurate recovery techniques;code dependencies;ground-truth architecture;input dependencies;recovery quality;software architecture recovery techniques;software architectures;software implementations;submodule-based technique;symbol dependencies,,,,,,,20170220,Feb. 1 2018,,IEEE,IEEE Journals & Magazines,,22
A PVS-Simulink Integrated Environment for Model-Based Analysis of Cyber-Physical Systems,C. Bernardeschi; A. Domenici; P. Masci,"Department of Information Engineering, University of Pisa, PI, Italy",IEEE Transactions on Software Engineering,20180612,2018,44,6,512,533,"This paper presents a methodology, with supporting tool, for formal modeling and analysis of software components in cyber-physical systems. Using our approach, developers can integrate a simulation of logic-based specifications of software components and Simulink models of continuous processes. The integrated simulation is useful to validate the characteristics of discrete system components early in the development process. The same logic-based specifications can also be formally verified using the Prototype Verification System (PVS), to gain additional confidence that the software design complies with specific safety requirements. Modeling patterns are defined for generating the logic-based specifications from the more familiar automata-based formalism. The ultimate aim of this work is to facilitate the introduction of formal verification technologies in the software development process of cyber-physical systems, which typically requires the integrated use of different formalisms and tools. A case study from the medical domain is used to illustrate the approach. A PVS model of a pacemaker is interfaced with a Simulink model of the human heart. The overall cyber-physical system is co-simulated to validate design requirements through exploration of relevant test scenarios. Formal verification with the PVS theorem prover is demonstrated for the pacemaker model for specific safety aspects of the pacemaker design.",0098-5589;00985589,,10.1109/TSE.2017.2694423,"North Portugal Regional Operational Programme; PORTUGAL 2020 Partnership Agreement, and through the European Regional Development Fund (ERDF); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900400,Real-time and embedded systems;formal methods;modeling techniques;specification,Analytical models;Automata;Cyber-physical systems;Data models;Mathematical model;Software packages,,,,,,,,,20170414,June 1 2018,,IEEE,IEEE Journals & Magazines,,21
Specialising Software for Different Downstream Applications Using Genetic Improvement and Code Transplantation,J. Petke; M. Harman; W. B. Langdon; W. Weimer,"University College London, London, United Kingdom",IEEE Transactions on Software Engineering,20180612,2018,44,6,574,594,"Genetic improvement uses automated search to find improved versions of existing software. Genetic improvement has previously been concerned with improving a system with respect to all possible usage scenarios. In this paper, we show how genetic improvement can also be used to achieve specialisation to a specific set of usage scenarios. We use genetic improvement to evolve faster versions of a C++ program, a Boolean satisfiability solver called MiniSAT, specialising it for three different applications, each with their own characteristics. Our specialised solvers achieve between 4 and 36 percent execution time improvement, which is commensurate with efficiency gains achievable using human expert optimisation for the general solver. We also use genetic improvement to evolve faster versions of an image processing tool called ImageMagick, utilising code from GraphicsMagick, another image processing tool which was forked from it. We specialise the format conversion functionality to greyscale images and colour images only. Our specialised versions achieve up to 3 percent execution time improvement.",0098-5589;00985589,,10.1109/TSE.2017.2702606,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962212,GI;Genetic improvement;GraphicsMagick;ImageMagick;SAT;code specialisation;code transplants,C++ languages;Genetic programming;Image processing;Optimization;Software;Software engineering,,,,,,,,,20170629,June 1 2018,,IEEE,IEEE Journals & Magazines,,20
EnergyPatch: Repairing Resource Leaks to Improve Energy-Efficiency of Android Apps,A. Banerjee; L. K. Chong; C. Ballabriga; A. Roychoudhury,"School of Computing, National University of Singapore, Singapore",IEEE Transactions on Software Engineering,20180514,2018,44,5,470,490,"Increased usage of mobile devices, such as smartphones and tablets, has led to widespread popularity and usage of mobile apps. If not carefully developed, such apps may demonstrate energy-inefficient behaviour, where one or more energy-intensive hardware components (such as Wifi, GPS, etc) are left in a high-power state, even when no apps are using these components. We refer to such kind of energy-inefficiencies as energy bugs. Executing an app with an energy bug causes the mobile device to exhibit poor energy consumption behaviour and a drastically shortened battery life. Since mobiles apps can have huge input domains, therefore exhaustive exploration is often impractical. We believe that there is a need for a framework that can systematically detect and fix energy bugs in mobile apps in a scalable fashion. To address this need, we have developed EnergyPatch, a framework that uses a combination of static and dynamic analysis techniques to detect, validate and repair energy bugs in Android apps. The use of a light-weight, static analysis technique enables EnergyPatch to quickly narrow down to the potential program paths along which energy bugs may occur. Subsequent exploration of these potentially buggy program paths using a dynamic analysis technique helps in validations of the reported bugs and to generate test cases. Finally, EnergyPatch generates repair expressions to fix the validated energy bugs. Evaluation with real-life apps from repositories such as F-droid and Github, shows that EnergyPatch is scalable and can produce results in reasonable amount of time. Additionally, we observed that the repair expressions generated by EnergyPatch could bring down the energy consumption on tested apps up to 60 percent.",0098-5589;00985589,,10.1109/TSE.2017.2689012,Singapore MoE Tier 2; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889026,Mobile apps;energy bugs;energy-aware test generation;non-functional testing,Androids;Batteries;Computer bugs;Energy consumption;Humanoid robots;Maintenance engineering;Mobile handsets,Android (operating system);energy consumption;mobile computing;power aware computing;program debugging;program diagnostics;program testing;smart phones,Android apps;EnergyPatch;F-droid;Github;buggy program paths;dynamic analysis techniques;energy bug;energy-efficiency;energy-inefficiencies;energy-inefficient behaviour;energy-intensive hardware components;mobile apps;mobile device;mobiles apps;poor energy consumption behaviour;real-life apps;smartphones;static analysis techniques;tablets;tested apps;validated energy bugs,,1,,,,,20170329,May 1 2018,,IEEE,IEEE Journals & Magazines,,20
Reviving Sequential Program Birthmarking for Multithreaded Software Plagiarism Detection,Z. Tian; T. Liu; Q. Zheng; E. Zhuang; M. Fan; Z. Yang,"Department of Computer Science and Technology, Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Xi'an Jiaotong University, Xi&#x0027;an, China",IEEE Transactions on Software Engineering,20180509,2018,44,5,491,511,"As multithreaded programs become increasingly popular, plagiarism of multithreaded programs starts to plague the software industry. Although there has been tremendous progress on software plagiarism detection technology, existing dynamic birthmark approaches are applicable only to sequential programs, due to the fact that thread scheduling nondeterminism severely perturbs birthmark generation and comparison. We propose a framework called TOB (Thread-oblivious dynamic Birthmark) that revives existing techniques so they can be applied to detect plagiarism of multithreaded programs. This is achieved by thread-oblivious algorithms that shield the influence of thread schedules on executions. We have implemented a set of tools collectively called TOB-PD (TOB based Plagiarism Detection tool) by applying TOB to three existing representative dynamic birthmarks, including SCSSB (System Call Short Sequence Birthmark), DYKIS (DYnamic Key Instruction Sequence birthmark) and JB (an API based birthmark for Java). Our experiments conducted on large number of binary programs show that our approach exhibits strong resilience against state-of-the-art semantics-preserving code obfuscation techniques. Comparisons against the three existing tools SCSSB, DYKIS and JB show that the new framework is effective for plagiarism detection of multithreaded programs. The tools, the benchmarks and the experimental results are all publicly available.",0098-5589;00985589,,10.1109/TSE.2017.2688383,Fok Ying-Tong Education Foundation; Fundamental Research Funds for the Central Universities; Ministry of Education Innovation Research Team; National Key Research and Development Program of China; National Science Foundation of China; Science and Technology Project in Shaanxi Province of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888597,Software plagiarism detection;multithreaded program;software birthmark;thread-oblivious birthmark,Computer science;Dynamic scheduling;Electronic mail;Indexes;Instruction sets;Plagiarism,computer crime;fraud;multi-threading;scheduling,API based birthmark;DYnamic Key Instruction Sequence birthmark;Plagiarism Detection tool;System Call Short Sequence Birthmark;Thread-oblivious dynamic Birthmark;binary programs;birthmark generation;dynamic birthmark approaches;multithreaded programs;multithreaded software;sequential program birthmarking;software industry;software plagiarism detection technology;thread schedules;thread scheduling nondeterminism;thread-oblivious algorithms,,,,,,,20170328,May 1 2018,,IEEE,IEEE Journals & Magazines,,20
Semantic Slicing of Software Version Histories,Y. Li; C. Zhu; J. Rubin; M. Chechik,"Department of Computer Science, University of Toronto, Toronto, ON, Canada",IEEE Transactions on Software Engineering,20180212,2018,44,2,182,201,"Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, _ÑÒinheriting_Ñù additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and propose techniques to minimize the produced slice. We then instantiate the overall approach, CSlicer, in a specific implementation for Java projects managed in Git and evaluate its correctness and effectiveness on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones.",0098-5589;00985589,,10.1109/TSE.2017.2664824,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843626,Software changes;dependency;program analysis;version control,Computer bugs;Context;History;Java;Minimization;Semantics;Software,Java;configuration management;program slicing;public domain software,CSlicer;configuration management system;open-source software repositories;semantic slicing problem;semantically-related commits;software developers;software version histories,,1,,,,,20170206,Feb. 1 2018,,IEEE,IEEE Journals & Magazines,,19
A Developer Centered Bug Prediction Model,D. Di Nucci; F. Palomba; G. De Rosa; G. Bavota; R. Oliveto; A. De Lucia,"University of Salerno, Fisciano, SA, Italy",IEEE Transactions on Software Engineering,20180108,2018,44,1,5,24,"Several techniques have been proposed to accurately predict software defects. These techniques generally exploit characteristics of the code artefacts (e.g., size, complexity, etc.) and/or of the process adopted during their development and maintenance (e.g., the number of developers working on a component) to spot out components likely containing bugs. While these bug prediction models achieve good levels of accuracy, they mostly ignore the major role played by human-related factors in the introduction of bugs. Previous studies have demonstrated that focused developers are less prone to introduce defects than non-focused developers. According to this observation, software components changed by focused developers should also be less error prone than components changed by less focused developers. We capture this observation by measuring the scattering of changes performed by developers working on a component and use this information to build a bug prediction model. Such a model has been evaluated on 26 systems and compared with four competitive techniques. The achieved results show the superiority of our model, and its high complementarity with respect to predictors commonly used in the literature. Based on this result, we also show the results of a _ÑÒhybrid_Ñù prediction model combining our predictors with the existing ones.",0098-5589;00985589,,10.1109/TSE.2017.2659747,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835258,Scattering metrics;bug prediction;empirical study;mining software repositories,Complexity theory;Computer bugs;Entropy;Measurement;Predictive models;Scattering;Software,object-oriented programming;program debugging;software maintenance;software reliability,code artefacts;developer centered bug prediction model;hybrid prediction model;software components;software defects prediction,,2,,,,,20170126,Jan. 1 2018,,IEEE,IEEE Journals & Magazines,,19
A Templating System to Generate Provenance,L. Moreau; B. V. Batlajery; T. D. Huynh; D. Michaelides; H. Packer,"Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom",IEEE Transactions on Software Engineering,20180212,2018,44,2,103,121,"PROV-TEMPLATEIS a declarative approach that enables designers and programmers to design and generate provenance compatible with the PROV standard of the World Wide Web Consortium. Designers specify the topology of the provenance to be generated by composing templates, which are provenance graphs containing variables, acting as placeholders for values. Programmers write programs that log values and package them up in sets of bindings, a data structure associating variables and values. An expansion algorithm generates instantiated provenance from templates and sets of bindings in any of the serialisation formats supported by PROV. A quantitative evaluation shows that sets of bindings have a size that is typically 40 percent of that of expanded provenance templates and that the expansion algorithm is suitably tractable, operating in fractions of milliseconds for the type of templates surveyed in the article. Furthermore, the approach shows four significant software engineering benefits: separation of responsibilities, provenance maintenance, potential runtime checks and static analysis, and provenance consumption. The article gathers quantitative data and qualitative benefits descriptions from four different applications making use of PROV-TEMPLATE. The system is implemented and released in the open-source library ProvToolbox for provenance processing.",0098-5589;00985589,,10.1109/TSE.2017.2659745,EPSRC SOCIAM; ESRC eBook; FP7 SmartSociety; ORCHID; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7909036,"<sc xmlns:ali=""http://www.niso.org/schemas/ali/1.0/"" xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"">prov</sc>;Provenance;provenance generation;template",Automobiles;Electronic publishing;Instruments;Libraries;Maintenance engineering;Runtime;Standards,Internet;data structures;graph theory;software engineering;software maintenance,PROV standard;PROV-TEMPLATE;World Wide Web Consortium;data structure;expanded provenance templates;expansion algorithm;open-source library ProvToolbox;provenance consumption;provenance generation;provenance graphs;provenance maintenance;provenance processing;quantitative data;software engineering;templating system,,,,,,,20170424,Feb. 1 2018,,IEEE,IEEE Journals & Magazines,,18
Refactoring Inspection Support for Manual Refactoring Edits,E. L. G. Alves; M. Song; T. Massoni; P. D. L. Machado; M. Kim,"Computer Science Department, Federal University of Campina Grande, Campina Grande, Brazil",IEEE Transactions on Software Engineering,20180416,2018,44,4,365,383,"Refactoring is commonly performed manually, supported by regression testing, which serves as a safety net to provide confidence on the edits performed. However, inadequate test suites may prevent developers from initiating or performing refactorings. We propose RefDistiller, a static analysis approach to support the inspection of manual refactorings. It combines two techniques. First, it applies predefined templates to identify potential missed edits during manual refactoring. Second, it leverages an automated refactoring engine to identify extra edits that might be incorrect. RefDistiller also helps determine the root cause of detected anomalies. In our evaluation, RefDistiller identifies 97 percent of seeded anomalies, of which 24 percent are not detected by generated test suites. Compared to running existing regression test suites, it detects 22 times more anomalies, with 94 percent precision on average. In a study with 15 professional developers, the participants inspected problematic refactorings with RefDistiller versus testing only. With RefDistiller, participants located 90 percent of the seeded anomalies, while they located only 13 percent with testing. The results show RefDistiller can help check the correctness of manual refactorings.",0098-5589;00985589,,10.1109/TSE.2017.2679742,CNPq/Brasil; Google Faculty Award; National Institute of Science and Technology for Software Engineering; National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874212,Refactoring;code inspection;refactoring anomalies,Computer bugs;Detectors;Engines;Inspection;Manuals;Testing;Transforms,program diagnostics;program testing;regression analysis;software maintenance,RefDistiller;automated refactoring engine;extra edits;generated test suites;inadequate test suites;manual refactoring edits;potential missed edits;problematic refactorings;refactoring inspection support;regression testing;running existing regression test suites;seeded anomalies,,,,,,,20170308,April 1 2018,,IEEE,IEEE Journals & Magazines,,18
Eliminating Path Redundancy via Postconditioned Symbolic Execution,Q. Yi; Z. Yang; S. Guo; C. Wang; J. Liu; C. Zhao,"National Engineering Research Center for Fundamental Software, Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Software Engineering,20180108,2018,44,1,25,43,"Symbolic execution is emerging as a powerful technique for generating test inputs systematically to achieve exhaustive path coverage of a bounded depth. However, its practical use is often limited by path explosion because the number of paths of a program can be exponential in the number of branch conditions encountered during the execution. To mitigate the path explosion problem, we propose a new redundancy removal method called postconditioned symbolic execution. At each branching location, in addition to determine whether a particular branch is feasible as in traditional symbolic execution, our approach checks whether the branch is subsumed by previous explorations. This is enabled by summarizing previously explored paths by weakest precondition computations. Postconditioned symbolic execution can identify path suffixes shared by multiple runs and eliminate them during test generation when they are redundant. Pruning away such redundant paths can lead to a potentially exponential reduction in the number of explored paths. Since the new approach is computationally expensive, we also propose several heuristics to reduce its cost. We have implemented our method in the symbolic execution engine KLEE [1] and conducted experiments on a large set of programs from the GNU Coreutils suite. Our results confirm that redundancy due to common path suffix is both abundant and widespread in real-world applications.",0098-5589;00985589,,10.1109/TSE.2017.2659751,; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835264,Symbolic execution;testing and debugging;testing tools,Concrete;Explosions;Input variables;Redundancy;Software;Syntactics;Testing,program diagnostics;program testing,branch conditions;branching location;common path suffix;exhaustive path coverage;path explosion problem;path redundancy elimination;path suffixes;postconditioned symbolic execution;redundancy removal method;traditional symbolic execution,,,,,,,20170126,Jan. 1 2018,,IEEE,IEEE Journals & Magazines,,18
Towards Model Checking Android Applications,G. Bai; Q. Ye; Y. Wu; H. Botha; J. Sun; Y. Liu; J. S. Dong; W. Visser,"Singapore Institute of Technology, Singapore",IEEE Transactions on Software Engineering,20180612,2018,44,6,595,612,"As feature-rich Android applications (<italic>apps</italic> for short) are increasingly popularized in security-sensitive scenarios, methods to verify their security properties are highly desirable. Existing approaches on verifying Android apps often have limited effectiveness. For instance, static analysis often suffers from a high false-positive rate, whereas approaches based on dynamic testing are limited in coverage. In this work, we propose an alternative approach, which is to apply the <italic>software model checking</italic> technique to verify Android apps. We have built a general framework named <sc>DroidPF</sc> upon Java PathFinder (JPF), towards model checking Android apps. In the framework, we craft an executable mock-up Android OS which enables JPF to dynamically explore the concrete state spaces of the tested apps; we construct programs to generate user interaction and environmental input so as to drive the dynamic execution of the apps; and we introduce Android specific reduction techniques to help alleviate the state space explosion. <sc>DroidPF</sc> focuses on common security vulnerabilities in Android apps including sensitive data leakage involving a non-trivial flow- and context-sensitive taint-style analysis. <sc>DroidPF </sc> has been evaluated with 131 apps, which include real-world apps, third-party libraries, malware samples and benchmarks for evaluating app analysis techniques like ours. <sc>DroidPF</sc> precisely identifies nearly all of the previously known security issues and nine previously unreported vulnerabilities/bugs.",0098-5589;00985589,,10.1109/TSE.2017.2697848,"NRF; National Cybersecurity R&D Directorate; National Cybersecurity R&D Program; National Research Foundation, Prime Ministers Office, Singapore; National University of Singapore and Singapore University of Technology and Design; Singapore NRF; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911333,Software model checking;android application;security verification,Androids;Humanoid robots;Java;Libraries;Model checking;Security;Software,,,,,,,,,20170425,June 1 2018,,IEEE,IEEE Journals & Magazines,,17
MAHAKIL: Diversity Based Oversampling Approach to Alleviate the Class Imbalance Issue in Software Defect Prediction,K. E. Bennin; J. Keung; P. Phannachitta; A. Monden; S. Mensah,"Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong",IEEE Transactions on Software Engineering,20180612,2018,44,6,534,550,"Highly imbalanced data typically make accurate predictions difficult. Unfortunately, software defect datasets tend to have fewer defective modules than non-defective modules. Synthetic oversampling approaches address this concern by creating new minority defective modules to balance the class distribution before a model is trained. Notwithstanding the successes achieved by these approaches, they mostly result in over-generalization (high rates of false alarms) and generate near-duplicated data instances (less diverse data). In this study, we introduce MAHAKIL, a novel and efficient synthetic oversampling approach for software defect datasets that is based on the chromosomal theory of inheritance. Exploiting this theory, MAHAKIL interprets two distinct sub-classes as parents and generates a new instance that inherits different traits from each parent and contributes to the diversity within the data distribution. We extensively compare MAHAKIL with SMOTE, Borderline-SMOTE, ADASYN, Random Oversampling and the No sampling approach using 20 releases of defect datasets from the PROMISE repository and five prediction models. Our experiments indicate that MAHAKIL improves the prediction performance for all the models and achieves better and more significant <italic>pf</italic> values than the other oversampling approaches, based on Brunner's statistical significance test and Cliff's effect sizes. Therefore, MAHAKIL is strongly recommended as an efficient alternative for defect prediction models built on highly imbalanced datasets.",0098-5589;00985589,,10.1109/TSE.2017.2731766,City University of Hong Kong; General Research Fund of the Research Grants Council of Hong Kong; JSPS KAKENHI; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990590,Software defect prediction;class imbalance learning;classification problems;data sampling methods;synthetic sample generation,Animals;Biological cells;Electronic mail;Predictive models;Sampling methods;Software,,,,,,,,,20170725,June 1 2018,,IEEE,IEEE Journals & Magazines,,16
Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction,S. McIntosh; Y. Kamei,"Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada",IEEE Transactions on Software Engineering,20180514,2018,44,5,412,428,"Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.",0098-5589;00985589,,10.1109/TSE.2017.2693980,JSPS KAKENHI; Natural Sciences and Engineering Research Council of Canada (NSERC); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898457,Just-In-Time prediction;defect prediction;mining software repositories,Calibration;Complexity theory;Context modeling;Data models;Market research;Predictive models;Software,data mining;just-in-time;learning (artificial intelligence);public domain software;software management;software quality;source code (software),JIT models;Just-In-Time models;OpenStack systems;Qt systems;code change properties;fix-inducing changes;fix-inducing code changes;just-in-time defect prediction;mining software repositories;target moving,,,,,,,20170412,May 1 2018,,IEEE,IEEE Journals & Magazines,,16
Discipline Matters: Refactoring of Preprocessor Directives in the <monospace>#ifdef </monospace> Hell,F. Medeiros; M. Ribeiro; R. Gheyi; S. Apel; C. KÕ_stner; B. Ferreira; L. Carvalho; B. Fonseca,"Federal Institute of Alagoas, AL, Brazil",IEEE Transactions on Software Engineering,20180514,2018,44,5,453,469,"The C preprocessor is used in many C projects to support variability and portability. However, researchers and practitioners criticize the C preprocessor because of its negative effect on code understanding and maintainability and its error proneness. More importantly, the use of the preprocessor hinders the development of tool support that is standard in other languages, such as automated refactoring. Developers aggravate these problems when using the preprocessor in undisciplined ways (e.g., conditional blocks that do not align with the syntactic structure of the code). In this article, we proposed a catalogue of refactorings and we evaluated the number of application possibilities of the refactorings in practice, the opinion of developers about the usefulness of the refactorings, and whether the refactorings preserve behavior. Overall, we found 5,670 application possibilities for the refactorings in 63 real-world C projects. In addition, we performed an online survey among 246 developers, and we submitted 28 patches to convert undisciplined directives into disciplined ones. According to our results, 63 percent of developers prefer to use the refactored (i.e., disciplined) version of the code instead of the original code with undisciplined preprocessor usage. To verify that the refactorings are indeed behavior preserving, we applied them to more than 36 thousand programs generated automatically using a model of a subset of the C language, running the same test cases in the original and refactored programs. Furthermore, we applied the refactorings to three real-world projects: BusyBox, OpenSSL, and SQLite. This way, we detected and fixed a few behavioral changes, 62 percent caused by unspecified behavior in the C programming language.",0098-5589;00985589,,10.1109/TSE.2017.2688333,"AFRL; CAPES; CNPq; DARPA; DEVASSES; European Union's Seventh Framework Programme for research, technological development and demonstration; FAPEAL PPGs; German Research Foundation; NSF; Science of Security Lablet; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888579,Configurable systems;and refactoring;preprocessors,C languages;Guidelines;Kernel;Linux;Standards;Syntactics,C language;program compilers;program diagnostics;program processors;public domain software;software maintenance,BusyBox;C preprocessor;C programming language;OpenSSL;SQLite;automated refactoring;preprocessor directives;preprocessor usage;refactored programs,,1,,,,,20170328,May 1 2018,,IEEE,IEEE Journals & Magazines,,16
Formulating Criticality-Based Cost-Effective Fault Tolerance Strategies for Multi-Tenant Service-Based Systems,Y. Wang; Q. He; D. Ye; Y. Yang,"State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China",IEEE Transactions on Software Engineering,20180313,2018,44,3,291,307,"The proliferation of cloud computing has fueled the rapid growth of multi-tenant service-based systems (SBSs), which serve multiple tenants simultaneously by composing existing services in the form of business processes. In a distributed and volatile operating environment, runtime anomalies may occur to the component services of an SBS and cause end-to-end quality violations. Engineering multi-tenant SBSs that can quickly handle runtime anomalies cost effectively has become a significant challenge. Different approaches have been proposed to formulate fault tolerance strategies for engineering SBSs. However, none of the existing approaches has sufficiently considered the service criticality based on multi-tenancy where multiple tenants share the same SBS instance with different multi-dimensional quality preferences. In this paper, we propose Criticality-based Fault Tolerance for Multi-Tenant SBSs (CFT4MTS), a novel approach that formulates cost-effective fault tolerance strategies for multi-tenant SBSs by providing redundancy for the critical component services. First, the criticality of each component service is evaluated based on its multi-dimensional quality and multiple tenants sharing the component service with differentiated quality preferences. Then, the fault tolerance problem is modelled as an Integer Programming problem to identify the optimal fault tolerance strategy. The experimental results show that, compared with three existing representative approaches, CFT4MTS can alleviate degradation in the quality of multi-tenant SBSs in a much more effective and efficient way.",0098-5589;00985589,,10.1109/TSE.2017.2681667,Australian Research Council Discovery; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876832,Cloud computing;criticality;fault tolerance;multi-tenancy;redundancy;service-based system,Business;Cloud computing;Fault tolerant systems;Redundancy;Runtime;Streaming media,cloud computing;integer programming;service-oriented architecture;software fault tolerance,CFT4MTS;Criticality-based Fault Tolerance for Multi-Tenant SBSs;Integer Programming problem;cloud computing;critical component services;criticality-based cost-effective fault tolerance;end-to-end quality violations;multi-tenant service-based systems;multidimensional quality preferences;multitenancy;optimal fault tolerance;runtime anomalies cost;service criticality,,2,,,,,20170313,March 1 2018,,IEEE,IEEE Journals & Magazines,,16