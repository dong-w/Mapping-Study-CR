"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Eye movements in code review","A. Begel; H. Vrzakova","Microsoft Research","Proceedings of the Workshop on Eye Movements in Programming","20180614","2018","","","1","5","<p>In order to ensure sufficient quality, software engineers conduct code reviews to read over one another's code looking for errors that should be fixed before committing to their source code repositories. Many kinds of errors are spotted, from simple spelling mistakes and syntax errors, to architectural flaws that may span several files. However, we know little about how software developers read code when looking for defects. What kinds of code trigger engineers to check more deeply into suspected defects? How long do they take to verify whether a defect is really there? We conducted a study of 35 software engineers performing 40 code reviews while capturing their gaze with an eye tracker. We classified each code defect the developers found and captured the patterns of eye gazes used to deliberate about each one. We report how long it took to confirm defect suspicions for each type of defect and the fraction of time spent skimming the code vs. carefully reading it. This work provides a starting point for automating code reviews that could help engineers spend more time focusing on the difficult task of defect confirmation rather than the tedious task of defect discovery.</p>","","","10.1145/3216723.3216727","","","code review;eye tracking","","","","","","","","","","","15-15 June 2018","","ACM","ACM Conferences"
"CFar: A Tool to Increase Communication, Productivity, and Review Quality in Collaborative Code Reviews","A. Z. Henley; K. Mu√ßlu; M. Christakis; S. D. Fleming; C. Bird","University of Memphis, Memphis, TN, USA","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","20180426","2018","","","1","13","<p>Collaborative code review has become an integral part of the collaborative design process in the domain of software development. However, there are well-documented challenges and limitations to collaborative code review---for instance, high-quality code reviews may require significant time and effort for the programmers, whereas faster, lower-quality reviews may miss code defects. To address these challenges, we introduce CFar, a novel tool design for extending collaborative code review systems with an automated code reviewer whose feedback is based on program-analysis technologies. To validate this design, we implemented CFar as a production-quality tool and conducted a mixed-method empirical evaluation of the tool usage at Microsoft. Through the field deployment of our tool and a laboratory study of professional programmers using the tool, we produced several key findings showing that CFar enhances communication, productivity, and review quality in human--human collaborative code review.</p>","","","10.1145/3173574.3173731","","","code review;collaborative design;programming environments","","","","","","","","","","","21-26 April 2018","","ACM","ACM Conferences"
"Diggit: Automated code review via software repository mining","R. Chatley; L. Jones","Imperial College London, 180 Queen's Gate, London, UK","2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20180405","2018","","","567","571","We present Diggit, a tool to automatically generate code review comments, offering design guidance on prospective changes, based on insights gained from mining historical changes in source code repositories. We describe how the tool was built and tuned for use in practice as we integrated Diggit into the working processes of an industrial development team. We focus on the developer experience, the constraints that had to be met in adapting academic research to produce a tool that was useful to developers, and the effectiveness of the results in practice.","","Electronic:978-1-5386-4969-5; POD:978-1-5386-4970-1","10.1109/SANER.2018.8330261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330261","data mining;software maintenance","Complexity theory;Data mining;Feathers;History;Market research;Software;Tools","data mining;program compilers;software development management;software maintenance;software quality;source code (software);team working","Diggit;automated code review;code review comments;design guidance;historical changes mining;industrial development team;software repository mining;source code repositories","","","","","","","","20-23 March 2018","","IEEE","IEEE Conferences"
"Review participation in modern code review: An empirical study of the Android, Qt, and OpenStack projects (journal-first abstract)","P. Thongtanunam; S. McIntosh; A. E. Hassan; H. Iida","The University of Adelaide, McGill University, Queen's University, Nara Institute of Science and Technology","2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20180405","2018","","","475","475","This paper empirically investigates the factors influence review participation in the MCR process. Through a case study of the Android, Qt, and OpenStack open source projects, we find that the amount of review participation in the past is a significant indicator of patches that will suffer from poor review participation. Moreover, the description length of a patch and the purpose of introducing new features also share a relationship with the likelihood of receiving poor review participation. This paper is an extended abstract of a paper published in the Empirical Software Engineering journal. The original paper is communicated by Jeffrey C. Carver.","","Electronic:978-1-5386-4969-5; POD:978-1-5386-4970-1","10.1109/SANER.2018.8330241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330241","","","","","","","","","","","","20-23 March 2018","","IEEE","IEEE Conferences"
"Multi-Perspective Visualization to Assist Code Change Review","C. Wang; X. Xie; P. Liang; J. Xuan","State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China","2017 24th Asia-Pacific Software Engineering Conference (APSEC)","20180305","2017","","","564","569","Change-based code review plays an important role in open-source project development. Due to the large amount of human involvement and tight time schedule, tools that can facilitate this activity would be of great help. Current tools mainly focus on difference extraction, code style examination, static analysis, comment and discussion, etc. However, there is little support to change impact analysis for code change review. In this paper, we serve this purpose by providing a change review assistance tool, namely, MultiViewer, for the most popular OSS GitHub. We define metrics to characterize code changes from multiple perspectives. Specifically, these metrics mine coupling relations among related files in the changes, as well as estimate the change effort, risk and impact. Such information is visualized by MultiViewer in two formats. We demonstrate the helpfulness of MultiViewer by showing its ability as indicators to some important project features with real-life case studies.","","Electronic:978-1-5386-3681-7; POD:978-1-5386-3682-4","10.1109/APSEC.2017.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305982","GitHub;change impact analysis;code review;visualization","Color;Computer bugs;Correlation;Couplings;Measurement;Tools;Visualization","data visualisation;program diagnostics;program visualisation;public domain software;software quality","MultiViewer;OSS GitHub;agile software development;assist code change review;change review assistance tool;change-based code review;code review;code style examination;multiperspective visualization;open-source project development;software quality assurance;static analysis","","","","","","","","4-8 Dec. 2017","","IEEE","IEEE Conferences"
"Comparing sequential and parallel code review techniques for formative feedback","A. Luxton-Reilly; A. Lewis; B. Plimmer","University of Auckland, Auckland, New Zealand","Proceedings of the 20th Australasian Computing Education Conference","20180301","2018","","","45","52","<p>The practice of Peer Review is widespread across a range of academic disciplines. We report on a study that compared two different approaches of peer reviewing program code --- reviewing a sequence of solutions to the same problem (sequential code review), and reviewing a set of multiple solutions side-by-side (parallel code review). We found that the parallel approach was preferred by the majority of participants in the study and there were some indications that it might be more helpful for reviewers, but the sequential approach elicited more written comments in general, and more specific critical comments compared with the parallel approach. Although parallel reviews may be preferred by reviewers, using sequential reviews appears to result in increased levels of formative feedback for the recipient.</p>","","","10.1145/3160489.3160498","","","CS1;code review;formative feedback;novice programmers;parallel code review;peer assessment;peer feedback;peer review;sequential code review","","","","","","","","","","","Jan. 30 2018-Feb. 2 2018","","ACM","ACM Conferences"
"Investigating the Effectiveness of Peer Code Review in Distributed Software Development","E. W. dos Santos; I. Nunes","Universidade Federal do Rio Grande do Sul (UFRGS), Porto Alegre, Brazil","Proceedings of the 31st Brazilian Symposium on Software Engineering","20180215","2017","","","84","93","<p>Code review is a potential means of improving software quality. To be effective, it depends on different factors, and many have been investigated in the literature to identify the scenarios in which it adds quality to the final code. However, factors associated with distributed software development, which is becoming increasingly common, have been little explored. Geographic distance can impose additional challenges to the reviewing process. We thus in this paper present the results of a quantitative study of the effectiveness of code review in a distributed software project involving 201 members. We investigate factors that can potentially influence the outcomes of peer code review. Our results show that a high number of changed lines of code tends to increase the review duration with a reduced number of messages, while the number of involved teams, locations, and participant reviewers generally improve reviewer contributions, but with a severe penalty to the duration.</p>","","","10.1145/3131151.3131161","","","Code Review;Distributed Software Development;Empirical Study","","","","","","","","","","","20-22 Sept. 2017","","ACM","ACM Conferences"
"Semantics-assisted code review: An efficient tool chain and a user study","M. Menarini; Y. Yan; W. G. Griswold","Department of Computer Science and Engineering, University of California at San Diego, La Jolla, CA, USA","2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)","20171123","2017","","","554","565","Code changes are often reviewed before they are deployed. Popular source control systems aid code review by presenting textual differences between old and new versions of the code, leaving developers with the difficult task of determining whether the differences actually produced the desired behavior. Fortunately, we can mine such information from code repositories. We propose aiding code review with inter-version semantic differential analysis. During review of a new commit, a developer is presented with summaries of both code differences and behavioral differences, which are expressed as diffs of likely invariants extracted by running the system's test cases. As a result, developers can more easily determine that the code changes produced the desired effect. We created an invariant-mining tool chain, Getty, to support our concept of semantically-assisted code review. To validate our approach, 1) we applied Getty to the commits of 6 popular open source projects, 2) we assessed the performance and cost of running Getty in different configurations, and 3) we performed a comparative user study with 18 developers. Our results demonstrate that semantically-assisted code review is feasible, effective, and that real programmers can leverage it to improve the quality of their reviews.","","Electronic:978-1-5386-2684-9; POD:978-1-5386-3976-4","10.1109/ASE.2017.8115666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115666","Software behavior;code review;dynamic impact analysis;likely invariants;mining software repository;scalability;software testing","Computer bugs;Navigation;Semantics;Software;Testing;Tools","Internet;data mining;project management;software maintenance","code changes;code differences;code repositories;inter-version semantic differential analysis;invariant-mining tool chain;semantic-assisted code review","","","","","","","","Oct. 30 2017-Nov. 3 2017","","IEEE","IEEE Conferences"
"SentiCR: A customized sentiment analysis tool for code review interactions","T. Ahmed; A. Bosu; A. Iqbal; S. Rahimi","Department of Computer Science & Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladeshi","2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)","20171123","2017","","","106","111","Sentiment Analysis tools, developed for analyzing social media text or product reviews, work poorly on a Software Engineering (SE) dataset. Since prior studies have found developers expressing sentiments during various SE activities, there is a need for a customized sentiment analysis tool for the SE domain. On this goal, we manually labeled 2000 review comments to build a training dataset and used our dataset to evaluate seven popular sentiment analysis tools. The poor performances of the existing sentiment analysis tools motivated us to build SentiCR, a sentiment analysis tool especially designed for code review comments. We evaluated SentiCR using one hundred 10-fold cross-validations of eight supervised learning algorithms. We found a model, trained using the Gradient Boosting Tree (GBT) algorithm, providing the highest mean accuracy (83%), the highest mean precision (67.8%), and the highest mean recall (58.4%) in identifying negative review comments.","","Electronic:978-1-5386-2684-9; POD:978-1-5386-3976-4","10.1109/ASE.2017.8115623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115623","","Algorithm design and analysis;Dictionaries;Sentiment analysis;Social network services;Supervised learning;Tools;Training","learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis;trees (mathematics)","GBT algorithm;SE activities;SE domain;SentiCR;code review comments;code review interactions;customized sentiment analysis tool;gradient boosting tree algorithm;negative review comments;social media text;software engineering dataset","","","","","","","","Oct. 30 2017-Nov. 3 2017","","IEEE","IEEE Conferences"
"How is IF Statement Fixed Through Code Review? A Case Study of Qt Project","Y. Ueda; A. Ihara; T. Hirao; T. Ishio; K. Matsumoto","Grad. Sch. of Inf. Sci., Nara Inst. of Sci. & Technol., Nara, Japan","2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","20171116","2017","","","207","213","Peer code review is key to ensuring the absence of software defects. To improve the review process, many code review tools provide OSS(Open Source Software) project CI(Continuous Integration) tests that automatically verify code quality issues such as a code convention issues. However, these tests do not cover project policy issues and a code readability issues. In this study, our main goal is to understand how a code owner fixes conditional statement issues based on reviewers feedback. We conduct an empirical study to understand if statement changes after review. Using 69,325 review requests in the Qt project, we analyze changes of the if conditional statements that (1) are requested to be reviewed, and (2) that are implemented after review. As a result, we find the most common symbolic changes are ""("" and "")"" (35%), ""!"" operator (20%) and ""->"" operator (12%). Also, ""!"" operator is frequently replaced with ""("" and "")"".","","Electronic:978-1-5386-2387-9; POD:978-1-5386-2388-6","10.1109/ISSREW.2017.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109285","code review;conditional statement;fix detection;if statement;static code analysis","Data mining;Itemsets;Manuals;Programming;Software;Syntactics;Tools","program debugging;program testing;program verification;project management;public domain software;software quality;source code (software)","CI;OSS;Qt project;code convention;code quality verification;code readability;code review tools;continuous integration;open source software;peer code review;project policy;review process;reviewers feedback;software defects","","","","","","","","23-26 Oct. 2017","","IEEE","IEEE Conferences"
"WAP: Does Reviewer Age Affect Code Review Performance?","Y. Murakami; M. Tsunoda; H. Uwano","Grad. Sch. of Sci. & Eng., Kindai Univ., Higashi-Osaka, Japan","2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)","20171116","2017","","","164","169","We focus on developer code review performance, and analyze whether the age of a subject affects the efficiency and preciseness of their code. Generally, older coders have more experience. Therefore, the age is considered to positively affect code review. However, in our past study, code understanding speed was relatively slow for older subjects, and memory is needed to understand programs. Similarly, during code review, a subject's age may affect efficiency (e.g., the number of indications per unit time). In the experiment, subjects reviewed source code, referring to mini specification documents. When the code did not follow the document, the subjects indicated the error. We classified subjects into senior and junior groups. In the analysis, we stratified the results based on age, and used correlation coefficients and multiple linear regression to clarify the relationship between age and review performance. We found that age does not affect the efficiency and correctness of code review. Also, the software development experience of subjects is not significantly correlated to performance.","","Electronic:978-1-5386-0941-5; POD:978-1-5386-0942-2","10.1109/ISSRE.2017.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109083","developer's performance;gerontology;software review","Correlation coefficient;Encoding;Gerontology;Java;Measurement;Software","program compilers;program debugging;program diagnostics;software maintenance;software performance evaluation;training","WAP;code review performance;code understanding speed;developer code review performance;older coders;older subjects;software development experience;source code","","","","","","","","23-26 Oct. 2017","","IEEE","IEEE Conferences"
"Refactoring-aware code review","X. Ge; S. Sarkar; J. Witschey; E. Murphy-Hill","Apple Inc., USA","2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","20171113","2017","","","71","79","Code review, where developers manually inspect one another's code changes, improves software quality and transfers knowledge in a team. Unfortunately, tools that support code review treat behavior-preserving changes, or refactorings, and behavior-altering changes, or non-refactorings, the same way, so developers have to spend effort differentiating between the two before they can evaluate the impact of a change set. In this paper, we describe a formative study of 35 developers that motivates the need for separating refactorings from non-refactorings during code review. Then, we present a refactoring-aware code review tool, called ReviewFactor, that differentiates between refactoring and non-refactoring, and allows developers to focus on one of them at a time. Finally, a case study of two open source projects suggests that ReviewFactor detects refactorings in 39% of the commits, and identifies 4.6% of the total lines of code change as refactorings. Our results also show that the precision and recall of ReviewFactor's refactoring detection algorithm are 92.5% and 94.2%, respectively.","","Electronic:978-1-5386-0443-4; POD:978-1-5386-0444-1","10.1109/VLHCC.2017.8103453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103453","","Detection algorithms;Java;Manuals;Software;Solids;Tools;Visualization","software maintenance;software quality;software reviews","ReviewFactor;behavior-altering changes;behavior-preserving changes;code change;refactoring detection algorithm;refactoring-aware code review tool;software quality","","","","","","","","11-14 Oct. 2017","","IEEE","IEEE Conferences"
"Evaluating how static analysis tools can reduce code review effort","D. Singh; V. R. Sekar; K. T. Stolee; B. Johnson","Department of Computer Science, North Carolina State University","2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","20171113","2017","","","101","105","Peer code reviews are important for giving and receiving peer feedback, but the code review process is time consuming. Static analysis tools can help reduce reviewer effort by catching common mistakes prior to peer code review. Ideally, contributors would use static analysis tools prior to pull request submission so common mistakes could be addressed first, before invoking the reviewer. To explore the potential efficiency gains for peer reviewers, we explore the overlap between reviewer comments on pull requests and warnings from the PMD static analysis tool. In an empirical study of 274 comments from 92 pull requests on GitHub, we observed that PMD overlapped with nearly 16% of the reviewer comments, indicating a time benefit to the reviewer if static analyzers would have been used prior to pull request submission. Using the non-overlapping set of comments, we identify four additional rules that, if implemented, could further reduce reviewer effort.","","Electronic:978-1-5386-0443-4; POD:978-1-5386-0444-1","10.1109/VLHCC.2017.8103456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103456","","Encoding;Java;Manuals;Standards;Tools;Visualization","program diagnostics;public domain software","GitHub;PMD static analysis tool;code review effort;code review process;peer code reviews;peer reviewers;pull requests;reviewer comments;static analysis tools;static analyzers","","","","","","","","11-14 Oct. 2017","","IEEE","IEEE Conferences"
"A Large-Scale Study of Modern Code Review and Security in Open Source Projects","C. Thompson; D. Wagner","University of California, Berkeley","Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering","20171107","2017","","","83","92","<p>Background: Evidence for the relationship between code review process and software security (and software quality) has the potential to help improve code review automation and tools, as well as provide a better understanding of the economics for improving software security and quality. Prior work in this area has primarily been limited to case studies of a small handful of software projects. Aims: We investigate the effect of modern code review on software security. We extend and generalize prior work that has looked at code review and software quality. Method: We gather a very large dataset from GitHub (3,126 projects in 143 languages, with 489,038 issues and 382,771 pull requests), and use a combination of quantification techniques and multiple regression modeling to study the relationship between code review coverage and participation and software quality and security. Results: We find that code review coverage has a significant effect on software security. We confirm prior results that found a relationship between code review coverage and software defects. Most notably, we find evidence of a negative relationship between code review of pull requests and the number of security bugs reported in a project. Conclusions: Our results suggest that implementing code review policies within the pull request model of development may have a positive effect on the quality and security of software.</p>","","Electronic:978-1-4503-5305-2","10.1145/3127005.3127014","","","code review;mining software repositories;multiple regression models;quantification models;software quality;software security","","","","","","","","","","","8-8 Nov. 2017","","ACM","ACM Conferences"
"On the Optimal Order of Reading Source Code Changes for Review","T. Baum; K. Schneider; A. Bacchelli","FG Software Eng., Leibniz Univ. Hannover, Hannover, Germany","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20171107","2017","","","329","340","Change-based code review, e.g., in the form of pull requests, is the dominant style of code review in practice. An important option to improve review's efficiency is cognitive support for the reviewer. Nevertheless, review tools present the change parts under review sorted in alphabetical order of file path, thus leaving the effort of understanding the construction, connections, and logic of the changes on the reviewer. This leads to the question: How should a code review tool order the parts of a code change to best support the reviewer? We answer this question with a middle-range theory, which we generated inductively in a mixed methods study, based on interviews, an online survey, and existing findings from related areas. Our results indicate that an optimal order is mainly an optimal grouping of the change parts by relatedness. We present our findings as a collection of principles and formalize them as a partial order relation among review orders.","","Electronic:978-1-5386-0992-7; POD:978-1-5386-0993-4","10.1109/ICSME.2017.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094433","Change-based code review;Cognitive support review tools;Modern code review;Program comprehension;Pull-based development","Industries;Interviews;Navigation;Software;Software engineering;Tools","program diagnostics;public domain software;software quality;source code (software)","alphabetical order;change parts;change-based code review;code review tool;middle-range theory;optimal grouping;optimal order;partial order relation;review efficiency;review orders;review tools;reviewer;source code change reading","","","","","","","","17-22 Sept. 2017","","IEEE","IEEE Conferences"
"Confusion Detection in Code Reviews","F. Ebert; F. Castor; N. Novielli; A. Serebrenik","Fed. Univ. of Pernambuco, Recife, Brazil","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20171107","2017","","","549","553","Code reviews are an important mechanism for assuring quality of source code changes. Reviewers can either add general comments pertaining to the entire change or pinpoint concerns or shortcomings about a specific part of the change using inline comments. Recent studies show that reviewers often do not understand the change being reviewed and its context.Our ultimate goal is to identify the factors that confuse code reviewers and understand how confusion impacts the efficiency and effectiveness of code review(er)s. As the first step towards this goal we focus on the identification of confusion in developers' comments. Based on an existing theoretical framework categorizing expressions of confusion, we manually classify 800 comments from code reviews of the Android project. We observe that confusion can be reasonably well-identified by humans: raters achieve moderate agreement (Fleiss' kappa 0.59 for the general comments and 0.49 for the inline ones). Then, for each kind of comment we build a series of automatic classifiers that, depending on the goals of the further analysis, can be trained to achieve high precision (0.875 for the general comments and 0.615 for the inline ones), high recall (0.944 for the general comments and 0.988 for the inline ones), or substantial precision and recall (0.696 and 0.542 for the general comments and 0.434 and 0.583 for the inline ones, respectively). These results motivate further research on the impact of confusion on the code review process. Moreover, other researchers can employ the proposed classifiers to analyze confusion in other contexts where software development-related discussions occur, such as mailing lists.","","Electronic:978-1-5386-0992-7; POD:978-1-5386-0993-4","10.1109/ICSME.2017.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094460","code review;confusion;machine learning","Androids;Humanoid robots;Labeling;Manuals;Software;Training;Uncertainty","Android (operating system);human factors;pattern classification;program testing;software quality","Android project;automatic classifiers;code review process;confusion detection;inline comments;precision value;recall value;source code changes","","1","","","","","","17-22 Sept. 2017","","IEEE","IEEE Conferences"
"Continuous Code Reviews: A Social Coding tool for Code Reviews inside the IDE","T. D√ºrschmid","Hasso Plattner Institute, University of Potsdam, Potsdam, Germany","Companion to the first International Conference on the Art, Science and Engineering of Programming","20170914","2017","","","1","3","<p>Code reviews play an important and successful role in modern software development. But usually they happen only once before new code is merged into the main branch. We present a concept which helps developers to continuously give feedback on their source code directly in the integrated development environment (IDE) by using the metaphor of social networks. This reduces context switches for developers, improves the software development process and allows to give feedback to developers of external libraries and frameworks.</p>","","","10.1145/3079368.3079374","","","code quality;code review;feedback;social coding","","","","","","","","","","","3-6 April 2017","","ACM","ACM Conferences"
"Interactively Decomposing Composite Changes to Support Code Review and Regression Testing","B. Guo; M. Song","Dept. of Comput. Sci., Univ. of Nebraska, Omaha, NE, USA","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","20170911","2017","1","","118","127","Developers often address multiple development issues to make composite code changes, as opposed to atomic changes that address one single issue. Investigating and testing such code changes is a tedious and error-prone process for developers. To address the problem, this paper presents a technique, called CHGCUTTER, for (1) interactively decomposing composite changes into atomic changes, (2) building related change subsets using program dependence relationships without syntactic violation, and (3) safely selecting only related test cases from the test suite to reduce the time to conduct regression testing. In the evaluation, CHGCUTTER analyzes 28 composite changes in four open source projects. It identifies related change subsets with 95.7% accuracy, and it selects test cases affected by these changes with 89.0% accuracy. Our results show that CHGCUTTER should help developers effectively inspect changes and validate modified applications during development.","0730-3157;07303157","Electronic:978-1-5386-0367-3; POD:978-1-5386-0368-0","10.1109/COMPSAC.2017.153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029599","Code review;maintenance;regression testing;software evolution","Bars;Buildings;Control systems;Paints;Software;Syntactics;Testing","program testing;public domain software;regression analysis;software maintenance;source code (software)","CHGCUTTER;atomic changes;code review;composite code changes;open source projects;program dependence relationships;regression testing;software development","","","","","","","","4-8 July 2017","","IEEE","IEEE Conferences"
"Tool Support for Managing Clone Refactorings to Facilitate Code Review in Evolving Software","Z. Chen; M. Mohanavilasam; Y. W. Kwon; M. Song","Univ. of Nebraska, Omaha, NE, USA","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","20170911","2017","1","","288","297","Developers often perform copy-and-paste activities. This practice causes the similar code fragment (aka code clones) to be scattered throughout a code base. Refactoring for clone removal is beneficial, preventing clones from having negative effects on software quality, such as hidden bug propagation and unintentional inconsistent changes. However, recent research has provided evidence that factoring out clones is not always to reduce the risk of introducing defects, and it is often difficult or impossible to remove clones using standard refactoring techniques. To investigate which or how clones can be refactored, developers typically spend a significant amount of their time managing individual clone instances or clone groups scattered across a large code base. To address the problem, this paper presents a technique for managing clone refactorings, Pattern-based clone Refactoring Inspection (PRI), using refactoring pattern templates. By matching the refactoring pattern templates against a code base, it summarizes refactoring changes of clones, and detects the clone instances not consistently factored out as potential anomalies. PRI also provides novel visualization user interfaces specifically designed for inspecting clone refactorings. In the evaluation, PRI analyzes clone instances in six open source projects. It identifies clone refactorings with 94.1% accuracy and detects inconsistent refactorings with 98.4% accuracy. Our results show that PRI should help developers effectively inspect evolving clones and correctly apply refactorings to clone groups.","0730-3157;07303157","Electronic:978-1-5386-0367-3; POD:978-1-5386-0368-0","10.1109/COMPSAC.2017.242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029620","Code clones;maintenance;refactorings;software evolution","Cloning;Computer bugs;Inspection;Pattern matching;Software;Standards;Tools","data visualisation;public domain software;software maintenance;software management;software quality;software tools;source code (software);user interfaces","PRI;clone instances analysis;clone refactorings management;clone removal;code base;code clones;code fragment;code review facilitation;open source projects;pattern-based clone refactoring inspection;refactoring pattern templates matching;software quality;tool support;visualization user interfaces","","","","","","","","4-8 July 2017","","IEEE","IEEE Conferences"
"Decoding the Representation of Code in the Brain: An fMRI Study of Code Review and Expertise","B. Floyd; T. Santander; W. Weimer","","2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)","20170720","2017","","","175","186","Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79%, p <; 0.001). Further, we find that the same set of brain regions distinguish between code and prose (near-perfect correlation, r = 0.99, p <; 0.001). Finally, we find that task distinctions are modulated by expertise, such that greater skill predicts a less differentiated neural representation (r = -0.44, p = 0.016) indicating that more skilled participants treat code and prose more similarly at a neural activation level.","","Electronic:978-1-5386-3868-2; POD:978-1-5386-3869-9","10.1109/ICSE.2017.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985660","code comprehension;medical imaging;prose review","Biomedical imaging;Brain;Computer science;Natural languages;Software;Software engineering;Tools","biomedical MRI;medical image processing;software engineering","code comprehension;code review;fMRI study;functional magnetic resonance imaging;medical imaging techniques;natural languages;neural representations;programming languages;prose review;software engineering tasks","","","","","","","","20-28 May 2017","","IEEE","IEEE Conferences"
"Using Metrics to Track Code Review Performance","D. Izquierdo-Cortazar; N. Sekitoleko; J. M. Gonzalez-Barahona; L. Kurth","Bitergia","Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering","20170711","2017","","","214","223","<p>During 2015, some members of the Xen Project Advisory Board became worried about the performance of their code review process. The Xen Project is a free, open source software project developing one of the most popular virtualization platforms in the industry. They use a pre-commit peer review process similar to that in the Linux kernel, based on email messages. They had observed a large increase over time in the number of messages related to code review, and were worried about how this could be a signal of problems with their code review process.</p> <p>To address these concerns, we designed and conducted, with their continuous feedback, a detailed analysis focused on finding these problems, if any. During the study, we dealt with the methodological problems of Linux-like code review, and with the deeper issue of finding metrics that could uncover the problems they were worried about. For having a benchmark, we run the same analysis on a similar project, which uses very similar code review practices: the Linux Netdev (Netdev) project. As a result, we learned how in fact the Xen Project had some problems, but at the moment of the analysis those were already under control. We found as well how different the Xen and Netdev projects were behaving with respect to code review performance, despite being so similar from many points of view.</p> <p>In this paper we show the results of both analyses, and propose a comprehensive methodology, fully automated, to study Linux-style code review. We discuss also the problems of getting significant metrics to track improvements or detect problems in this kind of code review.</p>","","Electronic:978-1-4503-4804-1","10.1145/3084226.3084247","","","Code review;Data mining;Software development analytics","","","","","","","","","","","15-16 June 2017","","ACM","ACM Conferences"
"Predicting Usefulness of Code Review Comments Using Textual Features and Developer Experience","M. M. Rahman; C. K. Roy; R. G. Kula","Univ. of Saskatchewan, Saskatoon, SK, Canada","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","20170703","2017","","","215","226","Although peer code review is widely adopted in both commercial and open source development, existing studies suggest that such code reviews often contain a significant amount of non-useful review comments. Unfortunately, to date, no tools or techniques exist that can provide automatic support in improving those non-useful comments. In this paper, we first report a comparative study between useful and non-useful review comments where we contrast between them using their textual characteristics, and reviewers' experience. Then, based on the findings from the study, we develop RevHelper, a prediction model that can help the developers improve their code review comments through automatic prediction of their usefulnessduring review submission. Comparative study using 1,116 review comments suggested that useful comments share more vocabulary with the changed code, contain salient items like relevant code elements, and their reviewers are generally more experienced. Experiments using 1,482 review comments report that our model can predict comment usefulness with 66% prediction accuracy which is promising. Comparison with three variants of a baseline model using a case study validates our empirical findings and demonstrates the potential of our model.","","Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4","10.1109/MSR.2017.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962371","Code review quality;change triggering capability;code element;review comment usefulness;reviewing experience","Companies;Inspection;Libraries;Predictive models;Software;Tools;Vocabulary","public domain software;software quality;source code (software);text analysis","RevHelper;code review comments;commercial source development;developer experience;open source development;relevant code elements;review submission;software development;textual characteristics;textual features;usefulness prediction","","","","","","","","20-21 May 2017","","IEEE","IEEE Conferences"
"Impact of Continuous Integration on Code Reviews","M. M. Rahman; C. K. Roy","Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","20170703","2017","","","499","502","Peer code review and continuous integration often interleave with each other in the modern software quality management. Although several studies investigate how non-technical factors (e.g., reviewer workload), developer participation and even patch size affect the code review process, the impact of continuous integration on code reviews is not yet properly understood. In this paper, we report an exploratory study using 578K automated build entries where we investigate the impact of automated builds on the code reviews. Our investigation suggests that successfully passed builds are more likely to encourage new code review participation in a pull request. Frequently built projects are found to be maintaining a steady level of reviewing activities over the years, which was quite missing from the rarely built projects. Experiments with 26,516 automated build entries reported that our proposed model can identify 64% of the builds that triggered new code reviews later.","","Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4","10.1109/MSR.2017.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962406","Automated build status;build frequency;code review quality;review participation","Correlation;Encoding;Frequency measurement;Manuals;Software quality;Standards","software management;software quality;software reviews","continuous integration;developer participation;nontechnical factors;patch size;peer code review;software quality management","","","","","","","","20-21 May 2017","","IEEE","IEEE Conferences"
"Code Reviewing in the Trenches: Understanding Challenges and Best Practices","L. MacLeod; M. Greiler; M. A. Storey; C. Bird; J. Czerwonka","","IEEE Software","","2017","Early Access","Early Access","1","1","Code review is a software practice that is widely adopted by and adapted to open source and industrial projects.Code review practices have been researched extensively, with most studies relying on trace data from tool reviews, augmented by surveys and interviews in a few cases. Several recent industrial research studies-in addition to blog posts and white papers-have exposed additional insights on code reviewing ""from the trenches'. Unfortunately, the lessons learned about code reviewing are widely dispersed and poorly summarized by existing literature. In particular, practitioners wishing to adopt or reflect on a new or existing code review process may find it difficult to know which challenges to expect and which best practices to adopt for their specific development context. Building on the existing literature, we add insights from a recent large-scale study of the code review practices of Microsoft developers to summarize the challenges faced by code change authors and reviewers, suggest best practices for code reviewing, and mention trade-offs that practitioners should consider.","0740-7459;07407459","","10.1109/MS.2017.265100500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950877","D Software/Software Engineering;D.2.5.a Code inspections and walkthroughs < D.2.5 Testing and Debugging < D.2 Software Engineering < D Software/Software Enginee;N. Learning Technologies;N.3 Social Technologies;N.3.f Peer reviewing","Best practices;Context;Interviews;Object recognition;Organizations;Stakeholders;Tools","","","","","","","","","20170616","","","IEEE","IEEE Early Access Articles"
"Impact of Gamification on Code review process: An Experimental Study","S. Khandelwal; S. K. Sripada; Y. R. Reddy","Software Engineering Reseach Center, IIIT Hyderabad, India","Proceedings of the 10th Innovations in Software Engineering Conference","20170424","2017","","","122","126","<p>Researchers have supported the idea of gamification to enhance students' interest in activities like code reviews, change management, knowledge management, issue tracking, etc. which might otherwise be repetitive and monotonous. We performed an experimental study consisting of nearly 180+ participants to measure the impact of gamification on code review process using 5 different code review tools, including one gamified code review instance from our extensible architectural framework. We assess the impact of gamification based on the code smells and bugs identified in a gamified and non-gamified environment as per code inspection report. Further, measurement and comparison of the quantity and usefulness of code review comments was done using machine learning techniques.</p>","","","10.1145/3021460.3021474","","","Architectural Framework;Classification;Code Reviews;Evaluation;Gamification;Text Analysis","","","","","","","","","","","5-7 Feb. 2017","","ACM","ACM Conferences"
"Factors influencing code review processes in industry","T. Baum; O. Liskin; K. Niklas; K. Schneider","Leibniz Universit&#228;t Hannover, Germany","Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering","20170413","2016","","","85","96","<p> Code review is known to be an efficient quality assurance technique. Many software companies today use it, usually with a process similar to the patch review process in open source software development. However, there is still a large fraction of companies performing almost no code reviews at all. And the companies that do code reviews have a lot of variation in the details of their processes. For researchers trying to improve the use of code reviews in industry, it is important to know the reasons for these process variations. We have performed a grounded theory study to clarify process variations and their rationales. The study is based on interviews with software development professionals from 19 companies. These interviews provided insights into the reasons and influencing factors behind the adoption or non-adoption of code reviews as a whole as well as for different process variations. We have condensed these findings into seven hypotheses and a classification of the influencing factors. Our results show the importance of cultural and social issues for review adoption. They trace many process variations to differences in development context and in desired review effects. </p>","","","10.1145/2950290.2950323","","","Code reviews;Empirical software engineering","","","","","","","","","","","13-18 Nov. 2016","","ACM","ACM Conferences"
"Visualizing code and coverage changes for code review","S. Oosterwaal; A. v. Deursen; R. Coelho; A. A. Sawant; A. Bacchelli","Delft University of Technology, Netherlands","Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering","20170413","2016","","","1038","1041","<p> One of the tasks of reviewers is to verify that code modifications are well tested. However, current tools offer little support in understanding precisely how changes to the code relate to changes to the tests. In particular, it is hard to see whether (modified) test code covers the changed code. To mitigate this problem, we developed Operias, a tool that provides a combined visualization of fine-grained source code differences and coverage impact. Operias works both as a stand-alone tool on specific project versions and as a service hooked to GitHub. In the latter case, it provides automated reports for each new pull request, which reviewers can use to assess the code contribution. Operias works for any Java project that works with maven and its standard Cobertura coverage plugin. We present how Operias could be used to identify test-related problems in real-world pull requests. Operias is open source and available on GitHub with a demo video: https://github.com/SERG-Delft/operias </p>","","","10.1145/2950290.2983929","","","code review;software evolution;software testing","","","","","","","","","","","13-18 Nov. 2016","","ACM","ACM Conferences"
"Revisiting Code Ownership and Its Relationship with Software Quality in the Scope of Modern Code Review","P. Thongtanunam; S. McIntosh; A. E. Hassan; H. Iida","Nara Inst. of Sci. & Technol., Nara, Japan","2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)","20170403","2016","","","1039","1050","Code ownership establishes a chain of responsibility for modules in large software systems. Although prior work uncovers a link between code ownership heuristics and software quality, these heuristics rely solely on the authorship of code changes. In addition to authoring code changes, developers also make important contributions to a module by reviewing code changes. Indeed, recent work shows that reviewers are highly active in modern code review processes, often suggesting alternative solutions or providing updates to the code changes. In this paper, we complement traditional code ownership heuristics using code review activity. Through a case study of six releases of the large Qt and OpenStack systems, we find that: (1) 67%-86% of developers did not author any code changes for a module, but still actively contributed by reviewing 21%-39% of the code changes, (2) code ownership heuristics that are aware of reviewing activity share a relationship with software quality, and (3) the proportion of reviewers without expertise shares a strong, increasing relationship with the likelihood of having post-release defects. Our results suggest that reviewing activity captures an important aspect of code ownership, and should be included in approximations of it in future studies.","","Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3","10.1145/2884781.2884852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886978","Expertise;Ownership;Software Quality","Birds;Conferences;Organizations;Software design;Software quality;Software systems","software quality;source code (software)","OpenStack systems;code change authorship;code ownership heuristics;code review activity;large Qt systems;large software systems;modern code review processes;post-release defects;software quality","","4","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"Code Review Quality: How Developers See It","O. Kononenko; O. Baysal; M. W. Godfrey","Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada","2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)","20170403","2016","","","1028","1038","In a large, long-lived project, an effective code review process is key to ensuring the long-term quality of the code base. In this work, we study code review practices of a large, open source project, and we investigate how the developers themselves perceive code review quality. We present a qualitative study that summarizes the results from a survey of 88 Mozilla core developers. The results provide developer insights into how they define review quality, what factors contribute to how they evaluate submitted code, and what challenges they face when performing review tasks. We found that the review quality is primarily associated with the thoroughness of the feedback, the reviewer's familiarity with the code, and the perceived quality of the code itself. Also, we found that while different factors are perceived to contribute to the review quality, reviewers often find it difficult to keep their technical skills up-to-date, manage personal priorities, and mitigate context switching.","","Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3","10.1145/2884781.2884840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886977","Code review;developer perception;review quality;survey","Computer bugs;Computer science;Data mining;Electronic mail;Face;Measurement;Software","software quality","Mozilla core developers;effective code review process;long-term quality;open source project","","1","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"A Study of the Quality-Impacting Practices of Modern Code Review at Sony Mobile","J. Shimagaki; Y. Kamei; S. Mcintosh; A. E. Hassan; N. Ubayashi","Sony Mobile, Japan","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","212","221","Nowadays, a flexible, lightweight variant of the code review process (i.e., the practice of having other team members critique software changes) is adopted by open source and proprietary software projects. While this flexibility is a blessing (e.g., enabling code reviews to span the globe), it does not mandate minimum review quality criteria like the formal code inspections of the past. Recent work shows that lax reviewing can impact the quality of open source systems. In this paper, we investigate the impact that code reviewing practices have on the quality of a proprietary system that is developed by Sony Mobile. We begin by replicating open source analyses of the relationship between software quality (as approximated by post-release defect-proneness) and: (1) code review coverage, i.e., the proportion of code changes that have been reviewed and (2) code review participation, i.e., the degree of reviewer involvement in the code review process. We also perform a qualitative analysis, with a survey of 93 stakeholders, semi-structured interviews with 15 stakeholders, and a follow-up survey of 25 senior engineers. Our results indicate that while past measures of review coverage and participation do not share a relationship with defect-proneness at Sony Mobile, reviewing measures that are aware of the Sony Mobile development context are associated with defect-proneness. Our results have lead to improvements of the Sony Mobile code review process.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883305","Code review;software quality","Inspection;Interviews;Measurement;Mobile communication;Software quality;Stakeholders","program testing;public domain software;software quality","Sony Mobile;code review coverage;code review participation;defect-proneness;open source systems;quality-impacting practices;software quality;software testing","","","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"Teaching Code Review Management Using Branch Based Workflows","S. Krusche; M. Berisha; B. Bruegge","Tech. Univ. Munchen, Munich, Germany","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","384","393","Developing software with high code quality in a university environment is a challenge for instructors of software engineering capstone courses. Teaching students how to achieve quality and how to conduct code reviews in projects is often neglected, although it helps to improve maintainability. In this paper we describe an informal review technique: branch based code reviews. Developers realize requirements in feature branches. Before they integrate source code into the main codebase, the code is reviewed asynchronously over the Internet in a quality gate to identify defects, design flaws and code flaws. Traditionally reviewing has been a task of the instructor. In our course, we delegate it to students of the development team. Our hypothesis is that students learn and adapt best practices from each other, improving code quality and their coding skills. We applied this technique in a workflow during a project- based capstone course over the period of three semesters. 300 students conducted 2939 code reviews with 4665 comments in 33 projects with industry customers. We evaluated the workflow in a qualitative study using interviews. Our key findings are that students do not longer see reviews as a bureaucratic burden and improve their skills through the comments of experienced team members. They are convinced that reviewing code leads to higher code quality and 89 % want to use the workflow again in future projects.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883324","Branching Model;Capstone Course;Code Quality;Distributed Version Control;Merge Request;Pair Programming;Peer Learning;Pull Request","Education;Industries;Inspection;Programming;Software;Software engineering;Taxonomy","computer science education;software quality;source code (software)","branch based code reviews;branch based workflows;code review management;high code quality;informal review technique;software development;software engineering capstone courses;university environment","","","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"Characterization of the Xen Project Code Review Process: an Experience Report","D. Izquierdo-Cortazar; L. Kurth; J. M. Gonz√°lez-Barahona; S. Due√±as; N. Sekitoleko","","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","20170126","2016","","","386","390","Many software development projects have introduced mandatory code review for every change to the code. This means that the project needs to devote a significant effort to review all proposed changes, and that their merging into the code base may get considerably delayed. Therefore, all those projects need to understand how code review is working, and the delays it is causing in time to merge. This is the case in the Xen project, which performs peer review using mailing lists. During the first half of 2015, some people in the project observed a large and sustained increase in the number of messages related to code review, which had started some years before. This observation led to concerns on whether the code review process was having some trouble, and too large an impact on the overall development process. Those concerns were addressed with a quantitative study, which is presented in this paper. Based on the information in code review messages, some metrics were defined to infer delays imposed by code review. The study produced quantitative data suitable for informed discussion, which the project is using to understand its code review process, and to take decisions to improve it.","","Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7","10.1109/MSR.2016.046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832917","code review;data mining;software process","Data mining;Databases;Delays;Market research;Merging;Message systems;Software","software engineering","Xen project code review process;code review messages;mailing lists;peer review;quantitative data;software development projects","","","","","","","","14-15 May 2016","","IEEE","IEEE Conferences"
"Mining the Modern Code Review Repositories: A Dataset of People, Process and Product","X. Yang; R. G. Kula; N. Yoshida; H. Iida","NAIST, Ikoma, Japan","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","20170126","2016","","","460","463","In this paper, we present a collection of Modern Code Review data for five open source projects. The data showcases mined data from both an integrated peer review system and source code repositories. We present an easy-to-use and richer data structure to retrieve the 1.) People 2.) Process and 3.) Product aspects of the peer review. This paper presents the extraction methodology, the dataset structure, and a collection of database dumps.","","Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7","10.1109/MSR.2016.054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832925","","Data mining;Electronic mail;History;Relational databases;Servers;Software","data mining;data structures;public domain software;software reviews;source code (software)","code review data;code review repositories;data mining;database dumps collection;dataset structure;extraction methodology;open source projects;peer review system;people dataset;process dataset;product dataset;source code repositories","","","","","","","","14-15 May 2016","","IEEE","IEEE Conferences"
"A Collaborative Code Review Platform for GitHub","A. Kalyan; M. Chiam; J. Sun; S. Manoharan","Dept. of Electr. & Comput. Eng., Univ. of Auckland, Auckland, New Zealand","2016 21st International Conference on Engineering of Complex Computer Systems (ICECCS)","20170116","2016","","","191","196","The incorporation of peer code reviews as being part of a developer's work flow, and hence the software development lifecycle, has steadily grown in popularity over the past three decades. During the process of statically inspecting code, developers of a codebase are able to collaboratively detect possible code defects, as well as use code reviews as a means of transferring knowledge to improve the overall understanding of a system. The uptake of such practices is dependent on several factors, specifically the availability of tools that aid in providing an easy-to-use and intuitive platform to perform code reviews, and is readily accessible to members of a project. This paper briefly explores the act of code review, identifies the pitfalls of existing code review tools, and proposes the design and development of a web-based code review application, along with the evaluation of the prototype. The code review tool developed, Fistbump, targets the popular Git based repository hosting service, GitHub, and provides a versatile tool to coordinate and manage discussions between the owner of a pull request and the elected participants of a review.","","Electronic:978-1-5090-5527-2; POD:978-1-5090-5528-9; Paper:978-1-5090-5526-5","10.1109/ICECCS.2016.032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816585","Code Review;Collaborative Platform;Team Programming","Collaboration;Computers;Industries;Inspection;Message systems;Servers;Software","Internet;groupware;program debugging;software development management;software tools;source code (software)","Fistbump;Git based repository hosting service;GitHub;Web-based code review application;code defect detection;code review tools;collaborative code review platform;software development lifecycle","","","","","","","","6-8 Nov. 2016","","IEEE","IEEE Conferences"
"Search-Based Peer Reviewers Recommendation in Modern Code Review","A. Ouni; R. G. Kula; K. Inoue","Dept. of Comput. Sci., Osaka Univ., Suita, Japan","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20170116","2016","","","367","377","Code review is of primary importance in modern software development. It is widely recognized that peer review is an efficient and effective practice for improving software quality and reducing defect proneness. For successful review process, peer reviewers should have a deep experience and knowledge with the code being reviewed, and familiar to work and collaborate together. However, one of the main challenging tasks in modern code review is to find the most appropriate reviewers for submitted code changes. So far, reviewers assignment is still a manual, costly and time-consuming task. In this paper, we introduce a search-based approach, namely RevRec, to provide decision-making support for code change submitters and/or reviewers assigners to identify most appropriate peer reviewers for their code changes. RevRec aims at finding reviewers to be assigned for a code change based on their expertise and collaboration in past reviews using genetic algorithm (GA). We evaluated our approach on a benchmark of three open-source software systems, Android, OpenStack, and Qt. Results indicate that RevRec accurately recommends code reviewers with up to 59% of precision and 74% of recall. Our experiments provide evidence that leveraging reviewers expertise from their prior reviews and the socio-technical aspects of the team work and collaboration is relevant in improving the performance of peer reviewers recommendation in modern code review.","","Electronic:978-1-5090-3806-0; POD:978-1-5090-3807-7","10.1109/ICSME.2016.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816482","Code review;Reviewer recommendation;Search-based Software Engineeging","Genetic algorithms;Search problems;Sociology;Software;Software engineering;Teamwork","Android (operating system);decision making;genetic algorithms;public domain software;search problems;software quality;software reviews;source code (software);team working","Android;OpenStack;Qt;RevRec;code review;decision making support;genetic algorithm;open-source software systems;peer review;precision;recall;search-based approach;search-based peer reviewer recommendation;software development;software quality;team work socio-technical aspects","","","","","","","","2-7 Oct. 2016","","IEEE","IEEE Conferences"
"Code Review Participation: Game Theoretical Modeling of Reviewers in Gerrit Datasets","N. Kitagawa; H. Hata; A. Ihara; K. Kogiso; K. Matsumoto","Nara Inst. of Sci. & Technol., Nara, Japan","2016 IEEE/ACM Cooperative and Human Aspects of Software Engineering (CHASE)","20170109","2016","","","64","67","Code review is a common practice for improving the quality of source code changes and expediting knowledge transfer in a development community. In modern code review, source code changes or patches are considered to be assessed and approved for integration by multiple reviews. However, from our empirical study, we found that some patches are reviewed by only one reviewer, and some reviewers did not continue the review discussion, which can have negative effects on software quality. To understand these reviewers' behaviors, we model the code review situation based on the snowdrift game, which is used to analyze social dilemmas. With this game-theoretical modeling, we found that it can explain reviewers' behaviors well.","","Electronic:978-1-4503-4155-4; POD:978-1-5090-2198-7","10.1109/CHASE.2016.022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809488","Code review;Empirical Study;Game Theory","Analytical models;Conferences;Games;Servers","game theory;software quality;source code (software)","Gerrit dataset;code review participation;code review situation model;game theoretical modeling;knowledge transfer;snowdrift game;social dilemma;source code change quality","","","","","","","","16-16 May 2016","","IEEE","IEEE Conferences"
"A Security Perspective on Code Review: The Case of Chromium","M. di Biase; M. Bruntink; A. Bacchelli","Software Improvement Group, Amsterdam, Netherlands","2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation (SCAM)","20161215","2016","","","21","30","Modern Code Review (MCR) is an established software development process that aims to improve software quality. Although evidence showed that higher levels of review coverage relates to less post-release bugs, it remains unknown the effectiveness of MCR at specifically finding security issues. We present a work we conduct aiming to fill that gap by exploring the MCR process in the Chromium open source project. We manually analyzed large sets of registered (114 cases) and missed (71 cases) security issues by backtracking in the project's issue, review, and code histories. This enabled us to qualify MCR in Chromium from the security perspective from several angles: Are security issues being discussed frequently? What categories of security issues are often missed or found? What characteristics of code reviews appear relevant to the discovery rate? Within the cases we analyzed, MCR in Chromium addresses security issues at a rate of 1% of reviewers' comments. Chromium code reviews mostly tend to miss language-specific issues (eg C++ issues and buffer overflows) and domain-specific ones (eg such as Cross-Site Scripting), when code reviews address issues, mostly they address those that pertain to the latter type. Initial evidence points to reviews conducted by more than 2 reviewers being more successful at finding security issues.","","Electronic:978-1-5090-3848-0; POD:978-1-5090-3849-7","10.1109/SCAM.2016.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7781793","code review;empirical software engineering;mining software repositories;modern code review;security flaw;software security","Chromium;Computer bugs;Inspection;Manuals;Security;Software quality","backtracking;program debugging;public domain software;safety-critical software;software quality;source code (software)","Chromium code reviews;Chromium open source project;MCR;backtracking;bugs;domain-specific ones;language-specific issues;modern code review;security issues;software development process;software quality improvement","","","","","","","","2-3 Oct. 2016","","IEEE","IEEE Conferences"
"Characterization of the Xen project code review process: an experience report","D. Izquierdo-Cortazar; L. Kurth; J. M. Gonzalez-Barahona; S. Due√±as; N. Sekitoleko","Bitergia","Proceedings of the 13th International Conference on Mining Software Repositories","20161111","2016","","","386","390","<p>Many software development projects have introduced mandatory code review for every change to the code. This means that the project needs to devote a significant effort to review all proposed changes, and that their merging into the code base may get considerably delayed. Therefore, all those projects need to understand how code review is working, and the delays it is causing in time to merge.</p> <p>This is the case in the Xen project, which performs peer review using mailing lists. During the first half of 2015, some people in the project observed a large and sustained increase in the number of messages related to code review, which had started some years before. This observation led to concerns on whether the code review process was having some trouble, and too large an impact on the overall development process.</p> <p>Those concerns were addressed with a quantitative study, which is presented in this paper. Based on the information in code review messages, some metrics were defined to infer delays imposed by code review. The study produced quantitative data suitable for informed discussion, which the project is using to understand its code review process, and to take decisions to improve it.</p>","","","10.1145/2901739.2901778","","","code review;data mining;software process","","","","","","","","","","","14-22 May 2016","","ACM","ACM Conferences"
"Characteristics of useful code reviews: an empirical study at Microsoft","A. Bosu; M. Greiler; C. Bird","University of Alabama, Tuscaloosa, Alabama","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","146","156","<p>Over the past decade, both open source and commercial software projects have adopted contemporary peer code review practices as a quality control mechanism. Prior research has shown that developers spend a large amount of time and effort performing code reviews. Therefore, identifying factors that lead to useful code reviews can benefit projects by increasing code review effectiveness and quality. In a three-stage mixed research study, we qualitatively investigated what aspects of code reviews make them useful to developers, used our findings to build and verify a classification model that can distinguish between useful and not useful code review feedback, and finally we used this classifier to classify review comments enabling us to empirically investigate factors that lead to more effective code review feedback.</p> <p>In total, we analyzed 1.5 millions review comments from five Microsoft projects and uncovered many factors that affect the usefulness of review feedback. For example, we found that the proportion of useful comments made by a reviewer increases dramatically in the first year that he or she is at Microsoft but tends to plateau afterwards. In contrast, we found that the more files that are in a change, the lower the proportion of comments in the code review that will be of value to the author of the change. Based on our findings, we provide recommendations for practitioners to improve effectiveness of code reviews.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Impact of developer reputation on code review outcomes in OSS projects: an empirical investigation","A. Bosu; J. C. Carver","University of Alabama, Tuscaloosa, AL","Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","20161111","2014","","","1","10","<p><i><u>Context:</u></i> Gaining an identity and building a good reputation are important motivations for Open Source Software (OSS) developers. It is unclear whether these motivations have any actual impact on OSS project success. <i><u>Goal:</u></i> <i>To identify how an OSS developer's reputation affects the outcome of his/her code review requests</i>. <i><u>Method:</u></i> We conducted a social network analysis (SNA) of the code review data from eight popular OSS projects. Working on the assumption that core developers have better reputation than peripheral developers, we developed an approach, Core Identification using K-means (CIK) to divide the OSS developers into core and periphery groups based on six SNA centrality measures. We then compared the outcome of the code review process for members of the two groups. <i><u>Results:</u></i> The results suggest that the core developers receive quicker first feedback on their review request, complete the review process in shorter time, and are more likely to have their code changes accepted into the project codebase. Peripheral developers may have to wait 2 - 19 times (or 12 - 96 hours) longer than core developers for the review process of their code to complete. <i><u>Conclusion:</u></i> We recommend that projects allocate resources or create tool support to triage the code review requests to motivate prospective developers through quick feedback.</p>","","","10.1145/2652524.2652544","","","code review;network structure;open source;peer impression;social network analysis","","","","","5","","","","","","18-19 Sept. 2014","","ACM","ACM Conferences"
"Code reviews do not find bugs: how the current code review best practice slows us down","J. Czerwonka; M. Greiler; J. Tilford","Microsoft Corporation Redmond, WA","Proceedings of the 37th International Conference on Software Engineering","20161111","2015","2","","27","28","<p>Because of its many uses and benefits, code reviews are a standard part of the modern software engineering workflow. Since they require involvement of people, code reviewing is often the longest part of the code integration activities. Using experience gained at Microsoft and with support of data, we posit (1) that code reviews often do not find functionality issues that should block a code submission; (2) that effective code reviews should be performed by people with specific set of skills; and (3) that the social aspect of code reviews cannot be ignored. We find that we need to be more sophisticated with our guidelines for the code review workflow. We show how our findings from code reviewing practice influence our code review tools at Microsoft. Finally, we assert that, due to its costs, code reviewing practice is a topic deserving to be better understood, systematized and applied to software engineering workflow with more precision than the best practice currently prescribes.</p>","","","","","","code integration;code reviews;software engineering workflow","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Gamifying software engineering tasks based on cognitive principles: the case of code review","N. Unkelos-Shpigel; I. Hadar","University of Haifa, Haifa, Israel","Proceedings of the Eighth International Workshop on Cooperative and Human Aspects of Software Engineering","20161111","2015","","","119","120","<p>Code review is an important task in software development. However, performing code review is perceived, for the most part, as an undesired task, presenting several challenges to the required collaboration and knowledge transfer between programmers and reviewers. In order to overcome these challenges and improve the effectiveness of code review, we developed SCRUT: Social Code Review Unifying Tool. By recruiting relevant cognitive theories and implementing gamification elements to motivate collaboration and knowledge sharing between programmers and reviewers, we plan to enhance the task of code review. This paper presents our vision for enhancing software engineering via gamification, and the theoretical cognitive foundation on which this vision is based, starting with the example of code review.</p>","","","","","","boundary object;code review;gamification","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Investigating code review practices in defective files: an empirical study of the Qt system","P. Thongtanunam; S. McIntosh; A. E. Hassan; H. Iida","Nara Institute of Science and Technology, Japan","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","168","179","<p>Software code review is a well-established software quality practice. Recently, Modern Code Review (MCR) has been widely adopted in both open source and proprietary projects. To evaluate the impact that characteristics of MCR practices have on software quality, this paper comparatively studies MCR practices in defective and clean source code files. We investigate defective files along two perspectives: 1) files that will eventually have defects (i.e., future-defective files) and 2) files that have historically been defective (i.e., risky files). Through an empirical study of 11,736 reviews of changes to 24,486 files from the Qt open source project, we find that both future-defective files and risky files tend to be reviewed less rigorously than their clean counterparts. We also find that the concerns addressed during the code reviews of both defective and clean files tend to enhance evolvability, i.e., ease future maintenance (like documentation), rather than focus on functional issues (like incorrect program logic). Our findings suggest that although functionality concerns are rarely addressed during code review, the rigor of the reviewing process that is applied to a source code file throughout a development cycle shares a link with its defect proneness.</p>","","","","","","code review;software quality","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"An approach for collaborative code reviews using multi-touch technology","S. M√ºller; M. W√ºrsch; T. Fritz; H. C. Gall","University of Zurich, Switzerland","Proceedings of the 5th International Workshop on Co-operative and Human Aspects of Software Engineering","20161111","2012","","","93","99","<p>Code reviews are an effective mechanism to improve software quality, but often fall short in the development of software. To improve the desirability and ease of code reviews, we introduce an approach that explores how multi-touch interfaces can support code reviews and can make them more collaborative. Our approach provides users with features to collaboratively find and investigate code smells, annotate source code and generate review reports using gesture recognition and a Microsoft Surface Table. In a preliminary evaluation, subjects generally liked the prototypical implementation of our approach for performing code review tasks.</p>","","","","","","code review;code smell;collaboration;gesture;multi-touch;software metrics","","","","","","","","","","","2-2 June 2012","","ACM","ACM Conferences"
"Removing stagnation from modern code review","G. Viviani; G. C. Murphy","University of British Columbia, Canada","Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity","20161111","2016","","","43","44","<p> Finding defects efficently is one of the major problems in software development, a problem that often still relies largely on human inspection of code to find defects. Many software development projects use code reviews as a mean to ensure this human inspection occurs. Known as modern code review, this approach is based on tools, such as Gerrit, that help the developers in the reviewing process. As part of this approach, developers are often presented with a list of open code reviews requiring attention; it is left to the developer to find a suitable review on which to work on from a long list of reviews. We present an investigation of two algorithms that recommend an ordering of the list of open reviews based on properties of the reviews. We use a simulation study over the JGit project from the Eclipse Foundation to show that an algorithm based on ordering reviews from least lines of codes changed in the code review to most lines of code out performs other algorithms. This algorithm shows promise for eliminating stagnation of reviews and optimizing the average duration reviews are open. </p>","","","10.1145/2984043.2989224","","","code inspection;software development process","","","","","","","","","","","Oct. 30 2016-Nov. 4 2016","","ACM","ACM Conferences"
"Partitioning composite code changes to facilitate code review","Y. Tao; S. Kim","The Hong Kong University of Science and Technology","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","180","190","<p>Developers expend significant effort on reviewing source code changes. Hence, the comprehensibility of code changes directly affects development productivity. Our prior study has suggested that <i>composite</i> code changes, which mix multiple development issues together, are typically difficult to review. Unfortunately, our manual inspection of 453 open source code changes reveals a non-trivial occurrence (up to 29%) of such composite changes.</p> <p>In this paper, we propose a heuristic-based approach to automatically partition composite changes, such that each subchange in the partition is more cohesive and self-contained. Our quantitative and qualitative evaluation results are promising in demonstrating the potential benefits of our approach for facilitating code review of composite code changes.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"An empirical investigation of socio-technical code review metrics and security vulnerabilities","A. Meneely; A. C. R. Tejeda; B. Spates; S. Trudeau; D. Neuberger; K. Whitlock; C. Ketant; K. Davis","Rochester Institute of Technology, USA","Proceedings of the 6th International Workshop on Social Software Engineering","20161111","2014","","","37","44","<p> One of the guiding principles of open source software development is to use crowds of developers to keep a watchful eye on source code. Eric Raymond declared Linus'' Law as ""many eyes make all bugs shallow"", with the socio-technical argument that high quality open source software emerges when developers combine together their collective experience and expertise to review code collaboratively. Vulnerabilities are a particularly nasty set of bugs that can be rare, difficult to reproduce, and require specialized skills to recognize. Does Linus' Law apply to vulnerabilities empirically? In this study, we analyzed 159,254 code reviews, 185,948 Git commits, and 667 post-release vulnerabilities in the Chromium browser project. We formulated, collected, and analyzed various metrics related to Linus' Law to explore the connection between collaborative reviews and vulnerabilities that were missed by the review process. Our statistical association results showed that source code files reviewed by more developers are, counter-intuitively, more likely to be vulnerable (even after accounting for file size). However, files are less likely to be vulnerable if they were reviewed by developers who had experience participating on prior vulnerability-fixing reviews. The results indicate that lack of security experience and lack of collaborator familiarity are key risk factors in considering Linus‚Äô Law with vulnerabilities. </p>","","","10.1145/2661685.2661687","","","code review;socio-technical;vulnerability","","","","","6","","","","","","17-17 Nov. 2014","","ACM","ACM Conferences"
"Characteristics of the vulnerable code changes identified through peer code review","A. Bosu","University of Alabama, USA","Companion Proceedings of the 36th International Conference on Software Engineering","20161111","2014","","","736","738","<p> To effectively utilize the efforts of scarce security experts, this study aims to provide empirical evidence about the characteristics of security vulnerabilities. Using a three-stage, manual analysis of peer code review data from 10 popular Open Source Software (OSS) projects, this study identified 413 potentially vulnerable code changes (VCC). Some key results include: 1) the most experienced contributors authored the majority of the VCCs, 2) while less experienced authors wrote fewer VCCs, their code changes were 1.5 to 24 times more likely to be vulnerable, 3) employees of the organization sponsoring the OSS projects are more likely to write VCCs. </p>","","","10.1145/2591062.2591200","","","code review;inspection;open source;security defects;vulnerability","","","","","2","","","","","","May 31 2014-June 7 2014","","ACM","ACM Conferences"
"Interactive code review for systematic changes","T. Zhang; M. Song; J. Pinedo; M. Kim","University of California, Los Angeles","Proceedings of the 37th International Conference on Software Engineering","20161111","2015","1","","111","122","<p>Developers often inspect a diff patch during peer code reviews. Diff patches show low-level program differences per file without summarizing <i>systematic changes</i>---similar, related changes to multiple contexts. We present C<scp>ritics</scp>, an interactive approach for inspecting systematic changes. When a developer specifies code change within a diff patch, C<scp>ritics</scp> allows developers to customize the change template by iteratively generalizing change content and context. By matching a generalized template against the codebase, it summarizes similar changes and detects potential mistakes. We evaluated C<scp>ritics</scp> using two methods. First, we conducted a user study at Salesforce.com, where professional engineers used C<scp>ritics</scp> to investigate diff patches authored by their own team. After using C<scp>ritics</scp>, all six participants indicated that they would like C<scp>ritics</scp> to be integrated into their current code review environment. This also attests to the fact that C<scp>ritics</scp> scales to an industry-scale project and can be easily adopted by professional engineers. Second, we conducted a user study where twelve participants reviewed diff patches using C<scp>ritics</scp> and Eclipse diff. The results show that human subjects using C<scp>ritics</scp> answer questions about systematic changes 47.3% more correctly with 31.9% saving in time during code review tasks, in comparison to the baseline use of Eclipse diff. These results show that C<scp>ritics</scp> should improve developer productivity in inspecting systematic changes during peer code reviews.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Mining the modern code review repositories: a dataset of people, process and product","X. Yang; R. G. Kula; N. Yoshida; H. Iida","NAIST, Japan","Proceedings of the 13th International Conference on Mining Software Repositories","20161111","2016","","","460","463","<p>In this paper, we present a collection of Modern Code Review data for five open source projects. The data showcases mined data from both an integrated peer review system and source code repositories. We present an easy-to-use and richer data structure to retrieve the (a) People, (b) Process, and (c) Product aspects of the peer review. This paper presents the extraction methodology, the dataset structure, and a collection of database dumps.</p>","","","10.1145/2901739.2903504","","","","","","","","2","","","","","","14-22 May 2016","","ACM","ACM Conferences"
"Lessons learned from building and deploying a code review analytics platform","C. Bird; T. Carnahan; M. Greiler","Microsoft, Redmond, WA","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","191","201","<p>Tool-based code review is growing in popularity and has become a standard part of the development process at Microsoft. Adoption of these tools makes it possible to mine data from code reviews and provide access to it. In this paper, we present an experience report for CodeFlow Analytics, a system that collects code review data, generates metrics from this data, and provides a number of ways for development teams to access the metrics and data. We discuss the design, design decisions and challenges that we encountered when building CodeFlow Analytics. We contacted teams that used CodeFlow Analytics over the past two years and discuss what prompted them to use CodeFlow Analytics, how they have used it, and what the impact has been. Further, we survey research that has been enabled by using the CodeFlow Analytics platform. We provide a series of lessons learned from this experience to help others embarking on a task of building an analytics platform in an enterprise setting.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Critics: an interactive code review tool for searching and inspecting systematic changes","T. Zhang; M. Song; M. Kim","University of California at Los Angeles, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","20161111","2014","","","755","758","<p> During peer code reviews, developers often examine program differences. When using existing program differencing tools, it is difficult for developers to inspect systematic changes‚Äîsimilar, related changes that are scattered across multiple files. Developers cannot easily answer questions such as ""what other code locations changed similar to this change?"" and ""are there any other locations that are similar to this code but are not updated?"" In this paper, we demonstrate Critics, an Eclipse plug-in that assists developers in inspecting systematic changes. It (1) allows developers to customize a context-aware change template, (2) searches for systematic changes using the template, and (3) detects missing or inconsistent edits. Developers can interactively refine the customized change template to see corresponding search results. Critics has potential to improve developer productivity in inspecting large, scattered edits during code reviews. The tool's demonstration video is available at https://www.youtube.com/watch?v=F2D7t_Z5rhk </p>","","","10.1145/2635868.2661675","","","Software Evolution;code reviews;program differencing","","","","","1","","","","","","16-21 Nov. 2014","","ACM","ACM Conferences"
"Reliability in the Assessment of Program Quality by Teaching Assistants During Code Reviews","M. J. Scott; G. Ghinea","Brunel University, London, United Kingdom","Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education","20161111","2015","","","346","346","<p>It is of paramount importance that formative feedback is meaningful in order to drive student learning. Achieving this, however, relies upon a clear and constructively aligned model of quality being applied consistently across submissions. This poster presentation raises concerns about the inter-rater reliability of code reviews conducted by teaching assistants in the absence of such a model. Five teaching assistants each reviewed 12 purposely selected programs submitted by introductory programming students. An analysis of their reliability revealed that while teaching assistants were self-consistent, they each assessed code quality in different ways. This suggests a need for standard models of program quality, alongside supporting rubrics and other tools, to be used during code reviews to improve the reliability of formative feedback.</p>","","","10.1145/2729094.2754844","","","agreement;assessment;code inspection;code review;consistency;grading;programming;quality;reliability","","","","","","","","","","","4-8 July 2015","","ACM","ACM Conferences"
"Helping developers help themselves: automatic decomposition of code review changesets","M. Barnett; C. Bird; J. Brunet; S. K. Lahiri","Microsoft Research Redmond, WA","Proceedings of the 37th International Conference on Software Engineering","20161111","2015","1","","134","144","<p>Code Reviews, an important and popular mechanism for quality assurance, are often performed on a changeset, a set of modified files that are meant to be committed to a source repository as an atomic action. Understanding a code review is more difficult when the changeset consists of multiple, independent, code differences. We introduce C<scp>luster</scp>C<scp>hanges</scp>, an automatic technique for decomposing changesets and evaluate its effectiveness through both a quantitative analysis and a qualitative user study.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"RefDistiller: a refactoring aware code review tool for inspecting manual refactoring edits","E. L. G. Alves; M. Song; M. Kim","University of Texas at Austin, USA / Federal University of Campina Grande, Brazil","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","20161111","2014","","","751","754","<p> Manual refactoring edits are error prone, as refactoring requires developers to coordinate related transformations and understand the complex inter-relationship between affected types, methods, and variables. We present RefDistiller, a refactoring-aware code review tool that can help developers detect potential behavioral changes in manual refactoring edits. It first detects the types and locations of refactoring edits by comparing two program versions. Based on the reconstructed refactoring information, it then detects potential anomalies in refactoring edits using two techniques: (1) a template-based checker for detecting missing edits and (2) a refactoring separator for detecting extra edits that may change a program's behavior. By helping developers be aware of deviations from pure refactoring edits, RefDistiller can help developers have high confidence about the correctness of manual refactoring edits. RefDistiller is available as an Eclipse plug-in at https://sites.google.com/site/refdistiller/ and its demonstration video is available at http://youtu.be/0Iseoc5HRpU. </p>","","","10.1145/2635868.2661674","","","Software evolution;refactoring","","","","","","","","","","","16-21 Nov. 2014","","ACM","ACM Conferences"
"Collaborative security code review","H. Assal","Carleton University, Ottawa, ON, Canada","Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia","20161111","2015","","","439","444","<p>With millions of mobile applications available for download, and the proliferation of these types of software in our daily lives, it is becoming increasingly important to ensure the security of these applications. Previous research showed that developers have little knowledge of security and privacy regulations, and existing vulnerability detection tools have usability issues that prevent developers from using them. In my research I look at the human factor; e.g., how developers conduct security reviews of their code and what issues they face when using existing tools. In addition, I aim to develop tools and methodologies that support vulnerability detection while seamlessly integrating with the Software Development Lifecycle.</p>","","","10.1145/2836041.2841224","","","","","","","","","","","","","","Nov. 30 2015-Dec. 2 2015","","ACM","ACM Conferences"
"Process Aspects and Social Dynamics of Contemporary Code Review: Insights from Open Source Development and Industrial Practice at Microsoft","A. Bosu; J. C. Carver; C. Bird; J. Orbeck; C. Chockley","Department of Computer Science, Southern Illinois University, Carbondale, IL","IEEE Transactions on Software Engineering","20170109","2017","43","1","56","75","Many open source and commercial developers practice contemporary code review, a lightweight, informal, tool-based code review process. To better understand this process and its benefits, we gathered information about code review practices via surveys of open source software developers and developers from Microsoft. The results of our analysis suggest that developers spend approximately 10-15 percent of their time in code reviews, with the amount of effort increasing with experience. Developers consider code review important, stating that in addition to finding defects, code reviews offer other benefits, including knowledge sharing, community building, and maintaining code quality. The quality of the code submitted for review helps reviewers form impressions about their teammates, which can influence future collaborations. We found a large amount of similarity between the Microsoft and OSS respondents. One interesting difference is that while OSS respondents view code review as an important method of impression formation, Microsoft respondents found knowledge dissemination to be more important. Finally, we found little difference between distributed and co-located Microsoft teams. Our findings identify the following key areas that warrant focused research: 1) exploring the non-technical benefits of code reviews, 2) helping developers in articulating review comments, and 3) assisting reviewers' program comprehension during code reviews.","0098-5589;00985589","","10.1109/TSE.2016.2576451","10.13039/100000001 - US National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484733","Code review;OSS;commercial projects;open source;peer impressions;survey","Collaboration;Context;Human factors;Inspection;Instruments;Measurement;Organizations","public domain software;software engineering;software management;software reviews;team working","Microsoft teams;OSS;code quality;contemporary code review;knowledge dissemination;open source software developers","","","","","","","20160607","Jan. 1 2017","","IEEE","IEEE Journals & Magazines"
"The impact of code review coverage and code review participation on software quality: a case study of the qt, VTK, and ITK projects","S. McIntosh; Y. Kamei; B. Adams; A. E. Hassan","Queen&#039;s University, Canada","Proceedings of the 11th Working Conference on Mining Software Repositories","20160129","2014","","","192","201","<p> Software code review, i.e., the practice of having third-party team members critique changes to a software system, is a well-established best practice in both open source and proprietary software domains. Prior work has shown that the formal code inspections of the past tend to improve the quality of software delivered by students and small teams. However, the formal code inspection process mandates strict review criteria (e.g., in-person meetings and reviewer checklists) to ensure a base level of review quality, while the modern, lightweight code reviewing process does not. Although recent work explores the modern code review process qualitatively, little research quantitatively explores the relationship between properties of the modern code review process and software quality. Hence, in this paper, we study the relationship between software quality and: (1) code review coverage, i.e., the proportion of changes that have been code reviewed, and (2) code review participation, i.e., the degree of reviewer involvement in the code review process. Through a case study of the Qt, VTK, and ITK projects, we find that both code review coverage and participation share a significant link with software quality. Low code review coverage and participation are estimated to produce components with up to two and five additional post-release defects respectively. Our results empirically confirm the intuition that poorly reviewed code has a negative impact on software quality in large systems using modern reviewing tools. </p>","","","10.1145/2597073.2597076","","","Code reviews;software quality","","","","","29","1","","","","","May 31 2014-June 1 2014","","ACM","ACM Conferences"
"Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation","V. Balachandran","VMware, India","Proceedings of the 2013 International Conference on Software Engineering","20160129","2013","","","931","940","<p> Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93% of all the automatically generated comments. There is only 14.71% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60%-92%, which is significantly better than a comparable method based on file change history. </p>","","","","","","","","","","","24","","","","","","18-26 May 2013","","ACM","ACM Conferences"
"Peer code review in open source communitiesusing reviewboard","A. Bosu; J. C. Carver","The University of Alabama, Tuscaloosa, AL, USA","Proceedings of the ACM 4th annual workshop on Evaluation and usability of programming languages and tools","20160129","2012","","","17","24","<p>Peer code review is an effective method to reduce the number of defects and maintain source code integrity. Peer re-views in most of the Open Source Software (OSS) communities are conducted via mailing list, which are difficult to manage at times. Code review tools aim to ease the review process and keep track of the review requests. In this paper, we describe preliminary results of our study to evaluate code review process using a popular open source code review tool (ReviewBoard) in OSS communities. Some of our study findings are similar to the findings of previous studies on code reviews. In the projects under our study, we found that, most of the revisions are not submitted for peer review. More than 80% of the review requests are responded by two or less number of reviewers. Top committers of the projects are also top contributors of code reviews. Most of the review requests get prompt feedback within a day; however, some requests might wait for feed-back for a long time. Most importantly, we have identified some interesting directions for future research.</p>","","","10.1145/2414721.2414726","","","code review;mining repositories;open source software;peer review;review board","","","","","1","","","","","","21-21 Oct. 2012","","ACM","ACM Conferences"
"Modern code reviews in open-source projects: which problems do they fix?","M. Beller; A. Bacchelli; A. Zaidman; E. Juergens","Delft University of Technology, Netherlands","Proceedings of the 11th Working Conference on Mining Software Repositories","20160129","2014","","","202","211","<p> Code review is the manual assessment of source code by humans, mainly intended to identify defects and quality problems. Modern Code Review (MCR), a lightweight variant of the code inspections investigated since the 1970s, prevails today both in industry and open-source software (OSS) systems. The objective of this paper is to increase our understanding of the practical benefits that the MCR process produces on reviewed source code. To that end, we empirically explore the problems fixed through MCR in OSS systems. We manually classified over 1,400 changes taking place in reviewed code from two OSS projects into a validated categorization scheme. Surprisingly, results show that the types of changes due to the MCR process in OSS are strikingly similar to those in the industry and academic systems from literature, featuring the similar 75:25 ratio of maintainability-related to functional problems. We also reveal that 7‚Äì35% of review comments are discarded and that 10‚Äì22% of the changes are not triggered by an explicit review comment. Patterns emerged in the review data; we investigated them revealing the technical factors that influence the number of changes due to the MCR process. We found that bug-fixing tasks lead to fewer changes and tasks with more altered files and a higher code churn have more changes. Contrary to intuition, the person of the reviewer had no impact on the number of changes. </p>","","","10.1145/2597073.2597082","","","Code Review;Defects;Open Source Software","","","","","24","","","","","","May 31 2014-June 1 2014","","ACM","ACM Conferences"
"Expectations, outcomes, and challenges of modern code review","A. Bacchelli; C. Bird","University of Lugano, Switzerland","Proceedings of the 2013 International Conference on Software Engineering","20160129","2013","","","712","721","<p> Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more lightweight than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classified hundreds of review comments across diverse teams at Microsoft. Our study reveals that while finding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional benefits such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we find that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers. </p>","","","","","","","","","","","88","","","","","","18-26 May 2013","","ACM","ACM Conferences"
"Teaching operating systems using code review","C. Dall; J. Nieh","Columbia University, New York, NY, USA","Proceedings of the 45th ACM technical symposium on Computer science education","20160129","2014","","","549","554","<p>Learning about operating systems often involves modifying a large and complex code base. Grading student projects can be difficult and time consuming, yet students often do not learn from their programming errors and struggle to understand core operating system concepts. We present GradeBoard, a code review system designed to simplify grading for instructors and enable students to understand and learn from their errors. GradeBoard provides an easy-to-use Web interface that allows instructors to annotate student code submissions with grading comments and scores, and students to discuss the comments and scores with instructors. GradeBoard presents student code changes with syntax highlighting and lets users collapse or expand code sections to provide a desired level of context, making it easier to read and understand student programming project submissions. Comments and scores are easily identifiable by visual cues, improving interaction between instructors and students. We have deployed and used GradeBoard in a large operating systems course involving Linux kernel programming projects. GradeBoard provided robust, easy-to-use functionality for reviewing Linux kernel code changes, improved the instructional staff grading experience, and over 90% of students surveyed indicated that GradeBoard improved their understanding of the kernel programming projects better than other alternatives.</p>","","","10.1145/2538862.2538894","","","code review;instructional tools;operating systems","","","","","","","","","","","5-8 March 2014","","ACM","ACM Conferences"
"Towards refactoring-aware code review","X. Ge; S. Sarkar; E. Murphy-Hill","North Carolina State University, USA","Proceedings of the 7th International Workshop on Cooperative and Human Aspects of Software Engineering","20160129","2014","","","99","102","<p> Software developers review changes to a code base to prevent new bugs from being introduced. However, some parts of a change are more likely to introduce bugs than others, and thus deserve more care in reviewing. In this short paper, we discuss our ongoing work to build a reviewing tool that automatically determines which changes in a change set are refactorings, uses this information to help the developer distinguish between refactoring and non-refactoring changes, and ultimately reduces the time it takes developers to review code accurately. We also discuss the challenges and opportunities we have faced when building this refactoring-aware code review tool. </p>","","","10.1145/2593702.2593706","","","Code Review;Refactoring;Refactoring Detection","","","","","1","","","","","","2-3 June 2014","","ACM","ACM Conferences"
"Talking about code: Integrating pedagogical code reviews into early computing courses","","","ACM Transactions on Computing Education (TOCE)","20160129","2013","13","3","1","28","<p>Given the increasing importance of soft skills in the computing profession, there is good reason to provide students with more opportunities to learn and practice those skills in undergraduate computing courses. Toward that end, we have developed an active learning approach for computing education called the <i>Pedagogical Code Review</i> (PCR). Inspired by the <i>code inspection</i> process used in the software industry, a PCR is a collaborative activity in which a small team of students, led by a trained moderator: (a) walk through segments of each other's programming solutions, (b) check the code against a list of best coding practices, and (c) discuss and log issues that arise. To evaluate the viability and effectiveness of this approach, we conducted a series of four mixed-method empirical studies of various implementations of PCRs in CS1 courses at Washington State University. The first study validated the viability of the PCR activity. Using a quasi-experimental design, the final three studies evaluated two alternative implementations of PCRs‚Äî<i>face-to-face</i> and <i>online</i>. Our results provide evidence that PCRs can promote positive attitudinal shifts, and hone skills in critical review, teamwork, and communication. Based on our findings, we present a set of best practices for implementing PCRs.</p>","","","10.1145/2499947.2499951","","","Studio-based learning and instruction;computer-supported collaborative learning;pedagogical code reviews","","","","","2","","","","","","August 2013","","ACM","ACM Journals & Magazines"
"Improving code review effectiveness through reviewer recommendations","P. Thongtanunam; R. G. Kula; A. E. C. Cruz; N. Yoshida; H. Iida","NAIST, Japan","Proceedings of the 7th International Workshop on Cooperative and Human Aspects of Software Engineering","20160129","2014","","","119","122","<p> Effectively performing code review increases the quality of software and reduces occurrence of defects. However, this requires reviewers with experiences and deep understandings of system code. Manual selection of such reviewers can be a costly and time-consuming task. To reduce this cost, we propose a reviewer recommendation algorithm determining file path similarity called FPS algorithm. Using three OSS projects as case studies, FPS algorithm was accurate up to 77.97%, which significantly outperformed the previous approach. </p>","","","10.1145/2593702.2593705","","","Open Source Software;Peer Code Review;Recommendation System;Software Quality","","","","","6","","","","","","2-3 June 2014","","ACM","ACM Conferences"
"Measuring increased engagement using tablet PCs in a code review class","W. Fagen; S. Kamin","Univ. of Illinois, Urbana, IL, USA","Proceeding of the 44th ACM technical symposium on Computer science education","20160129","2013","","","465","470","<p>The Programming Studio in the University of Illinois Computer Science department is a required course in which small groups of students participate in weekly code reviews of each other's programs. To increase student engagement in the discussions, Tablet PCs were introduced for several weeks in the middle of the semester. By recording the discussions before, during, and after the use of tablets, we measure the effectiveness of this intervention. In doing so, we develop a simple metric to measure the ""active engagement"" of the participants. We found each section was significantly more engaging when using Tablet PCs (p<0.0001) and the large majority of individual participants were more engaged. This paper contributes both an objective measurement of ""active engagement"" and a successful intervention in a programming studio-type course.</p>","","","10.1145/2445196.2445338","","","audio analysis;education;software engineering;tablet computers","","","","","","","","","","","6-9 March 2013","","ACM","ACM Conferences"
"Exploring the usability and effectiveness of interactive annotation and code review for the detection of security vulnerabilities","T. Thomas","Department of Software and Information Systems, University of North Carolina at Charlotte, 28223, USA","2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","20151217","2015","","","295","296","According to a recent IBM study, the average cost for a stolen record raised 9% to $145 in 2014. Since millions of credit card records are stolen every year, the cost can easily run into billions of dollars. Consequently, application security is a very important concern during the development of applications today. Resolving security problems later in the development process is very time consuming and expensive. Therefore, it is favorable to detect and resolve security vulnerabilities as soon as possible during the development process. By using a technique called static analysis, it is possible to partially overcome this problem. Static analysis tools examine source code statically (when not running), and attempt to detect security vulnerabilities. Unfortunately, however, static analysis tools generate very large amounts of false positives. In order for static analysis tools to be effective, extraordinarily complex custom rules must be written for the tool. This must be done by a security expert for every application the tool runs on. To make matters worse, communicating information about complex vulnerabilities to application developers presents a unique challenge in and of itself. If the developer does not understand why a certain line is flagged as potentially vulnerable and is not provided with detailed information, it will be far more difficult for him or her to resolve the problem. Consequently, static analysis tools are seldom used.","","Electronic:978-1-4673-7457-6; POD:978-1-4673-7458-3","10.1109/VLHCC.2015.7357234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357234","","","feature extraction;program diagnostics;security of data;software reviews;source code (software)","interactive annotation;security vulnerability detection;source code review;static analysis","","","","10","","","","18-22 Oct. 2015","","IEEE","IEEE Conferences"
"Investigating code review quality: Do people and participation matter?","O. Kononenko; O. Baysal; L. Guerrouj; Y. Cao; M. W. Godfrey","David R. Cheriton School of Computer Science, University of Waterloo, Canada","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20151123","2015","","","111","120","Code review is an essential element of any mature software development project; it aims at evaluating code contributions submitted by developers. In principle, code review should improve the quality of code changes (patches) before they are committed to the project's master repository. In practice, bugs are sometimes unwittingly introduced during this process. In this paper, we report on an empirical study investigating code review quality for Mozilla, a large open-source project. We explore the relationships between the reviewers' code inspections and a set of factors, both personal and social in nature, that might affect the quality of such inspections. We applied the SZZ algorithm to detect bug-inducing changes that were then linked to the code review information extracted from the issue tracking system. We found that 54% of the reviewed changes introduced bugs in the code. Our findings also showed that both personal metrics, such as reviewer workload and experience, and participation metrics, such as the number of involved developers, are associated with the quality of the code review process.","","Electronic:978-1-4673-7532-0; USB:978-1-4673-7531-3","10.1109/ICSM.2015.7332457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332457","Code review;Mozilla;bug-inducing changes;code review quality;empirical study;mining software repositories","Androids;Computer bugs;Control systems;Data mining;Electronic mail;Humanoid robots;Software","program debugging;project management;public domain software;software development management;software metrics;software quality;software reviews","Mozilla;SZZ algorithm;bug-inducing change detection;code change quality;code review process;code review quality;open-source project;personal metrics;project master repository;reviewer code inspections;reviewer workload;software development project","","7","","43","","","","Sept. 29 2015-Oct. 1 2015","","IEEE","IEEE Conferences"
"Four eyes are better than two: On the impact of code reviews on software quality","G. Bavota; B. Russo","Faculty of Computer Science, Free University of Bozen-Bolzano, Italy","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20151123","2015","","","81","90","Code review is advocated as one of the best practices to improve software quality and reduce the likelihood of introducing defects during code change activities. Recent research has shown how code components having a high review coverage (i.e., a high proportion of reviewed changes) tend to be less involved in post-release fixing activities. Yet the relationship between code review and bug introduction or the overall software quality is still largely unexplored.","","Electronic:978-1-4673-7532-0; USB:978-1-4673-7531-3","10.1109/ICSM.2015.7332454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332454","Code Review;Empirical Studies;Mining Software Repositories","Androids;Computer bugs;Couplings;Data mining;History;Humanoid robots;Software quality","software quality;source code (software)","bug introduction;code change activities;code components;code review;postrelease fixing activities;review coverage;software quality","","4","","34","","","","Sept. 29 2015-Oct. 1 2015","","IEEE","IEEE Conferences"
"Automatically Recommending Peer Reviewers in Modern Code Review","M. B. Zanjani; H. Kagdi; C. Bird","Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, Kansas","IEEE Transactions on Software Engineering","20160610","2016","42","6","530","543","Code review is an important part of the software development process. Recently, many open source projects have begun practicing code review through ‚Äúmodern‚Äù tools such as GitHub pull-requests and Gerrit. Many commercial software companies use similar tools for code review internally. These tools enable the owner of a source code change to request individuals to participate in the review, i.e., reviewers. However, this task comes with a challenge. Prior work has shown that the benefits of code review are dependent upon the expertise of the reviewers involved. Thus, a common problem faced by authors of source code changes is that of identifying the best reviewers for their source code change. To address this problem, we present an approach, namely cHRev, to automatically recommend reviewers who are best suited to participate in a given review, based on their historical contributions as demonstrated in their prior reviews. We evaluate the effectiveness of cHRev on three open source systems as well as a commercial codebase at Microsoft and compare it to the state of the art in reviewer recommendation. We show that by leveraging the specific information in previously completed reviews (i.e.,quantification of review comments and their recency), we are able to improve dramatically on the performance of prior approaches, which (limitedly) operate on generic review information (i.e., reviewers of similar source code file and path names) or source coderepository data. We also present the insights into why our approach cHRev outperforms the existing approaches.","0098-5589;00985589","","10.1109/TSE.2015.2500238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328331","Gerrit;Modern code review;code change;reviewer recommendation","Androids;Birds;Electronic mail;History;Humanoid robots;Inspection;Software","software engineering;software reviews","Gerrit tool;GitHub pull-requests tool;cHRev approach;code review;commercial codebase;open source projects;peer reviewer recommendation;software development process;source code change","","3","","55","","","20151112","June 1 2016","","IEEE","IEEE Journals & Magazines"
"Code Reviews Do Not Find Bugs. How the Current Code Review Best Practice Slows Us Down","J. Czerwonka; M. Greiler; J. Tilford","Microsoft Corp., Redmond, WA, USA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","2","","27","28","Because of its many uses and benefits, code reviews are a standard part of the modern software engineering workflow. Since they require involvement of people, code reviewing is often the longest part of the code integration activities. Using experience gained at Microsoft and with support of data, we posit (1) that code reviews often do not find functionality issues that should block a code submission; (2) that effective code reviews should be performed by people with specific set of skills; and (3) that the social aspect of code reviews cannot be ignored. We find that we need to be more sophisticated with our guidelines for the code review workflow. We show how our findings from code reviewing practice influence our code review tools at Microsoft. Finally, we assert that, due to its costs, code reviewing practice is a topic deserving to be better understood, systematized and applied to software engineering workflow with more precision than the best practice currently prescribes.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202946","Software engineering workflow;code integration;code reviews","Best practices;Guidelines;Inspection;Software;Software engineering;Standards;Switches","program compilers;program diagnostics;software engineering","Microsoft;bug finding;code integration;code review best practice;code review tools;code review workflow;code submission;software engineering workflow","","4","","6","","","","16-24 May 2015","","IEEE","IEEE Conferences"
"In Support of Peer Code Review and Inspection in an Undergraduate Software Engineering Course","S. Sripada; Y. R. Reddy; A. Sureka","Int. Inst. of Inf. Technol., Hyderabad, India","2015 IEEE 28th Conference on Software Engineering Education and Training","20150817","2015","","","3","6","Peer code review and inspection is a quality improvement software engineering activity consisting of systematic examination of source code. While peer code review is commonly used in industrial and open-source software projects, it is seldom taught or practiced in undergraduate level Software Engineering courses. We conduct a study on the use of peer code reviewin a sophomore level introductory Software Engineering course consisting of more than 200 students and present our experiences, findings and challenges. We use Bitbucket's (a free code distributed version control system hosting site for Git and Mercurial) in-built code-review system and web-based hosting service. We extract the peer code review comments using Bitbucket API for detecting coding standard or compliance violation and identification of defects (number and type) by reviewers. We also conduct a survey on the benefit of peer code review activity on peer cohesion and communication. Our experiments and survey reveal that employing peer code review in an undergraduate class has several learning benefits such as improvement in coding skills, program comprehension abilities, knowledge of coding standard, and compliance and peer communication.","1093-0175;10930175","Electronic:978-1-4673-6701-1; POD:978-1-4673-6702-8","10.1109/CSEET.2015.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203843","Code Reviews;Defects;Inspections;Static code Analysis","Computer bugs;Encoding;Inspection;Java;Software;Software engineering;Standards","","","","1","","8","","","","18-19 May 2015","","IEEE","IEEE Conferences"
"Interactive Code Review for Systematic Changes","T. Zhang; M. Song; J. Pinedo; M. Kim","Univ. of California, Los Angeles, Los Angeles, CA, USA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","1","","111","122","Developers often inspect a diff patch during peer code reviews. Diff patches show low-level program differences per file without summarizing systematic changes -- similar, related changes to multiple contexts. We present Critics, an interactive approach for inspecting systematic changes. When a developer specifies code change within a diff patch, Critics allows developers to customize the change template by iteratively generalizing change content and context. By matching a generalized template against the codebase, it summarizes similar changes and detects potential mistakes. We evaluated Critics using two methods. First, we conducted a user study at Salesforce.com, where professional engineers used Critics to investigate diff patches authored by their own team. After using Critics, all six participants indicated that they would like Critics to be integrated into their current code review environment. This also attests to the fact that Critics scales to an industry-scale project and can be easily adopted by professional engineers. Second, we conducted a user study where twelve participants reviewed diff patches using Critics and Eclipse diff. The results show that human subjects using Critics answer questions about systematic changes 47.3% more correctly with 31.9% saving in time during code review tasks, in comparison to the baseline use of Eclipse diff. These results show that Critics should improve developer productivity in inspecting systematic changes during peer code reviews.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194566","","Context;Data mining;Interviews;Software;Switches;Syntactics;Systematics","software engineering;software reviews;source code (software)","Critics;interactive code review;peer code review;software development;systematic change","","13","","44","","","","16-24 May 2015","","IEEE","IEEE Conferences"
"Helping Developers Help Themselves: Automatic Decomposition of Code Review Changesets","M. Barnett; C. Bird; J. Brunet; S. K. Lahiri","Microsoft Res., Redmond, WA, USA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","1","","134","144","Code Reviews, an important and popular mechanism for quality assurance, are often performed on a change set, a set of modified files that are meant to be committed to a source repository as an atomic action. Understanding a code review is more difficult when the change set consists of multiple, independent, code differences. We introduce CLUSTERCHANGES, an automatic technique for decomposing change sets and evaluate its effectiveness through both a quantitative analysis and a qualitative user study.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194568","","Computer bugs;Histograms;Manuals;Prototypes;Quality assurance;Software;Standards","software quality;software reviews;source code (software)","CLUSTERCHANGES;automatic decomposition technique;code review changeset;quality assurance;software development","","17","","28","","","","16-24 May 2015","","IEEE","IEEE Conferences"
"Characteristics of Useful Code Reviews: An Empirical Study at Microsoft","A. Bosu; M. Greiler; C. Bird","Dept. of Comput. Sci., Univ. of Alabama, Tuscaloosa, AL, USA","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","146","156","Over the past decade, both open source and commercial software projects have adopted contemporary peer code review practices as a quality control mechanism. Prior research has shown that developers spend a large amount of time and effort performing code reviews. Therefore, identifying factors that lead to useful code reviews can benefit projects by increasing code review effectiveness and quality. In a three-stage mixed research study, we qualitatively investigated what aspects of code reviews make them useful to developers, used our findings to build and verify a classification model that can distinguish between useful and not useful code review feedback, and finally we used this classifier to classify review comments enabling us to empirically investigate factors that lead to more effective code review feedback. In total, we analyzed 1.5 millions review comments from five Microsoft projects and uncovered many factors that affect the usefulness of review feedback. For example, we found that the proportion of useful comments made by a reviewer increases dramatically in the first year that he or she is at Microsoft but tends to plateau afterwards. In contrast, we found that the more files that are in a change, the lower the proportion of comments in the code review that will be of value to the author of the change. Based on our findings, we provide recommendations for practitioners to improve effectiveness of code reviews.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180075","code review;empirical;microsoft;recommendation","Data mining;Electronic mail;Interviews;Manuals;Reliability;Software","project management;public domain software;quality control;software development management;software quality","Microsoft projects;code quality;code review feedback;commercial software projects;contemporary peer code review practices;open source;quality control mechanism","","11","","36","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Investigating Code Review Practices in Defective Files: An Empirical Study of the Qt System","P. Thongtanunam; S. McIntosh; A. E. Hassan; H. Iida","Nara Inst. of Sci. & Technol., Nara, Japan","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","168","179","Software code review is a well-established software quality practice. Recently, Modern Code Review (MCR) has been widely adopted in both open source and proprietary projects. To evaluate the impact that characteristics of MCR practices have on software quality, this paper comparatively studies MCR practices in defective and clean source code files. We investigate defective files along two perspectives: 1) files that will eventually have defects (i.e., Future-defective files) and 2) files that have historically been defective (i.e., Risky files). Through an empirical study of 11,736 reviews of changes to 24,486 files from the Qt open source project, we find that both future-defective files and risky files tend to be reviewed less rigorously than their clean counterparts. We also find that the concerns addressed during the code reviews of both defective and clean files tend to enhance evolvability, i.e., Ease future maintenance (like documentation), rather than focus on functional issues (like incorrect program logic). Our findings suggest that although functionality concerns are rarely addressed during code review, the rigor of the reviewing process that is applied to a source code file throughout a development cycle shares a link with its defect proneness.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180077","Code Review;Software Quality","Data mining;Inspection;Maintenance engineering;Measurement;Software quality;Testing","program diagnostics;software development management;software maintenance;software quality","MCR;Qt system;code review practices;defect proneness;defective files;development cycle;functional issues;future maintenance;future-defective files;modern code review;risky files;software code review;software quality practice;source code file","","9","","40","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Partitioning Composite Code Changes to Facilitate Code Review","Y. Tao; S. Kim","Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Hong Kong, China","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","180","190","Developers expend significant effort on reviewing source code changes. Hence, the comprehensibility of code changes directly affects development productivity. Our prior study has suggested that composite code changes, which mix multiple development issues together, are typically difficult to review. Unfortunately, our manual inspection of 453 open source code changes reveals a non-trivial occurrence (up to 29%) of such composite changes. In this paper, we propose a heuristic-based approach to automatically partition composite changes, such that each sub-change in the partition is more cohesive and self-contained. Our quantitative and qualitative evaluation results are promising in demonstrating the potential benefits of our approach for facilitating code review of composite code changes.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180078","","Cloning;Force;Inspection;Libraries;Manuals;Pattern matching;Software","public domain software;software maintenance;source code (software)","code review;open source code;partitioning composite code changes;source code changes","","4","","35","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Lessons Learned from Building and Deploying a Code Review Analytics Platform","C. Bird; T. Carnahan; M. Greiler","Microsoft, Redmond, WA, USA","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","191","201","Tool-based code review is growing in popularity and has become a standard part of the development process at Mi-crosoft. Adoption of these tools makes it possible to mine data from code reviews and provide access to it. In this paper, we pre-sent an experience report for CodeFlow Analytics, a system that collects code review data, generates metrics from this data, and provides a number of ways for development teams to access the metrics and data. We discuss the design, design decisions and chal-lenges that we encountered when building CodeFlow Analytics. We contacted teams that used CodeFlow Analytics over the past two years and discuss what prompted them to use CodeFlow Ana-lytics, how they have used it, and what the impact has been. Fur-ther, we survey research that has been enabled by using the Code-Flow Analytics platform. We provide a series of lessons learned from this experience to help others embarking on a task of building an analytics platform in an enterprise setting.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180079","","Computational modeling;Data mining;Databases;Interviews;Measurement;Servers;Software","data mining;program diagnostics","CodeFlow analytics;Microsoft;analytics platform;code review analytics platform;data mining;design decisions;enterprise setting;tool-based code review","","4","","29","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Gamifying Software Engineering Tasks Based on Cognitive Principles: The Case of Code Review","N. Unkelos-Shpigel; I. Hadar","Dept. of Inf. Syst., Univ. of Haifa, Haifa, Israel","2015 IEEE/ACM 8th International Workshop on Cooperative and Human Aspects of Software Engineering","20150727","2015","","","119","120","Code review is an important task in software development. However, performing code review is perceived, for the most part, as an undesired task, presenting several challenges to the required collaboration and knowledge transfer between programmers and reviewers. In order to overcome these challenges and improve the effectiveness of code review, we developed SCRUT: Social Code Review Unifying Tool. By recruiting relevant cognitive theories and implementing gamification elements to motivate collaboration and knowledge sharing between programmers and reviewers, we plan to enhance the task of code review. This paper presents our vision for enhancing software engineering via gamification, and the theoretical cognitive foundation on which this vision is based, starting with the example of code review.","","Electronic:978-1-4673-7031-8; POD:978-1-4673-7032-5","10.1109/CHASE.2015.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166104","Gamification;boundary object;code review","Collaboration;Conferences;Context;Games;Software;Software engineering","program compilers;software engineering","Gamifying software engineering tasks;SCRUT;code review case;cognitive principles;knowledge sharing;knowledge transfer;social code review unifying tool;software development","","3","","7","","","","18-18 May 2015","","IEEE","IEEE Conferences"
"Who should review my code? A file location-based code-reviewer recommendation approach for Modern Code Review","P. Thongtanunam; C. Tantithamthavorn; R. G. Kula; N. Yoshida; H. Iida; K. i. Matsumoto","Nara Institute of Science and Technology, Japan","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20150409","2015","","","141","150","Software code review is an inspection of a code change by an independent third-party developer in order to identify and fix defects before an integration. Effectively performing code review can improve the overall software quality. In recent years, Modern Code Review (MCR), a lightweight and tool-based code inspection, has been widely adopted in both proprietary and open-source software systems. Finding appropriate code-reviewers in MCR is a necessary step of reviewing a code change. However, little research is known the difficulty of finding code-reviewers in a distributed software development and its impact on reviewing time. In this paper, we investigate the impact of reviews with code-reviewer assignment problem has on reviewing time. We find that reviews with code-reviewer assignment problem take 12 days longer to approve a code change. To help developers find appropriate code-reviewers, we propose RevFinder, a file location-based code-reviewer recommendation approach. We leverage a similarity of previously reviewed file path to recommend an appropriate code-reviewer. The intuition is that files that are located in similar file paths would be managed and reviewed by similar experienced code-reviewers. Through an empirical evaluation on a case study of 42,045 reviews of Android Open Source Project (AOSP), OpenStack, Qt and LibreOffice projects, we find that RevFinder accurately recommended 79% of reviews with a top 10 recommendation. RevFinder also correctly recommended the code-reviewers with a median rank of 4. The overall ranking of RevFinder is 3 times better than that of a baseline approach. We believe that RevFinder could be applied to MCR in order to help developers find appropriate code-reviewers and speed up the overall code review process.","1534-5351;15345351","Electronic:978-1-4799-8469-5; POD:978-1-4799-8470-1","10.1109/SANER.2015.7081824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081824","Code-Reviewer Recommendation;Distributed Software Development;Modern Code Review","Androids;History;Humanoid robots;Manuals;Open source software;Software systems","public domain software;software reviews","AOSP;Android open source project;LibreOffice project;MCR;OpenStack;Qt project;RevFinder;code-reviewer recommendation approach;file location;modern code review;open-source software system;proprietary software system;software code review","","6","","41","","","","2-6 March 2015","","IEEE","IEEE Conferences"
"Do code review practices impact design quality? A case study of the Qt, VTK, and ITK projects","R. Morales; S. McIntosh; F. Khomh","SWAT, Polytechnique Montr&#x00E9;al, Canada","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20150409","2015","","","171","180","Code review is the process of having other team members examine changes to a software system in order to evaluate its technical content and quality. A lightweight variant of this practice, often referred to as Modern Code Review (MCR), is widely adopted by software organizations today. Previous studies have established a relation between the practice of code review and the occurrence of post-release bugs. While the prior work studies the impact of code review practices on software release quality, it is still unclear what impact code review practices have on software design quality. Therefore, using the occurrence of 7 different types of anti-patterns (i.e., poor solutions to design and implementation problems) as a proxy for software design quality, we set out to investigate the relationship between code review practices and software design quality. Through a case study of the Qt, VTK and ITK open source projects, we find that software components with low review coverage or low review participation are often more prone to the occurrence of anti-patterns than those components with more active code review practices.","1534-5351;15345351","Electronic:978-1-4799-8469-5; POD:978-1-4799-8470-1","10.1109/SANER.2015.7081827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081827","","Analytical models;Complexity theory;Correlation;Measurement;Software design;Software systems","object-oriented programming;software quality","ITK project;Qt project;VTK project;code review practice;modern code review;open source project;post-release bugs;software components;software design quality;software release quality;software system","","16","","42","","","","2-6 March 2015","","IEEE","IEEE Conferences"
"Code review: Veni, ViDI, vici","Y. Tymchuk; A. Mocci; M. Lanza","REVEAL @ Faculty of Informatics - University of Lugano, Switzerland","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20150409","2015","","","151","160","Modern software development sees code review as a crucial part of the process, because not only does it facilitate the sharing of knowledge about the system at hand, but it may also lead to the early detection of defects, ultimately improving the quality of the produced software. Although supported by numerous approaches and tools, code review is still in its infancy, and indeed researchers have pointed out a number of shortcomings in the state of the art. We present a critical analysis of the state of the art of code review tools and techniques, extracting a set of desired features that code review tools should possess. We then present our vision and initial implementation of a novel code review approach named Visual Design Inspection (ViDI), illustrated through a set of usage scenarios. ViDI is based on a combination of visualization techniques, design heuristics, and static code analysis techniques.","1534-5351;15345351","Electronic:978-1-4799-8469-5; POD:978-1-4799-8470-1","10.1109/SANER.2015.7081825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081825","","Birds;Feature extraction;Inspection;Navigation;Software quality;Visualization","program diagnostics;program visualisation;software quality","ViDI;code review techniques;code review tools;defect detection;design heuristics;knowledge sharing;software development;software quality;static code analysis technique;visual design inspection;visualization techniques","","1","","24","","","","2-6 March 2015","","IEEE","IEEE Conferences"
"Would static analysis tools help developers with code reviews?","S. Panichella; V. Arnaoudova; M. Di Penta; G. Antoniol","University of Zurich, Department of Informatics, Switzerland","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20150409","2015","","","161","170","Code reviews have been conducted since decades in software projects, with the aim of improving code quality from many different points of view. During code reviews, developers are supported by checklists, coding standards and, possibly, by various kinds of static analysis tools. This paper investigates whether warnings highlighted by static analysis tools are taken care of during code reviews and, whether there are kinds of warnings that tend to be removed more than others. Results of a study conducted by mining the Gerrit repository of six Java open source projects indicate that the density of warnings only slightly vary after each review. The overall percentage of warnings removed during reviews is slightly higher than what previous studies found for the overall project evolution history. However, when looking (quantitatively and qualitatively) at specific categories of warnings, we found that during code reviews developers focus on certain kinds of problems. For such categories of warnings the removal percentage tend to be very high, often above 50% and sometimes up to 100%. Examples of those are warnings in the imports, regular expressions, and type resolution categories. In conclusion, while a broad warning detection might produce way too many false positives, enforcing the removal of certain warnings prior to the patch submission could reduce the amount of effort provided during the code review process.","1534-5351;15345351","Electronic:978-1-4799-8469-5; POD:978-1-4799-8470-1","10.1109/SANER.2015.7081826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081826","Code Review;Empirical Study;Mining Software Repositories;Static Analysis","Context;Data mining;Encoding;History;Java;Software;Standards","Java;project management;software management;software quality;software tools","Gerrit repository;Java open source projects;broad warning detection;code quality;code review process;code reviews developers;coding standards;patch submission;project evolution history;software projects;static analysis tools","","10","","30","","","","2-6 March 2015","","IEEE","IEEE Conferences"
"Mining Peer Code Review System for Computing Effort and Contribution Metrics for Patch Reviewers","R. Mishra; A. Sureka","Indraprastha Inst. of Inf. Technol., New Delhi, India","2014 IEEE 4th Workshop on Mining Unstructured Data","20141211","2014","","","11","15","Peer code review is a software quality assurance activity followed in several open-source and closed-source software projects. Rietveld and Gerrit are the most popular peer code review systems used by open-source software projects. Despite the popularity and usefulness of these systems, they do not record or maintain the cost and effort information for a submitted patch review activity. Currently there are no formal or standard metrics available to measure effort and contribution of a patch review activity. We hypothesize that the size and complexity of modified files and patches are significant indicators of effort and contribution of patch reviewers in a patch review process. We propose a metric for computing the effort and contribution of a patch reviewer based on modified file size, patch size and program complexity variables. We conduct a survey of developers involved in peer code review activity to test our hypothesis of causal relationship between proposed indicators and effort. We employ the proposed model and conduct an empirical analysis using the proposed metrics on open-source Google Android project.","","Electronic:978-1-4799-6793-3; POD:978-1-4799-6794-0","10.1109/MUD.2014.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980189","Contribution and Performance Assessment;Empirical Software Engineering and Measurements;Mining Software Repositories;Peer Code Review;Software Maintenance","Complexity theory;Data mining;Google;Size measurement;Smart phones;Software","data mining;public domain software;software metrics;software quality;software reviews;source code (software)","causal relationship;closed-source software projects;computing effort;contribution metrics;modified file size;open-source Google Android project;open-source software projects;patch review activity;patch review process;patch size;peer code review activity;peer code review system mining;program complexity variables;software quality assurance activity","","2","","9","","","","30-30 Sept. 2014","","IEEE","IEEE Conferences"
"ReDA: A Web-Based Visualization Tool for Analyzing Modern Code Review Dataset","P. Thongtanunam; X. Yang; N. Yoshida; R. G. Kula; A. E. C. Cruz; K. Fujiwara; H. Iida","Nara Inst. of Sci. & Technol., Ikoma, Japan","2014 IEEE International Conference on Software Maintenance and Evolution","20141206","2014","","","605","608","ReDA (http://reda.naist.jp/) is a web-based visualization tool for analyzing Modern Code Review (MCR) datasets for large Open Source Software (OSS) projects. MCR is a commonly practiced and lightweight inspection of source code using a support tool such as Gerrit system. Recently, mining code review history of such systems has received attention as a potentially effective method of ensuring software quality. However, due to increasing size and complexity of softwares being developed, these datasets are becoming unmanageable. ReDA aims to assist researchers of mining code review data by enabling better understand of dataset context and identifying abnormalities. Through real-time data interaction, users can quickly gain insight into the data and hone in on interesting areas to investigate. A video highlighting the main features can be found at: http://youtu.be/ fEoTRRas0U.","1063-6773;10636773","Electronic:978-1-4799-6146-7; POD:978-1-4799-6147-4","10.1109/ICSME.2014.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976150","Mining software repository;Modern Code Review;Visualization tool","Androids;Data mining;Data visualization;Google;History;Software quality","Internet;data visualisation;software quality;software tools;source code (software)","Gerrit system;MCR datasets;OSS projects;ReDA;Web-based visualization tool;mining code review data;mining code review history;modern code review dataset;open source software;real-time data interaction;software quality;source code","","12","","13","","","","Sept. 29 2014-Oct. 3 2014","","IEEE","IEEE Conferences"
"0-1 Programming Model-Based Method for Planning Code Review Using Bug Fix History","H. Aman","Center for Inf. Technol., Ehime Univ., Matsuyama, Japan","2013 20th Asia-Pacific Software Engineering Conference (APSEC)","20140306","2013","2","","37","42","Code review is a powerful activity for software quality improvement, and is ideal to review all source files being developed. However, such an exhaustive review would be difficult because the available time and effort are in reality limited. Thus, practitioners prioritize the source files in terms of bug-proneness by using related data such as bug fix history, and review them in decreasing order of priority-such strategy in this paper is called the ""conventional method."" While the conventional method is straightforward, it focuses only on the bug-proneness and cannot consider the review cost properly, so the method does not produce a cost-effective review plan. To take into account both the bug-proneness and the review cost, this paper proposes a 0-1 programming model-based method for planning code review. The proposed method formulates a review plan as a 0-1 programming problem, and the solution is the recommendation list of source files to be reviewed. Moreover, the proposed method considers the type of file-if the file is newly-developed or not. Such difference in file type may affect on how to evaluate the bug-proneness and the review strategy: newly-developed files are notable but not appeared in the bug fix history. This paper conducts a case study using popular open source software, shows that the proposed method is up to 42% more effective than the conventional method in recommending buggy files as the review targets.","1530-1362;15301362","CD-ROM:978-1-4799-2143-0; Electronic:978-1-4799-2144-7; POD:978-1-4799-2145-4","10.1109/APSEC.2013.109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754348","0-1 programming model;bug fix history;code review;cost-effectiveness","Complexity theory;History;Mathematical model;Measurement;Planning;Programming;Software","integer programming;linear programming;program debugging;software cost estimation;software quality;source code (software)","0-1 programming model-based method;bug fix history;bug-proneness;cost-effective review plan;file type;open source software;planning code review;review cost;software quality improvement;source files","","0","","15","","","","2-5 Dec. 2013","","IEEE","IEEE Conferences"
"Impact of Peer Code Review on Peer Impression Formation: A Survey","A. Bosu; J. C. Carver","Dept. of Comput. Sci., Univ. of Alabama, Tuscaloosa, AL, USA","2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","20131212","2013","","","133","142","Peer code review has been adopted as an effective quality improvement practice by many Open Source Software (OSS) communities. In addition to increasing software quality, there is anecdotal evidence that peer code review has other benefits, including: sharing knowledge, sharing expertise, sharing development techniques, and most importantly building accurate peer impressions between the code review participants. To further investigate the presence of these benefits, we surveyed members of popular OSS communities who were involved with peer code review. We used established scales from Psychology, Information science, and Organizational Behavior to create survey questions. We also enforced multiple reliability and validity measures to ensure higher confidence in the survey results. In this paper, we present a subset of the surveys results focused on better understanding four aspects of peer impression formation: trust, reliability, perception of expertise, and friendship. The results indicate that there is indeed a high level of trust, reliability, perception of expertise, and friendship between OSS peers who have participated in code review for a period of time. Because code review involves examining someone else's code, unsurprisingly, peer code review helped most in building a perception of expertise between code review partners.","1949-3770;19493770","Electronic:978-0-7695-5056-5; POD:978-1-4799-1144-8","10.1109/ESEM.2013.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681346","Code Review;OSS;Open Source;Peer impressions;Survey;expertise;friendship;reliability;trust","Communities;Correlation;Databases;Psychology;Reliability engineering;Software","public domain software;software quality;software reliability;source code (software);trusted computing","OSS communities;anecdotal evidence;development technique sharing;expertise perception;expertise sharing;friendship;information science;knowledge sharing;oganizational behavior;open source software communities;peer code review;peer impression formation;psychology;quality improvement practice;reliability;software quality;trust","","6","","35","","","","10-11 Oct. 2013","","IEEE","IEEE Conferences"
"The influence of non-technical factors on code review","O. Baysal; O. Kononenko; R. Holmes; M. W. Godfrey","David R. Cheriton Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada","2013 20th Working Conference on Reverse Engineering (WCRE)","20131121","2013","","","122","131","When submitting a patch, the primary concerns of individual developers are ‚ÄúHow can I maximize the chances of my patch being approved, and minimize the time it takes for this to happen?‚Äù In principle, code review is a transparent process that aims to assess qualities of the patch by their technical merits and in a timely manner; however, in practice the execution of this process can be affected by a variety of factors, some of which are external to the technical content of the patch itself. In this paper, we describe an empirical study of the code review process for WebKit, a large, open source project; we replicate the impact of previously studied factors - such as patch size, priority, and component and extend these studies by investigating organizational (the company) and personal dimensions (reviewer load and activity, patch writer experience) on code review response time and outcome. Our approach uses a reverse engineered model of the patch submission process and extracts key information from the issue tracking and code review systems. Our findings suggest that these nontechnical factors can significantly impact code review outcomes.","1095-1350;10951350","Electronic:978-1-4799-2931-3; POD:978-1-4799-2932-0","10.1109/WCRE.2013.6671287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671287","Code review;WebKit;non-technical factors;open source software;personal and organizational aspects","Browsers;Companies;Databases;Electronic mail;Google;Time factors","program diagnostics;reverse engineering;software quality","WebKit;code review outcome;code review process;code review response time;code review system;component;issue tracking system;nontechnical factors;open source project;patch size;patch submission process;priority;reverse engineered model;software quality","","25","","14","","","","14-17 Oct. 2013","","IEEE","IEEE Conferences"
"Fix-it: An extensible code auto-fix component in Review Bot","V. Balachandran","VMware, Bangalore, India","2013 IEEE 13th International Working Conference on Source Code Analysis and Manipulation (SCAM)","20131028","2013","","","167","172","Coding standard violations, defect patterns and non-conformance to best practices are abundant in checked-in source code. This often leads to unmaintainable code and potential bugs in later stages of software life cycle. It is important to detect and correct these issues early in the development cycle, when it is less expensive to fix. Even though static analysis techniques such as tool-assisted code review are effective in addressing this problem, there is significant amount of human effort involved in identifying the source code issues and fixing it. Review Bot is a tool designed to reduce the human effort and improve the quality in code reviews by generating automatic reviews using static analysis output. In this paper, we propose an extension to Review Bot- addition of a component called Fix-it for the auto-correction of various source code issues using Abstract Syntax Tree (AST) transformations. Fix-it uses built-in fixes to automatically fix various issues reported by the auto-reviewer component in Review Bot, thereby reducing the human effort to greater extent. Fix-it is designed to be highly extensible-users can add support for the detection of new defect patterns using XPath or XQuery and provide fixes for it based on AST transformations written in a high-level programming language. It allows the user to treat the AST as a DOM tree and run XQuery UPDATE expressions to perform AST transformations as part of a fix. Fix-it also includes a designer application which enables Review Bot administrators to design new defect patterns and fixes. The developer feedback on a stand-alone prototype indicates the possibility of significant human effort reduction in code reviews using Fix-it.","","Electronic:978-1-4673-5739-5; POD:978-1-4673-5738-8","10.1109/SCAM.2013.6648198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648198","","Abstracts;Conferences;Encoding;Java;Standards;XML","high level languages;human factors;program debugging;program diagnostics;software quality;software reviews;software tools;source coding;tree data structures","AST transformations;DOM tree;Fix-it;Review Bot administrators;XPath;XQuery UPDATE expressions;abstract syntax tree transformations;auto-reviewer component;automatic review generation;checked-in source code;code review quality;coding standard violations;defect patterns;extensible code auto-fix component;high-level programming language;human effort;software life cycle;source code issues;static analysis output;static analysis techniques;tool-assisted code review;unmaintainable code","","4","","19","","","","22-23 Sept. 2013","","IEEE","IEEE Conferences"
"Gerrit software code review data from Android","M. Mukadam; C. Bird; P. C. Rigby","Concordia University Montreal, QC, Canada","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","45","48","Over the past decade, a number of tools and systems have been developed to manage various aspects of the software development lifecycle. Until now, tool supported code review, an important aspect of software development, has been largely ignored. With the advent of open source code review tools such as Gerrit along with projects that use them, code review data is now available for collection, analysis, and triangulation with other software development data. In this paper, we extract Android peer review data from Gerrit. We describe the Android peer review process, the reverse engineering of the Gerrit JSON API, our data mining and cleaning methodology, database schema, and provide an example of how the data can be used to answer an empirical software engineering question. The database is available for use by the research community.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624002","","Androids;Data mining;Databases;Humanoid robots;Servers;Software","application program interfaces;data mining;database management systems;operating system kernels;public domain software;reverse engineering;software development management;software tools","Android;Android peer review data extract;Android peer review process;Gerrit JSON API;Gerrit software code review data;data cleaning methodology;data mining methodology;database schema;open source code review tools;reverse engineering;software development data;software development lifecycle management;software engineering","","11","","7","","","","18-19 May 2013","","IEEE","IEEE Conferences"
"Who does what during a code review? Datasets of OSS peer review repositories","K. Hamasaki; R. G. Kula; N. Yoshida; A. E. C. Cruz; K. Fujiwara; H. Iida","NAIST, Japan","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","49","52","We present four datasets that are focused on the general roles of OSS peer review members. With data mined from both an integrated peer review system and code source repositories, our rich datasets comprise of peer review data that was automatically recorded. Using the Android project as a case study, we describe our extraction methodology, the datasets and their application used for three separate studies. Our datasets are available online at http://sdlab.naist.jp/reviewmining/.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624003","Open Source Software;Peer Review Repository Mining;Quality Assurance","Androids;Complexity theory;Data mining;Humanoid robots;Radiation detectors;Social network services;Software","data mining;public domain software;software quality","Android project;OSS peer review repositories;code review;code source repositories;data mining;extraction methodology;integrated peer review system;software quality assurance activity","","6","","6","","","","18-19 May 2013","","IEEE","IEEE Conferences"
"On the understanding of programs with continuous code reviews","M. Bernhart; T. Grechenig","Res. Group for Ind. Software (INSO), Vienna Univ. of Technol., Vienna, Austria","2013 21st International Conference on Program Comprehension (ICPC)","20130930","2013","","","192","198","Code reviews are a very effective, but effortful quality assurance technique. A major problem is to read and understand source-code that was produced by someone else. With different programming styles and complex interactions, understanding the code under review is the most expensive sub-task of a code review. As with many other modern software engineering practices, code reviews may be applied as a continuous process to reduce the effort and support the concept of collective ownership. This study evaluates the effect of a continuous code review process on the understandability and collective ownership of the code base. A group of 8 subjects performed a total of 114 code reviews within 18 months in an industrial context and conducted an expert evaluation according to this research question. This study concludes that there is a clear positive effect on the understandability and collective ownership of the code base with continuous code reviews, but also limiting factors and drawbacks for complex review tasks.","1092-8138;10928138","Electronic:978-1-4673-3092-3; POD:978-1-4673-3090-9","10.1109/ICPC.2013.6613847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613847","Code review;IEEE-1028;aviation;code quality;continuous;inspection;safety","Airports;Coherence;Context;Planning;Quality assurance;Software;Timing","programming;quality assurance;reverse engineering;software quality","collective ownership;complex interactions;continuous code reviews;program understanding;programming styles;quality assurance technique;software engineering practices;source-code understanding","","3","","14","","","","20-21 May 2013","","IEEE","IEEE Conferences"
"Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation","V. Balachandran","VMware, Bangalore, India","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","931","940","Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93% of all the automatically generated comments. There is only 14.71% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60%-92%, which is significantly better than a comparable method based on file change history.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606642","","Algorithm design and analysis;Encoding;History;Inspection;Java;Software;Standards","program diagnostics;program testing;software engineering","Review Bot tool;automatic static analysis;coding standard violation checking automation;comment message;cost-effective software defect detection technique;defect pattern;developer feedback;file change history;peer code review;quality improvement;reviewer recommendation;source code line change history;static analysis tools;tool assisted code review","","24","1","24","","","","18-26 May 2013","","IEEE","IEEE Conferences"
"Expectations, outcomes, and challenges of modern code review","A. Bacchelli; C. Bird","REVEAL @ Faculty of Informatics, University of Lugano, Switzerland","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","712","721","Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more ‚Äúlightweight‚Äù than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classified hundreds of review comments across diverse teams at Microsoft. Our study reveals that while finding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional benefits such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we find that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606617","","Context;Guidelines;Inspection;Interviews;Knowledge transfer;Software;Sorting","software development management;software quality","Microsoft;industrial contexts;knowledge transfer;open source;software engineering practice;team awareness;tool-based code reviews","","88","","40","","","","18-26 May 2013","","IEEE","IEEE Conferences"
"The Effect of Checklist in Code Review for Inexperienced Students: An Empirical Study","G. Rong; J. Li; M. Xie; T. Zheng","State Key Lab. for Novel Software Technol., Nanjing, China","2012 IEEE 25th Conference on Software Engineering Education and Training","20120723","2012","","","120","124","Code review is believed to be an effective technique to remove defects in early development stage and improve software quality. Therefore, it is regarded as one of the basic skills of qualified software engineers. Consequently, most curricula for SE students incorporated knowledge about code review in different courses. However, how to teach students to conduct efficient code review remains challenging. Many reports claimed that using checklist during code review could increase review efficiency (percentage of defects removed in code review). Nevertheless, we found a quite different result through analyzing the data collected from a PSP course took by freshmen. Results indicate that checklist contributes more to helping beginners conduct code review than to improving review efficiency. This finding implies that educators need to properly recognize the role of checklist in code review for students and explore more approaches to help students master skills to conduct efficient code reviews.","1093-0175;10930175","Electronic:978-0-7695-4693-3; POD:978-1-4673-1592-0","10.1109/CSEET.2012.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245021","","Conferences;Education;Sensitivity;Software;Software engineering;Testing","computer science education;educational courses;program debugging;software quality;teaching","checklist;code review;course;curricula;defect removal;early development stage;inexperienced students;software engineering student;software quality;student teaching","","1","","8","","","","17-19 April 2012","","IEEE","IEEE Conferences"
"An approach for collaborative code reviews using multi-touch technology","S. M√ºller; M. W√ºrsch; T. Fritz; H. C. Gall","s.e.a.l. - software architecture and evolution lab, Department of Informatics, University of Zurich, Switzerland","2012 5th International Workshop on Co-operative and Human Aspects of Software Engineering (CHASE)","20120625","2012","","","93","99","Code reviews are an effective mechanism to improve software quality, but often fall short in the development of software. To improve the desirability and ease of code reviews, we introduce an approach that explores how multi-touch interfaces can support code reviews and can make them more collaborative. Our approach provides users with features to collaboratively find and investigate code smells, annotate source code and generate review reports using gesture recognition and a Microsoft Surface Table. In a preliminary evaluation, subjects generally liked the prototypical implementation of our approach for performing code review tasks.","","Electronic:978-1-4673-1824-2; POD:978-1-4673-1823-5","10.1109/CHASE.2012.6223031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223031","Multi-touch;code review;code smell;collaboration;gesture;software metrics","Collaboration;Data visualization;Gesture recognition;Prototypes;Software metrics;Surgery;Visualization","gesture recognition;groupware;haptic interfaces;software quality;software reviews","Microsoft Surface Table;code smells;collaborative code reviews;gesture recognition;multitouch interfaces;multitouch technology;prototypical implementation;software development;software quality","","4","","16","","","","2-2 June 2012","","IEEE","IEEE Conferences"
"Analyzing Involvements of Reviewers through Mining a Code Review Repository","J. Liang; O. Mizuno","Grad. Sch. of Sci. & Technol., Kyoto Inst. of Technol., Kyoto, Japan","2011 Joint Conference of the 21st International Workshop on Software Measurement and the 6th International Conference on Software Process and Product Measurement","20111229","2011","","","126","132","In order to assure the quality of software, early detection of defects is highly recommended. Code review is one of effective way for such early detection of defects in software. Code review activities must contain various useful insights for software quality. However, especially in open source software developments, records of code review merely exist. In this study, we try to analyze a code review repository of an open source software, Chromium, which adopts a code review tool in its development.Before analyzing the code review data, we address 7 research questions. We can find interesting answers for these questions by repositories mining.","","Electronic:978-0-7695-4565-3; POD:978-1-4577-1930-1","10.1109/IWSM-MENSURA.2011.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113052","code review;quality;repository mining","Chromium;Control systems;Correlation;Crawlers;Data mining;Measurement;Software","data mining;public domain software;software quality","Chromium;code review repository mining;defects detection;open source software;software quality","","2","","17","","","","3-4 Nov. 2011","","IEEE","IEEE Conferences"
"A Task-Based Code Review Process and Tool to Comply with the DO-278/ED-109 Standard for Air Traffic Managment Software Development: An Industrial Case Study","M. Bernhart; S. Reiterer; K. Matt; A. Mauczka; T. Grechenig","Res. Group for Ind. Software (INSO), Vienna Univ. of Technol., Vienna, Austria","2011 IEEE 13th International Symposium on High-Assurance Systems Engineering","20111229","2011","","","182","187","Software reviews are one of the most efficient quality assurance techniques in software engineering. They are required for the enhancement of the software quality in early phases of the development process and often used in development of safety critical systems. In the field of software engineering for Air Traffic Management (ATM) the standard DO-278/ED-109 requires the rigorous application of code reviews and fully traceable reporting of the results. This case study presents a process and an IDE-integrated tool that complies with the requirements of the standard.","1530-2059;15302059","Electronic:978-0-7695-4615-5; POD:978-1-4673-0107-7","10.1109/HASE.2011.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113895","Air Traffic Management;DO-278;ED-109;code inspection;code review","Asynchronous transfer mode;Companies;Inspection;Planning;Programming;Servers;Software","air traffic;safety-critical software;software quality;software standards;task analysis","DO-278/ED-109 standard;air traffic management software development;quality assurance;safety critical systems;software engineering;software quality;software reviews;task-based code review process","","2","","12","","","","10-12 Nov. 2011","","IEEE","IEEE Conferences"
"The design of an online environment to support pedagogical code reviews","C. Hundhausen; A. Agrawal; K. Ryan","Washington State University, Pullman, WA, USA","Proceedings of the 41st ACM technical symposium on Computer science education","20110708","2010","","","182","186","<p>Inspired by the <i>formal code inspection</i> process commonly used in the software industry, we have been exploring the use of <i>pedagogical code</i> reviews (PCRs), in which a team of three to four students, led by a trained moderator, (a) walk through segments of each other's programming assignments, (b) check the code against a list of best coding practices, and (c) discuss and log issues that arise. We have found that PCRs not only improve the quality of students' code, but also positively impact students' communication and sense of community. However, implementing PCRs also presents a key logistical challenge: how to make code solutions and review results accessible to team members before, during, and after the team reviews? To address this challenge, we are developing an online environment specifically tailored to support PCRs. Our environment enables students to submit their code solutions; to review team members' code solutions on-line prior to PCRs; to carry out PCRs; and to resubmit their solutions based upon the results of the PCRs. In an empirical evaluation of the environment in a CS1 course, we found that it not only eased the logistics of implementing PCRs, but also improved the organization and efficiency of the PCR process.</p>","","","10.1145/1734263.1734324","","","code inspection;cs1;online peer review environment;pedagogical code review;studio-based learning and instruction","","","","","1","","","","","","10-13 March 2010","","ACM","ACM Conferences"
"Design and Implementation of Java Sniper: A Community-Based Software Code Review Web Solution","X. Zhang; B. Dorn; W. Jester; J. Van Pelt; G. Gaeta; D. Firpo","California State Univ., Fresno, CA, USA","2011 44th Hawaii International Conference on System Sciences","20110222","2011","","","1","10","With the advent of the web, more and more information systems are being built web-centric, and more recently, these systems are leveraging the advantages of online community practice as well. This paper describes a lightweight community-based software code review web solution that allows a user to easily upload source code, review, and vote on peer reviews and reviewers.","1530-1605;15301605","Electronic:978-0-7695-4282-9; POD:978-1-4244-9618-1","10.1109/HICSS.2011.145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718483","","Communities;Databases;Electronic mail;Java;Software;User interfaces;Web sites","Java;information systems;social aspects of automation;software reviews","Java sniper;community based software code review Web solution;information systems;online community practice;peer reviews;source code","","0","","7","","","","4-7 Jan. 2011","","IEEE","IEEE Conferences"
"A Socio-Psychological Approach to Improve Student Participation and Review Quality in Peer Code Reviews","P. Agarwal; C. D. Hundhausen","Human-Centered Environments for Learning & Programming (HELP) Lab., Washington State Univ., Pullman, WA, USA","2010 IEEE Symposium on Visual Languages and Human-Centric Computing","20101111","2010","","","263","264","Modern day computing jobs are increasingly emphasizing on soft skills such as design, collaboration and communication skills in addition to programming skills. Having students participate in peer reviews is one possible way to develop such soft skills. However, we have observed that low student participation and low review quality can impede the educational value of the peer review process. To address this issue and improve student learning in computer programming courses, we propose to implement and evaluate social comparisons - a method suggested by socio-psychological research.","1943-6092;19436092","Electronic:978-0-7695-4206-5; POD:978-1-4244-8485-0","10.1109/VLHCC.2010.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635258","","Algorithm design and analysis;Book reviews;Communities;Encoding;Programming profession","computer science education;educational courses;programming;psychology;social sciences","collaboration skill;communication skill;computer programming course;computing job;design skill;educational value;peer code review;programming skill;review quality;social comparison;socio-psychological research;soft skill;student learning;student participation","","0","","5","","","","21-25 Sept. 2010","","IEEE","IEEE Conferences"
"Adopting Code Reviews for Agile Software Development","M. Bernhart; A. Mauczka; T. Grechenig","Res. Group for Ind. Software, Vienna Univ. of Technol., Vienna, Austria","2010 Agile Conference","20100907","2010","","","44","47","Code reviews have many benefits, most importantly to find bugs early in the development phase and to enforce coding standards. Still, it is widely accepted that formal code reviews are time-consuming and the practical applicability in agile development is controversial. This work presents a continuous differential-based method and tool for code reviews. By using a continuous approach to code reviews, the review overhead can be reduced and the effectiveness and applicability in agile environments shall be improved.","","Electronic:978-0-7695-4125-9; POD:978-1-4244-7731-9","10.1109/AGILE.2010.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562809","Code design;Code reviews;Code tuning;Continuous;Tools","Book reviews;Inspection;Java;Programming;Software;Software engineering;USA Councils","program debugging;software development management;software prototyping;software tools","agile development;agile software development;code review;coding standards;continuous differential based method","","8","1","26","","","","9-13 Aug. 2010","","IEEE","IEEE Conferences"
"Collaborations and Code Reviews","J. C. Carver; B. Caglayan; M. Habayeb; B. Penzenstadler; A. Yamashita","University of Alabama","IEEE Software","20150821","2015","32","5","27","29","This article discusses five papers presented at events connected with the 2015 International Conference on Software Engineering. The papers cover topics related to industry-academic collaborations and modern code review.","0740-7459;07407459","","10.1109/MS.2015.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7217781","2015 International Conference on Software Engineering;ClusterChanges;ICSE;changesets;code review;feedback cycles;industry-academic collaboration;software bugs;software engineering","","","","","0","","","","","","Sept.-Oct. 2015","","IEEE","IEEE Journals & Magazines"
"Reactive Power Management in Renewable Rich Power Grids: A Review of Grid-Codes, Renewable Generators, Support Devices, Control Strategies and Optimization Algorithms","M. N. I. Sarkar; L. G. Meegahapola; M. Datta","School of Engineering, RMIT University, Melbourne, VIC 3000, Australia.","IEEE Access","","2018","Early Access","Early Access","1","1","Power electronic converter (PEC) interfaced renewable energy generators (REGs) are increasingly being integrated to the power gird. With the high renewable power penetration levels, one of the key power system parameters, namely reactive power is affected, provoking steady-state voltage and dynamic/ transient stability issues. Therefore, it is imperative to maintain and manage adequate reactive power reserve to ensure a stable and reliable power grid. This paper presents a comprehensive literature review on reactive power management in renewable rich power grids. Reactive power requirements stipulated in different grid-codes for REGs are summarized to assess their adequacy for future network requirements. The PEC interfaced REGs are discussed with special emphasis on their reactive power compensation capability and control schemes. Along with REGs, conventional reactive power support devices (e.g. capacitor banks) and PEC interfaced reactive power support devices (e.g. static synchronous compensators (STATCOMs)) play an indispensable role in reactive power management of renewable rich power grids, and thus their reactive power control capabilities and limitations are thoroughly reviewed in this paper. Then, various reactive power control strategies are reviewed with a special emphasis on their advantages/ disadvantages. Reactive power coordination between support devices and their optimal capacity are vital for an efficient and stable management of the power grid. Accordingly, the prominent reactive power coordination and optimization algorithms are critically examined and discussed in the paper. Finally, key issues pertinent to the reactive power management in renewable rich power grids are enlisted with some important technical recommendations for the power industry, policymakers and academic researchers.","","","10.1109/ACCESS.2018.2838563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8361423","Control strategies;grid codes;optimization algorithms;reactive power;renewable energy generators (REGs);solar photovoltaic (PV);wind generation","Generators;Power grids;Power system stability;Reactive power;Renewable energy sources;Voltage control;Wind power generation","","","","","","","","","20180521","","","IEEE","IEEE Early Access Articles"
"Multilingual Source Code Analysis: A Systematic Literature Review","Z. Mushtaq; G. Rasool; B. Shehzad","COMSATS Institute of Information Technology, Lahore Campus, Lahore, Pakistan","IEEE Access","20180315","2017","5","","11307","11336","Contemporary software applications are developed using cross-language artifacts, which are interdependent with each other. The source code analysis of these applications requires the extraction and examination of artifacts, which are build using multiple programming languages along with their dependencies. A large number of studies presented on multilingual source code analysis and its applications in the last one and half decade. The objective of this systematic literature review (SLR) is to summarize state of the art and prominent areas for future research. This SLR is based on different techniques, tools, and methodologies to analyze multilingual source code applications. We finalized 56 multi-discipline published papers relevant to multilingual source code analysis and its applications out of 3820 papers, filtered through multi-stage search criterion. Based on our findings, we highlight research gaps and challenges in the field of multilingual applications. The research findings are presented in the form of research problems, research contributions, challenges, and future prospects. We identified 46 research issues and requirements for analyzing multilingual applications and grouped them in 13 different software engineering domains. We examined the research contributions and mapped them with individual research problems. We presented the research contributions in the form of tools techniques and approaches that are presented in the form of research models, platforms, frameworks, prototype models, and case studies. Every research has its limitations or prospects for future research. We highlighted the limitations and future perspectives and grouped them in various software engineering domains. Most of the research trends and potential research areas are identified in static source code analysis, program comprehension, refactoring, reverse engineering, detection, and traceability of cross-language links, code coverage, security analysis, cross-language parsing, a- d abstraction of source code models.","","","10.1109/ACCESS.2017.2710421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953501","Software engineering;reverse engineering;software architecture;software design;software maintenance","Analytical models;Bibliographies;Data mining;Manuals;Reverse engineering;Software;Systematics","program diagnostics;reverse engineering;security of data;software maintenance;source code (software)","contemporary software applications;cross-language artifacts;cross-language parsing;multi-stage search criterion;multilingual applications;multilingual source code analysis;program comprehension;refactoring;reverse engineering;security analysis;source code models;static source code analysis","","1","","","","","20170619","2017","","IEEE","IEEE Journals & Magazines"
"Review of Algebraic Coding Theory Revised Edition by Elwyn Berlekamp","S. V. Nagaraj","VIT University, Chennai Campus, India","ACM SIGACT News","20170511","2017","48","1","23","26","","0163-5700;01635700","","10.1145/3061640.3061645","","","","","","","","","","","","","","March 2017","","ACM","ACM Journals & Magazines"
"Analyzing reviews and code of mobile apps for better release planning","A. Ciurumelea; A. Schaufelb√ºhl; S. Panichella; H. C. Gall","University of Zurich, Department of Informatics, Switzerland","2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20170323","2017","","","91","102","The mobile applications industry experiences an unprecedented high growth, developers working in this context face a fierce competition in acquiring and retaining users. They have to quickly implement new features and fix bugs, or risks losing their users to the competition. To achieve this goal they must closely monitor and analyze the user feedback they receive in form of reviews. However, successful apps can receive up to several thousands of reviews per day, manually analysing each of them is a time consuming task. To help developers deal with the large amount of available data, we manually analyzed the text of 1566 user reviews and defined a high and low level taxonomy containing mobile specific categories (e.g. performance, resources, battery, memory, etc.) highly relevant for developers during the planning of maintenance and evolution activities. Then we built the User Request Referencer (URR) prototype, using Machine Learning and Information Retrieval techniques, to automatically classify reviews according to our taxonomy and recommend for a particular review what are the source code files that need to be modified to handle the issue described in the user review. We evaluated our approach through an empirical study involving the reviews and code of 39 mobile applications. Our results show a high precision and recall of URR in organising reviews according to the defined taxonomy.","","Electronic:978-1-5090-5501-2; POD:978-1-5090-5502-9","10.1109/SANER.2017.7884612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884612","Code Localization;Mobile Applications;Text Classification;User Reviews","Batteries;Computer bugs;Maintenance engineering;Mobile applications;Mobile communication;Prototypes;Taxonomy","information retrieval;learning (artificial intelligence);mobile computing;program debugging;software development management;software maintenance;source code (software)","URR;bug fixing;evolution activities;information retrieval techniques;machine learning;maintenance activities;mobile applications industry;mobile specific categories;release planning;source code files;user feedback;user request referencer prototype","","3","","","","","","20-24 Feb. 2017","","IEEE","IEEE Conferences"
"Empirical Evaluation of the Impact of Object-Oriented Code Refactoring on Quality Attributes: A Systematic Literature Review","J. Al Dallal; A. Abdin","Department of Information Science, Kuwait University, Safat, Kuwait","IEEE Transactions on Software Engineering","20180108","2018","44","1","44","69","Software refactoring is a maintenance task that addresses code restructuring to improve its quality. Many studies have addressed the impact of different refactoring scenarios on software quality. This study presents a systematic literature review that aggregates, summarizes, and discusses the results of 76 relevant primary studies (PSs) concerning the impact of refactoring on several internal and external quality attributes. The included PSs were selected using inclusion and exclusion criteria applied to relevant articles published before the end of 2015. We analyzed the PSs based on a set of classification criteria, including software quality attributes and measures, refactoring scenarios, evaluation approaches, datasets, and impact results. We followed the vote-counting approach to determine the level of consistency among the PS reported results concerning the relationship between refactoring and software quality. The results indicated that different refactoring scenarios sometimes have opposite impacts on different quality attributes. Therefore, it is false that refactoring always improves all software quality aspects. The vote-counting study provided a clear view of the impacts of some individual refactoring scenarios on some internal quality attributes such as cohesion, coupling, complexity, inheritance, and size, but failed to identify their impacts on external and other internal quality attributes due to insufficient findings.","0098-5589;00985589","","10.1109/TSE.2017.2658573","; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833023","quality attribute;quality measure;refactoring scenario;systematic literature review","Bibliographies;Libraries;Object oriented modeling;Software quality;Systematics;Unified modeling language","software maintenance;software metrics;software quality","76 relevant primary studies;PSs;classification criteria;code restructuring;different quality attributes;different refactoring scenarios;evaluation approaches;external quality;individual refactoring scenarios;internal quality;maintenance task;opposite impacts;software quality aspects;software quality attributes;software refactoring;systematic literature review;vote-counting approach;vote-counting study","","","","","","","20170125","Jan. 1 2018","","IEEE","IEEE Journals & Magazines"
"Refining students' coding and reviewing skills","P. Guo","University of Rochester","Communications of the ACM","20161111","2014","57","9","10","11","<p>The <i>Communications</i> Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of <i>Communications</i>, we'll publish selected posts or excerpts.<br /><br /><b>twitter</b><br /><b>Follow us on Twitter at http://twitter.com/blogCACM</b><br /><br /><b>http://cacm.acm.org/blogs/blog-cacm</b><br /><br />Philip Guo sees code reviews providing students ""lots of pragmatic learning.""</p>","0001-0782;00010782","","10.1145/2641221","","","","","","","","","","","","","","September 2014","","ACM","ACM Journals & Magazines"
"Review code evolution history in OSS universe","J. Zhu; H. Lin; M. Zhou; H. Mei","Peking University, Beijing, China","Proceedings of the Fourth Asia-Pacific Symposium on Internetware","20160129","2012","","","1","4","<p>Software evolves all the time because of the changing requirements, in particular, in the diverse Internet environment. Evolution history recorded in software repositories, e.g., Version Control Systems, reflects people's software development practice. Exploring this history could help practitioners to reuse the best practices therefore improve productivity and software quality. Because of the difficulty of collecting and standardizing data, most existing work could only utilize small project set. In this study, we target the open source software universe to build a universal code evolution model for large-scale data. We consider code evolution from two aspects: code version changing history in a single project and code reuse history in the whole universe. In the model, files/modules are built as nodes, and relations (version change or reuse) between files/modules are built as connections. Based on the model, we design and implement a code evolution review framework, i.e., Code Evolution Reviewer (CER), which provides a series of data interfaces to review code evolution history, in particular, code version changing in single project and code reuse among projects. Further, CER could be utilized to explore best practices across large-scale project set.</p>","","","10.1145/2430475.2430488","","","OSS universe;best practice;code evolution;code reuse","","","","","","","","","","","30-31 Oct. 2012","","ACM","ACM Conferences"
"A systematic literature review of traceability approaches between software architecture and source code","M. A. Javed; U. Zdun","University of Vienna, Austria","Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering","20160129","2014","","","1","10","<p>The links between the software architecture and the source code of a software system should be based on solid traceability mechanisms in order to effectively perform quality control and maintenance of the software system. There are several primary studies on traceability between software architecture and source code but so far no systematic literature review (SLR) has been undertaken. This study presents an SLR which has been carried out to discover the existing traceability approaches and tools between software architecture and source code, as well as the empirical evidence for these approaches, their benefits and liabilities, their relations to software architecture understanding, and issues, barriers, and challenges of the approaches. In our SLR the ACM Guide to Computing Literature has been electronically searched to accumulate the biggest share of relevant scientific bibliographic citations from the major publishers in computing. The search strategy identified 742 citations, out of which 11 have been included in our study, dated from 1999 to July, 2013, after applying our inclusion and exclusion criteria. Our SLR resulted in the identification of the current state-of-the-art of traceability approaches and tools between software architecture and source code, as well as gaps and pointers for further research. Moreover, the classification scheme developed in this paper can serve as a guide for researchers and practitioners to find a specific approach or set of approaches that is of interest to them.</p>","","","10.1145/2601248.2601278","","","software architecture;source code;systematic literature review;traceability","","","","","1","","","","","","13-14 May 2014","","ACM","ACM Conferences"
"Alternatives to lecture: experience peer instruction and pedagogical code reviews","S. Grissom; C. Hundhausen; P. Conrad","Grand Valley State University, Allendale, MI, USA","Proceedings of the 45th ACM technical symposium on Computer science education","20160129","2014","","","275","276","<p>This session will demonstrate best practices for introducing peer instruction and pedagogical code reviews into the computer science classroom. Presenters play the role of instructors on the first day of class. Audience members play the role of students as they collaborate on a variety of sample activities. By providing models of active learning in computing contexts, we seek to motivate instructors to adopt these pedagogies in their classrooms. Sufficient time will be reserved for questions and discussion.</p>","","","10.1145/2538862.2538985","","","active learning;collaborative learning;pedagogical code reviews;peer instruction;student engagement pedagogies","","","","","1","","","","","","5-8 March 2014","","ACM","ACM Conferences"
"Review of applied algebra: codes, ciphers and discrete algorithms, by Darel W. Hardy, Fred Richman, and Carol L. Walker","Y. Xie","","ACM SIGACT News","20160129","2012","43","3","25","27","","0163-5700;01635700","","10.1145/2421096.2421101","","","","","","","","","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"Review of variable-length codes for data compression by David Salomon","F. Nasim","Sylhet, Banglade","ACM SIGACT News","20160129","2013","44","4","24","26","","0163-5700;01635700","","10.1145/2556663.2556666","","","","","","","","","","","","","","December 2013","","ACM","ACM Journals & Magazines"
"Tutorial 1: Mining Unstructured Data from Code Reviews: A Hands-On Tutorial by Nicolas Bettenburg","","","2014 IEEE 4th Workshop on Mining Unstructured Data","20141211","2014","","","x","xi","Provides an abstract of the tutorial presentation and may include a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.","","Electronic:978-1-4799-6793-3; POD:978-1-4799-6794-0","10.1109/MUD.2014.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980185","","","","","","0","","","","","","30-30 Sept. 2014","","IEEE","IEEE Conferences"
"Improving Review of Clustered-Code Analysis Warnings","T. Muske","Res. Dev. & Design Center, Pune, India","2014 IEEE International Conference on Software Maintenance and Evolution","20141206","2014","","","569","572","While software verification using static analysis tools has proved its significance in early defect detection, the tools often fail to analyze many real world systems. Clustering, breaking a system into multiple clusters, is commonly used technique to scale these tools to large systems. Due to the imprecise nature of static analysis and the conservative approach taken for the inter-cluster communication (data sharing), a large number of analysis warnings are generated. All these reported warnings need to be reviewed manually to check if they represent a defect, and it is a time consuming process. We observe many of the reported warnings are common to multiple clusters and reviewing each of them individually incurs redundancy. We present an approach to improve the manual review process by eliminating the redundancy. This is achieved by grouping the inter-cluster common warnings such that review of a grouped warning under given constraint guarantees same review judgment for the other cluster warnings in the same group. Empirical results obtained with the presented approach indicate that -- a) on an average, 45% of total warnings are common to multiple clusters, and b) with the proposed grouping technique, the manual efforts required to review these common warnings are reduced by 60%.","1063-6773;10636773","Electronic:978-1-4799-6146-7; POD:978-1-4799-6147-4","10.1109/ICSME.2014.97","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976141","Clustering;False Positives;Manual Review of Warnings;Static Analysis","Arrays;Context;Indexes;Manuals;Redundancy;Software;Software engineering","pattern clustering;program diagnostics;program verification","clustered-code analysis warnings;early defect detection;grouping technique;intercluster common warnings;intercluster communication;manual review process;review judgment;software verification;static analysis tools","","3","","12","","","","Sept. 29 2014-Oct. 3 2014","","IEEE","IEEE Conferences"
"Geometric distortion measurement for shape coding: A contemporary review","F. A. Sohel; G. C. Karmakar; L. S. Dooley; M. Bennamoun","The University of Western Australia, Australia","ACM Computing Surveys (CSUR)","20121018","2011","43","4","1","22","<p>Geometric distortion measurement and the associated metrics involved are integral to the Rate Distortion (RD) shape coding framework, with importantly the efficacy of the metrics being strongly influenced by the underlying measurement strategy. This has been the catalyst for many different techniques with this article presenting a comprehensive review of geometric distortion measurement, the diverse metrics applied, and their impact on shape coding. The respective performance of these measuring strategies is analyzed from both a RD and complexity perspective, with a recent distortion measurement technique based on arc-length-parameterization being comparatively evaluated. Some contemporary research challenges are also investigated, including schemes to effectively quantify shape deformation.</p>","0360-0300;03600300","","10.1145/1978802.1978808","","","Object-based video coding;geometric distortion measurement;shape coding","","","","","2","","","","","","October 2011","","ACM","ACM Journals & Magazines"
"The importance of reviewing the code","J. A. A√±el","Universidade de Vigo at Ourense, Spain","Communications of the ACM","20121018","2011","54","5","40","41","<p>Highlighting the significance of the often overlooked underlying software used to produce research results.</p>","0001-0782;00010782","","10.1145/1941487.1941502","","","","","","","","0","","","","","","May 2011","","ACM","ACM Journals & Magazines"
"Review of codes: an introduction to information communication and cryptography by Norman L Biggs","R. J. Low","","ACM SIGACT News","20121018","2012","43","1","27","29","","0163-5700;01635700","","10.1145/2160649.2160658","","","","","","","","0","","","","","","March 2012","","ACM","ACM Journals & Magazines"
"Review of algebraic function fields and codes by Henning Stichtenoth","S. Kopparty","","ACM SIGACT News","20121018","2011","42","2","19","24","","0163-5700;01635700","","10.1145/1998037.1998043","","","","","","","","0","","","","","","June 2011","","ACM","ACM Journals & Magazines"
"Review of the Cryptoclub: using mathematics to make and break secret codes by Janet Beissinger and Vera Pless","A. M. O. S. Uddin Ayub","University of Maryland, Baltimore County","ACM SIGACT News","20121018","2012","43","1","9","14","","0163-5700;01635700","","10.1145/2160649.2160652","","","","","","","","0","","","","","","March 2012","","ACM","ACM Journals & Magazines"
"Review and Implementation of the Emerging CCSDS Recommended Standard for Multispectral and Hyperspectral Lossless Image Coding","J. E. S¬¥nchez; E. Auge; J. Santalo; I. Blanes; J. Serra-Sagrista; A. Kiely","Dept. of Inf. & Commun. Eng., Univ. Autonoma de Barcelona, Barcelona, Spain","2011 First International Conference on Data Compression, Communications and Processing","20111027","2011","","","222","228","A new standard for image coding is being developed by the MHDC working group of the CCSDS, targeting onboard compression of multi- and hyper-spectral imagery captured by aircraft and satellites. The proposed standard is based on the ""Fast Lossless"" adaptive linear predictive compressor, and is adapted to better overcome issues of on-board scenarios. In this paper, we present a review of the state of the art in this field, and provide an experimental comparison of the coding performance of the emerging standard in relation to other state-of-the-art coding techniques. Our own independent implementation of the MHDC Recommended Standard, as well as of some of the other techniques, has been used to provide extensive results over the vast corpus of test images from the CCSDS-MHDC.","","Electronic:978-0-7695-4528-8; POD:978-1-4577-1458-0","10.1109/CCP.2011.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061129","CCSDS-MHDC-123 standard;lossless image coding;multi-hyper-spectral imagery","Encoding;Hyperspectral imaging;Image coding;Sensors;Table lookup","geophysical image processing;image coding","CCSDS recommended standard;MHDC;adaptive linear predictive compressor;hyperspectral lossless image coding;remote-sensing sensors","","4","","23","","","","21-24 June 2011","","IEEE","IEEE Conferences"
"Review of coding for data and computer communications by David Salomon Springer, 2005","D. A. Werden","2115 Shadebrush Court, Augusta, GA","ACM SIGACT News","20110708","2010","41","4","32","34","","0163-5700;01635700","","10.1145/1907450.1907516","","","","","","","","0","","","","","","December 2010","","ACM","ACM Journals & Magazines"
"Magnetotelluric 3-D inversion‚Äîa review of two successful workshops on forward and inversion code testing and comparison","M. P. Miensopust; P. Queralt; A. G. Jones","","Geophysical Journal International","20180118","2013","193","3","1216","1238","Over the last half decade the need for, and importance of, three-dimensional (3-D) modelling of magnetotelluric (MT) data have increased dramatically and various 3-D forward and inversion codes are in use and some have become commonly available. Comparison of forward responses and inversion results is an important step for code testing and validation prior to ‚Äòproduction‚Äô use. The various codes use different mathematical approximations to the problem (finite differences, finite elements or integral equations), various orientations of the coordinate system, different sign conventions for the time dependence and various inversion strategies. Additionally, the obtained results are dependent on data analysis, selection and correction as well as on the chosen mesh, inversion parameters and regularization adopted, and therefore, a careful and knowledge-based use of the codes is essential. In 2008 and 2011, during two workshops at the Dublin Institute for Advanced Studies over 40 people from academia (scientists and students) and industry from around the world met to discuss 3-D MT inversion. These workshops brought together a mix of code writers as well as code users to assess the current status of 3-D modelling, to compare the results of different codes, and to discuss and think about future improvements and new aims in 3-D modelling. To test the numerical forward solutions, two 3-D models were designed to compare the responses obtained by different codes and/or users. Furthermore, inversion results of these two data sets and two additional data sets obtained from unknown models (secret models) were also compared. In this manuscript the test models and data sets are described (supplementary files are available) and comparisons of the results are shown. Details regarding the used data, forward and inversion parameters as well as computational power are summarized for each case, and the main discussion points of the workshops are revi- wed. In general, the responses obtained from the various forward models are comfortingly very similar, and discrepancies are mainly related to the adopted mesh. For the inversions, the results show how the inversion outcome is affected by distortion and the choice of errors, as well as by the completeness of the data set. We hope that these compilations will become useful not only for those that were involved in the workshops, but for the entire MT community and also the broader geoscience community who may be interested in the resolution offered by MT.","0956-540X;0956540X","","10.1093/gji/ggt066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8179980","Geomagnetic induction;Inverse theory;Magnetotelluric;Numerical solutions","","","","","","","","","","","June 2013","","OUP","OUP Journals & Magazines"
"Reviews [reviews of ""The Computer Boys Take Over: Computers, Programmers, and the Politics of Technical Expertise"" (Ensmenger, N; 2020) and ""Gender Codes: Why Women Are Leaving Computing"" (Misa, T.J.; 2010)]","","","IEEE Annals of the History of Computing","20110519","2011","33","2","102","104","Jeffrey R. Yost and Atsusshi Akera review the books The Computer Boys Take Over: Computers, Programmers, and the Politics of Technical Expertise by Nathan Ensmenger and Gender Codes: Why Women Are Leaving Computing by Thomas J. Misa.","1058-6180;10586180","","10.1109/MAHC.2011.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771316","Gender Codes: Why Women Are Leaving Computing;Nathan Ensmenger;Programemrs;The Computer Boys Take Over: Computers;Thomas J. Misa;a nd the Politics of Technical Expertise","Book reviews;Career development;Computer industry;Computer science education;Engineering profession;Gender issues;Professional aspects;Programming profession","","","","0","","1","","","","Feb. 2011","","IEEE","IEEE Journals & Magazines"
"From Black Codes to Recodification: Removing the Veil from Regulatory Writing Book Review","M. F. Williams","","IEEE Transactions on Professional Communication","20101122","2010","53","4","426","427","In this book, Miriam F. Williams explores regulatory writing that promotes distrust in historical and contemporary African American constituencies. Focusing specifically on Texas regulatory laws, she examines how writers of legislation and policies struggle with making language of legislation and policies clearer and objective while grappling with long-held feelings of distrust of government legislation in African American citizens--feelings that have been perpetuated by historical use of veiled language in laws and policies. She explores the possibility of mitigation an audience's mistrust by writing these policies in Plain English in an attempt to be more objective and transparent. The book is recommended to all who seek to explore the connections between public policy and technical communication and the implications of these fields of study on social and cultural concerns.","0361-1434;03611434","","10.1109/TPC.2010.2077851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625094","Contextual inquiry;Plain English;cultural studies;public policy;regulatory writing","","","","","0","","","","","","Dec. 2010","","IEEE","IEEE Journals & Magazines"
"Bayesian Brain: Probabilistic Approaches to Neural Coding (Doya, K., Eds., et al.; 2007) [Book Review]","S. Deutsch","","IEEE Pulse","20101122","2010","1","3","64","65","As stated in the preface, ""This book is based on lectures given at the First Okinawa Computational Neuroscience Course, held in November 2004 at Bankoku Shinryokan, Okinawa, Japan. The intention of the course was to bring together both experimental and theoretical neuroscientists employing the principles of Bayesian estimation to understand the brain mechanisms of perception, decision, and control."" There are 13 chapters written by 23 contributors. While there are some noted omissions and errors, this book belongs in the library of every neuroscientist who wishes to be at the forefront of theoretical and/or experimental analysis.","2154-2287;21542287","","10.1109/MPUL.2010.939182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5642158","","Bayesian methods;Book reviews;Brain models;Probability","","","","0","","","","","","Nov.-Dec. 2010","","IEEE","IEEE Journals & Magazines"
"Everyday Codes [Book Review]","A. Riddle","","IEEE Microwave Magazine","20100520","2010","11","4","120","120","","1527-3342;15273342","","10.1109/MMM.2010.936478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5462804","","Book reviews","","","","0","","","","","","June 2010","","IEEE","IEEE Journals & Magazines"
"Context is king: The developer perspective on the usage of static analysis tools","C. Vassallo; S. Panichella; F. Palomba; S. Proksch; A. Zaidman; H. C. Gall","University of Zurich, Switzerland","2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20180405","2018","","","38","49","Automatic static analysis tools (ASATs) are tools that support automatic code quality evaluation of software systems with the aim of (i) avoiding and/or removing bugs and (ii) spotting design issues. Hindering their wide-spread acceptance are their (i) high false positive rates and (ii) low comprehensibility of the generated warnings. Researchers and ASATs vendors have proposed solutions to prioritize such warnings with the aim of guiding developers toward the most severe ones. However, none of the proposed solutions considers the development context in which an ASAT is being used to further improve the selection of relevant warnings. To shed light on the impact of such contexts on the warnings configuration, usage and adopted prioritization strategies, we surveyed 42 developers (69% in industry and 31% in open source projects) and interviewed 11 industrial experts that integrate ASATs in their workflow. While we can confirm previous findings on the reluctance of developers to configure ASATs, our study highlights that (i) 71% of developers do pay attention to different warning categories depending on the development context, and (ii) 63% of our respondents rely on specific factors (e.g., team policies and composition) when prioritizing warnings to fix during their programming. Our results clearly indicate ways to better assist developers by improving existing warning selection and prioritization strategies.","","Electronic:978-1-5386-4969-5; POD:978-1-5386-4970-1","10.1109/SANER.2018.8330195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330195","Code Review;Continuous Integration;Development Context;Empirical Study;Static Analysis","Computer bugs;Encoding;Interviews;Lead;Programming;Software;Tools","program debugging;program diagnostics;software quality","ASAT;ASATs vendors;automatic code quality evaluation;automatic static analysis tools;bugs removal;development context;high false positive rates;prioritization strategies;software systems;warning categories;warning selection;warnings configuration","","","","","","","","20-23 March 2018","","IEEE","IEEE Conferences"
"Detecting Duplicate Pull-requests in GitHub","Z. Li; G. Yin; Y. Yu; T. Wang; H. Wang","National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, China","Proceedings of the 9th Asia-Pacific Symposium on Internetware","20171225","2017","","","1","6","<p>The widespread use of pull-requests boosts the development and evolution for many open source software projects. However, due to the parallel and uncoordinated nature of development process in GitHub, duplicate pull-requests may be submitted by different contributors to solve the same problem. Duplicate pull-requests increase the maintenance cost of GitHub, result in the waste of time spent on the redundant effort of code review, and even frustrate developers' willing to offer continuous contribution. In this paper, we investigate using text information to automatically detect duplicate pull-requests in GitHub. For a new-arriving pull-request, we compare the textual similarity between it and other existing pull-requests, and then return a candidate list of the most similar ones. We evaluate our approach on three popular projects hosted in GitHub, namely Rails, Elasticsearch and Angular.JS. The evaluation shows that about 55.3% -- 71.0% of the duplicates can be found when we use the combination of title similarity and description similarity.</p>","","","10.1145/3131704.3131725","","","Pull-request;code review;duplicate detection;textual similarity","","","","","","","","","","","23-23 Sept. 2017","","ACM","ACM Conferences"
"On the Influence of Human Factors for Identifying Code Smells: A Multi-Trial Empirical Study","R. M. d. Mello; R. F. Oliveira; A. F. Garcia","Banco do Brazil, Rio de Janeiro, Brazil","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","20171211","2017","","","68","77","Context: Code smells are symptoms in the source code that represent poor design choices. Professional developers often perceive several types of code smells as indicators of actual design problems. However, the identification of code smells involves multiple steps that are subjective in nature, requiring the engagement of humans. Human factors are likely to play a key role in the precise identification of code smells in industrial settings. Unfortunately, there is limited knowledge about the influence of human factors on smell identification. Goal: We aim at investigating whether the precision of smell identification is influenced by three key human factors, namely reviewer's professional background, reviewer's module knowledge and collaboration of reviewers during the task. We also aim at deriving recommendations for allocating human resources to smell identification tasks. Method: We performed 19 comparisons among different subsamples from two trials of a controlled experiment conducted in the context of an empirical study on code smell identification. One trial was conducted in industrial settings while the other had involved graduate students. The diversity of the samples allowed us to analyze the influence of the three factors in isolation and in conjunction. Results: We found that (i) reviewers' collaboration significantly increases the precision of smell identification, but (ii) some professional background is required from the reviewers to reach high precision. Surprisingly, we also found that: (iii) having previous knowledge of the reviewed module does not affect the precision of reviewers with higher professional background. However, this factor was influential on successful identification of more complex smells. Conclusion: We expect that our findings are helpful to support researchers in conducting proper experimental procedures in the future. Besides, they may also be useful for supporting project managers in allocating resources for smell identific- tion tasks.","","Electronic:978-1-5090-4039-1; POD:978-1-5090-4040-7","10.1109/ESEM.2017.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170086","code review;code smell identification;collaboration;context;human factors;replication","Collaboration;Human factors;Software measurement;Software systems;Tools","human factors;software maintenance;software quality","code smell identification;code smells;complex smells;human factors;human resources;multitrial empirical study;smell identification tasks","","","","","","","","9-10 Nov. 2017","","IEEE","IEEE Conferences"
"Are developers aware of the architectural impact of their changes?","M. Paixao; J. Krinke; D. Han; C. Ragkhitwetsagul; M. Harman","University College London, United Kingdom","2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)","20171123","2017","","","95","105","Although considered one of the most important decisions in a software development lifecycle, empirical evidence on how developers perform and perceive architectural changes is still scarce. Given the large implications of architectural decisions, we do not know whether developers are aware of their changes' impact on the software's architecture, whether awareness leads to better changes, and whether automatically making developers aware would prevent degradation. Therefore, we use code review data of 4 open source systems to investigate the intent and awareness of developers when performing changes. We extracted 8,900 reviews for which the commits are available. 2,152 of the commits have changes in their computed architectural metrics, and 338 present significant changes to the architecture. We manually inspected all reviews for commits with significant changes and found that only in 38% of the time developers are discussing the impact of their changes on the architectural structure, suggesting a lack of awareness. Finally, we observed that developers tend to be more aware of the architectural impact of their changes when the architectural structure is improved, suggesting that developers should be automatically made aware when their changes degrade the architectural structure.","","Electronic:978-1-5386-2684-9; POD:978-1-5386-3976-4","10.1109/ASE.2017.8115622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115622","Code Reviews;Software Architecture","Computer architecture;Couplings;Degradation;History;Java;Measurement;Software systems","software architecture;software engineering;software maintenance;software metrics;software quality","architectural changes;architectural decisions;architectural impact;architectural structure;computed architectural metrics;software development lifecycle;time developers","","","","","","","","Oct. 30 2017-Nov. 3 2017","","IEEE","IEEE Conferences"
"Reviewing Career Paths of the OpenStack Developers","P. v. Wesel; B. Lin; G. Robles; A. Serebrenik","Eindhoven Univ. of Technol., Eindhoven, Netherlands","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20171107","2017","","","544","548","Career perspectives are known to motivate software engineers. However, so far, career perspectives have been mostly studied within traditional software development companies. In our work we take a complementary approach and study career paths of open source developers, focusing on their advancement through the code review hierarchy, from developers to reviewers and further to core reviewers.To gain understanding of code review career paths we conduct an exploratory case study of the OpenStack community. Based on the case study we have publicized anonymized research data and formulated four hypotheses pertaining to career paths of contributors in modern multi-company open source projects. We conjecture that(i) developers and reviewers are separate subpopulations with little movement between them, (ii-a) the turnover of the core reviewers is high and rapid, (ii-b) companies are interested in having core reviewers among their staff, and (iii) being a core reviewer is beneficial for career.Validity of those hypotheses in other multi-company open source projects should be investigated in the follow-up studies.","","Electronic:978-1-5386-0992-7; POD:978-1-5386-0993-4","10.1109/ICSME.2017.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094459","OpenStack;career;code review;open source software","Bibliographies;Companies;Couplings;Databases;Engineering profession;Focusing;Software","human factors;personnel;public domain software;software engineering","OpenStack developers;career perspectives;code review career paths;code review hierarchy;multicompany open source projects;open source developers;software development companies;software engineers","","","","","","","","17-22 Sept. 2017","","IEEE","IEEE Conferences"
"The Human Factor","J. C. Carver; B. Penzenstadler; A. Serebrenik; A. Yamashita","University of Alabama","IEEE Software","20170922","2017","34","5","90","92","This installment reports on five papers from the 39th International Conference on Software Engineering and its collocated events. These papers focus on human factors in software engineering, with the last three dealing with open source software.","0740-7459;07407459","","10.1109/MS.2017.3571580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048655","39th International Conference on Software Engineering;Docker;GitHub;ICSE;brainstorming;code review;inclusiveness;open source licenses;privacy;privacy requirements;software development;software engineering","","","","","","","","","","","2017","","IEEE","IEEE Journals & Magazines"
"Source Code Patterns of SQL Injection Vulnerabilities","F. Schuckert; B. Katt; H. Langweg","HTWG Konstanz, Department of Computer Science, Konstanz, Germany, Norwegian University of Science and Technology, Department of Information Security and Communication Technology, Gj&#248;vik, Norway","Proceedings of the 12th International Conference on Availability, Reliability and Security","20170914","2017","","","1","7","<p>Many secure software development methods and tools are well-known and understood. Still, the same software security vulnerabilities keep occurring. To find out if new source code patterns evolved or the same patterns are reoccurring, we investigate SQL injections in PHP open source projects. SQL injections are well-known and a core part of software security education. For each common part of SQL injections, the source code patterns are analysed. Examples are pointed out showing that developers had software security in mind, but nevertheless created vulnerabilities. A comparison to earlier work shows that some categories are not found as often as expected. Our main contribution is the categorization of source code patterns.</p>","","","10.1145/3098954.3103173","","","Code Review;Data Mining;PHP;Software Security;Sql Injection;Vulnerabilities","","","","","","","","","","","Aug. 29 2017-Sept. 1 2017","","ACM","ACM Conferences"
"Broadcast vs. Unicast Review Technology: Does It Matter?","F. Armstrong; F. Khomh; B. Adams","SWAT-MCIS, Ecole Polytech. de Montreal, Montreal, QC, Canada","2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)","20170518","2017","","","219","229","Code review is the process of having other team members examine changes to a software system in order to evaluate their technical content and quality. Over the years, multiple tools have been proposed to help software developers conduct and manage code reviews. Some software organizations have been migrating from broadcast review technology to a more advanced unicast review approach such as Jira, but it is unclear if these unicast review technology leads to better code reviews. This paper empirically studies review data of five Apache projects that switched from broadcast based code review to unicast based, to understand the impact of review technology on review effectiveness and quality. Results suggest that broadcast based review is twice faster than review done with unicast based review technology. However, unicast's review quality seems to be better than that of the broadcast based. Our findings suggest that the medium (i.e., broadcast or unicast) technology used for code reviews can relate to the effectiveness and quality of reviews activities.","","Electronic:978-1-5090-6031-3; POD:978-1-5090-6032-0","10.1109/ICST.2017.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927977","Apache;Bug-inducing Changes;Code Review;Empirical Study;Medium;Patches;Quality Assurance","Broadcast technology;Electronic mail;Switches;Unicast","software quality","Apache projects;broadcast review technology;code review;software organizations;software system;unicast review technology","","","","","","","","13-17 March 2017","","IEEE","IEEE Conferences"
"Comparing Pre Commit Reviews and Post Commit Reviews Using Process Simulation","T. Baum; F. Kortum; K. Schneider; A. Brack; J. Schauder","FG Software Eng., Leibniz Univ. Hannover, Hannover, Germany","2016 IEEE/ACM International Conference on Software and System Processes (ICSSP)","20170126","2016","","","26","35","Code review in practice is often performed change-based, i.e. using the code changes belonging to a task to determine which code to review. In previous studies, it was found that two variations of this process are used in industry: Pre commit review (review-then-commit) and post commit review (commit-then-review). The choice for one of these variants has implications not only for practitioners deciding on a code review process to use, but also for the development of review tools and for experimentation with review processes. In some situations, a specific variant is clearly preferable due to the nature of the development process or team. In other situations, there are conflicting opinions, and both variants have proponents arguing for their method of choice. So we asked: Are there practically relevant performance differences between pre commit and post commit reviews, and how are these differences influenced by contextual factors? To assess this question, we designed a parametric discrete event simulation model of certain agile development processes. We validated this model with practitioner's feedback and in part also with empirical data from industry. Our analysis of the simulation results indicates that the best choice does depend on the context, but also that there are many situations with no practically relevant difference between both choices. We identified the main influencing factors and underlying effects and condensed our findings into heuristic rules.","","Electronic:978-1-4503-4188-2; POD:978-1-5090-2244-1","10.1109/ICSSP.2016.012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831532","Agile Software Development;Code review;Discrete Event Simulation;Post Commit Review;Pre Commit Review","Context;Data models;Discrete event simulation;Software;Software engineering;Unified modeling language","discrete event simulation;software prototyping;software reviews;source code (software)","agile development processes;change-based code review;commit-then-review;parametric discrete event simulation model;post commit reviews;practitioner feedback;precommit reviews;process simulation;review tools;review-then-commit","","","","","","","","14-15 May 2016","","IEEE","IEEE Conferences"
"Why are Commits Being Reverted?: A Comparative Study of Industrial and Open Source Projects","J. Shimagaki; Y. Kamei; S. McIntosh; D. Pursehouse; N. Ubayashi","Sony Mobile Commun. Inc., Japan","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20170116","2016","","","301","311","Software development is a cyclic process of integrating new features while introducing and fixing defects. During development, commits that modify source code files are uploaded to version control systems. Occasionally, these commits need to be reverted, i.e., the code changes need to be completely backed out of the software project. While one can often speculate about the purpose of reverted commits (e.g., the commit may have caused integration or build problems), little empirical evidence exists to substantiate such claims. The goal of this paper is to better understand why commits are reverted in large software systems. To that end, we quantitatively and qualitatively study two proprietary and four open source projects to measure: (1) the proportion of commits that are reverted, (2) the amount of time that commits that are eventually reverted linger within a codebase, and (3) the most frequent reasons why commits are reverted. Our results show that 1%-5% of the commits in the studied systems are reverted. Those commits that are eventually reverted linger within the studied codebases for 1-35 days (median). Furthermore, we identify 13 common reasons for reverting commits, and observe that the frequency of reverted commits of each reason varies broadly from project to project. A complementary qualitative analysis suggests that many reverted commits could have been avoided with better team communication and change awareness. Our findings made Sony Mobile's stakeholders aware that internally reverted commits can be reduced by paying more attention to their own changes. On the other hand, externally reverted commits could be minimized only if external stakeholders are involved to improve inter-company communication or requirements elicitation.","","Electronic:978-1-5090-3806-0; POD:978-1-5090-3807-7","10.1109/ICSME.2016.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816476","code inspections;code review;mining software repository;revert;software evolution","Androids;Humanoid robots;Mobile communication;Mobile handsets;Software systems;Stakeholders","project management;public domain software;software engineering;software management;source code (software)","industrial project;large software systems;open source project;qualitative analysis;reverted commits;software development;software project;source code;version control systems","","","","","","","","2-7 Oct. 2016","","IEEE","IEEE Conferences"
"Robot ON!: A Serious Game for Improving Programming Comprehension","M. A. Miljanovic; J. S. Bradbury","Software Quality Res. Lab., Univ. of Ontario, Oshawa, ON, Canada","2016 IEEE/ACM 5th International Workshop on Games and Software Engineering (GAS)","20170109","2016","","","33","36","A number of educational games have been created to help students programming. Many of these games focus on problem solving and the actual act of writing programs, while very few focus on programming comprehension. We introduce a serious game called Robot ON! aimed at players who have never programmed before. Unlike other serious programming games, Robot ON! focuses on comprehension rather than problem-solving challenges; players do not actually write any programs, but are instead given the task of demonstrating their knowledge and understanding of a program's behavior. Robot ON! includes tools that allow players to demonstrate understanding of variable values, data types, program statements, and control flow. We include an evaluation plan to assess Robot ON!'s playability, enjoyment, and benefits to program comprehension.","","Electronic:978-1-4503-4160-8; POD:978-1-5090-2203-8","10.1109/GAS.2016.014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809514","code review;code walkthrough;computer science;education;game-based learning;program comprehension;programming;serious games;software engineering","Educational robots;Games;Programming profession;Writing","computer aided instruction;computer science education;serious games (computing);software engineering","Robot ON!;educational games;program statements;programming comprehension;serious game;student programming;writing programs","","","","","","","","16-16 May 2016","","IEEE","IEEE Conferences"
"Integrated Formal Methods for Constructing Assurance Cases","C. C√¢rlan; T. A. Beyene; H. Ruess","fortiss-An-Inst., Tech. Univ. Munchen, Munich, Germany","2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","20161219","2016","","","221","228","The use of formal methods in verification activities is well established in various dedicated safety standards. Deficits in the verification process may have a negative impact on the confidence of verification results. Safety standards promote the use of integrated formal methods when a single method cannot achieve the verification objective without specifying how. In this paper, we take on the problem of using outputs from integrated formal methods as evidence in assurance cases, which are used in certification of safety-critical systems. We first present two workflows that employ integrated formal methods - code review workflow and code coverage workflow - corresponding to two of the most important activities of the verification phase. Then, we show how each workflow and the outputs from its integrated formal methods can be used in creating an assurance argument. These assurance arguments offer evidence for undeveloped goals identified in previous works from the field.","","Electronic:978-1-5090-3601-1; POD:978-1-5090-3602-8","10.1109/ISSREW.2016.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789404","assurance cases;assurance deficits;code coverage;code review;confidence;integrated formal methods;testing","Employment;Encoding;Safety;Software;Standards;Testing;Uncertainty","certification;formal verification;safety-critical software;software reviews","assurance argument;assurance case construction;certification;code coverage workflow;code review workflow;integrated formal methods;safety-critical systems;verification phase","","","","","","","","23-27 Oct. 2016","","IEEE","IEEE Conferences"
"Comparing pre commit reviews and post commit reviews using process simulation","T. Baum; F. Kortum; K. Schneider; A. Brack; J. Schauder","Leibniz Universit&#228;t Hannover, Hannover, Germany","Proceedings of the International Conference on Software and Systems Process","20161111","2016","","","26","35","<p>Code review in practice is often performed change-based, i.e. using the code changes belonging to a task to determine which code to review. In previous studies, it was found that two variations of this process are used in industry: Pre commit review (review-then-commit) and post commit review (commit-then-review). The choice for one of these variants has implications not only for practitioners deciding on a code review process to use, but also for the development of review tools and for experimentation with review processes. In some situations, a specific variant is clearly preferable due to the nature of the development process or team. In other situations, there are conflicting opinions, and both variants have proponents arguing for their method of choice. So we asked: Are there practically relevant performance differences between pre commit and post commit reviews, and how are these differences influenced by contextual factors? To assess this question, we designed a parametric discrete event simulation model of certain agile development processes. We validated this model with practitioner's feedback and in part also with empirical data from industry. Our analysis of the simulation results indicates that the best choice does depend on the context, but also that there are many situations with no practically relevant difference between both choices. We identified the main influencing factors and underlying effects and condensed our findings into heuristic rules.</p>","","","10.1145/2904354.2904362","","","Agile software development;code review;discrete event simulation;post commit review;pre commit review","","","","","","","","","","","14-22 May 2016","","ACM","ACM Conferences"
"Predicting Defectiveness of Software Patches","B. Soltanifar; A. Erdem; A. Bener","Data Science Lab, Ryerson, University, Canada","Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","20161111","2016","","","1","10","<p>Context: Software code review, as an engineering best practice, refers to the inspection of the code change in order to find possible defects and ensure change quality. Code reviews, however, may not guarantee finding the defects. Thus, there is a risk for a defective code change in a given patch, to pass the review process and be submitted.</p> <p>Goal: In this research, we aim to apply different machine learning algorithms in order to predict the defectiveness of a patch after being reviewed, at the time of its submission.</p> <p>Method: We built three models using three different machine learning algorithms: Logistic Regression, Na√É≈ïve Bayes, and Bayesian Network model. To build the models, we consider different factors involved in review process in terms of Product, Process and People (3P).</p> <p>Results: Our empirical results show that, Bayesian Networks is able to better predict the defectiveness of the changed code with 76% accuracy.</p> <p>Conclusions: Predicting defectiveness of change code is beneficial in making patch release decisions. The Bayesian Network model outperforms the others since it capturs the relationship among the factors in the review process.</p>","","","10.1145/2961111.2962601","","","Code Review Quality;Code review;Defect Prediction;Software Patch Defectiveness","","","","","","","","","","","8-9 Sept. 2016","","ACM","ACM Conferences"
"Identifying the characteristics of vulnerable code changes: an empirical study","A. Bosu; J. C. Carver; M. Hafiz; P. Hilley; D. Janni","University of Alabama, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","20161111","2014","","","257","268","<p> To focus the efforts of security experts, the goals of this empirical study are to analyze which security vulnerabilities can be discovered by code review, identify characteristics of vulnerable code changes, and identify characteristics of developers likely to introduce vulnerabilities. Using a three-stage manual and automated process, we analyzed 267,046 code review requests from 10 open source projects and identified 413 Vulnerable Code Changes (VCC). Some key results include: (1) code review can identify common types of vulnerabilities; (2) while more experienced contributors authored the majority of the VCCs, the less experienced contributors' changes were 1.8 to 24 times more likely to be vulnerable; (3) the likelihood of a vulnerability increases with the number of lines changed, and (4) modified files are more likely to contain vulnerabilities than new files. Knowing which code changes are more prone to contain vulnerabilities may allow a security expert to concentrate on a smaller subset of submitted code changes. Moreover, we recommend that projects should: (a) create or adapt secure coding guidelines, (b) create a dedicated security review team, (c) ensure detailed comments during review to help knowledge dissemination, and (d) encourage developers to make small, incremental changes rather than large changes. </p>","","","10.1145/2635868.2635880","","","code review;inspection;open source;security defects;vulnerability","","","","","7","","","","","","16-21 Nov. 2014","","ACM","ACM Conferences"
"Architecting an extensible framework for Gamifying Software Engineering concepts","S. K. Sripada; Y. R. Reddy; S. Khandelwal","IIIT-Hyderabad, Hyderabad, India","Proceedings of the 9th India Software Engineering Conference","20161111","2016","","","119","130","<p>Software engineering activities like code reviews, change management, knowledge management, issue tracking, etc. tend to be heavily process oriented. Gamification of such activities by composing the core activities with game design elements like badges and points can increase developers' interest in performing such activities. While there are various frameworks/applications that assist in gamification, extending the frameworks to add any/all desired game design elements has not been adequately addressed. In this paper, we propose an extensible architectural framework for gamification of software engineering activities where in the game design elements are modeled as services. We create an example instance of our framework by building a prototype for code review activity and note the challenges of designing such an extensible architectural framework. The example instance uses python's Flask micro framework and has five game design elements implemented as services, and exposed using restful APIs.</p>","","","10.1145/2856636.2856649","","","Architecture;Code review;Game Design Elements;Gamification;REST API;Services;Web Hooks","","","","","1","","","","","","18-20 Feb. 2016","","ACM","ACM Conferences"
"Software security: a study in technology transfer","G. E. McGraw","Cigital, Dulles, VA, USA","Proceedings of the companion publication of the 2014 ACM SIGPLAN conference on Systems, Programming, and Applications: Software for Humanity","20161111","2014","","","1","1","<p>Where do security technologies come from? Academics propose research and government (sometimes) funds it. Startups move technologies across the ""research valley of death"" to early adopters. Global corporations make technology widely available by acquiring startups. At every step there are gaps and pitfalls. Adoption is the acid test of innovation. Idea-generation is perhaps ten per cent of innovation; most of the work is on technology transfer and adoption. Chance plays a big role in creating opportunities (e.g., R&D involves a lot of luck), but a company's success depends on its ability to make opportunities more likely to occur, and to capitalize on those opportunities when they arise. Passionate individuals drive technology transfer more than does process; indeed, some people believe that the original researchers need to be involved all the way along the chain. Prototyping is an important practice, often resulting in ""researchware"" that proves a concept but is not ready for wide use. Transforming a prototype from the lab to the real-world is a multi-stage, multi-year undertaking.</p> <p>This talk will use the decade-long evolution of static analysis in code review as a driver for discussion. We'll talk startups, big companies, venture capital, research agencies, and subject-matter expertise. In general, technologists don't appreciate business people enough and business people don't appreciate technology enough. Most successful companies are brilliant at one, but also need to be adequate at the other.</p>","","","10.1145/2660252.2661745","","","code review;security;static analysis;technolgy adoption;technology transfer","","","","","1","","","","","","20-24 Oct. 2014","","ACM","ACM Conferences"
"The Sweep: Essential Examples for In-Flow Peer Review","J. G. Politz; J. M. Collard; A. Guha; K. Fisler; S. Krishnamurthi","Brown University, Providence, RI, USA","Proceedings of the 47th ACM Technical Symposium on Computing Science Education","20161111","2016","","","243","248","<p>In in-flow peer review, students provide feedback to one another on intermediate artifacts on their way to a final submission. Prior work has studied examples and tests as a potentially useful initial artifact for review. Unfortunately, large test suites are onerous to produce and especially to review. We instead propose the notion of a sweep, an artificially constrained set of tests that illustrates common and interesting behavior. We present experimental data across several courses that show that sweeps have reasonable quality, and are also a good target for peer review; for example, students usually (over half the time) suggest new tests to one another in a review.</p>","","","10.1145/2839509.2844626","","","code review;example-first programming;peer review;testing","","","","","","","","","","","2-5 March 2016","","ACM","ACM Conferences"
"Untangling development tasks with software developer's activity","M. Konopka; P. Navrat","Slovak University of Technology in Bratislava, Bratislava, Slovakia","Proceedings of the Second International Workshop on Context for Software Development","20161111","2015","","","13","14","<p>A combination of several activities is required to solve a development task, but in the end, developer reports only part of it. It is difficult to understand whether all committed files were changed because of the reason in a given description. Software developers work on multiple tasks at once and often fail to distinguish them with separate commits because of their unknowingness, as well as of limitations of the current tools for source code versioning. Our idea is to address this problem with identification of software developer's activities from a stream of interaction data in real time. We attempt to identify situations when a developer has worked on multiple tasks, to prevent him from tangling them in a single commit, or to aid him to separate certain activities from the task, e.g., floss refactoring.</p>","","","","","","code change;code review;composite change;developer activity;interaction data;tangled change;task context","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Convergent contemporary software peer review practices","P. C. Rigby; C. Bird","Concordia University, Canada","Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering","20160129","2013","","","202","212","<p> Software peer review is practiced on a diverse set of software projects that have drastically different settings, cultures, incentive systems, and time pressures. In an effort to characterize and understand these differences we examine two Google-led projects, Android and Chromium OS, three Microsoft projects, Bing, Office, and MS SQL, and projects internal to AMD. We contrast our findings with data taken from traditional software inspection conducted on a Lucent project and from open source software peer review on six projects, including Apache, Linux, and KDE. Our measures of interest include the review interval, the number of developers involved in review, and proxy measures for the number of defects found during review. We find that despite differences among projects, many of the characteristics of the review process have independently converged to similar values which we think indicate general principles of code review practice. We also introduce a measure of the degree to which knowledge is shared during review. This is an aspect of review practice that has traditionally only had experiential support. Our knowledge sharing measure shows that conducting peer review increases the number of distinct files a developer knows about by 66% to 150% depending on the project. This paper is one of the first studies of contemporary review in software firms and the most diverse study of peer review to date. </p>","","","10.1145/2491411.2491444","","","Empirical Software Engineering;Inspection;Open source software;Peer code review;Software firms","","","","","27","","","","","","18-26 Aug. 2013","","ACM","ACM Conferences"
"Process mining multiple repositories for software defect resolution from control and organizational perspective","M. Gupta; A. Sureka; S. Padmanabhuni","IIIT Delhi, India","Proceedings of the 11th Working Conference on Mining Software Repositories","20160129","2014","","","122","131","<p> Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities. </p>","","","10.1145/2597073.2597081","","","Empirical Software Engineering and Measurements;Issue Tracking System;Peer Code Review System;Process Mining;Social Network Analysis;Software Maintenance","","","","","2","","","","","","May 31 2014-June 1 2014","","ACM","ACM Conferences"
"CaptainTeach: a platform for in-flow peer review of programming assignments","J. G. Politz; S. Krishnamurthi; K. Fisler","Brown University, Providence, RI, USA","Proceedings of the 2014 conference on Innovation & technology in computer science education","20160129","2014","","","332","332","<p>Peer review is effective for teaching students to evaluate approaches to problems, fostering collaboration, and assessing other students' work. Peer review often happens after assignments are turned in, on complete artifacts that other students have created. We've been experimenting with a different style of peer review, which we call in-flow reviewing, in which programming assignments are broken into reviewable stages. After students complete each stage they review one anothers' work, allowing for feedback early on in the assignment. We've built a system, dubbed Captain Teach, for exploring in-flow reviewing for both programming and written assignments. In our demonstration and tutorial, we will show what the student experience looks like for a Captain Teach assignment, explain the interface that instructors have for creating assignments in Captain Teach, outline some of the mechanisms for anonymously assigning reviews and distributing feedback, and discuss future directions for the tool.</p>","","","10.1145/2591708.2602687","","","code review;peer review","","","","","","","","","","","21-25 June 2014","","ACM","ACM Conferences"
"How do software engineers understand code changes?: an exploratory study in industry","Y. Tao; Y. Dang; T. Xie; D. Zhang; S. Kim","The Hong Kong University of Science and Technology, Hong Kong, China","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","20160129","2012","","","1","11","<p>Software evolves with continuous source-code changes. These code changes usually need to be understood by software engineers when performing their daily development and maintenance tasks. However, despite its high importance, such change-understanding practice has not been systematically studied. Such lack of empirical knowledge hinders attempts to evaluate this fundamental practice and improve the corresponding tool support.</p> <p>To address this issue, in this paper, we present a large-scale quantitative and qualitative study at Microsoft. The study investigates the role of understanding code changes during software-development process, explores engineers' information needs for understanding changes and their requirements for the corresponding tool support. The study results reinforce our beliefs that understanding code changes is an indispensable task performed by engineers in software-development process. A number of insufficiencies in the current practice also emerge from the study results. For example, it is difficult to acquire important information needs such as a change's completeness, consistency, and especially the risk imposed by it on other software components. In addition, for understanding a composite change, it is valuable to decompose it into sub-changes that are aligned with individual development issues; however, currently such decomposition lacks tool support.</p>","","","10.1145/2393596.2393656","","","code change;code review;information needs;tool support","","","","","18","","","","","","11-16 Nov. 2012","","ACM","ACM Conferences"
"An Overview of Source Code Audit","X. Lingzi; L. Zhi","Nat. Eng. Res. Center of Inf. Security, Beijing, China","2015 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration","20160107","2015","","","26","29","Software vulnerability reports and reports of software exploitations continue to grow at an alarming rate in recent years. Many security issues are appeared in codes. The source code audit can improve the source code quality and avoid potential vulnerabilities in application system. This paper firstly expounded the principles of code audit and the purpose of Code audit is to make sure developers strictly follow the security technology, also briefly introducing the CERT secure coding standards which provide a detailed enumeration of coding errors that have caused vulnerabilities. Next, summarized the audit methods and techniques and compared the analysis tools for source code audit, then, show the value and significance of code audit. Finally, the development trend of audit technology is estimated.","","Electronic:978-1-4673-8312-7; POD:978-1-4673-8313-4","10.1109/ICIICII.2015.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373782","information security;source code review;static analysis","Encoding;Java;Safety;Security;Software;Standards","program diagnostics;security of data;software reliability;source code (software)","CERT secure coding standard;audit technology;coding error;security issue;security technology;software exploitation;software vulnerability report;source code audit;source code quality","","","","15","","","","3-4 Dec. 2015","","IEEE","IEEE Conferences"
"Using a case study to teach students about finding and fixing logic flaws in software","L. Simpkins; X. Yuan; H. Yu; K. Williams","Computer Science Department, North Carolina A&T State University, Greensboro, NC, USA","2015 IEEE Frontiers in Education Conference (FIE)","20151207","2015","","","1","7","An application logic flaw is a type of software vulnerability related to privilege manipulation or transaction control manipulation. They are often difficult to identify using automated scanners. A case study on the eCommerce merchant software Bigcommerce, integrated with PayPal Express as a third party payment collector, was created to teach students about this topic. Case studies provide students with a real-world context, and help them understand complex topics better than traditional teaching methods. However, the computer science field, especially computer security, does not have many case studies available. The case study on logic flaws in software was taught in Spring 2015, and the teaching experience is discussed.","","Paper:978-1-4799-8454-1","10.1109/FIE.2015.7344155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344155","application logic flaws;case study;computing education;manual code review method;software security","Computer science;Education;Fault diagnosis;Manuals;Security;Software;Testing","computer science education;program debugging;security of data;teaching","Bigcommerce;PayPal Express;application logic flaw;automated scanners;computer science field;computer security;ecommerce merchant software;logic flaw fixing;software vulnerability;student teaching;transaction control manipulation","","","","40","","","","21-24 Oct. 2015","","IEEE","IEEE Conferences"
"Who should review this change?: Putting text and file location analyses together for more accurate recommendations","X. Xia; D. Lo; X. Wang; X. Yang","College of Computer Science and Technology, Zhejiang University, Hangzhou, China","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20151123","2015","","","261","270","Software code review is a process of developers inspecting new code changes made by others, to evaluate their quality and identify and fix defects, before integrating them to the main branch of a version control system. Modern Code Review (MCR), a lightweight and tool-based variant of conventional code review, is widely adopted in both open source and proprietary software projects. One challenge that impacts MCR is the assignment of appropriate developers to review a code change. Considering that there could be hundreds of potential code reviewers in a software project, picking suitable reviewers is not a straightforward task. A prior study by Thongtanunam et al. showed that the difficulty in selecting suitable reviewers may delay the review process by an average of 12 days. In this paper, to address the challenge of assigning suitable reviewers to changes, we propose a hybrid and incremental approach Tie which utilizes the advantages of both Text mIning and a filE location-based approach. To do this, Tie integrates an incremental text mining model which analyzes the textual contents in a review request, and a similarity model which measures the similarity of changed file paths and reviewed file paths. We perform a large-scale experiment on four open source projects, namely Android, OpenStack, QT, and LibreOffice, containing a total of 42,045 reviews. The experimental results show that on average Tie can achieve top-1, top-5, and top-10 accuracies, and Mean Reciprocal Rank (MRR) of 0.52, 0.79, 0.85, and 0.64 for the four projects, which improves the state-of-the-art approach RevFinder, proposed by Thongtanunam et al., by 61%, 23%, 8%, and 37%, respectively.","","Electronic:978-1-4673-7532-0; USB:978-1-4673-7531-3","10.1109/ICSM.2015.7332472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332472","Modern Code Review;Path Similarity;Recommendation System;Text Mining","Accuracy;Analytical models;Computational modeling;Control systems;Feature extraction;Software;Text mining","configuration management;data mining;public domain software;text analysis","Android;LibreOffice;MCR;MRR;OpenStack;QT;TIE;filE location-based approach;mean reciprocal rank;modern code review;open source;software code review;software project;text mining model;textual content analysis;version control system","","6","","36","","","","Sept. 29 2015-Oct. 1 2015","","IEEE","IEEE Conferences"
"A New Framework of Security Vulnerabilities Detection in PHP Web Application","Z. Jingling; G. Rulin","Nat. Eng. Lab. for Mobile Network Security, Beijing Univ. of Posts & Telecommun., Beijing, China","2015 9th International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing","20151001","2015","","","271","276","Nowadays, Web applications provide us most of the Internet services, but also give birth to more and more new types of Internet applications. While, according to the developers' programming techniques and safety awareness, there are many kinds of Web application security flaws and vulnerabilities hiding in the program. So it is very important to improve their reliability and security. Usually people use code review based on static or dynamic analysis to detect security vulnerabilities, but each method has shortcomings that can't overcome easily which can result in a big number of false positives and omission. To address this issue, this paper proposed a new framework of detecting security vulnerabilities of PHP web application. In this framework, we combine dynamic and static analysis to make full use of the advantages of the two, greatly improve the efficiency of detection. An implementation based on this framework has also been completed and it will also be presented in the paper.","","CD-ROM:978-1-4799-8872-3; Electronic:978-1-4799-8873-0; POD:978-1-4799-8874-7","10.1109/IMIS.2015.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284959","PHP code review;code audit;dynamic analysis;static analysis","Data structures;Indexes;Security;Testing;Uniform resource locators;Virtual machining","Internet;telecommunication security","Internet applications;Internet services;PHP Web application;code review;dynamic analysis;safety awareness;security flaws;security vulnerabilities detection;static analysis","","","","12","","","","8-10 July 2015","","IEEE","IEEE Conferences"
"Empirical Metadata Maintenance in Source Code Development Process","K. Rastocny; M. Bielikova","Fac. of Inf. & Inf. Technol., Slovak Univ. of Technol. in Bratislava, Bratislava, Slovakia","2015 4th Eastern European Regional Conference on the Engineering of Computer Based Systems","20150928","2015","","","25","31","A management of software development process is a crucial part of the software engineering, from which the success of software projects is dependent. This management mostly relays upon quality and freshness of software metrics, especially empirical software metrics. But empirical software metrics are sensitive to source code modifications and also to developers' activities. In this paper we proposed approach for a maintenance of empirical metadata stored in information tags. The approach covers main maintenance actions - creating missing, repairing invalidated and removing unrepairable information tags. The maintenance is provided via tagging rules executed after matching predefined templates in a stream of developers' actions. We also introduces a set of information tags for supporting software development process and we describe their utilization in teaching a subject Team project.","","Electronic:978-1-4673-7967-0; POD:978-1-4673-7968-7","10.1109/ECBS-EERC.2015.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275222","code review;developers' activities;empirical metadata;information tag;metadata management;software metrics","Generators;Maintenance engineering;Resource description framework;Semantics;Software;Software metrics;Tagging","meta data;software development management;software maintenance;software metrics;software quality","empirical metadata maintenance;maintenance actions;software development process management;software engineering;software metrics;source code development process;subject team project;tagging rules;unrepairable information tags","","","","20","","","","27-28 Aug. 2015","","IEEE","IEEE Conferences"
"Virtual Reality in Software Engineering: Affordances, Applications, and Challenges","A. Elliott; B. Peiris; C. Parnin","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","2","","547","550","Software engineers primarily interact with source code using a keyboard and mouse, and typically view software on a small number of 2D monitors. This interaction paradigm does not take advantage of many affordances of natural human movement and perception. Virtual reality (VR) can use these affordances more fully than existing developer environments to enable new creative opportunities and potentially result in higher productivity, lower learning curves, and increased user satisfaction. This paper describes the affordances offered by VR, demonstrates the benefits of VR and software engineering in prototypes for live coding and code review, and discusses future work, open questions, and the challenges of VR.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203009","code review;live coding;software engineering;virtual reality","Cognition;Encoding;Keyboards;Navigation;Software;Software engineering;Three-dimensional displays","software engineering;software reviews;source code (software);virtual reality","VR;code review;live coding;software engineering;source code;virtual reality","","3","","21","","","","16-24 May 2015","","IEEE","IEEE Conferences"
"Untangling Development Tasks with Software Developer's Activity","M. Konopka; P. Navrat","Fac. of Inf. & Inf. Technol., Slovak Univ. of Technol. in Bratislava, Bratislava, Slovakia","2015 IEEE/ACM 2nd International Workshop on Context for Software Development","20150813","2015","","","13","14","A combination of several activities is required to solve a development task, but in the end, developer reports only part of it. It is difficult to understand whether all committed files were changed because of the reason in a given description. Software developers work on multiple tasks at once and often fail to distinguish them with separate commits because of their unknowingness, as well as of limitations of the current tools for source code versioning. Our idea is to address this problem with identification of software developer's activities from a stream of interaction data in real time. We attempt to identify situations when a developer has worked on multiple tasks, to prevent him from tangling them in a single commit, or to aid him to separate certain activities from the task, e.g., Floss refactoring.","","Electronic:978-1-4673-7037-0; POD:978-1-4673-7038-7","10.1109/CSD.2015.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181499","Tangled change;code change;code review;composite change;developer activity;interaction data;task context","Context;Hidden Markov models;Monitoring;Real-time systems;Software;Switches;Testing","software engineering","software developers;source code versioning","","2","","8","","","","19-19 May 2015","","IEEE","IEEE Conferences"
"Will They Like This? Evaluating Code Contributions with Language Models","V. J. Hellendoorn; P. T. Devanbu; A. Bacchelli","SORCERERS @ Software Eng. Res. Group, Delft Univ. of Technol., Delft, Netherlands","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","157","167","Popular open-source software projects receive and review contributions from a diverse array of developers, many of whom have little to no prior involvement with the project. A recent survey reported that reviewers consider conformance to the project's code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively evaluate the existence and effects of this phenomenon. To this aim we use language models, which were shown to accurately capture stylistic aspects of code. We find that rejected change sets do contain code significantly less similar to the project than accepted ones, furthermore, the less similar change sets are more likely to be subject to thorough review. Armed with these results we further investigate whether new contributors learn to conform to the project style and find that experience is positively correlated with conformance to the project's code style.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180076","code review;language model;pull request","Context;Context modeling;Data mining;Entropy;Java;Mathematical model;Software","public domain software;software engineering","Github;code contributions;language models;open-source software projects;software development","","4","","35","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Process Cube for Software Defect Resolution","M. Gupta; A. Sureka","Indraprastha Inst. of Inf. Technol. - Delhi (IIITD), New Delhi, India","2014 21st Asia-Pacific Software Engineering Conference","20150423","2014","1","","239","246","Online Analytical Processing (OLAP) cube is a multi-dimensional dataset used for analyzing data in a Data Warehouse (DW) for the purpose of extracting actionable intelligence. Process mining consists of analyzing event log data produced from Process Aware Information Systems (PAIS) for the purpose of discovering and improving business processes. Process cube is a concept which falls at the intersection of OLAP cube and process mining. Process cube facilitates process mining from multiple-dimensions and enables comparison of process mining results across various dimensions. We present an application of process cube to software defect resolution process to analyze and compare process data from a multi-dimensional perspective. We present a framework, a novel perspective to mine software repositories using process cube. Each cell of process cube is defined by metrics from multiple process mining perspectives like control flow, time, conformance and organizational perspective. We conduct a case-study on Google Chromium project data in which the software defect resolution process spans three software repositories: Issue Tracking System (ITS), Peer Code Review System (PCR) and Version Control System (VCS). We define process cube with 9 dimensions as issue report timestamp, priority, state, closed status, OS, component, bug type, reporter and owner. We define hierarchies along various dimensions and cluster members to handle sparsity. We apply OLAP cube operations such as slice, dice, roll-up and drill-down, and create materialized sub log for each cell. We demonstrate the solution approach by discovering process map and compare process mining results from Control Flow and Time perspective for Performance and Security issues.","1530-1362;15301362","Electronic:978-1-4799-7426-9; POD:978-1-4799-7427-6","10.1109/APSEC.2014.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091316","Empirical Software Engineering;Issue Tracking System;Mining Software Repositories;OLAP;Peer Code Review System;Process Cube;Process Mining;Version Control System","Chromium;Data mining;Google;Measurement;Process control;Security;Software","configuration management;data mining;data warehouses;program debugging;software engineering;software maintenance","DW;Google Chromium project data;ITS;OLAP cube operations;PAIS;PCR;VCS;actionable intelligence extraction;bug type;conformance perspective;control flow;data warehouse;event log data analysis;issue report OS;issue report closed status;issue report component;issue report priority;issue report state;issue report timestamp;issue tracking system;multidimensional dataset;online analytical processing cube;organizational perspective;owner;peer code review system;performance issues;process aware information systems;process mining;reporter;security issues;software defect resolution process;software repositories;version control system","","2","","16","","","","1-4 Dec. 2014","","IEEE","IEEE Conferences"
"Assessing MCR Discussion Usefulness Using Semantic Similarity","T. Pangsakulyanont; P. Thongtanunam; D. Port; H. Iida","Kasetsart Univ., Bangkok, Thailand","2014 6th International Workshop on Empirical Software Engineering in Practice","20141206","2014","","","49","54","Modern Code Review (MCR) is an informal practice whereby reviewers virtually discuss proposed changes by adding comments through a code review tool or mailing list. It has received much research attention due to its perceived cost-effectiveness and popularity with industrial and OSS projects. Recent studies indicate there is a positive relationship between the number of review comments and code quality. However, little research exists investigating how such discussion impacts software quality. The concern is that the informality of MCR encourages a focus on trivial, tangential, or unrelated issues. Indeed, we have observed that such comments are quite frequent and may even constitute the majority. We conjecture that an effective MCR actually depends on having a substantive quantity of comments that directly impact a proposed change (or are ""useful""). To investigate this, a necessary first step requires distinguishing review comments that are useful to a proposed change from those that are not. For a large OSS projects such as our Qt case study, manual assessment of the over 72,000 comments is a daunting task. We propose to utilize semantic similarity as a practical, cost efficient, and empirically assurable approach for assisting with the manual usefulness assessment of MCR comments. Our case study results indicate that our approach can classify comments with an average F-measure score of 0.73 and reduce comment usefulness assessment effort by about 77%.","","Electronic:978-1-4799-6666-0; POD:978-1-4799-7139-8","10.1109/IWESEP.2014.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976022","Modern Code Review;Software Quality;Text Mining","Data models;Manuals;Semantics;Software quality;Standards;Training;Training data","data mining;semantic networks;software quality","F-measure score;MCR comments;MCR discussion usefulness;OSS projects;code quality;code review tool;comment usefulness assessment;cost effectiveness;mailing list;modern code review;semantic similarity;software quality","","7","","10","","","","12-13 Nov. 2014","","IEEE","IEEE Conferences"
"A tool for authoring programs that automatically distribute feedback to novice programmers","M. Ichinco; Y. Dosouto; C. Kelleher","Dept. of Comput. Sci. & Eng., Washington Univ. in St. Louis, St. Louis, MO, USA","2014 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","20140828","2014","","","207","208","One way to provide feedback to independent novice programmers is by leveraging experienced programmers as code reviewers. To provide this feedback at a large scale, experienced programmers can author heuristic programs, or rules, that automatically determine whether a novice program should receive certain feedback. This work presents the lessons learned from designing a tool to enable rule authoring.","1943-6092;19436092","Electronic:978-1-4799-4035-6; POD:978-1-4799-4034-9","10.1109/VLHCC.2014.6883058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6883058","code review;crowdsourcing;novice programming","Documentation;Educational institutions;Programming;Programming environments;Standards;Syntactics;Testing","authoring systems;computer science education;programming","authoring program tool;automatic feedback distribution;code reviewers;experienced programmers;heuristic programs;novice programmers","","0","","3","","","","July 28 2014-Aug. 1 2014","","IEEE","IEEE Conferences"
"The Secret Life of Patches: A Firefox Case Study","O. Baysal; O. Kononenko; R. Holmes; M. W. Godfrey","David R. Cheriton Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada","2012 19th Working Conference on Reverse Engineering","20121220","2012","","","447","455","The goal of the code review process is to assess the quality of source code modifications (submitted as patches) before they are committed to a project's version control repository. This process is particularly important in open source projects to ensure the quality of contributions submitted by the community, however, the review process can promote or discourage these contributions. In this paper, we study the patch lifecycle of the Mozilla Fire fox project. The model of a patch lifecycle was extracted from both the qualitative evidence of the individual processes (interviews and discussions with developers), and the quantitative assessment of the Mozilla process and practice. We contrast the lifecycle of a patch in pre- and post-rapid release development. A quantitative comparison showed that while the patch lifecycle remains mostly unchanged after switching to rapid release, the patches submitted by casual contributors are disproportionately more likely to be abandoned compared to core contributors. This suggests that patches from casual developers should receive extra care to both ensure quality and encourage future community contributions.","1095-1350;10951350","Electronic:978-0-7695-4891-3; POD:978-1-4673-4536-1","10.1109/WCRE.2012.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385140","Open source software;code review;patch lifecycle","Communities;Computer bugs;Educational institutions;Linux;Open source software;Process control;Switches","online front-ends;public domain software;software quality","Firefox case study;Mozilla Firefox project;code review process;contribution quality;open source projects;patch lifecycle;post-rapid release development;pre rapid release development;source code modification quality assessment;version control repository","","14","","15","","","","15-18 Oct. 2012","","IEEE","IEEE Conferences"
"Software certification - coding, code, and coders","K. Havelund; G. J. Holzmann","Laboratory for Reliable Software (LaRS), Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, 91109-8099, USA","2011 Proceedings of the Ninth ACM International Conference on Embedded Software (EMSOFT)","20111031","2011","","","205","210","We describe a certification approach for software development that has been adopted at our organization. JPL develops robotic spacecraft for the exploration of the solar system. The flight software that controls these spacecraft is considered to be mission critical. We argue that the goal of a software certification process cannot be the development of ‚Äúperfect‚Äù software, i.e., software that can be formally proven to be correct under all imaginable and unimaginable circumstances. More realistically, the goal is to guarantee a software development process that is conducted by knowledgeable engineers, who follow generally accepted procedures to control known risks, while meeting agreed upon standards of workmanship. We target three specific issues that must be addressed in such a certification procedure: the coding process, the code that is developed, and the skills of the coders. The coding process is driven by standards. The code is mechanically checked against the standards with the help of state-of-the-art static source code analyzers. The coders, finally, are certified in on-site training courses that include formal exams.","","Electronic:978-1-4503-0714-7; POD:978-1-4503-0714-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064527","Coding standards;code review;logic model checking;safety- and mission-critical software.;static source code analysis;unit testing","Certification;Encoding;Programming;Software;Software reliability;Standards","program testing;program verification;software reliability","coder;coding process;software certification;software development process;static source code analyzer","","0","","11","","","","9-14 Oct. 2011","","IEEE","IEEE Conferences"
"Technology Transfer: A Software Security Marketplace Case Study","G. McGraw","Cigital","IEEE Software","20110818","2011","28","5","9","11","This paper presents the software security (application security) solutions. It is an idea of engineering software so that it continues to function correctly under malicious attack. Although as a discipline software security is relatively young, much progress has been made on ways to integrate security best practices into the software development life cycle. Microsoft, for example, has helped spearhead soft ware security through its Trustworthy Computing Initiative and the resulting Security Development Lifecycle (SDL). Cigital has also been instrumental in bringing software security to the wider market through its professional services.","0740-7459;07407459","","10.1109/MS.2011.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984788","code review;software security;technology transfer","Best practices;Computer security;Software engineering;Technology transfer","security of data;software engineering;technology transfer","Cigital;Microsoft;computer security;malicious attack;software development life cycle;software engineering;software security;technology transfer;trustworthy computing initiative","","1","","2","","","","Sept.-Oct. 2011","","IEEE","IEEE Journals & Magazines"
"Does studio-based instruction work in CS 1?: an empirical comparison with a traditional approach","C. Hundhausen; A. Agrawal; D. Fairbrother; M. Trevisan","Washington State University, Pullman, WA, USA","Proceedings of the 41st ACM technical symposium on Computer science education","20110708","2010","","","500","504","<p>Given the increasing importance of communication, teamwork, and critical thinking skills in the computing profession, we believe there is good reason to provide students with increased opportunities to learn and practice those skills in undergraduate computing courses. Toward that end, we have been exploring <i>studio-based</i> instructional methods, which have been successfully employed in architecture and fine arts education for over a century. We have developed an adaptation of studio-based instruction for computing education called the <i>pedagogical code review</i>, which is modeled after the code inspection process used in the software industry. To evaluate its effectiveness, we carried out a quasi-experimental comparison of a ""studio-based"" CS 1 course with pedagogical code reviews and an identical ""traditional"" CS 1 course without pedagogical code reviews. We found no learning outcome differences between the two courses; however, we did observe two interesting attitudinal trends: (a) self-efficacy decreased more in the traditional course than in the studio-based course; and (b) peer learning decreased in the traditional course, but increased in the studio-based course. Additional questionnaire and interview data provide further evidence of the positive impact of studio-based instruction.</p>","","","10.1145/1734263.1734432","","","code inspection;cs1;pedagogical code review;peer review;studio-based learning and instruction","","","","","1","","","","","","10-13 March 2010","","ACM","ACM Conferences"
"Empirical software evolvability - code smells and human evaluations","M. V. M√°ntyl√°","SoberIT, Department of Computer Science, School of Science and Technology, Aalto University, P.O. Box 19210, FI-00760, Finland","2010 IEEE International Conference on Software Maintenance","20101025","2010","","","1","6","Low software evolvability may increase costs of software development for over 30%. In practice, human evaluations and discoveries of software evolvability dictate the actions taken to improve the software evolvability, but the human side has often been ignored in prior research. This dissertation synopsis proposes a new group of code smells called the solution approach, which is based on a study of 563 evolvability issues found in industrial and student code reviews. Solution approach issues require re-thinking of the existing implementation rather than just reorganizing the code through refactoring. This work also contributes to the body of knowledge about software quality assurance practices by confirming that 75% of defects found in code reviews affect software evolvability rather than functionality. We also found evidence indicating that context-specific demographics, i.e., role in organization and code ownership, affect evolvability evaluations, but general demographics, i.e., work experience and education, do not.","1063-6773;10636773","Electronic:978-1-4244-8629-8; POD:978-1-4244-8630-4; USB:978-1-4244-8628-1","10.1109/ICSM.2010.5609545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609545","Doctoral dissertation synopsis;code review;code smells;empirical study;human evaluation;software maintainability","Book reviews;Humans;Silicon;Software","program diagnostics;software performance evaluation;software prototyping;software quality","code smells;empirical software evolvability;human evaluations;refactoring;software development;software quality assurance","","0","","19","","","","12-18 Sept. 2010","","IEEE","IEEE Conferences"
"Guest editors' introduction: Software Assurance for the Masses","B. Chess; C. Wysopal","","IEEE Security & Privacy","20120626","2012","10","3","14","15","The guest editors of this special theme issue describe how they selected articles from a wide variety of static analysis experts from research teams, academia, government, and commercial software companies. The broad spectrum of ideas covered range from the practicality of building and making static analysis tools usable in a major software company to ways of cataloging the problem space of vulnerabilities in software.","1540-7993;15407993","","10.1109/MSP.2012.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226539","code review;software development;static analysis","Quality assurance;Software quality;Special issues and sections","","","","0","","","","","","May-June 2012","","IEEE","IEEE Journals & Magazines"
"Automatically recommending code reviewers based on their expertise: An empirical comparison","C. Hannebauer; M. Patalas; S. St√ºnkelt; V. Gruhn","paluno - The Ruhr Institute for Software Technology, University of Duisburg-Essen, Germany","2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)","20161006","2016","","","99","110","Code reviews are an essential part of quality assurance in Free, Libre, and Open Source Software (FLOSS) projects. However, finding a suitable reviewer can be difficult, and delayed or forgotten reviews are the consequence. Automating reviewer selection with suitable algorithms can mitigate this problem. We compare empirically six algorithms based on modification expertise and two algorithms based on review expertise on four major FLOSS projects. Our results indicate that the algorithms based on review expertise yield better recommendations than those based on modification expertise. The algorithm Weighted Review Count (WRC) recommends at least one out of five reviewers correctly in 69 % to 75 % of all cases, which is one of the best results achieved in the comparison.","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582749","Code reviewer recommendation;code reviews;expertise metrics;issue tracker;open source;patches;recommendation system","Algorithm design and analysis;History;Machine learning algorithms;Measurement;Prediction algorithms;Software;Software algorithms","project management;public domain software;software quality","FLOSS projects;WRC;code reviewer automatic recommendation;free, libre, and open source software projects;modification expertise;quality assurance;review expertise;reviewer selection automation;weighted review count","","","","","","","","3-7 Sept. 2016","","IEEE","IEEE Conferences"
"The promises and perils of mining GitHub","E. Kalliamvakou; G. Gousios; K. Blincoe; L. Singer; D. M. German; D. Damian","University of Victoria, Canada","Proceedings of the 11th Working Conference on Mining Software Repositories","20160129","2014","","","92","101","<p> With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub. </p>","","","10.1145/2597073.2597074","","","Mining software repositories;bias;code reviews;git;github","","","","","43","","","","","","May 31 2014-June 1 2014","","ACM","ACM Conferences"
"Incorporating real world non-coding features into block IDEs","M. Streeter","Georgia Institute of Technology, Atlanta, GA","2015 IEEE Blocks and Beyond Workshop (Blocks and Beyond)","20160104","2015","","","103","104","We often see block-based coding environments as toy environments that let novice programmers have fun as they learn the basics of programming. While these environments do have an engaging low floor, they are missing out on other aspects of introductory programming that could further engage students and better replicate the real work of software developers. In this paper, I describe real world non-coding features (e.g. collaboration, code reviews, version control, aesthetic appeal, galleries/community) that should be incorporated into block IDEs. There are likely other important non-coding features that are not included.","","Electronic:978-1-4673-8367-7; POD:978-1-4673-8368-4","10.1109/BLOCKS.2015.7369013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7369013","aesthetic appeal;blocks;code reviews;collaboration;galleries;novice;programming;version control","Collaboration;Encoding;Floors;Programming profession;Software","computer science education;programming;software engineering;source code (software)","block IDE;block-based coding environment;noncoding feature;novice programmer;programming education;software developer","","","","","","","","22-22 Oct. 2015","","IEEE","IEEE Conferences"
"Code Comprehension Activities in Undergraduate Software Engineering Course - A Case Study","S. K. Sripada; Y. R. Reddy","Int. Inst. of Inf. Technol., Hyderabad, India","2015 24th Australasian Software Engineering Conference","20151228","2015","","","68","77","In industry, inspections, reviews, and refactoring are considered as necessary software engineering activities for enhancing quality of code. In academia, such activities are rarely taught and practiced at Undergraduate level due to various reasons like limited skill set, limited knowledge of the available tools, time constraints, project setting, project client availability, flexibility with Syllabus, etc. However, we argue that such activities are an essential part of introductory software engineering courses and can result in improvement of coding skills, knowledge of coding standard and compliance to the same, and peer communication within teams. We have studied the use of such activities in a sophomore level Software Engineering course consisting of more than 200 students working in teams on projects from start-ups and present our experiences, findings and challenges. We present the results of quantitative evaluation of the impact of code comprehension activities before and after each iteration of the team projects.","1530-0803;15300803","Electronic:978-1-4673-9390-4; POD:978-1-4673-9391-1","10.1109/ASWEC.2015.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365795","Code Inspection;Code Reviews;Program Comprehension;Refactoring;Static Code Analysis;Textual Analysis","Encoding;Inspection;Measurement;Software;Software engineering;Standards;Writing","computer science education;educational courses;further education;software engineering","code comprehension activities;introductory software engineering courses;undergraduate software engineering course","","","","23","","","","Sept. 28 2015-Oct. 1 2015","","IEEE","IEEE Conferences"
"A hybrid approach to code reviewer recommendation with collaborative filtering","Z. Xia; H. Sun; J. Jiang; X. Wang; X. Liu","State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China 100191","2017 6th International Workshop on Software Mining (SoftwareMining)","20171109","2017","","","24","31","Code review is known to be of paramount importance for software quality assurance. However, finding a reviewer for certain code can be very challenging in Modern Code Review environment due to the difficulty of learning the expertise and availability of candidate reviewers. To tackle this problem, existing efforts mainly concern how to model a reviewer's expertise with the review history, and making recommendation based on how well a reviewer's expertise can meet the requirement of a review task. Nonetheless, as there are both explicit and implicit relations in data that affect whether a reviewer is suitable for a given task, merely modeling review expertise with explicit relations often fails to achieve expected recommendation accuracy. To that end, we propose a recommendation algorithm that takes implicit relations into account. Furthermore, we utilize a hybrid approach that combines latent factor models and neighborhood methods to capture implicit relations. Finally, we have conducted extensive experiments by comparing with the state-of-the-art methods using the data of 5 popular GitHub projects. The results demonstrate that our approach outperforms the comparing methods for all top-k recommendations and reaches a 15.3% precision promotion in top-1 recommendation.","","Electronic:978-1-5386-1389-4; POD:978-1-5386-1390-0","10.1109/SOFTWAREMINING.2017.8100850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100850","Code reviewer;collaborative filtering;github","Algorithm design and analysis;Collaboration;Data models;Feature extraction;Filtering;Software","collaborative filtering;recommender systems;software quality","candidate reviewers;code reviewer recommendation;hybrid approach;modern code review environment;recommendation accuracy;recommendation algorithm;review history;software quality assurance","","","","","","","","3-3 Nov. 2017","","IEEE","IEEE Conferences"
"Graph Data Management of Evolving Dependency Graphs for Multi-versioned Codebases","O. Goonetilleke; D. Meibusch; B. Barham","Oracle Labs., Brisbane, QLD, Australia","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20171107","2017","","","574","583","FrappeÃÅ is a code comprehension tool developed by Oracle Labs that extracts the code dependencies from a codebase and stores them in a graph database enabling advanced comprehension tasks. In addition to traditional text-based queries, such context-sensitive tools allow developers to express navigational queries of the form Does function X or something it calls write to global variable Y? providing more insight into the underlying codebases. Frappe captures the dependencies based on the most recent snapshot of the codebase.In this work we focus on the challenges associated with the management of multiple source code revisions, and investigate strategies to enable advanced code comprehension when the underlying codebase evolves over time. To find the deltas, we detail how entities can be resolved across versions, and propose a model for representing evolving dependency graphs. Our versioned graphs are built using snapshots of large codebases in the order of 13 million lines of code.We show growth and storage benefits of versioned graphs compared to independently storing individual snapshots. We also demonstrate how existing Frappe queries can be executed on versioned graphs and new queries can retrieve a history of changes in a function for a code review use case.","","Electronic:978-1-5386-0992-7; POD:978-1-5386-0993-4","10.1109/ICSME.2017.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094463","entity resolution;evolving dependency graphs;graph database;versioned codebase","Buildings;Data mining;Databases;History;Navigation;Tools","graph theory;query processing;software maintenance;software tools;text analysis","Frappe queries;Oracle Labs;code comprehension tool;code dependencies;code review;comprehension tasks;context-sensitive tools;graph data management;graph database;multiple source code revisions;navigational queries;text-based queries;versioned graphs","","","","","","","","17-22 Sept. 2017","","IEEE","IEEE Conferences"
"From Smashed Screens to Smashed Stacks: Attacking Mobile Phones Using Malicious Aftermarket Parts","O. Shwartz; G. Shitrit; A. Shabtai; Y. Oren","Ben-Gurion Univ., Beer-Sheva, Israel","2017 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","20170703","2017","","","94","98","In this preliminary study we present the first practical attack on a modern smartphone which is mounted through a malicious after market replacement part (specifically, a replacement touchscreen). Our attack exploits the lax security checks on the packets traveling between the touch screen's embedded controller and the phone's main CPU, and isable to achieve kernel-level code execution privileges on modern Android phones protected by SELinux. This attack is memory independent and survives data wipes and factory resets. We evaluate two phones from major vendors and present a proof-of-concept attack in actual hardware on one phone and an emulation level attack on the other. Through a semi-automated source code review of 26 recent Android phones from 8 different vendors, we believe that ourattack vector can be applied to many other phones, and that it is very difficult to protect against. Similar attacks should also be possible on other smart devices such as printers, cameras and cars, which similarly contain user-replaceable sub-units.","","Electronic:978-1-5386-2244-5; POD:978-1-5386-2245-2","10.1109/EuroSPW.2017.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966977","android;cyber security;driver;hardware security;smarthphone","Europe;Hardware;Kernel;Privacy;Security;Smart phones","Linux;security of data;smart phones","SELinux;kernel-level code execution privileges;mobile phones;modern Android phones;modern smartphone;semi-automated source code review","","","","","","","","26-28 April 2017","","IEEE","IEEE Conferences"
"Examination of Coding Violations Focusing on Their Change Patterns over Releases","A. E. Burhandenny; H. Aman; M. Kawahara","Grad. Sch. of Sci. & Eng., Ehime Univ., Matsuyama, Japan","2016 23rd Asia-Pacific Software Engineering Conference (APSEC)","20170403","2016","","","121","128","Code review is an essential activity to ensure the quality of code being developed, and there have been static code checkers for aiding an effective code review. However, such tools have not been actively utilized in the world of programmers due to a lot of coding violations (warning) produced by tools and their false-positiveness. In order to analyze the automatically pointed violations and the actual attentions which programmers paid to those violations, this paper proposes a novel metric-the Index of Programmers' Attention (IPA)-and conducts an empirical study focusing on the change patterns of violations over the releases of popular seven open source software products, under two research questions (RQs): (RQ1) What kind of coding violations are related to the parts that many programmers tend to improve? and what kind of coding violations are likely to be disregarded?; (RQ2) How can we reduce the meaningless violations for programmers by omitting disregarded coding violations? The empirical results showed the following findings: (1) important violations (having high IPA values) may vary from project to project; (2) there are some unimportant violations common to different projects, but they are a minority of automatically detected violations (about 12%). Therefore, while many violations may be made by a code checker, most of them are likely to be worthy in improving the code quality, and it is ineffective to reduce the violations by eliminating such unimportant violations.","1530-1362;15301362","Electronic:978-1-5090-5575-3; POD:978-1-5090-5576-0","10.1109/APSEC.2016.027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890579","Code Checker;Programmer Attention;Trend Analysis;Violation","Aerodynamics;Computer bugs;Data collection;Encoding;Java;Market research;Testing","public domain software;software quality;software reviews;source code (software)","IPA;change pattern;code quality;code review;coding violation;index of programmers attention;open source software product release","","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conferences"
"Which review feedback did long-term contributors get on OSS projects?","T. Norikane; A. Ihara; K. Matsumoto","Graduate School of Information Science, Nara Institute of Science and Technology, JAPAN","2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20170323","2017","","","571","572","Open Source Software (OSS) cannot exist without contributions from the community. In particular, long-term contributors (LTCs) (e.g., committer), defined as contributors who spend at least one year on OSS projects, play crucial role in a project success because they would have permission to add (commit) code changes to a project's version control system, and to become a mentor for a beginner in OSS projects. However, contributors often leave a project before becoming a LTC because most contributors are volunteers. If contributors are motivated in their work in OSS projects, they might not leave the projects. In this study, we examine the phenomena involved in becoming a LTC in terms of motivation to continue in OSS projects. In particular, our target motivation is to understand what is involved in long-term contribution with other expert contributors. We study classifier to identify a LTC who will contribute patch submissions for more than one year based on collaboration in terms of the code review process. In detail, we analyze what review feedbacks encourage a contributor to continue with OSS project. Using a Qt project dataset, we understand review feedback which affected contribution period of the developer.","","Electronic:978-1-5090-5501-2; POD:978-1-5090-5502-9","10.1109/SANER.2017.7884682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884682","","Collaboration;Control systems;Databases;Documentation;Open source software;Software engineering","configuration management;public domain software","LTC;OSS projects;Qt project dataset;code review process;commit code changes;long-term contributors;open source software;patch submissions;project version control system;review feedbacks","","","","","","","","20-24 Feb. 2017","","IEEE","IEEE Conferences"
"Software Analytics: Challenges and Opportunities","L. Guerrouj; O. Baysal; D. Lo; F. Khomh","Ecole de Technol. Super., Montre&#x0301;al, QC, Canada","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","902","903","Nowadays, software development projects produce a large number of software artifacts including source code, execution traces, end-user feedback, as well as informal documentation such as developers' discussions, change logs, StackOverflow, and code reviews. Such data embeds rich and significant knowledge about software projects, their quality and services, as well as the dynamics of software development. Most often, this data is not organized, stored, and presented in a way that is immediately useful to software developers and project managers to support their decisions. To help developers and managers understand their projects, how they evolve, as well as support them during their decision-making process, software analytics -- use of analysis, data, and systematic reasoning for making decisions -- has become an emerging field of modern data analysis. While results obtained from analytics-based solutions suggested so far are promising, there are still several challenges associated with the adoption of software analytics into software development processes, as well as the development and integration of analytics tools in practical settings. We therefore propose a tutorial on software analytics. The tutorial will start with an introduction of software analytics. Next, we will discuss the main challenges and opportunities associated with software analytics based on the examples from our own research. These examples will cover a range of topics leveraging software analytics. The topics include mobile apps quality, code review process and its quality, analytics for the software engineering Twitter space, as well as the use of analytics to solve scheduling problems in the cloud.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883436","Challenges;Opportunities;Software Analytics","Conferences;Data mining;Mobile communication;Software;Software engineering;Tutorials;Twitter","data analysis;software development management","cloud scheduling problems;code quality;code review process;data analysis;mobile apps quality;software analytics;software development processes;software engineering Twitter space analytics","","","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"Identifying Code Smells with Collaborative Practices: A Controlled Experiment","R. Oliveira; B. Est√°cio; A. Garcia; S. Marczak; R. Prikladnicki; M. Kalinowski; C. Lucena","PUC-Rio, Rio de Janeiro, Brazil","2016 X Brazilian Symposium on Software Components, Architectures and Reuse (SBCARS)","20161219","2016","","","61","70","Code smells are often considered as key indicators of software quality degradation. If code smells are not systematically removed from a program, its continuous degradation may lead to either major maintenance effort or the complete redesign of the system. For several reasons, software developers introduce smells in their code as soon as they start to learn programming. If novice developers are ought to become either proficient programmers or skilled code reviewers, they should be early prepared to effectively identify code smells in existing programs. However, effective identification of code smells is often not a non-trivial task in particular to a novice developer working in isolation. Thus, the use of collaborative practices may have the potential to support developers in improving their effectiveness on this task at their early stages of their careers. These practices offer the opportunity for two or more developers analyzing the source code together and collaboratively reason about potential smells prevailing on it. Pair Programming (PP) and Coding Dojo Randori (CDR) are two increasingly adopted practices for improving the effectiveness of developers with limited or no knowledge in software engineering tasks, including code review tasks. However, there is no broad understanding about the impact of these collaborative practices on the effectiveness of code smell identification. This paper presents a controlled experiment involving 28 novice developers, aimed at assessing the effectiveness of collaborative practices in the identification of code smells. We compared PP and CDR with solo programming in order to better distinguish their impact on the effective identification of code smells. Our study is also the first in the literature to observe how novice developers work individually and together to identify smells. Our results suggest that collaborative practices contribute to the effectiveness on the identification of a wide range of code smells. Our findings - an also be used in practice to guide educators, researchers or teams on improving detection and training on code smell identification.","","Electronic:978-1-5090-5086-4; POD:978-1-5090-5087-1","10.1109/SBCARS.2016.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789840","Code Smells;Collaborative Practices;Controlled Experiment;Program Comprehension;Software Degradation","Collaboration;Degradation;Encoding;Programming profession;Software;Software engineering","software maintenance;software quality;software reviews;source code (software)","CDR;Coding Dojo Randori;PP;code review tasks;code smell identification;pair programming;software engineering tasks;software maintenance;software quality degradation;solo programming;source code","","2","","","","","","19-20 Sept. 2016","","IEEE","IEEE Conferences"
"CORRECT: Code reviewer recommendation at GitHub for Vendasta technologies","M. M. Rahman; C. K. Roy; J. Redl; J. A. Collins","University of Saskatchewan, Canada","2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)","20161006","2016","","","792","797","Peer code review locates common coding standard violations and simple logical errors in the early phases of software development, and thus, reduces overall cost. Unfortunately, at GitHub, identifying an appropriate code reviewer for a pull request is challenging given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation tool-CORRECT-that considers not only the relevant cross-project work experience (e.g., external library experience) of a developer but also her experience in certain specialized technologies (e.g., Google App Engine) associated with a pull request for determining her expertise as a potential code reviewer. We design our tool using client-server architecture, and then package the solution as a Google Chrome plug-in. Once the developer initiates a new pull request at GitHub, our tool automatically analyzes the request, mines two relevant histories, and then returns a ranked list of appropriate code reviewers for the request within the browser's context. Demo: https://www.youtube.com/watch?v=rXU1wTD6QQ0.","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582817","Code reviewer recommendation;GitHub;cross-project experience;pull request;specialized technology experience","Authentication;Browsers;Collaboration;Encoding;History;Libraries;Software","DP industry;Internet;client-server systems;recommender systems;software architecture;software reviews;software tools;source code (software)","CORRECT;GitHub;Google Chrome plug-in;Vendasta Technologies;client-server architecture;code reviewer recommendation tool;peer code review;software development","","","","","","","","3-7 Sept. 2016","","IEEE","IEEE Conferences"
"Treating software quality as a first-class entity","Y. Tymchuk","REVEAL @ Faculty of Informatics - University of Lugano, Switzerland","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20151123","2015","","","594","597","Quality is a crucial property of any software system and consists of many aspects. On the one hand, quality measures how well a piece of software satisfies its functional requirements. On the other hand, it captures how easy it is to understand, test and modify a software system. While functional requirements are provided by the product owner, maintainability of software is often underestimated. Currently software quality is either assessed by experts, or presented as a list of rule violations reported by some kind of static analyzer. Both these approaches are working with a sense of quality outside of the software itself.","","Electronic:978-1-4673-7532-0; USB:978-1-4673-7531-3","10.1109/ICSM.2015.7332521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332521","","Encoding;Measurement;Object oriented modeling;Software quality;Software systems;Visualization","decision making;program diagnostics;software maintenance;software quality;software reviews","code review approach;first-class entity;functional requirements;quality aware tools;quality-aware IDE plugins;reviewer decision making;software development lifecycle;software maintainability;software model;software system quality;static analyzer","","1","","28","","","","Sept. 29 2015-Oct. 1 2015","","IEEE","IEEE Conferences"
"Synthesizing Continuous Deployment Practices Used in Software Development","A. A. U. Rahman; E. Helms; L. Williams; C. Parnin","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","2015 Agile Conference","20151001","2015","","","1","10","Continuous deployment speeds up the process of existing agile methods, such as Scrum, and Extreme Programming (XP) through the automatic deployment of software changes to end-users upon passing of automated tests. Continuous deployment has become an emerging software engineering process amongst numerous software companies, such as Facebook, Github, Netflix, and Rally Software. A systematic analysis of software practices used in continuous deployment can facilitate a better understanding of continuous deployment as a software engineering process. Such analysis can also help software practitioners in having a shared vocabulary of practices and in choosing the software practices that they can use to implement continuous deployment. The goal of this paper is to aid software practitioners in implementing continuous deployment through a systematic analysis of software practices that are used by software companies. We studied the continuous deployment practices of 19 software companies by performing a qualitative analysis of Internet artifacts and by conducting follow-up inquiries. In total, we found 11 software practices that are used by 19 software companies. We also found that in terms of use, eight of the 11 software practices are common across 14 software companies. We observe that continuous deployment necessitates the consistent use of sound software engineering practices such as automated testing, automated deployment, and code review.","","Electronic:978-1-4673-7153-7; POD:978-1-4673-7154-4","10.1109/Agile.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284592","agile;continuous delivery;continuous deployment;follow-up inquiries;industry practices;internet artifacts","Companies;Facebook;Industries;Internet;Software;Software engineering;Testing","program testing;software prototyping","Internet artifacts;Scrum;XP;agile methods;automated deployment;automated tests;code reviewing;continuous deployment;extreme programming;software companies;software development;software engineering process","","7","","43","","","","3-7 Aug. 2015","","IEEE","IEEE Conferences"
"phpSAFE: A Security Analysis Tool for OOP Web Application Plugins","P. J. C. Nunes; J. Fonseca; M. Vieira","Polytech. Inst. of Guarda, Univ. of Coimbra, Guarda, Portugal","2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks","20150917","2015","","","299","306","There is nowadays an increasing pressure to develop complex Web applications at a fast pace. The vast majority is built using frameworks based on third-party server-side plugins that allow developers to easily add new features. However, as many plugin developers have limited programming skills, there is a spread of security vulnerabilities related to their use. Best practices advise the use of systematic code review for assure security, but free tools do not support OOP, which is how most Web applications are currently developed. To address this problem we propose phpSAFE, a static code analyzer that identifies vulnerabilities in PHP plugins developed using OOP. We evaluate phpSAFE against two well-known tools using 35 plugins for a widely used CMS. Results show that phpSAFE clearly outperforms other tools, and that plugins are being shipped with a considerable number of vulnerabilities, which tends to increase over time.","1530-0889;15300889","Electronic:978-1-4799-8629-3; POD:978-1-4799-8630-9","10.1109/DSN.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7266859","Static analysis;security;vulnerabilities;web application plugins","Arrays;Context;Databases;Filtering;Measurement;Security;Software","Web services;client-server systems;program diagnostics;security of data","CMS;OOP Web application plugins;PHP plugin vulnerability identification;phpSAFE;security analysis tool;security vulnerabilities;static code analyzer;systematic code review;third-party server-side plugins","","3","","20","","","","22-25 June 2015","","IEEE","IEEE Conferences"
"ViDI: The Visual Design Inspector","Y. Tymchuk; A. Mocci; M. Lanza","REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","2","","653","656","We present ViDI (Visual Design Inspector), a novel code review tool which focuses on quality concerns and design inspection as its cornerstones. It leverages visualization techniques to represent the reviewed software and augments the visualization with the results of quality analysis tools. To effectively understand the contribution of a reviewer in terms of the impact of her changes on the overall system quality, ViDI supports the recording and further inspection of reviewing sessions. ViDI is an advanced prototype which we will soon release to the Pharo open-source community.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203035","Lugano","Birds;Inspection;Quality assessment;Software engineering;Software systems;Visualization","data visualisation;software quality;software reviews;software tools","ViDI;code review tool;system quality;visual design inspector;visualization technique","","1","","11","","","","16-24 May 2015","","IEEE","IEEE Conferences"
"The Firefox Temporal Defect Dataset","M. Habayeb; A. Miranskyy; S. S. Murtaza; L. Buchanan; A. Bener","Data Sci. Lab., Ryerson Univ., Toronto, ON, Canada","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","498","501","The bug tracking repositories of software projects capture initial defect (bug) reports and the history of interactions among developers, testers, and customers. Extracting and mining information from these repositories is time consuming and daunting. Researchers have focused mostly on analyzing the frequency of the occurrence of defects and their attributes (e.g., The number of comments and lines of code changed, count of developers). However, the counting process eliminates information about the temporal alignment of events leading to changes in the attributes count. Software quality teams could plan and prioritize their work more efficiently if they were aware of these temporal sequences and knew their frequency of occurrence. In this paper, we introduce a novel dataset mined from the Fire fox bug repository (Bugzilla) which contains information about the temporal alignment of developer interactions. Our dataset covers eight years of data from the Fire fox project on activities throughout the project's lifecycle. Some of these activities have not been reported in frequency-based or other temporal datasets. The dataset we mined from the Fire fox project contains new activities, such as reporter experience, file exchange events, code-review process activities, and setting of milestones. We believe that this new dataset will improve analysis of bug reports and enable mining of temporal relationships so that practitioners can enhance their bug-fixing process.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180127","Bug reports;Bug repositories;Dataset;Defect tracking;Temporal activities","Communities;Computer bugs;Data mining;Feature extraction;History;Software;Software engineering","data mining;program debugging;project management;search engines","Bugzilla;Firefox bug repository;Firefox temporal defect dataset;attribute count;bug report analysis improvement;bug tracking repositories;bug-fixing process;code lines;code-review process activities;defect attributes;defect occurrence;developer count;developer interactions;file exchange events;frequency analysis;frequency-based activities;information extraction;information mining;occurrence frequency;project lifecycle;reporter experience;software projects;software quality teams;temporal datasets;temporal event alignment;temporal relationship mining;temporal sequences","","1","","12","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Automated techniques and tools for program analysis: Survey","A. K. Ashish; J. Aghav","Dept. of Comput. Eng., Coll. of Eng., Pune, India","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","20140130","2013","","","1","7","Dealing with program analysis for software development and the understanding of source code of software exhibit many research possibilities. The field of program analysis scrutinizes the approaches and techniques to analyze the properties of program including its analysis, flow, program development, algorithm, reverse engineering and other invisible features. The goal is to improve our understanding of inherently invisible and intangible software, particularly when dealing with large information spaces that characterize domains like software maintenance, reverse engineering, and collaborative development. The use of analytical methods to inspect and review source code to detect bugs has been a standard development practice. This process can be accomplished both manually and in an automated fashion. With automation, software tools provide assistance with the code review and inspection process. Program analysis includes Static and Dynamic program analysis. Static analysis techniques range from routine to more complex semantic analysis based structure.","","Electronic:978-1-4799-3926-8; POD:978-1-4799-3927-5","10.1109/ICCCNT.2013.6726693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726693","","Abstracts;Algorithm design and analysis;Computer bugs;Optimization;Performance analysis;Runtime;Software","program diagnostics;software reviews;software tools;source code (software)","automated techniques;bugs detection;code review;collaborative development;complex semantic analysis based structure;dynamic program analysis techniques;inspection process;intangible software;invisible software;program development;reverse engineering;software development;software maintenance;software tools;source code;static program analysis techniques","","0","","26","","","","4-6 July 2013","","IEEE","IEEE Conferences"
"Informing development decisions: From data to information","O. Baysal","David R. Cheriton School of Computer Science, University of Waterloo, ON, Canada","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","1407","1410","Software engineers generate vast quantities of development artifacts such as source code, bug reports, test cases, usage logs, etc., as they create and maintain their projects. The information contained in these artifacts could provide valuable insights into the software quality and adoption, as well as development process. However, very little of it is available in the way that is immediately useful to various stakeholders. This research aims to extract and analyze data from software repositories to provide software practitioners with up-to-date and insightful information that can support informed decisions related to the business, management, design, or development of software systems. This data-centric decision-making is known as analytics. In particular, we demonstrate that by employing software development analytics, we can help developers make informed decisions around user adoption of a software project, code review process, as well as improve developers' awareness of their working context.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606729","","Communities;Data mining;Decision making;Market research;Open source software;Software systems","program diagnostics;program testing;project management;software development management;software quality","bug reports;code review process;data-centric decision-making;developer awareness improvement;development artifacts;development process;informed decision;software adoption;software development analytics;software development decision;software engineering;software project adoption;software quality;software repository;software system;source code;test cases;usage logs","","1","","30","","","","18-26 May 2013","","IEEE","IEEE Conferences"
"Situational awareness: Personalizing issue tracking systems","O. Baysal; R. Holmes; M. W. Godfrey","Software Architecture Group (SWAG), David R. Cheriton School of Computer Science, University of Waterloo, Canada","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","1185","1188","Issue tracking systems play a central role in ongoing software development; they are used by developers to support collaborative bug fixing and the implementation of new features, but they are also used by other stakeholders including managers, QA, and end-users for tasks such as project management, communication and discussion, code reviews, and history tracking. Most such systems are designed around the central metaphor of the ‚Äúissue‚Äù (bug, defect, ticket, feature, etc.), yet increasingly this model seems ill fitted to the practical needs of growing software projects; for example, our analysis of interviews with 20 Mozilla developers who use Bugzilla heavily revealed that developers face challenges maintaining a global understanding of the issues they are involved with, and that they desire improved support for situational awareness that is difficult to achieve with current issue management systems. In this paper we motivate the need for personalized issue tracking that is centered around the information needs of individual developers together with improved logistical support for the tasks they perform. We also describe an initial approach to implement such a system - extending Bugzilla - that enhances a developer's situational awareness of their working context by providing views that are tailored to specific tasks they frequently perform; we are actively improving this prototype with input from Mozilla developers.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606674","","Computer bugs;Electronic mail;Interviews;Prototypes;Software;Software engineering;Target tracking","online front-ends;project management;software development management","Bugzilla;Mozilla developers;code review;collaborative bug fixing;developer situational awareness;history tracking;information needs;issue management system;issue tracking system personalization;logistical support;project management;software development;software project","","3","","20","","","","18-26 May 2013","","IEEE","IEEE Conferences"
"Extracting artifact lifecycle models from metadata history","O. Baysal; O. Kononenko; R. Holmes; M. W. Godfrey","David R. Cheriton School of Computer Science, University of Waterloo, Canada","2013 1st International Workshop on Data Analysis Patterns in Software Engineering (DAPSE)","20130919","2013","","","17","19","Software developers and managers make decisions based on the understanding they have of their software systems. This understanding is both built up experientially and through investigating various software development artifacts. While artifacts can be investigated individually, being able to summarize characteristics about a set of development artifacts can be useful. In this paper we propose lifecycle models as an effective way to gain an understanding of certain development artifacts. Lifecycle models capture the dynamic nature of how various development artifacts change over time in a graphical form that can be easily understood and communicated. Lifecycle models enables reasoning of the underlying processes and dynamics of the artifacts being analyzed. In this paper we describe how lifecycle models can be generated and demonstrate how they can be applied to the code review process of a development project.","","Electronic:978-1-4673-6296-2; POD:978-1-4673-6295-5","10.1109/DAPSE.2013.6603803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603803","","Control systems;Data mining;Data models;Educational institutions;History;Software;Time measurement","meta data;software engineering","artifact lifecycle model extraction;code review process;metadata history;software development artifacts;software development project;software systems","","0","","2","","","","21-21 May 2013","","IEEE","IEEE Conferences"
"Security Engineering Based on Structured Formal Reasoning","A. Fuchs; C. Rudolph","Fraunhofer Inst. for Secure Inf. Technol., Darmstadt, Germany","2012 ASE/IEEE International Conference on BioMedical Computing (BioMedCom)","20130516","2012","","","145","152","Security by Design and Secure Engineering are among the most pressing challenges in IT Security research and practice. Increased attacker potential and dependence on IT-Systems in economy and in critical infrastructures cause a higher demand in securely engineered systems and thus in new approaches and methodologies. This paper introduces a consistent methodology for designing secure systems during the specification phase. The Security Modeling Framework SeMF serves as basis for its security vocabulary. We extend SeMF by the concept of SeMF Building Blocks SeBBs as reasoning tool and provide a security design process utilizing them as refinement artifacts. This process guides the decision making during the system specification phase focused on the security aspects and integrates with refinement driven functional engineering processes. Our approach further results in a security design documentation and residual assumptions that can serve as a basis for risk assessment, code review, and organizational security means during deployment.","","Electronic:978-0-7695-4938-5; POD:978-1-4673-5495-0","10.1109/BioMedCom.2012.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516443","formal languages;formal methods;security by design;security engineering","","formal specification;functional programming;reasoning about programs;risk analysis;security of data","IT security research;IT system;SeBB;SeMF building blocks;Security Modeling Framework;attacker potential;code review;critical infrastructure;decision making;economy;organizational security;reasoning tool;refinement artifact;refinement driven functional engineering process;residual assumption;risk assessment;secure engineering;secure system design;security aspect;security by design;security design documentation;security engineering;security vocabulary;structured formal reasoning;system specification","","0","","29","","","","14-16 Dec. 2012","","IEEE","IEEE Conferences"
"An Empirical Study on Factors Impacting Bug Fixing Time","F. Zhang; F. Khomh; Y. Zou; A. E. Hassan","Sch. of Comput., Queen's Univ., Kingston, ON, Canada","2012 19th Working Conference on Reverse Engineering","20121220","2012","","","225","234","Fixing bugs is an important activity of the software development process. A typical process of bug fixing consists of the following steps: 1) a user files a bug report, 2) the bug is assigned to a developer, 3) the developer fixes the bug, 4) changed code is reviewed and verified, and 5) the bug is resolved. Many studies have investigated the process of bug fixing. However, to the best of our knowledge, none has explicitly analyzed the interval between bug assignment and the time when bug fixing starts. After a bug assignment, some developers will immediately start fixing the bug while others will start bug fixing after a long period. We are blind on developer's delays when fixing bugs. This paper explores such delays of developers through an empirical study on three open source software systems. We examine factors affecting bug fixing time along three dimensions: bug reports, source code involved in the fix, and code changes that are required to fix the bug. We further compare different factors by descriptive logistic regression models. Our results can help development teams better understand factors behind delays, and then improve bug fixing process.","1095-1350;10951350","Electronic:978-0-7695-4891-3; POD:978-1-4673-4536-1","10.1109/WCRE.2012.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385118","bug fixing process;bug report;change request;empirical software engineering;fixing time;mylyn","Computer bugs;Delay;Distributed Bragg reflectors;History;Logistics;Operating systems","program debugging;public domain software;regression analysis;software engineering","bug assignment;bug fixing;bug fixing time;bug report;bug resolution;code change;code review;code verification;descriptive logistic regression model;open source software system;software development process;source code","","14","","21","","","","15-18 Oct. 2012","","IEEE","IEEE Conferences"
"Contemporary Peer Review in Action: Lessons from Open Source Development","P. Rigby; B. Cleary; F. Painchaud; M. A. Storey; D. German","Concordia University, Montreal, Canada","IEEE Software","20121022","2012","29","6","56","61","Do you use software peer reviews? Are you happy with your current code review practices? Even though formal inspection is recognized as one of the most effective ways to improve software quality, many software organizations struggle to effectively implement a formal inspection regime. Open source projects use an agile peer review process-based on asynchronous, frequent, incremental reviews that are carried out by invested codevelopers-that contrasts with heavyweight inspection processes. The authors describe lessons from the OSS process that transfer to proprietary software development. They also present a selection of popular tools that support lightweight, collaborative, code review processes and nonintrusive metric collection.","0740-7459;07407459","","10.1109/MS.2012.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6148202","agile development;inspection;open source software development;software peer review;software quality","Agile manufacturing;Electronic mail;Programming;Software development;Software engineering;Software quality","DP industry;formal verification;inspection;public domain software;software quality;software reviews","OSS process;agile peer review process;code review practices;code review process;collaborative review process;contemporary peer review;formal inspection regime;heavyweight inspection processes;invested codevelopers;lightweight review process;nonintrusive metric collection;open source development;open source projects;proprietary software development;software organizations;software peer reviews;software quality","","27","","15","","","20120207","Nov.-Dec. 2012","","IEEE","IEEE Journals & Magazines"
"[Title page i]","","","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","i","i","The following topics are dealt with: software repository mining; interaction data; app mining; code review; ecosystems; API; architecture; bugs; computer musicians; bullies; gists; licenses; deep learning; and process mining.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180054","","","application program interfaces;data mining;law;learning (artificial intelligence);program debugging;software engineering;source code (software)","API;app mining;architecture;bugs;bullies;code review;computer musicians;deep learning;ecosystems;gists;interaction data;licenses;process mining;software repository mining","","0","","","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Table of contents","","","2014 IEEE 4th Workshop on Mining Unstructured Data","20141211","2014","","","v","v","The following topics are dealt with: unstructured data mining; software microblogger behaviors; changeset topic modeling; peer code review system mining; text mining and visualization techniques; and team behavioral processes.","","Electronic:978-1-4799-6793-3; POD:978-1-4799-6794-0","10.1109/MUD.2014.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980181","","","data mining","changeset topic modeling;peer code review system mining;software microblogger behaviors;team behavioral processes;text mining;text visualization;unstructured data mining","","0","","","","","","30-30 Sept. 2014","","IEEE","IEEE Conferences"
"Lightweight realization of UML ports for safety-critical real-time embedded software","A. T. Kocata≈ü; M. Can; A. H. Doƒüru","Avionics Software Design Department, Aselsan Inc., Ankara, Turkey","2016 4th International Conference on Model-Driven Engineering and Software Development (MODELSWARD)","20170626","2016","","","258","265","UML ports are widely used in the modeling of real-time software due to their advantages in flexibility and expressiveness. When realizing UML ports in object oriented languages, using objects for each port is one option. However, this approach causes runtime overhead and renders significant amount of additional generated code. To meet the performance constraints and decrease the costs of code reviews required in development of safety-critical real-time embedded software, more efficient approaches are required. In this article, we propose an approach, which introduces relatively less runtime overhead and results in more compact source code. A structural model defined with UML ports is transformed into a model that uses associations instead of objects to efficiently implement the UML port semantics with less lines of code. Achieved improvements and validation of the proposed approach is demonstrated by a case study; the design of an existing avionics software.","","Electronic:978-989-758-232-5; POD:978-1-5090-5898-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954368","Embedded;Model;Object-Oriented;Port;Real-Time;Realization;Safety-Critical;Transformation;UML","Connectors;Object oriented modeling;Ports (Computers);Relays;Runtime;Software;Unified modeling language","Unified Modeling Language;aerospace computing;avionics;embedded systems;object-oriented languages;program verification;safety-critical software;source code (software)","UML port semantics;avionics software design;code reviews;compact source code;object oriented languages;performance constraints;real-time software;safety-critical real-time embedded software;structural model","","","","","","","","19-21 Feb. 2016","","IEEE","IEEE Conferences"
"Using (Bio)Metrics to Predict Code Quality Online","S. C. M√ºller; T. Fritz","Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)","20170403","2016","","","452","463","Finding and fixing code quality concerns, such as defects or poor understandability of code, decreases software development and evolution costs. A common industrial practice to identify code quality concerns early on are code reviews. While code reviews help to identify problems early on, they also impose costs on development and only take place after a code change is already completed. The goal of our research is to automatically identify code quality concerns while a developer is making a change to the code. By using biometrics, such as heart rate variability, we aim to determine the difficulty a developer experiences working on a part of the code as well as identify and help to fix code quality concerns before they are even committed to the repository. In a field study with ten professional developers over a two-week period we investigated the use of biometrics to determine code quality concerns. Our results show that biometrics are indeed able to predict quality concerns of parts of the code while a developer is working on, improving upon a naive classifier by more than 26% and outperforming classifiers based on more traditional metrics. In a second study with five professional developers from a different country and company, we found evidence that some of our findings from our initial study can be replicated. Overall, the results from the presented studies suggest that biometrics have the potential to predict code quality concerns online and thus lower development and evolution costs.","","Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3","10.1145/2884781.2884803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886925","","Biometrics (access control);Companies;Heart rate variability;Manuals;Software;Temperature measurement","software quality","biometrics;code quality prediction;code reviews;code understandability;heart rate variability;software development costs;software evolution costs","","1","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"What information about code snippets is available in different software-related documents? An exploratory study","P. Chatterjee; M. A. Nishi; K. Damevski; V. Augustine; L. Pollock; N. A. Kraft","University of Delaware, Newark, USA","2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20170323","2017","","","382","386","A large corpora of software-related documents is available on the Web, and these documents offer the unique opportunity to learn from what developers are saying or asking about the code snippets that they are discussing. For example, the natural language in a bug report provides information about what is not functioning properly in a particular code snippet. Previous research has mined information about code snippets from bug reports, emails, and Q&A forums. This paper describes an exploratory study into the kinds of information that is embedded in different software-related documents. The goal of the study is to gain insight into the potential value and difficulty of mining the natural language text associated with the code snippets found in a variety of software-related documents, including blog posts, API documentation, code reviews, and public chats.","","Electronic:978-1-5090-5501-2; POD:978-1-5090-5502-9","10.1109/SANER.2017.7884638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884638","","Blogs;Computer bugs;Context;Data mining;Documentation;Natural languages;Software","Web sites;application program interfaces;data mining;natural language processing;software engineering;text analysis","API documentation;Q&A forums;blog posts;bug report;code reviews;code snippets;emails;natural language text mining;public chats;software-related documents","","1","","","","","","20-24 Feb. 2017","","IEEE","IEEE Conferences"
"WAP: Understanding the Brain at Software Debugging","J. Duraes; H. Madeira; J. Castelhano; C. Duarte; M. C. Branco","DEIS, Polytech. Inst. of Coimbra, Coimbra, Portugal","2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)","20161208","2016","","","87","92","We propose that understanding functional patterns of activity in mapped brain regions associated with code comprehension tasks and, more specifically, to the activity of finding bugs in traditional code inspections could reveal useful insights to improve software reliability and to improve the software development process in general. This includes helping to select the best professionals for the debugging effort, improving the conditions for code inspections, and identify new directions to follow for training code reviewers. This paper presents an interdisciplinary study to analyze the brain activity during code inspection tasks using functional magnetic resonance imaging (fMRI), which is a well-established tool in cognitive neuroscience research. We used several programs where realistic bugs representing the most frequent types of software faults found in the field were injected. The code inspectors involved in the research include programmers with different levels of expertise and experience in real code reviews. The goal is to understand brain activity patterns associated with code comprehension tasks and, more specifically, the brain activity when the code reviewer identifies a bug in the code ('eureka' moment), which can be a true positive or a false positive. Our results confirmed that brain areas associated with language processing and mathematics are highly active during code reviewing and shows that there are specific brain activity patterns that can be related to the decision-making moment of suspicion/bug detection. Importantly, the activity at the anterior insula region that we find to play a relevant role in the process of identifying software bugs is positively correlated to the precision of bug detection by the inspectors. This finding provides a new perspective on the role of this region on error awareness and monitoring and of its potential predictive value in predicting the quality of bug removing.","","Electronic:978-1-4673-9002-6; POD:978-1-4673-9003-3","10.1109/ISSRE.2016.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774510","ODC;cognitive neuroscience;fRMI;software faults;software inspections","Brain;Computer bugs;Inspection;Magnetic resonance imaging;Neuroscience;Software;Software reliability","biomedical MRI;program debugging;software engineering;software reliability","WAP;activity functional patterns;anterior insula region;brain activity patterns;brain regions;bug removing quality prediction;code comprehension tasks;code inspections;code reviewing;cognitive neuroscience research;debugging effort;decision-making moment;error awareness;error monitoring;fMRI;functional magnetic resonance imaging;language processing;realistic bugs;software debugging;software development process;software faults;software reliability improvement;suspicion-bug detection","","","","","","","","23-27 Oct. 2016","","IEEE","IEEE Conferences"
"Web usage patterns of developers","C. S. Corley; F. Lois; S. Quezada","ABB Corporate Research, Raleigh, NC, USA","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20151123","2015","","","381","390","Developers often rely on the web-based tools for troubleshooting, collaboration, issue tracking, code reviewing, documentation viewing, and a myriad of other uses. Developers also use the web for non-development purposes, such as reading news or social media. In this paper we explore whether web usage is detriment to a developer's focus on work from a sample over 150 developers. Additionally, we investigate if highly-focused developers use the web differently than other developers. Our qualitative findings suggest highly-focused developers use the web differently, but we are unable to predict a developer's focused based on web usage alone. Further quantitative findings suggest that web usage does not have a negative impact on a developer's focus.","","Electronic:978-1-4673-7532-0; USB:978-1-4673-7531-3","10.1109/ICSM.2015.7332489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332489","developer focus;interruptions;personal software process;web activity","Blogs;Buildings;Collaboration;Debugging;Encoding;Heating;Software","Internet;programming environments;software engineering","Web usage pattern;Web-based tool;code reviewing;documentation viewing;troubleshooting","","1","","19","","","","Sept. 29 2015-Oct. 1 2015","","IEEE","IEEE Conferences"
"Software Analytics for Digital Games","T. Zimmermann","Microsoft Res., Redmond, WA, USA","2015 IEEE/ACM 4th International Workshop on Games and Software Engineering","20150730","2015","","","1","2","Software and its development generates an inordinate amount of data. Development activities such as check-ins, work items, bug reports, code reviews, and test executions are recorded in software repositories. User interactions that reflect how customers experience software are recorded in telemetry data, run-time traces, and log files and helps to track application and feature usage and expose performance and reliability. Software analytics takes this data and turns it into actionable insight to better inform decisions related to software. In this talk, I will summarize our efforts in the area of software analytics with a special focus on digital games. I will present several examples of games studies, which we have worked on at Microsoft Research such as how players are engaged in Project Gotham Racing, how skill develops over time in Halo Reach and Forza Motor sports, and the initial experience of game play. I will also point out important differences between games development and traditional software development. The work presented in this talk has been done by Nachi Nagappan, myself, and many others who have visited our group over the past years.","","Electronic:978-1-4673-7046-2; POD:978-1-4673-7047-9","10.1109/GAS.2015.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169461","digital games;software analytics","Committees;Data mining;Games;Software;Software engineering;Software reliability;Telemetry","computer games;software engineering","Forza Motorsports;Halo Reach;Microsoft research;Nachi Nagappan;Project Gotham Racing;bug reports;check-ins;code reviews;digital games;log files;run-time traces;software analytics;software development;software repositories;telemetry data;test executions;work items","","0","","","","","","18-18 May 2015","","IEEE","IEEE Conferences"
"Keynote: Lessons and Insights from Tech Transfers at Microsoft by Christian Bird","","","2014 IEEE 4th Workshop on Mining Unstructured Data","20141211","2014","","","viii","ix","The author discussed how a risk assessment system went from a website with a few visits per day to a web-service service thousands of requests per day. He discussed the tradeoffs they faced and the design decisions they made to a system that automatically adds high expertise reviewers to code reviews. In addition, he discussed what they learned from the research projects that never succeeded at Tech Transfer. Finally, he proposed what he believe to be open problems in tech transfer and the way the research community interacts with software development practitioners today.","","Electronic:978-1-4799-6793-3; POD:978-1-4799-6794-0","10.1109/MUD.2014.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980184","","","Web services;risk management;software engineering","Microsoft;Web site;Web-service;code reviews;risk assessment system;software development;tech transfers","","0","","","","","","30-30 Sept. 2014","","IEEE","IEEE Conferences"
"Improving manual analysis of automated code inspection results: Need and effectiveness","","","2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","20131219","2013","","","53","53","Automated code inspection using static analysis tools has been found to be useful and cost-effective over manual code reviews. This is due to ability of these tools to detect programming bugs (or defects) early in the software development cycle without running the code. Further, using sound static analysis tools, even large industry applications can be certified to be free of certain types of the programming bugs such as Division by Zero, Null/Illegal Dereference of a Pointer, Memory Leaks, and so on. In spite of these merits, as per various surveys, the static analysis tools are used infrequently and inconsistently in practice to ensure software quality. Large number of false alarms generated and the efforts required to manually analyze them are the primary reasons for this. Similar has been the experience of our team with the usage of these tools.","","Electronic:978-1-4799-2552-0; POD:978-1-4799-2553-7","10.1109/ISSREW.2013.6688867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688867","","Computer bugs;Inspection;Manuals;Programming;Software quality","program diagnostics;software quality","automated code inspection results;manual analysis;manual code reviews;memory leaks;null-illegal pointer dereference;programming bugs;software development cycle;software quality;static analysis tools","","0","","","","","","4-7 Nov. 2013","","IEEE","IEEE Conferences"
"IPOL: Reviewed publication and public testing of research software","N. Limare; L. Oudre; P. Getreuer","CMLA, ENS Cachan, Cachan, France","2012 IEEE 8th International Conference on E-Science","20130110","2012","","","1","8","With the journal Image Processing On Line (IPOL), we propose to promote software to the status of regular research material and subject it to the same treatment as research papers: it must be reviewed, it must be reusable and verifiable by the research community, it must follow style and quality guidelines. In IPOL, algorithms are published with their implementation, codes are peer-reviewed, and a web-based test interface is attached to each of these articles. This results in more software released by the researchers, a better software quality achieved with the review process, and a large collection of test data gathered for each article. IPOL has been active since 2010, and has already published thirty articles.","","Electronic:978-1-4673-4466-1; POD:978-1-4673-4467-8","10.1109/eScience.2012.6404449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6404449","","Communities;Guidelines;Image processing;Licenses;Patents;Software;Software algorithms","Internet;program testing;research and development;software quality;user interfaces","IPOL;Web-based test interface;image processing on line;peer-reviewed codes;public testing;research software;reviewed publication;software quality","","1","","18","","","","8-12 Oct. 2012","","IEEE","IEEE Conferences"
"Evaluating awareness information in distributed collaborative editing by software-engineers","J. Schenk","Institute of Computer Science, Freie Universit&#x00E4;t Berlin, Germany","2012 First International Workshop on User Evaluation for Software Engineering Researchers (USER)","20120628","2012","","","35","38","In co-located collaborative software development activities like pair programming, side-by-side programming, code reviews or code walkthroughs, the individuals automatically gain a fine granular mutual understanding of where in the shared workspace the other participants are, what they are doing and what their levels of interest are. These points of so called awareness information are critical for an efficient and smooth collaboration but cannot be obtained via the natural mechanisms in virtual teams. Application sharing and groupware for collaborative editing are widely used for collaborative tasks in distributed software development but considered from the awareness and flexibility aspect they are far off the co-located setting. To better support virtual team collaboration by improving tools for distributed software development it is neccesary to evaluate awareness and its impacts to certain collaborative situations. Awareness itself is an invisible phenomenon and due to its intangible nature cannot be easily observed or measured. Thus we recorded virtual teams using Saros, a groupware for distributed collaborative party programming, respectively VNC and now analyse these videos using the grounded theory methodology. This approach for evaluating awareness leads to various problems concerning the recording setup and time exposure for analysis.","","Electronic:978-1-4673-1859-4; POD:978-1-4673-1858-7","10.1109/USER.2012.6226580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226580","Application Sharing;CSCW;Collaborative Editing;Distributed Software Development;Groupware;Workspace Awareness","Collaborative software;Collaborative work;Programming;Synchronization;Videos;Virtual groups","distributed programming;groupware;software engineering;video recording","Saros;VNC;application sharing;awareness information evaluation;code reviews;code walkthroughs;colocated collaborative software development activities;distributed collaborative editing;distributed collaborative party programming;distributed software development;granular mutual understanding;grounded theory methodology;groupware;pair programming;recording setup;shared workspace;side-by-side programming;software engineers;time exposure;video analysis;virtual team collaboration","","1","","6","","","","5-5 June 2012","","IEEE","IEEE Conferences"
"Assessing the Impact of Using Fault Prediction in Industry","R. M. Bell; E. J. Weyuker; T. J. Ostrand","AT&T Labs. - Res., Florham Park, NJ, USA","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","20110714","2011","","","561","565","Software developers and testers need realistic ways to measure the practical effects of using fault prediction models to guide software quality improvement methods such as testing, code reviews, and refactoring. Will the availability of fault predictions lead to discovery of different faults, or to more efficient means of finding the same faults? Or do fault predictions have no practical impact at all? In this challenge paper we describe the difficulties of answering these questions, and the issues involved in devising meaningful ways to assess the impact of using prediction models. We present several experimental design options and discuss the pros and cons of each.","","Electronic:978-0-7695-4345-1; POD:978-1-4577-0019-4","10.1109/ICSTW.2011.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954465","assessing causal effects;experimental design;industrial systems;software fault prediction;software testing","Computer bugs;Fault detection;Measurement;Predictive models;Software;Software testing","program testing;software reliability;system recovery","code reviews;fault-prediction;prediction models;software developers;software quality improvement methods","","2","","8","","","","21-25 March 2011","","IEEE","IEEE Conferences"
"Code Recommendation with Natural Language Tags and Other Heterogeneous Data","F. Qiu; W. Ge; X. Dai","National Key Laboratory for Novel Software Technology; Nanjing University, Nanjing, China","Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence","20180308","2017","","","137","142","<p>Recommender systems solve the problem of information overload by efficiently utilizing huge quantities of data and trying its best to predict potential preference which aims at a certain user. They are widely applied in numerous fields. However, hardly can we see a code recommender system for programmers though it is desperately expected. Raw data of the code are not so convenient to handle for the difference in structure and lack of relevance. Fortunately, in real world, there are abundant data affiliated to the code, such as context, tags, social relations of users and view histories. In this paper, we firstly formulate a new task of code recommendation. Then, we propose a hybrid linear algorithm for recommending source codes, in which we maximize the utility of multivariate heterogeneous auxiliary data with code. Experiments on the dataset from Code Review Community show that our proposed method works for the new code recommendation task. Our system is hopefully designed to be adaptive to new source of heterogeneous information, and hopefully performs better with more significant data and new inspired components.</p>","","","10.1145/3168390.3168407","","","Code Recommendation;Heterogeneous Data;Hybrid Linear Model;Recommender Systems","","","","","","","","","","","5-7 Dec. 2017","","ACM","ACM Conferences"
"Understanding Technical Debt at the Code Level from the Perspective of Software Developers","J. C. Rocha; V. Zapalowski; I. Nunes","Universidade Federal do Rio Grande do Sul (UFRGS), Porto Alegre, Brazil","Proceedings of the 31st Brazilian Symposium on Software Engineering","20180215","2017","","","64","73","<p>Keeping the source code clean and organized throughout the software development and evolution is a challenging task. Due to many factors, design choices that cause the overall code structure to decay may be made and implemented, so that benefits, such as reduced development time, can be obtained in the short term. In order to deal with these situations, the metaphor of technical debt emerged to allow such situations to be systematically managed. Although this concept is already known in academia, there are limited evidences that the industry widely adopts it. Therefore, this paper presents the results of a survey involving 74 participants that work in the Brazilian software industry, in order to understand why technical debt is introduced, eliminated and how it is managed in practice, with a focus on the code level. Our survey is not limited to the explicit management of technical debt but also includes the notion that the introduction of poor code without the awareness that it is a poor design choice can also become a debt. Such a code can be acknowledged as a debt to be paid as the software evolves. Our results show that overload of work and lack of time, together with pressure from the management, are the main reasons for the creation of technical debt. However, when participants evaluate other developers, they believe that inexperience also plays a key role. Moreover, the most effective practice to avoid the creation of technical debt is code review, in the opinion of participants.</p>","","","10.1145/3131151.3131164","","","Programming Best Practices;Survey;Technical Debt","","","","","","","","","","","20-22 Sept. 2017","","ACM","ACM Conferences"
"A reactive specification formalism for enhancing system development, analysis and adaptivity","A. Marron","Weizmann Institute of Science, Rehovot, Israel","Proceedings of the 15th ACM-IEEE International Conference on Formal Methods and Models for System Design","20180215","2017","","","161","164","<p>During system development, external reviewers, especially ones with expertise in the problem domain or in system and software engineering (SE), often contribute insights that up to that point were not noticed by the engineers and other project stakeholders. These reviewers apparently do so by employing special human competencies that presently are not, or even cannot be, automated. Consider, for example, the competencies that enable the following review comments about a home-assistant robot: (a) during code review: ""I see that the robot can defers command execution until it charges its battery for the task; When done charging, does the robot check if the action is still needed?""; (b) following a demo: ""Will the robot trip over a thin ,transparent phone cord?""; and (c) ""Some clear voice commands had to be repeated. Perhaps sometimes the robot isn't listening?"".</p>","","","10.1145/3127041.3127064","","","","","","","","","","","","","","Sept. 29 2017-Oct. 2 2017","","ACM","ACM Conferences"
"Towards Self-Verification in Finite Difference Code Generation","J. H√ºckelheim; Z. Luo; F. Luporini; N. Kukreja; M. Lange; G. Gorman; S. Siegel; M. Dwyer; P. Hovland","Imperial College London, London, UK","Proceedings of the First International Workshop on Software Correctness for HPC Applications","20171225","2017","","","42","49","<p>Code generation from domain-specific languages is becoming increasingly popular as a method to obtain optimised low-level code that performs well on a given platform and for a given problem instance. Ensuring the correctness of generated codes is crucial. At the same time, testing or manual inspection of the code is problematic, as the generated code can be complex and hard to read. Moreover, the generated code may change depending on the problem type, domain size, or target platform, making conventional code review or testing methods impractical. As a solution, we propose the integration of formal verification tools into the code generation process. We present a case study in which the CIVL verification tool is combined with the Devito finite difference framework that generates optimised stencil code for PDE solvers from symbolic equations. We show a selection of properties of the generated code that can be automatically specified and verified during the code generation process. Our approach allowed us to detect a previously unknown bug in the Devito code generation tool.</p>","","","10.1145/3145344.3145488","","","Code generation;Equivalence checking;Formal methods;HPC;Specification;Symbolic execution;Verification","","","","","","","","","","","12-17 Nov. 2017","","ACM","ACM Conferences"
"An Empirical Study of Reviewer Recommendation in Pull-based Development Model","C. Yang; X. Zhang; L. Zeng; Q. Fan; G. Yin; H. Wang","National Laboratory for Parallel and Distributed Processing, College of Computer, Changsha, Hunan, China","Proceedings of the 9th Asia-Pacific Symposium on Internetware","20171225","2017","","","1","6","<p>Code review is an important process to reduce code defects and improve software quality. However, in social coding communities using the pull-based model, everyone can submit code changes, which increases the required code review efforts. Therefore, there is a great need of knowing the process of code review and analyzing the pre-existing reviewer recommendation algorithms. In this paper, we do an empirical study about the PRs and their reviewers in Rails project. Moreover, we reproduce a popular and effective IR-based code reviewer recommendation algorithm and validate it on our dataset which contains 16,049 PRs. We find that the inactive reviewers are very important to code reviewing process, however, the pre-existing method's recommendation result strongly depends on the activeness of reviewers.</p>","","","10.1145/3131704.3131718","","","GitHub;code reviewer recommendation;pull request","","","","","","","","","","","23-23 Sept. 2017","","ACM","ACM Conferences"
"Sentiment Identification for Collaborative, Geographically Dispersed, Cross-Functional Software Development Teams","A. Patwardhan","DevOps AssetMark, Financial Inc., Concord, MA, USA","2017 IEEE 3rd International Conference on Collaboration and Internet Computing (CIC)","20171214","2017","","","20","26","The process of software development is a collaborative effort that may consist of various geographically dispersed teams. It involves extensive communication over emails, intra-company forums, blogs, surveys and code reviews. The high level of message and opinion exchange evokes positive and negative emotions regarding the software project delivery. This paper proposed techniques to identify the underlying emotion polarity across various collaborating teams by analyzing the communication during the software release cycle. It also evaluated whether there was a relation between emotion polarity and social factors and software artifacts such as gender, location of team, experience level, release duration, team size, code issue count and code review comments. The sentiment analysis was used to implement an emotion dashboard to measure the project success and emotional health across various cross-functional software development teams. 80% of the managers found the tool useful in improving the overall team morale and 70% managers found the dashboard useful for identifying the emotional health of the collaborating teams.","","Electronic:978-1-5386-2565-1; POD:978-1-5386-2566-8","10.1109/CIC.2017.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8181475","Emotion Aware Software Development Process and Monitoring;Emotion mining;Sentiment Analysis;Team collaboration","Business;Collaboration;Electronic mail;Feature extraction;Sentiment analysis;Software;Tools","project management;sentiment analysis;social aspects of automation;software development management;team working","collaborative effort;cross-functional software development teams;emotion dashboard;emotion polarity;geographically dispersed teams;opinion exchange;sentiment analysis;sentiment identification;software project delivery;software release cycle;team collaboration;team morale","","","","","","","","15-17 Oct. 2017","","IEEE","IEEE Conferences"
"Are One-Time Contributors Different? A Comparison to Core and Periphery Developers in FLOSS Repositories","A. Lee; J. C. Carver","Dept. of Comput. Sci., Univ. of Alabama, Tuscaloosa, AL, USA","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","20171211","2017","","","1","10","Context: Free/Libre Open Source Software (FLOSS) communities consist of different types of contributors. Core contributors and peripheral contributors work together to create a successful project, each playing a different role. One-Time Contributors (OTCs), who are on the very fringe of the peripheral developers, are largely unstudied despite offering unique insights into the development process. In a prior survey, we identified OTCs and discovered their motivations and barriers. Aims: The objective of this study is to corroborate the survey results and provide a better understand of OTCs. We compare OTCs to other peripheral and core contributors to determine whether they are distinct. Method: We mined data from the same code-review repository used to identify survey respondents in our previous study. After identifying each contributor as core, periphery, or OTC, we compared them in terms of patch size, time interval from submission to decision, the nature of their conversations, and patch acceptance rates. Results: We identified a continuum between core developers and OTCs. OTCs create smaller patches, face longer time intervals between patch submission and rejection, have longer review conversations, and face lower patch acceptance rates. Conversely, core contributors create larger patches, face shorter time intervals for feedback, have shorter review conversations, and have patches accepted at the highest rate. The peripheral developers fall in between the OTCs and the core contributors. Conclusion: OTCs do, in fact, face the barriers identified in our prior survey. They represent a distinct group of contributors compared to core and peripheral developers.","","Electronic:978-1-5090-4039-1; POD:978-1-5090-4040-7","10.1109/ESEM.2017.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8169979","FLOSS;OTC;One-Time Contributors;repository mining","Computer bugs;Computer science;Data mining;Electronic mail;Face;Open source software","data mining;public domain software","Free/Libre Open Source Software communities;OTC;core contributors;core developers;development process;face longer time intervals;face shorter time intervals;one-time contributors;peripheral contributors;peripheral developers;periphery developers;playing a different role;time interval","","","","","","","","9-10 Nov. 2017","","IEEE","IEEE Conferences"
"File-Level Defect Prediction: Unsupervised vs. Supervised Models","M. Yan; Y. Fang; D. Lo; X. Xia; X. Zhang","Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","20171211","2017","","","344","353","Background: Software defect models can help software quality assurance teams to allocate testing or code review resources. A variety of techniques have been used to build defect prediction models, including supervised and unsupervised methods. Recently, Yang et al. [1] surprisingly find that unsupervised models can perform statistically significantly better than supervised models in effort-aware change-level defect prediction. However, little is known about relative performance of unsupervised and supervised models for effort-aware file-level defect prediction. Goal: Inspired by their work, we aim to investigate whether a similar finding holds in effort-aware file-level defect prediction. Method: We replicate Yang et al.'s study on PROMISE dataset with totally ten projects. We compare the effectiveness of unsupervised and supervised prediction models for effort-aware file-level defect prediction. Results: We find that the conclusion of Yang et al. [1] does not hold under within-project but holds under cross-project setting for file-level defect prediction. In addition, following the recommendations given by the best unsupervised model, developers needs to inspect statistically significantly more files than that of supervised models considering the same inspection effort (i.e., LOC). Conclusions: (a) Unsupervised models do not perform statistically significantly better than state-of-art supervised model under within-project setting, (b) Unsupervised models can perform statistically significantly better than state-ofart supervised model under cross-project setting, (c) We suggest that not only LOC but also number of files needed to be inspected should be considered when evaluating effort-aware filelevel defect prediction models.","","Electronic:978-1-5090-4039-1; POD:978-1-5090-4040-7","10.1109/ESEM.2017.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170121","Effortaware Defect Prediction;Inspection Effort;Replication Study","Data models;Inspection;Logistics;Measurement;Predictive models;Software;Software engineering","software quality","effort-aware change-level defect prediction;effort-aware file-level defect prediction;software defect models;supervised prediction models;unsupervised prediction models","","","","","","","","9-10 Nov. 2017","","IEEE","IEEE Conferences"
"Let's Talk Money: Evaluating the Security Challenges of Mobile Money in the Developing World","S. Castle; F. Pervaiz; G. Weld; F. Roesner; R. Anderson","University of Washington","Proceedings of the 7th Annual Symposium on Computing for Development","20170914","2016","","","1","10","<p>Digital money drives modern economies, and the global adoption of mobile phones has enabled a wide range of digital financial services in the developing world. Where there is money, there must be security, yet prior work on mobile money has identified discouraging vulnerabilities in the current ecosystem. We begin by arguing that the situation is not as dire as it may seem---many reported issues can be resolved by security best practices and updated mobile software. To support this argument, we diagnose the problems from two directions: (1) a large-scale analysis of existing financial service products and (2) a series of interviews with 7 developers and designers in Africa and South America. We frame this assessment within a novel, systematic threat model. In our large-scale analysis, we evaluate 197 Android apps and take a deeper look at 71 products to assess specific organizational practices. We conclude that although attack vectors are present in many apps, service providers are generally making intentional, security-conscious decisions. The developer interviews support these findings, as most participants demonstrated technical competency and experience, and all worked within established organizations with regimented code review processes and dedicated security teams.</p>","","","10.1145/3001913.3001919","","","Android;ICTD;finance;fraud;human factors;mBanking;mobile money","","","","","","","","","","","18-20 Nov. 2016","","ACM","ACM Conferences"
"An Empirical Study on the Correctness of Formally Verified Distributed Systems","P. Fonseca; K. Zhang; X. Wang; A. Krishnamurthy","University of Washington","Proceedings of the Twelfth European Conference on Computer Systems","20170831","2017","","","328","343","<p>Recent advances in formal verification techniques enabled the implementation of distributed systems with machine-checked proofs. While results are encouraging, the importance of distributed systems warrants a large scale evaluation of the results and verification practices.</p> <p>This paper thoroughly analyzes three state-of-the-art, formally verified implementations of distributed systems: Iron-Fleet, Verdi, and Chapar. Through code review and testing, we found a total of 16 bugs, many of which produce serious consequences, including crashing servers, returning incorrect results to clients, and invalidating verification guarantees. These bugs were caused by violations of a wide-range of assumptions on which the verified components relied. Our results revealed that these assumptions referred to a small fraction of the trusted computing base, mostly at the interface of verified and unverified components. Based on our observations, we have built a testing toolkit called PK, which focuses on testing these parts and is able to automate the detection of 13 (out of 16) bugs.</p>","","","10.1145/3064176.3064183","","","","","","","","","","","","","","23-26 April 2017","","ACM","ACM Conferences"
"E-Mentoring for Software Engineering: a Socio-Technical Perspective","E. H. Trainer; A. Kalyanasundaram; J. D. Herbsleb","Inst. for Software Res., Carnegie Mellon Univ., Pittsburgh, PA, USA","2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET)","20170824","2017","","","107","116","Mentoring is one of the most effective pedagogical tools, holding great promise for software engineering education. When done badly, however, it can lead to dysfunctional interpersonal relationships and may turn off mentees from careers in software engineering. In this qualitative interview-based study we examine how socio-technical dimensions of software impact the formation of social ties important for satisfying two goals of mentorship, building technical skill and interpersonal development. We find that mentees working on user facing, interdependent software form a balance of ties that facilitate both goals, while mentees working on non-user facing software mostly form ties important for building technical skill. Work practices that create opportunities for unstructured contact between mentees and community members, such as code review in a mentee cohort, can help to overcome this imbalance. Our findings have important implications for task definition in software engineering e-mentoring program schemes.","","Electronic:978-1-5386-2671-9; POD:978-1-5386-2672-6","10.1109/ICSE-SEET.2017.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7964335","E-mentoring;expressive ties;free and open-source software;instrumental ties;qualitative methods;tie content","Engineering profession;Mentoring;Organizations;Social network services;Software;Software engineering","computer science education;social aspects of automation;software engineering","community members;e-mentoring program;interdependent software;interpersonal development;mentees;nonuser facing software;qualitative interview-based study;social ties;sociotechnical dimensions;software engineering education;software impact;technical skill;user facing software","","","","","","","","20-28 May 2017","","IEEE","IEEE Conferences"
"Eliph: Effective Visualization of Code History for Peer Assessment in Programming Education","J. Park; Y. H. Park; S. Kim; A. Oh","Korea Advanced Institute of Science and Technology, Daejoen, Republic of Korea","Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing","20170417","2017","","","33","36","<p>Peer assessment is an effective pedagogical tool in which students engage in the process of evaluating other student's work. In programming education, peer assessment involves peer code review, where students mark and give feedback other peer's code. We introduce Eliph, a web-based peer assessment tool for programming education with code history visualization. Eliph incorporates the visualization of character-level code history, selection-based history tracking and the integration of execution events. In a controlled experiment performed in an undergraduate CS course, we found that Eliph helps students understand code structure and the author's intention more clearly, and promotes higher quality of peer feedback.</p>","","","10.1145/3022198.3023266","","","code history;data visualization;peer assessment;peer review;time series visualization","","","","","","","","","","","Feb. 25 2017-March 1 2017","","ACM","ACM Conferences"
"Effective assignment and assistance to software developers and reviewers","M. B. Zanjani","Wichita State University, USA","Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering","20170413","2016","","","1091","1093","<p> Human reliance and dominance are ubiquitous in sustaining a high-quality large software system. Automatically assigning the right solution providers to the maintenance task at hand is arguably as important as providing the right tool support for it, especially in the far too commonly found state of inadequate or obsolete documentation of large-scale software systems. Two maintenance tasks related to assignment and assistance to software developers and reviewers are addressed, and solutions are proposed. The key insight behind these proposed solutions is the analysis and use of micro-levels of human-to-code and human-to-human interactions (eg., code review). We analyzed code reviews that are managed by Gerrit and found different markers of developer expertise associated with the source code changes and their acceptance, time line, and human roles and feedback involved in the reviews. We formed a developer-expertise model from these markers and showed its application in bug triaging. Specifically, we derived a developer recommendation approach for an incoming change request, named rDevX , from this expertise model. Additionally, we present an approach, namely cHRev, to automatically recommend reviewers who are best suited to participate in a given review, based on their historical contributions as demonstrated in their prior reviews. Furthermore, a comparative study on other previous approaches for developer recommendation and reviewer recommendation was performed. The metrics recall and MRR were used to measure their quantitative effectiveness. Results show that the proposed approaches outperform the subjected competitors with statistical significance. </p>","","","10.1145/2950290.2983960","","","Developer Recommendation;Reviewer recommendation","","","","","","","","","","","13-18 Nov. 2016","","ACM","ACM Conferences"
"Developer workflow at google (showcase)","C. Sadowski","Google, USA","Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering","20170413","2016","","","26","26","<p> This talk describes the developer workflow at Google, and our use of program analysis, testing, metrics, and tooling to reduce errors when creating and committing changes to source code. Software development at Google has several unique characteristics such as our monolithic codebase and distributed hermetic build system. Changes are vetted both manually, via our internal code review tool, and automatically, via sources such as the Tricorder program analysis platform and our automated testing infrastructure. </p>","","","10.1145/2950290.2994156","","","Developer tools;developer workflow","","","","","","","","","","","13-18 Nov. 2016","","ACM","ACM Conferences"
"CORRECT: Code Reviewer Recommendation in GitHub Based on Cross-Project and Technology Experience","M. M. Rahman; C. K. Roy; J. A. Collins","Univ. of Saskatchewan, Saskatoon, SK, Canada","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","222","231","Peer code review locates common coding rule violations and simple logical errors in the early phases of software development, and thus reduces overall cost. However, in GitHub, identifying an appropriate code reviewer for a pull request is a non-trivial task given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation technique that considers not only the relevant cross-project work history (e.g., external library experience) but also the experience of a developer in certain specialized technologies associated with a pull request for determining her expertise as a potential code reviewer. We first motivate our technique using an exploratory study with 10 commercial projects and 10 associated libraries external to those projects. Experiments using 17,115 pull requests from 10 commercial projects and six open source projects show that our technique provides 85%-92% recommendation accuracy, about 86% precision and 79%--81% recall in code reviewer recommendation, which are highly promising. Comparison with the state-of-the-art technique also validates the empirical findings and the superiority of our recommendation technique.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883306","Code reviewer recommendation;GitHub;cross-project experience;pull request;specialized technology experience","Collaboration;Companies;Encoding;History;Libraries;Servers;Software","configuration management;public domain software;recommender systems","CORRECT;GitHub;code reviewer recommendation technique;cross-project work history;open source projects","","","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"Towards Property Driven Hardware Security","W. Hu; A. Althoff; A. Ardeshiricham; R. Kastner","Dept. of Comput. Sci. & Eng., Univ. of California, San Diego, La Jolla, CA, USA","2016 17th International Workshop on Microprocessor and SOC Test and Verification (MTV)","20170320","2016","","","51","56","Secure hardware design is a challenging task due to the fact that security properties are difficult or impossible to model and subsequently verify using traditional hardware design tools. The ""state of the art"" for hardware design security relies heavily on functional verification, manual inspection, and code review to identify security vulnerabilities. This labor intensive process significantly reduces productivity while proving no guarantee that a security flaw will be identified. In this paper, we describe a property driven approach to hardware security, which allows automatic synthesis and verification of both qualitative and quantitative security properties. We address hardware security by enforcing information flow and statistical security properties. By incorporating a new security property specification language, such security properties can be specified, translated and verified using hardware design tools. We present design examples to demonstrate our property driven hardware security solution for proving isolation, detecting timing channel, eliminating hardware Trojan, and enforcing security related statistical properties.","","Electronic:978-1-4673-8924-2; POD:978-1-4673-8925-9","10.1109/MTV.2016.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880823","","Computational modeling;Encryption;Hardware;Logic gates;Timing;Trojan horses","security","hardware Trojan;hardware design;hardware design tools;information flow;property driven hardware security;security properties;security vulnerabilities;timing channel","","","","","","","","12-13 Dec. 2016","","IEEE","IEEE Conferences"
"Refinement-based Specification and Security Analysis of Separation Kernels","Y. Zhao; D. Sanan; F. Zhang; Y. Liu","School of Computer Science and Engineering, Beihang Univerisity, Beijing, China (e-mail: zhaoyw@buaa.edu.cn).","IEEE Transactions on Dependable and Secure Computing","","2017","Early Access","Early Access","1","1","Assurance of information-flow security by formal methods is mandated in security certification of separation kernels. As an industrial standard for improving safety, ARINC 653 has been complied with by mainstream separation kernels. Due to the new trend of integrating safe and secure functionalities into one separation kernel, security analysis of ARINC 653 as well as a formal specification with security proofs are thus significant for the development and certification of ARINC 653 compliant Separation Kernels (ARINC SKs). This paper presents a specification development and security analysis method for ARINC SKs based on refinement. We propose a generic security model and a stepwise refinement framework. Two levels of functional specification are developed by the refinement. A major part of separation kernel requirements in ARINC 653 are modeled, such as kernel initialization, two-level scheduling, partition and process management, and inter-partition communication. The formal specification and its security proofs are carried out in the Isabelle/HOL theorem prover. We have reviewed the source code of one industrial and two open-source ARINC SK implementations, i.e. VxWorks 653, XtratuM, and POK, in accordance with the formal specification. During the verification and code review, six security flaws, which can cause information leakage, are found in the ARINC 653 standard and the implementations.","1545-5971;15455971","","10.1109/TDSC.2017.2672983","10.13039/501100001381 - National Research Foundation Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862167","ARINC 653;Common Criteria, Theorem Proving;Formal Specification;Information-flow Security;Refinement;Separation Kernels","Computer science;Kernel;Market research;Safety;Scheduling;Security;Standards","","","","","","","","","20170222","","","IEEE","IEEE Early Access Articles"
"Empirical evaluation of code smells in open source projects: preliminary results","A. Nanthaamornphong; A. Chaisutanon","Prince of Songkla University, Thailand","Proceedings of the 1st International Workshop on Software Refactoring","20161111","2016","","","5","8","<p> Open Source Software (OSS) now plays an important role in various industry domains. OSS is generally developed by highly experienced developers who have multiple perspectives. However, previous studies have indicated that OSS has quality limitations in software maintainability. In general, OSS developers typically focus on achieving the correct functionality. In contrast, in addition to focusing on building software functionality, software engineering practices also focus on the structure of the software and on its maintainability. Code with a well-designed structure is more likely to result in high quality software. To better understand how peer code review can reduce ``code smells"" in existing OSS projects, we examined comments from code reviewers that identified code smells in OSS projects. This paper is a proof-of-concept that presents the preliminary results from an analysis of comments we obtained for two OSS projects, OpenStack and WikiMedia, both of which use the code review data repository called Gerrit. The preliminary results of this ongoing research show that code reviewers comment on only a small number of code smells. The full-scale results would contribute to the empirical body of validated knowledge in the field of OSS quality and code review. </p>","","","10.1145/2975945.2975947","","","Software engineering;code smell;open source software","","","","","","","","","","","4-4 Sept. 2016","","ACM","ACM Conferences"
"VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits","H. Perl; S. Dechand; M. Smith; D. Arp; F. Yamaguchi; K. Rieck; S. Fahl; Y. Acar","Fraunhofer FKIE, Bonn, Germany","Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security","20161111","2015","","","426","437","<p>Despite the security community's best effort, the number of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. However, due to the enormous amount of code being produced, as well as a the lack of manpower and expertise, not all code is sufficiently audited. Thus, many vulnerabilities slip into production systems. A best-practice approach is to use a code metric analysis tool, such as Flawfinder, to flag potentially dangerous code so that it can receive special attention. However, because these tools have a very high false-positive rate, the manual effort needed to find vulnerabilities remains overwhelming. In this paper, we present a new method of finding potentially dangerous code in code repositories with a significantly lower false-positive rate than comparable systems. We combine code-metric analysis with metadata gathered from code repositories to help code review teams prioritize their work. The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database. Second, based on this database, we trained a SVM classifier to flag suspicious commits. Compared to Flawfinder, our approach reduces the amount of false alarms by over 99 % at the same level of recall. Finally, we present a thorough quantitative and qualitative analysis of our approach and discuss lessons learned from the results. We will share the database as a benchmark for future research and will also provide our analysis tool as a web service.</p>","","","10.1145/2810103.2813604","","","machine learning;static analysis;vulnerabilities","","","","","1","","","","","","12-16 Oct. 2015","","ACM","ACM Conferences"
"Comparative Evaluation of Architectural and Code-Level Approaches for Finding Security Vulnerabilities","R. Vanciu; E. Khalaj; M. Abi-Antoun","Wayne State University, Detroit, MI, USA","Proceedings of the 2014 ACM Workshop on Security Information Workers","20161111","2014","","","27","34","<p>During architectural risk analysis, Security Information Workers (SIWs) reason about security-relevant architectural flaws using a high-level representation of the system's structure instead of directly reading the code as in during a code review. It is still hard to extract from the code a high-level representation that is sound, conveys design intent, and enables expressive constraints that can find security vulnerabilities. As a result, architecture-level approaches are less mature than code-level ones that extract low-level representations that are not directly intended for use by SIWs.</p> <p>In this paper, we compare an architecture-level approach with a code-level approach in terms of effectiveness (precision and recall) across test cases with injected vulnerabilities that range from coding bugs to architectural flaws. The evaluation shows that an architecture-level approach can uncover some security vulnerabilities with better precision and recall than a code-level approach. Moreover, it shows that the effectiveness of the approaches varies greatly based on whether the security vulnerability is a coding bug or an architectural flaw. These results may help SIWs select the right tools for the job of securing their systems.</p>","","","10.1145/2663887.2663905","","","","","","","","","","","","","","7-7 Nov. 2014","","ACM","ACM Conferences"
"Social network analysis in open source software peer review","X. Yang","Nara Institute of Science and Technology, Japan","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","20161111","2014","","","820","822","<p> Software peer review (aka. code review) is regarded as one of the most important approaches to keep software quality and productivity. Due to the distributed collaborations and communication nature of Open Source Software (OSS), OSS review differs from traditional industry review. Unlike other related works, this study investigated OSS peer review pro- cesses from social perspective by using social network anal- ysis (SNA). We analyzed the review history from three typi- cal OSS projects. The results provide hints on relationships among the OSS reviewers which can help to understand how developers work and communicate with each other. </p>","","","10.1145/2635868.2661682","","","open source;peer review;social network","","","","","","","","","","","16-21 Nov. 2014","","ACM","ACM Conferences"
"ASystemC: an AOP extension for hardware description language","Y. Endoh","Toshiba Corporation, Kawasaki, Japan","Proceedings of the tenth international conference on Aspect-oriented software development companion","20161111","2011","","","19","28","<p>Hardware-design requirements are becoming increasingly complex. Accordingly, the hardware developer is also beginning to use modern programming languages instead of traditional hardware description languages. However, modularity of the current hardware design has not changed from that of the traditional design. In this paper, we first conducted empirical investigation by interviews with real-world developers of circuit products, and confirmed that there exist cross-cutting concerns in actual products. The cross-cutting concerns fall into two types: one in common with software development and one specific to hardware design. In light of these results, this paper proposes ASystemC, an AOP extension for the hardware description language SystemC. ASystemC provides AOP features based on the AspectJ-like pointcut-advice mechanism. The design principle of ASystemC is practicality; we designed ASystemC to accept existing SystemC source code, and to weave aspects by using source-to-source conversion that outputs human-readable SystemC code. This design allows a user to utilize not only existing codes but also the existing knowledge and development process, as much as possible. As a result, ASystemC does not require modification of the existing source code review process and source analysis/manipulation tools, even if there is a developer unfamiliar with ASystemC in a development team. In addition, we confirmed the practicality and fiexibility of ASystemC through case studies: estimation of circuit size by using simulation, feature-configurable products and LTL verification. These cases are abstracted from actual problems in our products. They require not only code-level changes but also structural changes.</p>","","","10.1145/1960314.1960322","","","hardware description language;systemc","","","","","2","1","","","","","21-25 March 2011","","ACM","ACM Conferences"
"Collaborative infrastructure for test-driven scientific model validation","C. Omar; J. Aldrich; R. C. Gerkin","Carnegie Mellon University, USA","Companion Proceedings of the 36th International Conference on Software Engineering","20161111","2014","","","524","527","<p> One of the pillars of the modern scientific method is model validation: comparing a scientific model's predictions against empirical observations. Today, a scientist demonstrates the validity of a model by making an argument in a paper and submitting it for peer review, a process comparable to code review in software engineering. While human review helps to ensure that contributions meet high-level goals, software engineers typically supplement it with unit testing to get a more complete picture of the status of a project. </p> <p> We argue that a similar test-driven methodology would be valuable to scientific communities as they seek to validate increasingly complex models against growing repositories of empirical data. Scientific communities differ from software communities in several key ways, however. In this paper, we introduce SciUnit, a framework for test-driven scientific model validation, and outline how, supported by new and existing collaborative infrastructure, it could integrate into the modern scientific process. </p>","","","10.1145/2591062.2591129","","","cyberinfrastructure;model validation;unit testing","","","","","","","","","","","May 31 2014-June 7 2014","","ACM","ACM Conferences"
"Learning natural coding conventions","M. Allamanis; E. T. Barr; C. Bird; C. Sutton","University of Edinburgh, UK","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","20161111","2014","","","281","293","<p> Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a project‚Äôs coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94 % accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted. </p>","","","10.1145/2635868.2635883","","","Coding conventions;naturalness of software","","","","","22","","","","","","16-21 Nov. 2014","","ACM","ACM Conferences"
"The MetricsGrimoire database collection","J. M. Gonzalez-Barahona; G. Robles; D. Izquierdo-Cortazar","Universidad Rey Juan Carlos, GSyC/LibreSoft","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","478","481","<p>The MetricsGrimoire system is composed by a set of tools designed to retrieve data from repositories related to software development. Their aim is to produce organized databases suitable for easy querying with research and industrial purposes. The data in those databases have a similar structure, to easy cross-database studies, and can be enriched with information such as linkage of the multiple identities of actors, or their affiliation. This paper presents the general structure of those databases, and a collection of up-to-date database dumps that are publicly available. They correspond to two well-known projects, OpenStack, and Eclipse, including data from source code management repositories, issue tracking systems, mailing lists, and code review systems.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Nirikshan: process mining software repositories to identify inefficiencies, imperfections, and enhance existing process capabilities","M. Gupta","IIIT Delhi, India","Companion Proceedings of the 36th International Conference on Software Engineering","20161111","2014","","","658","661","<p> Process mining is to extract knowledge about business processes from data stored implicitly in ad-hoc way or explicitly by information systems. The aim is to discover runtime process, analyze performance and perform conformance verification, using process mining tools like ProM and Disco, for single software repository and processes spanning across multiple repositories. Application of process mining to software repositories has recently gained interest due to availability of vast data generated during software development and maintenance. Process data are embodied in repositories which can be used for analysis to improve the efficiency and capability of process, however, involves a lot of challenges which have not been addressed so far. Project team defines workflow, design process and policies for tasks like issue tracking (defect or feature enhancement), peer code review (review the submitted patch to avoid defects before they are injected) etc. to streamline and structure the activities. The reality may not be the same as defined because of imperfections so the extent of non-conformance needs to be measured. We propose a research framework `Nirikshan' to process mine the data of software repositories from multiple perspectives like process, organizational, data and time. We apply process mining on software repositories to derive runtime process map, identify and remove inefficiencies and imperfections, extend the capabilities of existing software engineering tools to make them more process aware, and understand interaction pattern between various contributors to improve the efficiency of project. </p>","","","10.1145/2591062.2591080","","","Business Process Intelligence;Empirical Software Engineering and Measurements;Mining Software Repositories;Open-Source Software;Process Mining","","","","","","","","","","","May 31 2014-June 7 2014","","ACM","ACM Conferences"
"The Firefox temporal defect dataset","M. Habayeb; A. Miranskyy; S. S. Murtaza; L. Buchanan; A. Bener","Ryerson University, Canada","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","498","501","<p>The bug tracking repositories of software projects capture initial defect (bug) reports and the history of interactions among developers, testers, and customers. Extracting and mining information from these repositories is time consuming and daunting. Researchers have focused mostly on analyzing the frequency of the occurrence of defects and their attributes (e.g., the number of comments and lines of code changed, count of developers). However, the counting process eliminates information about the temporal alignment of events leading to changes in the attributes count. Software quality teams could plan and prioritize their work more efficiently if they were aware of these temporal sequences and knew their frequency of occurrence.</p> <p>In this paper, we introduce a novel dataset mined from the Firefox bug repository (Bugzilla) which contains information about the temporal alignment of developer interactions. Our dataset covers eight years of data from the Firefox project on activities throughout the project's lifecycle. Some of these activities have not been reported in frequency-based or other temporal datasets. The dataset we mined from the Firefox project contains new activities, such as reporter experience, file exchange events, code-review process activities, and setting of milestones. We believe that this new dataset will improve analysis of bug reports and enable mining of temporal relationships so that practitioners can enhance their bug-fixing process.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Virtual reality in software engineering: affordances, applications, and challenges","A. Elliott; B. Peiris; C. Parnin","North Carolina State University, Raleigh, North Carolina","Proceedings of the 37th International Conference on Software Engineering","20161111","2015","2","","547","550","<p>Software engineers primarily interact with source code using a keyboard and mouse, and typically view software on a small number of 2D monitors. This interaction paradigm does not take advantage of many affordances of natural human movement and perception. Virtual reality (VR) can use these affordances more fully than existing developer environments to enable new creative opportunities and potentially result in higher productivity, lower learning curves, and increased user satisfaction. This paper describes the affordances offered by VR; demonstrates the benefits of VR and software engineering in prototypes for live coding and code review; and discusses future work, open questions, and the challenges of VR.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"ViDI: the visual design inspector","Y. Tymchuk; A. Mocci; M. Lanza","University of Lugano, Switzerland","Proceedings of the 37th International Conference on Software Engineering","20161111","2015","2","","653","656","<p>We present ViDI (Visual Design Inspector), a novel code review tool which focuses on quality concerns and design inspection as its cornerstones. It leverages visualization techniques to represent the reviewed software and augments the visualization with the results of quality analysis tools. To effectively understand the contribution of a reviewer in terms of the impact of her changes on the overall system quality, ViDI supports the recording and further inspection of reviewing sessions. ViDI is an advanced prototype which we will soon release to the Pharo open-source community.</p> <p>Video URL: http://youtu.be/EtdkcNBJAec</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Identifying software process management challenges: survey of practitioners in a large global IT company","M. Gupta; A. Sureka; S. Padmanabhuni; A. M. Asadullah","Indraprastha Institute of Information Technology, Delhi, India","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","346","356","<p>Process mining consists of mining event logs generated from business process execution supported by Information Systems (IS). Process mining of software repositories has diverse applications because vast data is generated during Software Development Life Cycle (SDLC) and archived in IS such as Version Control System (VCS), Peer Code Review (PCR) System, Issue Tracking System (ITS), and mail archives. There is need to explore its applications on different repositories to aid managers in process management. We conduct two phase surveys and interviews with managers in a large, global, IT company. The first survey and in-person interviews identify the process challenges encountered by them that can be addressed by novel applications of process mining. We filter, group and abstract responses formulating 30 generic problem statements. On the basis of process mining type, we classify identified problems to eight categories such as control analysis, organizational analysis, conformance analysis, and preventive analysis. The second survey asks distinct participants the importance of solving identified problems. We calculate proposed Net Importance Metric (NIM) using 1262 ratings from 43 participants. Combined analysis of NIM and first survey responses reveals that the problems mentioned by few practitioners in first survey are considered important by majority in the second survey. We elaborate on possible solutions and challenges for most frequent and important problems. We believe solving these validated problems will help managers in improving project quality and productivity.</p>","","","","","","process mining;qualitative study;software development life cycle;software repositories","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Security Analysis of the Estonian Internet Voting System","D. Springall; T. Finkenauer; Z. Durumeric; J. Kitcat; H. Hursti; M. MacAlpine; J. A. Halderman","University of Michigan, Ann Arbor, MI, USA","Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security","20161111","2014","","","703","715","<p>Estonia was the first country in the world to use Internet voting nationally, and today more than 30% of its ballots are cast online. In this paper, we analyze the security of the Estonian I-voting system based on a combination of in-person election observation, code review, and adversarial testing. Adopting a threat model that considers the advanced threats faced by a national election system---including dishonest insiders and state-sponsored attacks---we find that the I-voting system has serious architectural limitations and procedural gaps that potentially jeopardize the integrity of elections. In experimental attacks on a reproduction of the system, we demonstrate how such attackers could target the election servers or voters' clients to alter election results or undermine the legitimacy of the system. Our findings illustrate the practical obstacles to Internet voting in the modern world, and they carry lessons for Estonia, for other countries considering adopting such systems, and for the security research community.</p>","","","10.1145/2660267.2660315","","","attacks;case studies;estonia;internet voting;security;security analysis;voting;vulnerabilities","","","","","12","","","","","","3-7 Nov. 2014","","ACM","ACM Conferences"
"Cesar: Visual representation of source code vulnerabilities","H. Assal; S. Chiasson; R. Biddle","School of Computer Science, Carleton University","2016 IEEE Symposium on Visualization for Cyber Security (VizSec)","20161110","2016","","","1","8","Code analysis tools are not widely accepted by developers, and software vulnerabilities are detected by the thousands every year. We take a user-centered approach to that problem, starting with analyzing one of the popular open source static code analyzers, and uncover serious usability issues facing developers. We then design Cesar, a system offering developers a visual analysis environment to support their quest to rid their code of vulnerabilities. We present a prototype implementation of Cesar, and perform a usability analysis of the prototype and the visualizations it employs. Our analysis shows that the prototype is promising in promoting collaboration, exploration, and enabling developers to focus on the overall quality of their code as well as inspect individual vulnerabilities. We finally provide general recommendations to guide future designs of code review tools to enhance their usability.","","Electronic:978-1-5090-1605-1; POD:978-1-5090-1606-8","10.1109/VIZSEC.2016.7739576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7739576","","Collaboration;Computer science;Prototypes;Security;Usability;Visualization","program diagnostics;program visualisation;source code (software)","Cesar;open source static code analyzers;source code vulnerabilities;usability issues;user-centered approach;visual analysis environment","","1","","","","","","24-24 Oct. 2016","","IEEE","IEEE Conferences"
"Investigating code reading techniques for novice inspectors: an industrial case study","G. Rong; H. Zhang; D. Shao","Nanjing University, Nanjing China","Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering","20160129","2014","","","1","10","<p>Code inspection is believed to be an effective technique to remove defects and improve software quality. However, the adoption of code inspection in industry is far less than it should be, which may lead to many novice inspectors in industry. For these novice inspectors, a suitable reading technique should be of the first step to begin this quality journey. While reports indicated that Checklist-Based Reading (CBR) and Ad Hoc Reading (AHR) had been the most adopted inspection techniques in industry, we deem it is necessary to investigate these two techniques first. In this paper, we present a case study of the adoption of code reading techniques in one small-sized software company. In this study, five engineers used different techniques (i.e., CBR vs. AHR) to read source code in 20 modules. Both quantitative data and qualitative data are collected during the case study. Initial analysis of these data indicates that industrial novice inspectors using CBR tended to have a lower reading speed than those using AHR. Both techniques could help these novice inspectors to remove a certain portion of defects during code review, and compared to AHR approach, CBR may help them find larger percentage of defects. However, there still exist several issues, for example, missing large portion of review-removable defects could not be avoided for novice inspectors. What's more, CBR may limit reviewers' ability to find defects outside the checklist, and to establish effective checklist remains a big challenge for novice inspectors. Besides, both internal factors (e.g., faith in inspection to achieve high quality) as well as external factors (e.g., schedule pressure) may also impact novice inspectors to adopt code reading.</p>","","","10.1145/2601248.2601280","","","code reading;industrial case study;novice inspectors","","","","","","","","","","","13-14 May 2014","","ACM","ACM Conferences"
"A semantic integrated development environment","F. Logozzo; M. Barnett; M. A. F√§hndrich; P. Cousot; R. Cousot","Microsoft Research, Redmond, WA, USA","Proceedings of the 3rd annual conference on Systems, programming, and applications: software for humanity","20160129","2012","","","15","16","<p>We present SIDE, a Semantic Integrated Development Environment. SIDE uses static analysis to enrich existing IDE features and also adds new features. It augments the way existing compilers find syntactic errors - in real time, as the programmer is writing code without execution - by also finding semantic errors, <i>e.g.</i>, arithmetic expressions that may overflow. If it finds an error, it suggests a repair in the form of code - <i>e.g.</i>, providing an equivalent yet non-overflowing expression. Repairs are correct by construction. SIDE also enhances code refactoring (by suggesting precise yet general contracts), code review (by answering what-if questions), and code searching (by answering questions like ""<i>find all the callers where</i> x < y"").</p> <p>SIDE is built on the top of CodeContracts and the Roslyn CTP. CodeContracts provide a lightweight and programmer-friendly specification language. SIDE uses the abstract interpretation-based CodeContracts static checker (cccheck/Clousot) to obtain a deep semantic understanding of what the program does.</p>","","","10.1145/2384716.2384724","","","abstract interpretation;design by contract;integrated development enviroment;method extraction;program repair;program transformation;refactoring;static analysis","","","","","","","","","","","19-26 Oct. 2012","","ACM","ACM Conferences"
"Informing development decisions: from data to information","O. Baysal","University of Waterloo, Canada","Proceedings of the 2013 International Conference on Software Engineering","20160129","2013","","","1407","1410","<p> Software engineers generate vast quantities of development artifacts such as source code, bug reports, test cases, usage logs, etc., as they create and maintain their projects. The information contained in these artifacts could provide valuable insights into the software quality and adoption, as well as development process. However, very little of it is available in the way that is immediately useful to various stakeholders. This research aims to extract and analyze data from software repositories to provide software practitioners with up-to-date and insightful information that can support informed decisions related to the business, management, design, or development of software systems. This data-centric decision-making is known as analytics. In particular, we demonstrate that by employing software development analytics, we can help developers make informed decisions around user adoption of a software project, code review process, as well as improve developers' awareness of their working context. </p>","","","","","","","","","","","1","","","","","","18-26 May 2013","","ACM","ACM Conferences"
"The MetricsGrimoire Database Collection","J. M. Gonzalez-Barahona; G. Robles; D. Izquierdo-Cortazar","","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","478","481","The Metrics Grimoire system is composed by a set of tools designed to retrieve data from repositories related to software development. Their aim is to produce organized databases suitable for easy querying with research and industrial purposes. The data in those databases have a similar structure, to easy cross-database studies, and can be enriched with information such as linkage of the multiple identities of actors, or their affiliation. This paper presents the general structure of those databases, and a collection of up-to-date database dumps that are publicly available. They correspond to two well-known projects, Open Stack, and Eclipse, including data from source code management repositories, issue tracking systems, mailing lists, and code review systems.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180122","","Companies;Databases;Merging;Open source software;Standards organizations","database management systems;software engineering","Eclipse;Open Stack;metrics grimoire database collection;software development;source code management repositories;up-to-date database","","3","","4","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Identifying Software Process Management Challenges: Survey of Practitioners in a Large Global IT Company","M. Gupta; A. Sureka; S. Padmanabhuni; A. M. Asadullah","Indraprastha Inst. of Inf. Technol., Delhi, India","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","346","356","Process mining consists of mining event logs generated from business process execution supported by Information Systems (IS). Process mining of software repositories has diverse applications because vast data is generated during Software Development Life Cycle (SDLC) and archived in IS such as Version Control System (VCS), Peer Code Review (PCR) System, Issue Tracking System (ITS), and mail archives. There is need to explore its applications on different repositories to aid managers in process management. We conduct two phase surveys and interviews with managers in a large, global, IT company. The first survey and in-person interviews identify the process challenges encountered by them that can be addressed by novel applications of process mining. We filter, group and abstract responses formulating 30 generic problem statements. On the basis of process mining type, we classify identified problems to eight categories such as control analysis, organizational analysis, conformance analysis, and preventive analysis. The second survey asks distinct participants the importance of solving identified problems. We calculate proposed Net Importance Metric (NIM) using 1262 ratings from 43 participants. Combined analysis of NIM and first survey responses reveals that the problems mentioned by few practitioners in first survey are considered important by majority in the second survey. We elaborate on possible solutions and challenges for most frequent and important problems. We believe solving these validated problems will help managers in improving project quality and productivity.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180093","Process Mining;Qualitative Study;Software Development Life Cycle;Software Repositories","Companies;Data mining;Interviews;Measurement;Process control;Software","business process re-engineering;data mining;information systems;information technology;software process improvement","IS;NIM;SDLC;business process execution;event logs mining;global IT company;information systems;net importance metric;process mining;productivity;project quality;software development life cycle;software process management","","1","","32","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"Poster abstract: Formal analysis of fresenius infusion pump (FIP)","V. Sfyrla; S. Marcoux; C. Vittoria","Objet Direct 4 Avenue Doyen Louis Weil Grenoble, France","2013 ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS)","20130919","2013","","","253","253","Summary form only given. Today's medical devices are based on embedded architecture, with software used to control the underlying hardware. They are highly critical since errors in the software can endanger end users such as patients and medics. Medical devices should be designed and manufactured in such a way that when used, they perform as intended and they ensure a high level of safety. Current industrial practices are based on testing processes to check if the software meets the specifications and if it fulfills its purpose. However, testing does have several disadvantages that limit the reliability of this verification and validation process. Testing cannot guarantee that a device will function properly under all conditions and bugs can never be completely identified withing a program. Several attempts have already been made to provide standards for the formal verification of safety properties of medical devices, initiated by the Generic Infusion Pump project [2]. Our work is a collaboration between Objet Direct R&D and Fresenius [1]. Fresenius is a leading international health care group which produces and markets pharmaceuticals and medical devices. We aim to investigate innovative methods for software development, validation and verification. We study existing results provided amongst others by [3, 4] which we intend to extend by analyzing the Fresenius Infusion Pump (FIP) software. FIP automatizes the delivery process of fluid medical solution into patient's body. Its design is based on three layers. The highest level is the user interface and consists of three components, the administration protocol, the application system and the power management. The middle level consists of the pumping control components and the lowest level contains driver components such as Door, Watchdog, Optical Disk, Motor. FIP is modeled in UML (a total of 100 state machines) and the requirements are written in natural language. The implementation of the model is done in C- + with automatic code generation. For the V&V process, software testing checks if the implementation meets the requirements using fault scenarios written in UML. The main objective of this project is to use model-based design for migrating from software testing to formal based solution for verifying the Fresenius Infusion Pump. The goal is to use model checking technologies in order to verify requirements and eliminate bugs during the design process. Several faulty design patterns have already been identified to be caused by deadlocks, lost signal events, stack overflow, violation of real-time properties, incoherent behavior of UML state machines. We present and analyze the case study of the FIP's Motor component, a driver component of the lowest level. Its interest lies on the fact that while the Motor Control is stopped, the Motor Driver is still running. This faulty behavior was detected during the test checks and bug was partially corrected in code review.","","Electronic:978-1-4503-1996-6; POD:978-1-4503-1996-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604031","","Computer bugs;Medical services;Pumps;Safety;Software;Testing;Unified modeling language","biomedical equipment;embedded systems;health and safety;reliability","C++ language;FIP motor component;Generic Infusion Pump project;UML model;automatic code generation;embedded architecture;faulty design patterns;formal analysis;formal verification;fresenius infusion pump;industrial practices;medical devices;patient safety;safety property;testing processes;validation process;verification reliability","","0","","4","","","","8-11 April 2013","","IEEE","IEEE Conferences"
"Targeting Specific Customer Satisfactions Issues with ODC Analysis","R. Chillarege","","2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops","20130110","2012","","","64","64","Often, the task of quality improvement is complicated by having defect backlogs and process gaps that seem too hard to tackle in a short period of time. This is particularly true of most development organizations that have performed root cause analysis or process assessments which identify long list of potential improvement opportunities. The list include identification of one or more of a variety of issues - such as architecture weaknesses, test coverage inadequacy, inadequate regression opportunity, weak code review or inadequacy of appropriate skills.","","Electronic:978-0-7695-4928-6; POD:978-1-4673-5048-8","10.1109/ISSREW.2012.105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405419","","Abstracts;Customer satisfaction;IEEE Potentials;Organizations;Random access memory","customer satisfaction","ODC analysis;customer satisfaction;defect backlog;development organization;process assessment;process gap;root cause analysis","","0","","","","","","27-30 Nov. 2012","","IEEE","IEEE Conferences"
"A program differencing algorithm for verilog HDL","A. Duley; C. Spandikow; M. Kim","Intel Corporation, Austin, TX, USA","Proceedings of the IEEE/ACM international conference on Automated software engineering","20110708","2010","","","477","486","<p>During code review tasks, comparing two versions of a hardware design description using existing program differencing tools such as <i>diff</i> is inherently limited because existing program differencing tools implicitly assume sequential execution semantics, while hardware description languages are designed to model concurrent computation. We designed a position-independent differencing algorithm to robustly handle language constructs whose relative orderings do not matter. This paper presents <i>Vdiff</i>, an instantiation of this position-independent differencing algorithm for Verilog HDL. To help programmers reason about the differences at a high-level, Vdiff outputs syntactic differences in terms of Verilog-specific change types. We evaluated Vdiff on two open source hardware design projects. The evaluation result shows that Vdiff is very accurate, with overall 96.8% precision and 97.3% recall when using manually classified differences as a basis of comparison.</p>","","","10.1145/1858996.1859093","","","change types;empirical study;hardware description languages;program differencing;verilog","","","","","8","","","","","","20-24 Sept. 2010","","ACM","ACM Conferences"
"Better Software, Better Research","C. Goble","University of Manchester, UK","IEEE Internet Computing","20140828","2014","18","5","4","8","Modern scientific research isn't possible without software. However, its vital role is often overlooked by funders, universities, assessment committees, and even the research community itself. This is a serious issue that needs urgent attention. This article raises a number of points concerning quality, code review, and openness; development practices and training in scientific computing; career recognition of research software engineers; and sustainability models for funding scientific software. We must get software recognized to be the first-class experimental scientific instrument that it is and get ""better software for better research.""","1089-7801;10897801","","10.1109/MIC.2014.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6886129","engineering;scientific software;sustainability","","","","","9","","11","","","","Sept.-Oct. 2014","","IEEE","IEEE Journals & Magazines"
"Invited talk: How the Fundamental Assurance Question pervades certification","M. Feather","Jet Propulsion Laboratory, California Institute of Technology","2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","20131219","2013","","","1","1","Assurance Cases are promoted as a means by which to present an argument for why a system is sufficiently dependable (alternate terms for the same concept include ‚ÄúDependability Cases‚Äù, ‚ÄúSafety Cases‚Äù when the concern is safety, ‚ÄúSecurity Cases‚Äù, etc.). The purpose of such an argument is typically to inform a decision maker, often in the context of a key certification decision, so he/she will be better able to make that decision. Examples of such decisions include whether to deploy a system, whether to make an upgrade to an existing system, whether to advance a system to the next phase in its development. Assurance Cases are widely practiced in Europe, and are receiving growing attention in North America. For software systems in particular, an assurance-case-based approach is often contrasted to a standards-based approach, the latter being characterized as more ‚Äúprescriptive‚Äù in specifying the process and techniques to be applied to sufficiently assure software. The proponents of an assurance-case-based approach point out that the need to construct a sufficiently convincing Assurance Case puts the onus on the provider of the software to present the argument for its dependability, as compared to putting the onus on the regulator to have described in advance a sufficient process to be followed by the provider in their development of software. The distinction is not as clear-cut as it might at first seem. Both approaches have the need to assess by how much the outcomes of assurance activities (e.g., testing; code review; fault tree analysis; model-checking) raise confidence in decisions made about the system. For a standards-based approach, how is it possible to determine whether the required standard practice can be relaxed or waived entirely, when an alternate approach can be substituted, when additional activities are warranted? These determinations hinge on an understanding of the role of assurance act- vities, and the information conveyed by their outcome. These questions will arise more often and become more urgent to answer in the evolving world mentioned in the Call for Papers. For an assurance-case-based approach the outcome of an assurance activity will be evidence located within the assurance case, which makes it easier to see the role it plays in the overall assurance argument, but the same question arises ‚Äî what is its information contribution to confidence? Distilling these gives the ‚ÄúFundamental Assurance Question,‚Äù namely how much do assurance activities contribute to raising decision confidence about system qualities, such as safety? These questions ‚Äî and an intriguing start at answering them ‚Äî will be the focus of this talk.","","Electronic:978-1-4799-2552-0; POD:978-1-4799-2553-7","10.1109/ISSREW.2013.6688831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688831","","","","","","0","","","","","","4-7 Nov. 2013","","IEEE","IEEE Conferences"
"Identifying Security Spots for Data Integrity","P. Ogale; M. Shin; S. Abeysinghe","","2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)","20180622","2018","01","","462","467","This paper describes an approach to detecting malicious code introduced by insiders, which can compromise the data integrity in a program. The approach identifies security spots in a program, which are either malicious code or benign code. Malicious code is detected by reviewing each security spot to determine whether it is malicious or benign. The integrity breach conditions (IBCs) for object-oriented programs are specified to identify security spots in the programs. The IBCs are specified by means of the concepts of coupling within an object or between objects. A prototype tool is developed to validate the approach with a case study.","0730-3157;07303157","Electronic:978-1-5386-2666-5; POD:978-1-5386-2667-2","10.1109/COMPSAC.2018.10277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377905","Security spot, Data integrity, Malicious code, Insider attack, Coupling, Integrity breach condition","Couplings;Data integrity;Databases;Malware;Tools","","","","","","","","","","23-27 July 2018","","IEEE","IEEE Conferences"
"RadStream: An interactive visual display of radiology workflow for delay detection in the clinical imaging process","F. Abukhodair; K. Khashoggi; T. O'Connell; C. Shaw","School of Interactive Arts and Technology, Simon Fraser University","2017 IEEE Workshop on Visual Analytics in Healthcare (VAHC)","20180618","2017","","","69","76","Given the high caseload most radiology departments face on a daily basis, workflow streaming becomes a necessity. In addition, workflow optimization can significantly reduce cost and increase efficiency and productivity. In this paper, we present RadStream: a web-based retrospective, exploratory, interactive data visualization tool that provides a comprehensive overview of the radiology department's daily activities. We worked closely with radiology staff to analyze the department workflow and classify the analytical tasks required by domain experts in order to inform the design of the tool, and together we abstracted the steps and factors involved in the clinical imaging process. The visual representation emphasizes the time interval between the different steps and uses color-coding to denote the status of a process (on-time, acceptably late, late) in compliance with standard radiology turnaround times (TATs). RadStream is an ongoing research project. Its main focus is on monitoring performance with special attention to duration, delays, and compliance with standard TATs. RadStream is currently undergoing evaluation by radiology staff using hospital data and real scenarios to evaluate its effectiveness, and the initial feedback received is very promising. Working with domain experts, we seek to provide a tool that helps monitor and improve the radiology department's activities. And based on a few review sessions with staff radiologists, we sensed a general excitement about RadStream as a quality assurance tool. We have also collected some constructive feedback we can build upon for future releases.","","Electronic:978-1-5386-3187-4; POD:978-1-5386-3188-1","10.1109/VAHC.2017.8387543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8387543","Radiology;temporal data;visual analytics in healthcare;workflow","Data visualization;Imaging;Medical services;Production;Radiology;Tools;Visualization","","","","","","","","","","1-1 Oct. 2017","","IEEE","IEEE Conferences"
"Exploring How Software Developers Work with Mention Bot in GitHub","Z. Peng; J. Yoo; M. Xia; S. Kim; X. Ma","The Hong Kong University of Science and Technology, HKSAR, China","Proceedings of the Sixth International Symposium of Chinese CHI","20180614","2018","","","152","155","<p>Recently, major software development platforms have started to provide automatic reviewer recommendation (ARR) services for pull requests, to improve the collaborative coding review process. However, the user experience of ARR is under-investigated. In this paper, we use a two-stage mixed-methods approach to study how software developers perceive and work with the Facebook mention bot, one of the most popular ARR bots in GitHub. Specifically, in Stage I, we conduct archival analysis on projects employing mention bot and a user survey to investigate the bot's performance. A year later, in Stage II, we revisit these projects and conduct additional surveys and interviews with three user groups: project owners, contributors and reviewers. Results show that developers appreciate mention bot saving their effort, but are bothered by its unstable setting and unbalanced workload allocation. We conclude with design considerations for improving ARR services.</p>","","","10.1145/3202667.3202694","","","Automatic reviewer recommendation services;mixed-methods;software development platform;user experience","","","","","","","","","","","21-22 April 2018","","ACM","ACM Conferences"
"Building a Theoretical Framework for the Development of Digital Scholarship Services in China's Universities","L. Zhou; X. Lu; T. Zijlstra","Wuhan University, Wuhan , China","Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries","20180614","2018","","","119","122","<p>The provision of digital scholarship services (DSS) in China's university is very unsystematic and fragmented. This paper reports on a literature review that aims to develop a comprehensive theoretical framework, which can serve as a practical guide for the development of DSS in China's university libraries. The framework was developed through systematically searching, screening, assessing, coding and aggregating DSS as reported in the existing body of literature. Academic literature, both in Chinese and English, as well as relevant professional reports are carefully searched, selected and analysed. The analysis of the literature pointed to 25 DSS in six categories: supporting services, formulating research ideas, locating research partners, proposal writing, research process, and publication. This paper focuses on the development of DSS in China's university libraries, but the research findings and the framework developed can provide useful insights and indications that can be shared across international borders.</p>","","","10.1145/3197026.3197060","","","digital scholarship;digital scholarship services;theoretical framework;university libraries","","","","","","","","","","","3-7 June 2018","","ACM","ACM Conferences"
"Broadband Microwave Signal Processing Enabled by Polarization-based Photonic Microwave Phase Shifters","Y. Zhang; S. Pan","Key Laboratory of Radar Imaging and Microwave Photonics (Nanjing Univ. Aeronaut. Astronaut.), Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China.","IEEE Journal of Quantum Electronics","","2018","Early Access","Early Access","1","1","A polarization-based photonic microwave phase shifter that implemented with an orthogonal circularly-/linearly-polarized wavelength generator incorporated with a polarizer provides advantages of high scalability, high-speed phase tuning, large operation bandwidth, full-360-degree tunability, flat power response, and compact configuration, making it a key enabler for tunable microwave photonic filtering, optically controlled beamforming, high speed phase coding, large time-bandwidth product (TBWP) signal generation, phase noise measurement, and co-site RF signal cancellation. In this paper, the recent advances of polarization-based photonic microwave phase shifters are reviewed. A general model of the polarization-based photonic microwave phase shifter is established, and the applications of the polarization-based photonic microwave phase shifter are described and discussed.","0018-9197;00189197","","10.1109/JQE.2018.2847398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8385211","Microwave Photonics;frequency mixing;frequency multiplying;phase shifter;signal processing","Microwave amplifiers;Microwave filters;Microwave photonics;Optical attenuators;Phase shifters","","","","","","","","","20180614","","","IEEE","IEEE Early Access Articles"
"Do I Need an IRB?: Computer Science Education Research and Institutional Review Board (IRB) (Abstract Only)","J. Zhang; K. Huett; J. Gratch","Texas Woman's University, Denton, TX, USA","Proceedings of the 49th ACM Technical Symposium on Computer Science Education","20180611","2018","","","1083","1083","<p>The importance of rigorous standards in computer science education research to include a description of hypotheses, research questions, methodologies, and results has been recognized in the computer science education community. The driving force for computer science education research is to understand the learning needs of our students who are human subjects. Therefore, some computer science education researchers may need to answer a critical question before they start their planned research: Do I need Institutional Review Board (IRB) approval to conduct this research using my students as research subjects? The key goal of the IRB is to protect human subjects from physical or psychological harm (""Code of Federal Regulations, Title 45, Public Welfare, Part 46, Protection of Human Subjects""). Although commonly used in the fields of health and social sciences, the role and purpose of IRB, the different categories of IRB reviews, the timeline from planning and submission of an IRB application, and the general rules for citing IRB in publications and grant proposals are not widely understood in the computer science education research community. In this poster, the authors describe the history, the purpose, review categories, and guidelines for reporting on the IRB. The authors will tailor the discussions on the different IRB review categories to computer science educators interested in conducting computer science education research with their students.</p>","","","10.1145/3159450.3162253","","","computer science education research;human subjects;institutional review board","","","","","","","","","","","21-24 Feb. 2018","","ACM","ACM Conferences"
"Two-Stage Programming Projects: Individual Work Followed by Peer Collaboration","L. Battestilli; A. Awasthi; Y. Cao","North Carolina State University, Raleigh, NC, USA","Proceedings of the 49th ACM Technical Symposium on Computer Science Education","20180611","2018","","","479","484","<p>Programming projects are widely used in CS1 classes to develop students' coding skills. To improve the learning impact of these projects, we propose and study a special project format named two-stage project in an introductory computer science course. For the first stage, students submit their programming projects individually followed by a second stage where they are paired to work on the same project in order to create an improved solution. Through peer collaboration, students review each other's work from the first stage, and write correctly-styled, well-documented, and more thoroughly tested code during the second stage. We used isomorphic assessments before and after the second stage of a project to measure students' understanding of the course material. Results indicate that two-stage projects tend to improve student understanding of course learning objectives. We also studied students' perceptions and experiences with two-stage projects, and their confidence toward computing. Students liked working on two-stage projects because they saw new ways to approach the same problem, and they liked discussions with their peers.</p>","","","10.1145/3159450.3159486","","","CS1;collaborative project;peer collaboration;two-stage project","","","","","","","","","","","21-24 Feb. 2018","","ACM","ACM Conferences"
"SLASH: Automatically Generating Flash Cards for Reviewing Concepts in Lectures Slides (Abstract Only)","W. Wu","University of Southern California, Los Angeles, CA, USA","Proceedings of the 49th ACM Technical Symposium on Computer Science Education","20180611","2018","","","1107","1107","<p>We present SLASH, a learning tool currently under development in our graduate program. SLASH aims to help students review concepts in lectures slides using flash cards automatically generated from the slides. Many courses in our program have weekly quizzes and students can get stressed quite easily. So we hope that SLASH can make the process of reviewing lectures more fun and interesting to the students. Extracting concepts from lectures slides is itself an interesting but challenging problem, since the contents of the slides may be fragmented (e.g., point-based, with an incomplete sentence for each point) and noisy (e.g., containing formulas and codes). Past research on text mining has tried to ""glue"" together the points to construct a grammatically correct sentence, which is then used to extract concepts and relationships. In contrast, we focus on discovering popular concepts in the slides and generating flash cards with (just) sufficient contexts to help students recall the concepts. To the best of our knowledge, this is the first work on the automatic generation of concept-based flash cards from lecture slides. In the presentation, we will show our preliminary work, example flash cards, student feedback, and challenges in developing SLASH. We believe that SLASH may benefit all instructors who are using PowerPoint for lecture presentation, and may be used to largely stimulate students' interests in learning the subjects.</p>","","","10.1145/3159450.3162224","","","concepts;flash cards;knowledge discovery;lecture slides;natural language processing","","","","","","","","","","","21-24 Feb. 2018","","ACM","ACM Conferences"
"Key Concepts for a Data Science Ethics Curriculum","J. S. Saltz; N. I. Dewar; R. Heckman","Syracuse University, Syracuse, NY, USA","Proceedings of the 49th ACM Technical Symposium on Computer Science Education","20180611","2018","","","952","957","<p>Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.</p>","","","10.1145/3159450.3159483","","","big data;computing education;data science;ethics","","","","","","","","","","","21-24 Feb. 2018","","ACM","ACM Conferences"
"DIVAS: Outreach to the Natural Sciences through Image Processing","M. Meysenburg; T. Durham Brooks; R. Burks; E. Doyle; T. Frey","Doane University, Crete, NE, USA","Proceedings of the 49th ACM Technical Symposium on Computer Science Education","20180611","2018","","","777","782","<p>The DIVAS (Digital Imaging and Vision Applications in Science) project addresses workforce challenges in science, technology, engineering, and mathematics by creating a pedagogical and programmatic ""on-ramp"" that empowers natural science majors to engage in authentic computational problems as members of skilled, professional teams. We are developing and testing institutional practices and curricular innovations that engage and train STEM undergraduate students to use Python programming, and image processing in particular, in their undergraduate research projects. Students are recruited into the DIVAS program in the first semester of their first year. DIVAS scholars and other participating students can experience a variety of interventions including: 1) a one-credit DIVAS seminar exploring several imaging and computing topics; 2) image capture and analysis modules in introductory- or upper-level biology and chemistry courses; 3) a week-long, intensive coding bootcamp that introduces bash, git, Python programming, and the OpenCV image processing library; 4) pair programming exercises to solve genuine morphometric and colorimetric problems; 5) an extended summer research project involving image processing; and 6) weekly code reviews to check on progress and provide guidance. The DIVAS projects measures the impact of these interventions on students' self-reported efficacy in using computation to solve problems, their attitudes towards computation, and their computational thinking skills, using both established and newly developed instruments. Our first year results show that multiple interventions have had significant positive impact on students' self-efficacy and interest in using computing in their future careers, and certain computational thinking skills.</p>","","","10.1145/3159450.3159537","","","assessment;outreach;undergraduate instruction","","","","","","","","","","","21-24 Feb. 2018","","ACM","ACM Conferences"
"Channel Precoding Based Message Authentication in Wireless Networks: Challenges and Solutions","D. Chen; N. Zhang; R. Lu; N. Cheng; K. Zhang; Z. Qin","","IEEE Network","","2018","Early Access","Early Access","1","7","Due to the broadcast characteristic of the wireless medium, message impersonation and substitution attacks can possibly be launched by an adversary with low cost in wireless communication networks. As an ingenious solution, physical layer based message authentication can achieve perfect security by leveraging channel precoding techniques to meet high level security requirements. In this article, we focus on channel-precoding-based message authentication (CPC-based authentication) over a binary-input wiretap channel (BIWC). Specifically, message authentication with physical layer techniques is first reviewed. Then, a CPC-based authentication framework and its security requirements are presented. Based on the proposed framework, an authentication scheme with polar codes over a binary symmetric wiretap channel (BSWC) is developed. Moreover, a case study is provided as an example of message authentication with polar codes over BSWC. Finally, open research topics essential to CPC-based authentication are discussed.","0890-8044;08908044","","10.1109/MNET.2018.1700392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360842","","Authentication;Channel coding;Message authentication;Noise measurement;Receivers","","","","","","","","","20180517","","","IEEE","IEEE Early Access Articles"
"A Review on Human-Centered IoT-Connected Smart Labels for the Industry 4.0","T. M. Fern√°ndez-Caram√©s; P. Fraga-Lamas","Department of Computer Engineering, Faculty of Computer Science, Campus de Elvi&#x00F1;a s/n, Universidade da Coru&#x00F1;a, A Coru&#x00F1;a, Spain","IEEE Access","20180605","2018","6","","25939","25957","One of the challenges of Industry 4.0 is the creation of vertical networks that connect smart production systems with design teams, suppliers, and the front office. To achieve such a vision, information has to be collected from machines and products throughout a smart factory. Smart factories are defined as flexible and fully connected factories that are able to make use of constant streams of data from operations and production systems. In such scenarios, the arguably most popular way for identifying and tracking objects is by adding labels or tags, which have evolved remarkably over the last years: from pure hand-written labels to barcodes, QR codes, and RFID tags. The latest trend in this evolution is smart labels which are not only mere identifiers with some kind of internal storage, but also sophisticated context-aware tags with embedded modules that make use of wireless communications, energy efficient displays, and sensors. Therefore, smart labels go beyond identification and are able to detect and react to the surrounding environment. Moreover, when the industrial Internet of Things paradigm is applied to smart labels attached to objects, they can be identified remotely and discovered by other Industry 4.0 systems, what allows such systems to react in the presence of smart labels, thus triggering specific events or performing a number of actions on them. The amount of possible interactions is endless and creates unprecedented industrial scenarios, where items can talk to each other and with tools, machines, remote computers, or workers. This paper, after reviewing the basics of Industry 4.0 and smart labels, details the latest technologies used by them, their applications, the most relevant academic and commercial implementations, and their internal architecture and design requirements, providing researchers with the necessary foundations for developing the next generation of Industry 4.0 human-centered smart label applications.","","","10.1109/ACCESS.2018.2833501","Agencia Estatal de Investigaci&#243;n of Spain; ERDF funds of the EU (AEI/FEDER, UE); Xunta de Galicia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8355491","IIoT;Industry 40;IoT;Smart labels;cyber-physical system;human-centered design;human-computer interface;smart objects;traceability;tracking","Industries;Internet of Things;Production facilities;Production systems;Real-time systems;Smart manufacturing","Internet of Things;factory automation;manufacturing systems;production engineering computing","Industry 4.0;flexible connected factories;fully connected factories;human-centered IoT-connected smart labels;smart factory;smart label applications;smart production systems","","","","","","","20180507","2018","","IEEE","IEEE Journals & Magazines"
"Doppio: Tracking UI Flows and Code Changes for App Development","P. Y. (. Chi; S. P. Hu; Y. Li","Google Inc., Mountain View, CA, USA","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","20180426","2018","","","1","13","<p>Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes.</p>","","","10.1145/3173574.3174029","","","android;demonstrations;ides;mobile apps;screencast videos;screenflow diagram;software documentation","","","","","","","","","","","21-26 April 2018","","ACM","ACM Conferences"
"AI Techniques in Software Engineering Paradigm","M. R. Lyu","Chinese University of Hong Kong, Hong Kong, Hong Kong","Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering","20180426","2018","","","2","2","<p>In the next decade, Artificial Intelligent (AI) techniques can see wide adoption in our daily life to release human burden. In our recent Software Engineering research, we investigated on the design of novel AI methods to facilitate all three major phases in software engineering: development, operation, and analysis. In this talk, I will first introduce the AI techniques we employed, including machine learning framework, classification, clustering, matrix factorization, topic modeling, deep learning, and parallel computing platform. Then I will explain the challenges in each phase and describe our recently proposed methodologies. First in development phase, we suggested an automated code completion technique via deep learning. Our technique learns the code style from lots of existing code bases, and recommends the most suitable token based on the trained deep learning model and current coding context. Besides, to help developers in conducting effective logging, we designed a tool named LogAdvisor, which tells developers whether they should write a logging statement in the current code block or not. Secondly, in operation phase, we implemented a continuous and passive authentication method for mobile phones based on user touch biometrics. Different from the traditional password authentication scheme, our method can recognize malicious attackers based on abnormal user behaviors. Moreover, we developed PAID, which automatically prioritizes app issues by mining user reviews. Finally, in analysis phase, we designed systematic data analytics techniques for software reliability prediction. Besides, to make full use of the crucial runtime information, we proposed effective methods for every step in log analysis, including log parsing, feature extraction, and log mining. Furthermore, we developed a CNN-based defect prediction method to help developers find the buggy code. In the end, we expect to establish a comprehensive framework for systematic employment of AI techniq- es in the Software Engineering paradigm.</p>","","","10.1145/3184407.3184440","","","artificial intelligence;software engineering","","","","","","","","","","","9-13 April 2018","","ACM","ACM Conferences"
"What proportion of vulnerabilities can be attributed to ordinary coding errors?: poster","R. Kuhn; M. Raunak; R. Kacker","National Institute of Standards and Technology","Proceedings of the 5th Annual Symposium and Bootcamp on Hot Topics in the Science of Security","20180426","2018","","","1","1","<p>The analysis reported in this poster developed from questions that arose in discussions of the Reducing Software Vulnerabilities working group, sponsored by the White House Office of Science and Technology Policy in 2016 [1]. The key question we sought to address is the degree to which vulnerabilities arise from ordinary program errors, which may be detected in code reviews and functional testing, rather than post-release.</p>","","","10.1145/3190619.3191686","","","","","","","","","","","","","","10-11 April 2018","","ACM","ACM Conferences"
"Visualizing Gaze Direction to Support Video Coding of Social Attention for Children with Autism Spectrum Disorder","K. Higuchi; S. Matsuda; R. Kamikubo; T. Enomoto; Y. Sugano; J. Yamamoto; Y. Sato","University of Tokyo, Tokyo, Japan","23rd International Conference on Intelligent User Interfaces","20180426","2018","","","571","582","<p>This paper presents a novel interface to support video coding of social attention in the assessment of children with autism spectrum disorder. Video-based evaluations of social attention during therapeutic activities allow observers to find target behaviors while handling the ambiguity of attention. Despite the recent advances in computer vision-based gaze estimation methods, fully automatic recognition of social attention under diverse environments is still challenging. The goal of this work is to investigate an approach that uses automatic video analysis in a supportive manner for guiding human judgment. The proposed interface displays visualization of gaze estimation results on videos and provides GUI support to allow users to facilitate agreement between observers by defining social attention labels on the video timeline. Through user studies and expert reviews, we show how the interface helps observers perform video coding of social attention and how human judgment compensates for technical limitations of the automatic gaze analysis.</p>","","Electronic:978-1-4503-4945-1","10.1145/3172944.3172960","","","children with asd;social attention;video coding support","","","","","","","","","","","7-11 March 2018","","ACM","ACM Conferences"
"Benchmarks for software clone detection: A ten-year retrospective","C. K. Roy; J. R. Cordy","Department of Computer Science, University of Saskatchewan, Canada","2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20180405","2018","","","26","37","There have been a great many methods and tools proposed for software clone detection. While some work has been done on assessing and comparing performance of these tools, very little empirical evaluation has been done. In particular, accuracy measures such as precision and recall have only been roughly estimated, due both to problems in creating a validated clone benchmark against which tools can be compared, and to the manual effort required to hand check large numbers of candidate clones. In order to cope with this issue, over the last 10 years we have been working towards building cloning benchmarks for objectively evaluating clone detection tools. Beginning with our WCRE 2008 paper, where we conducted a modestly large empirical study with the NiCad clone detection tool, over the past ten years we have extended and grown our work to include several languages, much larger datasets, and model clones in languages such as Simulink. From a modest set of 15 C and Java systems comprising a total of 7 million lines in 2008, our work has progressed to a benchmark called BigCloneBench with eight million manually validated clone pairs in a large inter-project source dataset of more than 25,000 projects and 365 million lines of code. In this paper, we present a history and overview of software clone detection benchmarks, and review the steps of ourselves and others to come to this stage. We outline a future for clone detection benchmarks and hope to encourage researchers to both use existing benchmarks and to contribute to building the benchmarks of the future.","","Electronic:978-1-5386-4969-5; POD:978-1-5386-4970-1","10.1109/SANER.2018.8330194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330194","","Benchmark testing;Cloning;Reliability;Software measurement;Software systems;Tools","software maintenance","NiCad clone detection tool;candidate clones;clone detection tools;cloning benchmarks;software clone detection benchmarks","","","","","","","","20-23 March 2018","","IEEE","IEEE Conferences"
"Towards Semantic Approaches for General-Purpose End-User Development","M. Atzeni; M. Atzori","","2018 Second IEEE International Conference on Robotic Computing (IRC)","20180405","2018","","","369","376","Despite the demand for increasing automation within specified tasks by a large spectrum of different users, software development is still a complex task mainly oriented to professional programmers. Often, the exploration and understanding of large code bases is also a difficult task for experienced developers. Recently, semantic parsers and advances in research areas primarily investigated within the field of natural language human-robot interaction, have shown to be potentially useful for end-user development supported by natural language communication. Hence, this paper provides a structured review and categorization of approaches to ease software development, both for professional software programmers and for end-users with no prior programming skills. We then focus on semantic developments based on natural language understanding and on a comparison between the outlined approaches.","","Electronic:978-1-5386-4652-6; POD:978-1-5386-4653-3; USB:978-1-5386-4651-9","10.1109/IRC.2018.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8329941","End User Development;Natural Language Processing;Robotic Commands;Semantic Parsers;Semantic Robots","Data mining;Natural languages;Programming;Semantics;Software;Task analysis;Visualization","human-robot interaction;interactive systems;natural language processing;software engineering","experienced developers;general-purpose end-user development;natural language communication;natural language human-robot interaction;natural language understanding;professional programmers;professional software programmers;semantic developments;semantic parsers;software development;structured review","","","","","","","","Jan. 31 2018-Feb. 2 2018","","IEEE","IEEE Conferences"
"Large scale clone detection, analysis, and benchmarking: An evolutionary perspective (Keynote)","C. K. Roy","Department of Computer Science, University of Saskatchewan, Canada","2018 IEEE 12th International Workshop on Software Clones (IWSC)","20180329","2018","","","1","1","Copying a code fragment and then reusing it by pasting and adapting (e.g., adding/modifying/deleting statements) is a common practice in software development, which results in a significant amount of duplicated code in software systems. Developers consider cloning as one of the principled re-engineering approaches and often intentionally practice cloning for a variety of reasons such as faster development, avoiding risk by reusing stable old code, or for time pressure. On the other hand, duplicated code poses a number of threats to the maintenance of software systems such as clones are the #1 ‚Äúbad smell‚Äù in Flower's refactoring list and several recent studies including studies with industrial systems show that although for many cases clones are not really harmful, and even could be useful for some cases, they could be also detrimental to software maintenance. For example, reusing a fragment containing unknown bugs may result in bugs propagation, or any changes in requirements involving a cloned fragment may lead to changes to all the similar fragments to it, multiplying the work to be done. Furthermore, inconsistent changes to the cloned fragments during any updating processes may lead to severe unexpected behaviour. Software clones are thus considered to be one of the major contributors to the high software maintenance cost, which could be up to 80% of total software development cost. The era of Big Data has introduced new applications for clone detection. For example, clone detection has been used to find similar mobile applications, to intelligently tag code snippets, to identify code examples, and so on from large inter-project repositories. The dual role of clones in software development and maintenance, along with these many emerging new applications of clone detection, has led to a great many clone detection tools and analysis frameworks. In this keynote talk, I will review the cloning literature to date, in particular,- I will talk about our recent work on large scale clone detection, and the challenges in evaluating such clone detectors and how we have overcome them at least in part with our BigCloneBench and Mutation framework. I will then talk about the recent advances in clone analysis and management along with a vision for a comprehensive clone management system.","","Electronic:978-1-5386-6430-8; POD:978-1-5386-6431-5","10.1109/IWSC.2018.8327311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327311","","","","","","","","","","","","20-20 March 2018","","IEEE","IEEE Conferences"
"Are there functionally similar code clones in practice?","V. K√§fer; S. Wagner; R. Koschke","University of Stuttgart, Germany","2018 IEEE 12th International Workshop on Software Clones (IWSC)","20180329","2018","","","2","8","Having similar code fragments, also called clones, in software systems can lead to unnecessary comprehension, review and change efforts. Syntactically similar clones can often be encountered in practice. The same is not clear for only functionally similar clones (FSC). We conducted an exploratory survey among developers to investigate whether they encounter functionally similar clones in practice and whether there is a difference in their inclination to remove them to syntactically similar clones. Of the 34 developers answering the survey, 31 have experienced FSC in their professional work, and 24 have experienced problems caused by FSCs. We found no difference in the inclination and reasoning for removing FSCs and syntactically similar clones. FSCs exist in practice and should be investigated to bring clone detectors to the same quality as for syntactically similar clones, because being able to detect them allows developers to manage and potentially remove them.","","Electronic:978-1-5386-6430-8; POD:978-1-5386-6431-5","10.1109/IWSC.2018.8327312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327312","code clones;survey","Cloning;Detectors;Power capacitors;Semantics;Sorting;Syntactics;Tools","software maintenance","clone detectors;functionally similar clones;similar code clones;similar code fragments;software systems;syntactically similar clones","","","","","","","","20-20 March 2018","","IEEE","IEEE Conferences"
"Design of Low-Density Parity Check Codes for 5G New Radio","T. Richardson; S. Kudekar","","IEEE Communications Magazine","20180315","2018","56","3","28","34","Turbo codes, prevalent in most modern cellular devices, are set to be replaced by LDPC codes as the code for forward error correction. This transition was ushered in mainly because of the high throughput demands for 5G New Radio (NR). The new channel coding solution also needs to support incremental-redundancy hybrid ARQ, and a wide range of blocklengths and coding rates, with stringent performance guarantees and minimal description complexity. In this article, we first briefly review the requirements of the new channel code for 5G NR. We then describe the LDPC code design philosophy and how the broad requirements of 5G NR channel coding led to the introduction of novel structural features in the code design, culminating in an LDPC code that satisfies all the demands of 5G NR.","0163-6804;01636804","","10.1109/MCOM.2018.1700839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8316763","","5G mobile communication;Complexity theory;Decoding;Parity check codes;Throughput;Turbo codes","5G mobile communication;automatic repeat request;cellular radio;channel coding;forward error correction;parity check codes;turbo codes","5G NR;5G New Radio;LDPC code;cellular devices;channel coding solution;forward error correction;incremental-redundancy hybrid ARQ;low-density parity check codes;turbo codes","","","","","","","","MARCH 2018","","IEEE","IEEE Journals & Magazines"
"Combination of linear power flow tools for voltages and power estimation on MV networks","J. Buire; X. Guillaud; F. Colas; J. Y. Dieulot; L. De Alvaro","L2EP, Lille, France","CIRED - Open Access Proceedings Journal","20180315","2017","2017","1","2157","2160","European grid codes define new network management rules. To answer these decrees, it is important to estimate accurately the voltages and powers inside distribution networks. The intermittent nature of renewable sources leads to consider stochastic variables in power flow algorithms. A review of power flow methods and their ability to comply with these requirements is done, which shows that computationally demanding non-linear methods have to be discarded when dealing with stochastic data and considering limited calculation time. A combination of linear methods is proposed, for which average errors in power and voltage are quite low, when applied to a real-life distribution network. The validity domain of the method is also presented.","","","10.1049/oap-cired.2017.0806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8316133","","","distribution networks;load flow;power grids;power system management","European grid codes;MV networks;distribution networks;linear power flow tools;network management;nonlinear methods;power estimation;power flow algorithms;power flow methods;real-life distribution network;renewable sources","","","","","","","","10 2017","","IET","IET Journals & Magazines"
"Framework Information Based Java Software Architecture Recovery","X. Li; L. Zhang; N. Ge","State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China","2017 24th Asia-Pacific Software Engineering Conference Workshops (APSECW)","20180312","2017","","","114","120","Software systems tend to become more and more complex as they evolve, which makes it difficult to review, understand, and maintain the source code without complete architectural information, especially in case of large-scale systems. Software architecture recovery is considered an important method contributing to solving this problem. Hierarchical clustering is one of the techniques used to extract architectural information from lower level software representations, such as the source code. This paper is aimed at improving the accuracy of existing hierarchical clustering algorithms by allowing users to parameterize and configure framework information as framework-specific features. We have implemented our approach as an Eclipse plugin and have applied it to recovering the architecture of Java programs. Experiments are carried out on our benchmark built upon Java web applications in which the Spring Framework is used. The experimental results show that our approach can improve the accuracy of the recovered architecture to some extent.","","Electronic:978-1-5386-2649-8; POD:978-1-5386-2650-4","10.1109/APSECW.2017.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8312533","framework information;hierarchical clustering;software architecture recovery","Clustering algorithms;Computer architecture;Feature extraction;Java;Software;Software algorithms;Software architecture","Java;large-scale systems;pattern clustering;software architecture;software maintenance","Java Web applications;Java programs;Java software architecture recovery;Spring Framework;architectural information;framework information;framework-specific features;hierarchical clustering algorithms;large-scale systems;lower level software representation;software systems","","","","","","","","4-8 Dec. 2017","","IEEE","IEEE Conferences"
"Low voltage ride-through capability enhancement of grid-connected permanent magnet synchronous generator driven directly by variable speed wind turbine: a review","M. H. Qais; H. M. Hasanien; S. Alghuwainem","King Saud University, Saudi Arabia","The Journal of Engineering","20180312","2017","2017","13","1750","1754","The large power penetration of wind farms into the power grids requires new regulations by transmission system operators to keep them connected to the grid as long as possible. Grid disturbances such as voltage dips cause islanding of wind farms on the way to protect its apparatus from damage due to a high current flowing. The grid stability will suffer due to the islanding of large-scale wind farms. Wind farms should keep on connecting to the grid during low voltages for a specific time [low voltage ride through (LVRT) capability] to support the grid stability restoring. LVRT capability of permanent magnet synchronous generator driven directly by a variable speed wind turbine (PMSG-VSWT) can be realised by modifying the control of grid side converter (GSC), machine side converter (MSC), pitch angle control, or using the existing energy storage system. This study presents a review of recent proposed improvements to enhance the LVRT capability of PMSG-VSWT. Many artificial intelligence and conventional controllers are used to enhance the control performance during voltage sags to keep the wind farms connected to the grid according to the recent grid codes.","","","10.1049/joe.2017.0632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8311335","","","distributed power generation;permanent magnet generators;power convertors;power generation control;power generation protection;power grids;power supply quality;power system stability;synchronous generators;variable speed drives;wind power plants;wind tunnels","GSC;LVRT capability;MSC;PMSG-VSWT;energy storage system;grid codes;grid disturbances;grid side converter;grid stability restoring;grid-connected permanent magnet synchronous generator;high current flowing;large-scale wind farms islanding;low voltage ride-through capability enhancement;machine side converter;pitch angle control;power grids;power penetration;transmission system operators;variable speed wind turbine;voltage dips;voltage sags","","","","","","","","2017","","IET","IET Journals & Magazines"
"Cloud Based Collaborative Software Development: A Review, Gap Analysis and Future Directions","S. Ewenike; E. Benkhelifa; C. Chibelushi","Cloud Comput. & Applic. Res. Lab., Staffordshire Univ., Stoke-on-Trent, UK","2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)","20180312","2017","","","901","909","Organizations who have transitioned their development environments to the Cloud have started realizing benefits such as: cost reduction in hardware; relatively accelerated development process via reduction of time and effort to set up development and testing environments; unified management; service and functionality expansion; on-demand provisioning and access to resources and development environments. These benefits represent only a fraction of the full potential that could be achieved via leveraging Cloud Computing for the collaborative software development process. Related efforts in this area have been mainly in the areas of: asynchronous collaboration; collaboration in isolated aspects of the Software Development process, such as coding activities; use of open-source tools for contributing, improving, and managing code, etcetera. Although these efforts represent valid contributions and important enablers, they are still missing important aspects which enable a more holistic process, with solid theoretical foundation. This paper reviews this research area, in order to better assess factors and gaps creating the need to enhance the collaborative software development process in the Cloud, to better meet the pressure to collaboratively create better cloud-agnostic applications.","","Electronic:978-1-5386-3581-0; POD:978-1-5386-3582-7","10.1109/AICCSA.2017.220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8308385","Cloud;Collaborative software development;collaboration;gap analysis","Adaptation models;Business;Cloud computing;Collaborative software;Tools","cloud computing;groupware;open systems;service-oriented architecture;software development management","asynchronous collaboration;cloud based collaborative software development;cloud computing;cloud-agnostic applications;cost reduction;gap analysis;testing environments","","","","","","","","Oct. 30 2017-Nov. 3 2017","","IEEE","IEEE Conferences"
"Teaching, Analyzing, Designing and Interactively Simulating Sliding Mode Control","R. Costa-Castell√≥; N. Carrero; S. Dormido; E. Fossas","Institut de Rob&#x00F2;tica i Inform&#x00E0;tica Industrial, Universitat Polit&#x00E8;cnica de Catalunya, Barcelona, Spain","IEEE Access","20180409","2018","6","","16783","16794","This paper introduces an interactive methodology to analize, design, and simulate sliding model controllers for <inline-formula> <tex-math notation=""LaTeX"">$mathbb {R}^{2}$ </tex-math></inline-formula> linear systems. This paper reviews sliding mode basic concepts and design methodologies and describes an interactive tool which has been developed to support teaching in this field. The tool helps students by generating a nice graphical and interactive display of most relevant concepts. This fact can be used so that students build their own intuition about the role of different parameters in a sliding mode controller. Described application has been coded with Sysquake using an event-driven solver technique. The Sysquake allows using precise integration methods in real time and handling interactivity in a simple manner.","","","10.1109/ACCESS.2018.2815043","Spanish State Research Agency through the Mar&#237;a de Maeztu Seal of Excellence to IRI; 10.13039/501100002809 - AGAUR of Generalitat de Catalunya through the Advanced Control Systems (SAC) Group; 10.13039/501100003329 - Spanish Ministry of Economy and Competitiveness (MINECO/FEDER); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8314690","Sliding mode control;control education;interactive simulations","Education;Linear systems;Switches;Tools;Trajectory;Visualization","","","","","","","","","20180312","2018","","IEEE","IEEE Journals & Magazines"
"Analysis and comparison of electronic digital signature state standards GOST R 34.10-1994, GOST R 34.10-2001 and GOST R 34.10-2012","A. Komarova; A. Menshchikov; T. Klyaus; A. Korobeynikov; Y. Gatchin; N. Tishukova","Saint-Petersburg National Research University of Information Technologies, Saint-Petersburg, Russia","Proceedings of the 10th International Conference on Security of Information and Networks","20180308","2017","","","262","267","<p>These days information systems that use asymmetric cryptography or public key cryptography are widely spread all over the world. The reason of this fact is a rather high reliability of existing algorithms. In terms of information security, we can distinguish the following important tasks: ensuring the availability of information, its authenticity and confidentiality. All these problems can be solved with the help of electronic digital signature (hereinafter-EDS). It ensures the integrity of information, its confidentiality, authenticity and the origin. EDS is widely used in various commercial and governmental organizations that are obliged to follow the state standard GOST R 34.10-2012 ""Information technology. Cryptographic protection of information. The processes of forming and checking electronic digital signature"". In the view of the above we perform a comparative analysis of the state standard GOST R 34.10-2012 with the state standards GOST R 34.10-2001 and R 34.10-1994 to find their relevance, reliability and complexity. Due to the fact that EDS is based on a special hash function, the authors of this article have also reviewed the state standards GOST R 34.11-2012 and GOST R 34.11-1994. To estimate hash function generation time, the Python Code which uses the Pygost Library has been written. Furthermore, hacking complexity assessments of the algorithms themselves have been realized. Therefore, estimations of speed and performance procedures generation and signature verification have been made and the hacking complexity assessments of the Russian state standards are realized. The approximate durations of forced entry are calculated and the advantage of the new state standard GOST R 34.10-2012 is justified. As a result of the analysis the relevance, the reliability and the complexity of the state standard GOST R 34.10-2012 have been turned out. In spite of all the strength benefits the new standard requires more time effort for hash-function formation and fo- the digital signature formation.</p>","","","10.1145/3136825.3136894","","","","","","","","","","","","","","13-15 Oct. 2017","","ACM","ACM Conferences"
"Hybrid Sorting Method for Successive Cancellation List Decoding of Polar Codes","H. Li; C. Guo; D. Wang","University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences, Beijing","Proceedings of the 2017 the 7th International Conference on Communication and Network Security","20180308","2017","","","23","26","<p>This paper proposes a hybrid metric sorting method (HMS) of successive cancellation list decoders for polar codes, which plays a critical role in decoding process. We review the state-of-the-art metric sorting methods and combine the advantages of them to generate the proposed method. Due to the optimized architecture, the proposed HMS method reduces the number of comparing stages effectively with little increase in comparisons. Evaluation results show that about 25 percent of comparing stages can be removed by HMS, compared with state-of-the-art methods. The proposed method enjoys a latency reduction for hardware implementation.</p>","","","10.1145/3163058.3163079","","","Polar codes;metric sorting;successive cancellation list decoding","","","","","","","","","","","24-26 Nov. 2017","","ACM","ACM Conferences"
"A Comprehensive Study of Implemented International Standards, Technical Challenges, Impacts and Prospects for Electric Vehicles","S. Habib; M. M. Khan; F. Abbas; L. Sang; M. U. Shahid; H. Tang","School of Electronic, Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","20180327","2018","6","","13866","13890","The impending environmental issues and growing concerns for global energy crises are driving the need for new opportunities and technologies that can meet significantly higher demand for cleaner and sustainable energy systems. This necessitates the development of transportation and power generation systems. The electrification of the transportation system is a promising approach to green the transportation systems and to reduce the issues of climate change. This paper inspects the present status, latest deployment, and challenging issues in the implementation of Electric vehicles (EVs) infrastructural and charging systems in conjunction with several international standards and charging codes. It further analyzes EVs impacts and prospects in society. A complete assessment of charging systems for EVs with battery charging techniques is explained. Moreover, the beneficial and harmful impacts of EVs are categorized and thoroughly reviewed. Remedial measures for harmful impacts are presented and benefits obtained therefrom are highlighted. Bidirectional charging offers the fundamental feature of vehicle to grid technology. In this paper, the current challenging issues due to the massive deployment of EVs, and upcoming research trends are also presented. It is envisioned that the researchers interested in such areas can find this paper valuable and an informative one-stop source.","","","10.1109/ACCESS.2018.2812303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8307044","Electric vehicles (EVs);impacts and challenging issues;infrastructure of charging systems;international standards;plug-in hybrid electric vehicles (PHEVs);vehicle to gird (V2G) technology","Batteries;Connectors;Electric vehicles;Fossil fuels;Power grids;Standards","battery powered vehicles;renewable energy sources;secondary cells;sustainable development","EV;battery charging techniques;beneficial impacts;bidirectional charging;charging codes;charging systems;cleaner energy systems;climate change;current challenging issues;electric vehicles;global energy crises;grid technology;harmful impacts;impending environmental issues;international standards;power generation systems;sustainable energy systems;technical challenges;transportation system electrification","","","","","","","20180306","2018","","IEEE","IEEE Journals & Magazines"
"On the Synchronization Bottleneck of OpenStack Swift-like Cloud Storage Systems","M. Ruan; T. Titcheu; E. Zhai; Z. Li; Y. Liu; J. E; Y. Cui; H. Xu","School of Software, Tsinghua University, 12442 Beijing, Beijing China 100084 (e-mail: brmk@vip.qq.com)","IEEE Transactions on Parallel and Distributed Systems","","2018","Early Access","Early Access","1","1","As one type of the most popular cloud storage services, OpenStack Swift and its follow-up systems replicate each object across multiple storage nodes and leverage object sync protocols to achieve high reliability and eventual consistency. The performance of object sync protocols heavily relies on two key parameters: r (number of replicas for each object) and n (number of objects hosted by each storage node). In existing tutorials and demos, the configurations are usually <formula><tex>$r = 3$</tex></formula> and <formula><tex>$n < 1000$</tex></formula> by default, and the sync process seems to perform well. However, we discover in data-intensive scenarios, e.g., when <formula><tex>$r > 3$</tex></formula> and <formula><tex>$n gg 1000$</tex></formula>, the sync process is significantly delayed and produces massive network overhead, referred to as the sync bottleneck problem. By reviewing the source code of OpenStack Swift, we find that its object sync protocol utilizes a fairly simple and network-intensive approach to check the consistency among replicas of objects. Hence in a sync round, the number of exchanged hash values per node is <formula><tex>$Theta(n times r)$</tex></formula>. To tackle the problem, we propose a lightweight and practical object sync protocol, LightSync, which not only remarkably reduces the sync overhead, but also preserves high reliability and eventual consistency. LightSync derives this capability from three novel building blocks: 1) Hashing of Hashes, which aggregates all the h hash values of each data partition into a single but representative hash value with the Merkle tree; 2) Circular Hash Checking, which checks the consistency of different partition replicas by only sending the aggregated hash value to the clockwise neighbor; and 3) Failed Neighbor Handling, which properly detects and handles node failures with moderate overhead to effectively strengthen the robustness of Li- htSync. The design of LightSync offers provable guarantee on reducing the per-node network overhead from <formula><tex>$Theta(n times r)$</tex></formula> to <formula><tex>$Theta (frac{n}{h})$</tex></formula>. Furthermore, we have implemented LightSync as an open-source patch and adopted it to OpenStack Swift, thus reducing the sync delay by up to <formula><tex>$879 times$</tex></formula> and the network overhead by up to <formula><tex>$47.5 times$</tex></formula>.","1045-9219;10459219","","10.1109/TPDS.2018.2810179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8303732","Cloud storage;OpenStack Swift;object synchronization;performance bottleneck","Cloud computing;Delays;Electronic mail;Open source software;Protocols;Reliability;Synchronization","","","","","","","","","20180227","","","IEEE","IEEE Early Access Articles"
"If Memory Serves: Towards Designing and Evaluating a Game for Teaching Pointers to Undergraduate Students","M. M. McGill; C. Johnson; J. Atlas; D. Bouchard; C. Messom; I. Pollock; M. J. Scott","Knox College, Galesburg, IL, USA","Proceedings of the 2017 ITiCSE Conference on Working Group Reports","20180222","2017","","","25","46","<p>Games can serve as a valuable tool for enriching computer science education, since they can facilitate a number of conditions that can promote learning and instigate affective change. As part of the 22nd ACM Annual Conference on Innovation and Technology in Computer Science Education (ITiCSE 2017), the Working Group on Game Development for Computer Science Education convened to extend their prior work, a review of the literature and a review of over 120 educational games that support computing instruction. The Working Group builds off this earlier work to design and develop a prototype of a game grounded in specific learning objectives. They provide the source code for the game to the computing education community for further review, adaptation, and exploration. To aid this endeavor, the Working Group also chose to explore the research methods needed to establish validity, highlighting a need for more rigorous approaches to evaluate the effectiveness of the use of games in computer science education.</p> <p> This report provides two distinct contributions to the body of knowledge in games for computer science education. We present an experience report in the form of a case study describing the design and development of If Memory Serves, a game to support teaching pointers to undergraduate students. We then propose guidelines to validate its effectiveness rooted in theoretical approaches for evaluating learning in games and media. We include an invitation to the computer science education community to explore the game's potential in classrooms and report on its ability to achieve the stated learning outcomes.</p>","","","10.1145/3174781.3174783","","","computer memory;design;development;educational;games;learning;pointers;research methods;serious;validation framework","","","","","","","","","","","3-5 July 2017","","ACM","ACM Conferences"
"Sensibility Testbed: Automated IRB Policy Enforcement in Mobile Research Apps","Y. Zhuang; A. Rafetseder; Y. Hu; Y. Tian; J. Cappos","University of Colorado, Colorado Springs, Colorado Springs, CO, USA","Proceedings of the 19th International Workshop on Mobile Computing Systems & Applications","20180222","2018","","","113","118","<p>Due to their omnipresence, mobile devices such as smartphones could be tremendously valuable to researchers. However, since research projects can extract data about device owners that could be personal or sensitive, there are substantial privacy concerns. Currently, the only regulation to protect user privacy for research projects is through Institutional Review Boards (IRBs) from researchers' institutions. However, there is no guarantee that researchers will follow the IRB protocol. Even worse, researchers without security expertise might build apps that are vulnerable to attacks.</p> <p>In this work, we present a platform, Sensibility Testbed, for automated enforcement of the privacy policies set by IRBs. Our platform enforces such policies when a researcher runs code on mobile devices. The enforcement mechanism is a set of obfuscation layers in a secure sandbox, that can be customized for any level of IRB compliance, and can be augmented by policies set by the device owner.</p>","","","10.1145/3177102.3177120","","","policy enforcement;privacy protections","","","","","","","","","","","12-13 Feb. 2018","","ACM","ACM Conferences"
"COSMIC Function Points Evaluation for Software Maintenance","A. Hira; B. Boehm","University of Southern California, Los Angeles, California","Proceedings of the 11th Innovations in Software Engineering Conference","20180222","2018","","","1","11","<p>The Common Software Measurement International Consortium (COSMIC) group reviewed the existing functional size methods, such as the International Function Points User Group (IFPUG)'s Function Points (FPs), to develop a functional size metric based on ""the basic principles"" that applies to a wide range of application domains. Though several empirical studies on the COSMIC method verify that COSMIC Function Points (CFPs) successfully accomplished the goal of being applicable to a wide range of application domains and that its size correlate well with effort over a very wide range of sizes, one study of telecom switching software noticed that the correlation between CFPs and cost is very low for small projects (5 CFPs or less). The COSMIC method does not explicitly size data manipulations (such as, mathematical algorithms), which causes it to be less effective for mathematically-intensive software. IFPUG's FPs method has the same drawback of not explicitly measuring mathematical operations, but IFPUG developed the Software Non-Functional Assessment Process (SNAP) to complement a project's functional size. This empirical analysis will determine whether CFPs can be an effective size metric for small, maintenance tasks (between 2 and 12 CFPs) using a dataset consisting of Unified Code Count (UCC)1's maintenance tasks. Additionally, this analysis will consider whether using IFPUG's SNAP with COSMIC's FPs can lead to better effort estimates, as the former provides a method to measure data manipulation. The authors found that tasks adding new features require a different effort estimate model from those that modify existing features.</p>","","","10.1145/3172871.3172874","","","CFPs;COCOMO;COSMIC Function Points;Cost Estimation;Effort Estimation;Function Point Analysis;IFPUG;Local Calibration;Project Management;Software Maintenance;Software Non-Functional Assessment Process","","","","","","","","","","","9-11 Feb. 2018","","ACM","ACM Conferences"
"A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets","S. Karanam; M. Gou; Z. Wu; A. Rates-Borras; O. Camps; R. J. Radke","Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, New York United States 12180 (e-mail: karans3@rpi.edu)","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2018","Early Access","Early Access","1","1","Person re-identification (re-id) is a critical problem in video analytics applications such as security and surveillance. The public release of several datasets and code for vision algorithms has facilitated rapid progress in this area over the last few years. However, directly comparing re-id algorithms reported in the literature has become difficult since a wide variety of features, experimental protocols, and evaluation metrics are employed. In order to address this need, we present an extensive review and performance evaluation of single- and multi-shot re-id algorithms. The experimental protocol incorporates the most recent advances in both feature extraction and metric learning. To ensure a fair comparison, all of the approaches were implemented using a unified code library that includes 11 feature extraction algorithms and 22 metric learning and ranking techniques. All approaches were evaluated using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 16 other publicly available datasets: VIPeR, GRID, CAVIAR, DukeMTMC4ReID, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK01, CHUK02, CUHK03, RAiD, iLIDSVID, HDA+ and Market1501. The evaluation codebase and results will be made publicly available for community use.","0162-8828;01628828","","10.1109/TPAMI.2018.2807450","Science and Technology Directorate; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8294254","","Benchmark testing;Cameras;Feature extraction;Histograms;Image color analysis;Measurement;Probes","","","","","","","","","20180219","","","IEEE","IEEE Early Access Articles"
"Heuristics for vehicle routing problems: Current challenges and future prospects","T. Vidal","Pontifical Catholic University of Rio de Janeiro Brazil","Proceedings of the Eighth International Symposium on Information and Communication Technology","20180219","2017","","","6","6","<p>Vehicle Routing Problems (VRP) involve designing least-cost delivery routes to visit a geographically-dispersed set of customers. Over the past 60 years, this class of problems has been the subject of considerable work, summing up to thousands of articles. In 2017, we can reasonably say that the classical capacitated VRP (with only capacity constraints) is fairly well solved by metaheuristic techniques. Yet, the research on VRPs keeps on expanding even further, as a consequence of the increasing diversity of applications, which bring forth new difficult constraints, objectives, and combined decisions to account for customers needs, vehicle and network restrictions, and to better integrate VRP optimization in the decision chains. Moreover, with the advent of collaborative logistics, green initiatives, smart cities, multi-modal transport, in contexts where multiples stakeholders and conflicting objectives have to be considered jointly, or in the presence of dynamic problems with a short response time, the efficient resolution of these problems becomes even more critical. In this talk, we will review some of the most challenging and recent VRP variants, and examine the heuristic solution techniques which are developed to tackle them. We will study the close connections between the structure of the problem decision sets, and the associated solution methods, showing how modern heuristics can effectively perform a search in a reduced space, defined by fewer groups of decision variables. Finally, a key challenge is to progress towards unified solution methods, which are not tailored for one single problem, but instead designed to solve a wide collection of problem variants with different constraints and objectives. For this purpose, we expose some of the main principles of the Unified Hybrid Genetic Search (UHGS), which has been recently extended to obtain state-of-the-art results --in a single code base-- for more than 50 difficult variants of vehicle routing and arc - outing problems.</p>","","","10.1145/3155133.3155139","","","","","","","","","","","","","","7-8 Dec. 2017","","ACM","ACM Conferences"
"Functional Requirements-Based Automated Testing for Avionics","Y. Sun; M. Brain; D. Kroening; A. Hawthorn; T. Wilson; F. Schanda; F. J. G. Jim√©nez; S. Daniel; C. Bryan; I. Broster","Univ. of Oxford, Oxford, UK","2017 22nd International Conference on Engineering of Complex Computer Systems (ICECCS)","20180215","2017","","","170","173","We propose and demonstrate a method for the reduction of testing effort in safety-critical software development using DO-178 guidance. We achieve this through the application of Bounded Model Checking (BMC) to formal low-level requirements, in order to generate tests automatically that are good enough to replace existing labor-intensive test writing procedures while maintaining independence from implementation artefacts. Given that manual processes are often empirical and subjective, we begin by formally defining a metric, which extends recognized best practice from code coverage analysis strategies to generate tests that adequately cover the requirements. We then implement it in an automated requirements testing procedure and apply it in a case study with industrial partners. In review, the toolchain developed here is demonstrated to significantly reduce the human effort for the qualification of software products under DO-178 guidance.","","Electronic:978-1-5386-2431-9; POD:978-1-5386-2432-6","10.1109/ICECCS.2017.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292819","","Compounds;Manuals;Model checking;Software;Syntactics;Tools","avionics;program testing;program verification;safety-critical software","BMC;Bounded Model Checking;DO-178 guidance;automated requirements;automated testing;avionics;code coverage analysis strategies;functional requirements;labor-intensive test writing procedures;low-level requirements;safety-critical software development;software products;testing effort","","","","","","","","5-8 Nov. 2017","","IEEE","IEEE Conferences"
"Workstation versus Free Open Source Software in radiology","K. M. Mujika; J. A. J. M√©ndez; A. F. d. Miguel","University Assistantce Complex of Salamanca, Salamanca, Spain","Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality","20180129","2017","","","1","6","<p>Over the last years, we have observed a constant increase in the number of technological applications to visualize medical images which are of great interest, both from a teaching and a radiological perspective. Graphic workstations in radiology allow their users to process, review, analyse, communicate and exchange multidimensional digital images acquired with different image-capturing radiological devices. Therefore, they are very sophisticated computing tools that combine software and hardware to process medical images, and they can work with digital pictures in several dimensions, obtained with different image-capturing radiological devices from CT (Computerised tomography), MRI (Magnetic Resonance Imaging), PET (Positron Emission Tomography), etc. However, the programs included in these workstations have a high cost which always depends on the software provider and is always subject to its norms and requirements.</p> <p>On the other hand, free open source software in the field of radiology, offers good ways to digitally manage a medical image and they can be easily installed in personal computers. Also, they do not cost anything and do not depend on the provider, since the source code is freely available.</p> <p>With this study, we aim to present the advantages and disadvantages of these radiological image management and visualization systems in the advanced management of radiological studies. We will compare the features of the VITREA2¬Æ radiology workstation with free open source software applications like OsiriX¬Æ and 3D Slicer¬Æ, with examples from specific studies.</p>","","","10.1145/3144826.3145411","","","3D Slicer;Free Open Source Software;Imaging diagnosis;Osirix lite;Vitrea2;Workstation","","","","","","","","","","","18-20 Oct. 2017","","ACM","ACM Conferences"
"Pharo Git Thermite: A Visual Tool for Deciding to Weld a Pull Request","R. Salgado; A. Bergel","Pleiad Lab, DCC, University of Chile","Proceedings of the 12th edition of the International Workshop on Smalltalk Technologies","20180129","2017","","","1","6","<p>Collaborative software development platforms such as GitHub simplify the process of contributing into open source projects by the use of a pull request. The decision of accepting or rejecting a pull request has to be made by an integrator. Because reviewing a pull request can be time consuming, social factors are known to have an important effect on the acceptation of a pull request. This effect can be especially important for large and complicated pull request.</p> <p>In this paper we present Git Thermite, a tool to assess the internal structure of a pull request and simplifying the job of the integrator. Git Thermite details the structural changes made on the source code. In Git Thermite we use a pull request business card visual metaphor for describing a pull request. In this business card, we present the pull request metadata and describe the modified files, and the structural changes in the modified source code.</p>","","","10.1145/3139903.3139916","","","","","","","","","","","","","","4-8 Sept. 2017","","ACM","ACM Conferences"
"The complexity of microRNAs in human cancer","J. Y. Y. Kwan; P. Psarianos; J. P. Bruce; K. W. Yip; F. F. Liu","","Journal of Radiation Research","20180118","2016","57","S1","i106","i111","MicroRNAs (miRNAs) are small non-coding RNA molecules that have key regulatory roles in cancer, acting as both oncogenes and tumor suppressors. Due to the potential roles of miRNAs in improving cancer prognostic, predictive, diagnostic and therapeutic approaches, they have become an area of intense research focus in recent years. MiRNAs harbor attractive features allowing for translation to the clinical world, such as relatively simple extraction methods, resistance to molecular degradation, and ability to be quantified. Numerous prognostic, predictive and diagnostic miRNA signatures have been developed. To date however, miRNA analysis has not been adopted for routine clinical use. The objectives of this article are to provide an overview of miRNA research and review a selection of miRNA studies in breast cancer, cervical cancer, sarcoma, and nasopharyngeal carcinoma to highlight advances and challenges in miRNA cancer research.","0449-3060;04493060","","10.1093/jrr/rrw009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8148831","breast cancer;cervical cancer;microRNA;nasopharyngeal carcinoma;sarcoma","","","","","","","","","","","Aug. 2016","","OUP","OUP Journals & Magazines"
"Who guards the guards? formal validation of the Arm v8-m architecture specification","A. Reid","ARM, UK","Proceedings of the ACM on Programming Languages","20180115","2017","1","OOPSLA","1","24","<p> Software and hardware are increasingly being formally verified against specifications, but how can we verify the specifications themselves? This paper explores what it means to formally verify a specification. We solve three challenges: (1) How to create a secondary, higher-level specification that can be effectively reviewed by processor designers who are not experts in formal verification; (2) How to avoid common-mode failures between the specifications; and (3) How to automatically verify the two specifications against each other. </p> <p> One of the most important specifications for software verification is the processor specification since it defines the behaviour of machine code and of hardware protection features used by operating systems. We demonstrate our approach on ARM's v8-M Processor Specification, which is intended to improve the security of Internet of Things devices. Thus, we focus on establishing the security guarantees the architecture is intended to provide. Despite the fact that the ARM v8-M specification had previously been extensively tested, we found twelve bugs (including two security bugs) that have all been fixed by ARM. </p>","","","10.1145/3133912","","","Formal Verification;ISA;Specification","","","","","","","","","","","October 2017","","ACM","ACM Journals & Magazines"
"Application of dynamic logistic regression with unscented Kalman filter in predictive coding","Y. S. Astle; X. Tang; C. Freeman","Vista Analytics, Washington D.C., USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1381","1389","Predictive coding, adapted from text categorization for litigation support, is an evolving process with identification of responsive documents and changing labeling decisions. The current state-of-art within predictive coding workflow uses Active Learning, where a new model is periodically rebuilt with additional documents reviewed, to continuously revise a model and improve the identification of responsive documents. We propose an alternative approach to recursively update the model using the Unscented Kalman Filter for each additional labeled document. With synthetic text streaming data and induced concept drift, we show that our approach learns new patterns at a faster rate, renders better accuracy and recall, and requires a reduced labeling cost, which when combined makes it potentially a better alternative in updating the model in the setting of Active Learning for predictive coding.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258071","Active Learning;Concept Drift;Logistic Regression;Predictive Coding;Unscented Kalman Filter","Kalman filters;Logistics;Prediction algorithms;Predictive coding;Predictive models;Text categorization;Training","Kalman filters;learning (artificial intelligence);nonlinear filters;pattern classification;regression analysis;text analysis","Active Learning;Unscented Kalman Filter;additional documents;additional labeled document;dynamic logistic regression;evolving process;labeling decisions;litigation support;predictive coding workflow;reduced labeling cost;responsive documents;synthetic text streaming data;text categorization;unscented Kalman filter","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"A Review of Recent Developments in NOMA & SCMA Schemes for 5G Technology","M. H. Alshammary; F. A. Alanezi","Dept. Comput. Sci., King's Coll., London, UK","2017 16th International Symposium on Distributed Computing and Applications to Business, Engineering and Science (DCABES)","20180111","2017","","","55","59","The race for developing 5G technology has taken a multiorganizational competition in the recent past, where the professional bodies, private organizations, and research units are carrying out research in developing various technologies relating to 5G. Multiple Access Scheme is one of the most important aspects that have been an area of interest for the majority as 5G is expected to accommodate large number of users with high speed of data transmission. This paper review the latest developments in the field of research concerning Sparse code multiple access (SCMA) and Non-orthogonal multiple access (NOMA). Ann extensive research has been carried out to identify the recent developments and their results are presented in this paper. This study has found that most of the recent developments in SCMA were focusing on achieving lower complexity levels, low BER, and effective and efficient energy utilization, and the recent developments identified regarding NOMA focused especially on the performance attributes like spectral efficiency, spatial efficiency, low BER and latency, and resource allocation and multiple access.","","Electronic:978-1-5386-2162-2; POD:978-1-5386-2163-9","10.1109/DCABES.2017.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253035","5g Technology;Non-orthogonal multiple access;Sparse code multiple access","5G mobile communication;Complexity theory;NOMA;Organizations;Receivers;Resource management;Uplink","5G mobile communication;error statistics;multi-access systems","5G technology;BER;Multiple Access Scheme;NOMA schemes;Non-orthogonal multiple access;SCMA schemes;Sparse code multiple access","","","","","","","","13-16 Oct. 2017","","IEEE","IEEE Conferences"
"Face Analysis in the Wild","F. H. d. B. Zavan; N. Gasparin; J. C. Batista; L. P. e. Silva; V. Albiero; O. R. P. Bellon; L. Silva","IMAGO Res. Group, Univ. Fed. do Parana, Curitiba, Brazil","2017 30th SIBGRAPI Conference on Graphics, Patterns and Images Tutorials (SIBGRAPI-T)","20180111","2017","","","9","16","With the global demand for extra security systems, and the growing of human-machine interaction, facial analysis in unconstrained environments (in the wild) became a hot-topic in recent computer vision research.Unconstrained environments include surveillance footage, social media photos and live broadcasts.This type of images and videos include no control over illumination, position, size, occlusion, and facial expressions. Successful facial processing methods for controlled scenarios are unable to pledge with challenging circumstances. Consequently, methods tailored for handling those situations are indispensable for the face analysis research progress. This work presents a comprehensive review of state-of-the-art methods, drawing attention to the complications derived from in the wild scenarios and the behavior differences when applied to the controlled images.The main topics to be covered are: (1) face detection; (2) facial image quality; (3) head pose estimation; (4) face alignment; (5) 3D face reconstruction; (6) gender and age estimation; (7) facial expressions and emotions; and (8) face recognition. Finally, available code and applications for in the wild face analysis are presented,followed by a discussion on future directions.","","Electronic:978-1-5386-0619-3; POD:978-1-5386-0620-9","10.1109/SIBGRAPI-T.2017.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8250221","face analysis;face processing","Estimation;Face;Face detection;Face recognition;Three-dimensional displays","computer vision;face recognition;image reconstruction;pose estimation;video surveillance","3D face reconstruction;age estimation;computer vision research;extra security systems;face alignment;face detection;face recognition;facial analysis;facial expressions;facial image quality;facial processing methods;gender estimation;global demand;human-machine interaction;live broadcasts;occlusion;social media photos;surveillance footage;videos;wild face analysis","","","","","","","","17-18 Oct. 2017","","IEEE","IEEE Conferences"
"Quantifying the Financial Value of Cloud Investments: A Systematic Literature Review","P. Rosati; G. Fox; D. Kenny; T. Lynn","Irish Centre for Cloud Comput. & Commerce, Dublin City Univ., Dublin, Ireland","2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)","20171228","2017","","","194","201","The importance of demonstrating the value achieved from IT investments is long established in the Computer Science (CS) and Information Systems (IS) literature. However, emerging technologies such as the ever-changing complex area of cloud computing present new challenges and opportunities for demonstrating how IT investments lead to business value. This paper conducts a multidisciplinary systematic literature review drawing from CS, IS, and Business disciplines to understand the current evidence on the quantification of financial value from cloud computing investments. The study identified 53 articles, which were coded in an analytical framework across six themes (measurement type, costs, benefits, adoption type, actor and service model). Future research directions were presented for each theme. The review highlights the need for multi-disciplinary research which both explores and further develops the conceptualization of value in cloud computing research, and research which investigates how IT value manifests itself across the chain of service provision and in inter-organizational scenarios.","","Electronic:978-1-5386-0692-6; POD:978-1-5386-0693-3","10.1109/CloudCom.2017.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8241108","Cost-Benefit Analysis;Return on Investment;Systematic Literature Review;Total Cost of Ownership","Bibliographies;Cloud computing;Computer science;Economics;Frequency measurement;Investment","business data processing;cloud computing;information systems;investment","Business disciplines;CS;IS;adoption type;business value;cloud computing investments;cloud investments;computer science and information systems;emerging technologies;financial value;future research directions;measurement type;multidisciplinary research;multidisciplinary systematic literature review;service model;theme","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"Review on Firmware","C. J. Tan; J. Mohamad-Saleh; K. A. M. Zain; Z. A. A. Aziz","School of Electrical & Electronic Engineering, Universiti Sains Malaysia, Pulau Pinang, Malaysia","Proceedings of the International Conference on Imaging, Signal Processing and Communication","20171225","2017","","","186","190","<p>This paper presents a review on firmware and the process of firmware development including firmware development model, current trend in firmware development, task scheduling, debugging, documenting the source code and discussed some information related to embedded microcontroller. Firmware was introduced by Ascher Opler in 1967, to solve confusion on defining the software code residing in the hardware Read Only Memory (ROM). Moore's law (1965) is rumored to coming to an end, but undeniable it has been envisioning semiconductor industry for the past 5 decades. Improvement in semiconductor manufacturing technology for integrated chip has at least 10 to 100-folded in term of performance. Embedded device which was then resource-constrained is now capable to run full fledge operating system (O/S). Firmware had been used to define the software code reside in ROM for the Basic Input Output System (BIOS) in our computer motherboard used for initialization of the hardware peripheral. Firmware is also used to define the software code reside in microcontroller and embedded system. Embedded microcontroller is used in various application due to its low cost, small size, reliability, less power consumption and lighter weight. The improvement of technology in term of both hardware and software development had introduce microcontroller like Arduino to allow people to carry out firmware coding relatively ease compared with previously available embedded system. The increased capacity of embedded memory had allowed O/S stack to reside with the embedded system firmware which has blurred the border between firmware and software as how the software code stored in the hardware ROM happened back in 1967. The definition of firmware should be redefined over time to further avoid confusion. Although the borderline between firmware and software is rather blur, undoubtedly firmware is a piece of software with close relation to the hardware within. This paper gives an overview about firmwar- and provides a clearer view on firmware in personal computer and embedded systems.</p>","","Electronic:978-1-4503-5289-5","10.1145/3132300.3132337","","","BIOS;Embedded;Firmware;Microcontroller;Software","","","","","","","","","","","26-28 July 2017","","ACM","ACM Conferences"
"The Register Allocation and Instruction Scheduling Challenge","J. F. N. Carvalho; B. L. Sousa; M. R. Ara√∫jo; M. A. S. Bigonha","Departamento de Ci&#234;ncia da Computa&#231;&#227;o, Universidade Federal de Minas Gerais, Belo Horizonte, M.G., Brasil","Proceedings of the 21st Brazilian Symposium on Programming Languages","20171225","2017","","","1","9","<p>Both register allocation and instruction scheduling are old and open issues in Computer Science, despite the efforts already made to address them separate or jointly. Register allocation may be seen as having two parts: allocation, which decides which values should be in registers, and assignment, which assigns a specific register to each value. Instruction scheduling aims at identifying and moving the instructions in the code, changing their original execution sequence, so that they may run in parallel. Register allocation and instruction scheduling attempt to minimize the execution time of the program, however, they are interdependent and are involved in a prioritization problem. This paper presents a Systematic Literature Review (SLR) related to this problem. From a total of 542 primary studies initially obtained on six databases, 25 studies closely related to this research theme were identified, 12 of them published between 2000 and October 2016. These studies were analyzed to answer the research questions proposed in this SLR, producing useful informations about this theme and about the approaches that, traditionally, have been used to solving this problem. An important finding of this research is the confirmation that this problem still has no definitive solution, and continues to be a relevant challenge for developers, since its solution is closely linked to the quality of the code generated by the compilers in general.</p>","","","10.1145/3125374.3125380","","","code optimization;instruction scheduling;register allocation","","","","","","","","","","","21-22 Sept. 2017","","ACM","ACM Conferences"
"Spark and Scala (keynote)","R. Xin","Databricks, USA","Proceedings of the 8th ACM SIGPLAN International Symposium on Scala","20171225","2017","","","1","1","<p> Written mostly in Scala and with over 1000 contributors, Apache Spark has become the de facto standard for big data processing. In this talk, I will review the evolution of Spark for the last seven years and our experience using Scala as the main programming language in a high profile open source project with a distributed team. I will outline language features that we can't live without, and features we wish were designed differently. Last but not least, I will discuss how we at Databricks are leveraging native code to further improve performance. </p>","","","10.1145/3136000.3148042","","","Big Data;SQL;Scala;Spark","","","","","","","","","","","22-23 Oct. 2017","","ACM","ACM Conferences"
"IDE Plugins for Detecting Input-Validation Vulnerabilities","A. Z. Baset; T. Denning","","2017 IEEE Security and Privacy Workshops (SPW)","20171221","2017","","","143","146","Many vulnerabilities in products and systems could be avoided if better secure coding practices were in place. There exist a number of Integrated Development Environment (IDE) plugins which help developers check for security flaws while they code. In this work, we present a review of these plugins. We specifically focus on the plugins that detect input-validation-related vulnerabilities. We list salient features such as their supported IDEs, applicable languages and specific types of vulnerability checks. We believe this work synthesizes information useful for future research on IDE plugins for detecting input-validation-related vulnerabilities.","","Electronic:978-1-5386-1968-1; POD:978-1-5386-1969-8","10.1109/SPW.2017.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8227300","","Androids;Documentation;Humanoid robots;Java;Tools","programming environments;security of data","IDE plugins;Integrated Development Environment plugins;applicable language;input-validation-related vulnerabilities detection;products;secure coding practices;security flaws;supported IDEs;systems;vulnerability checks","","","","","","","","25-25 May 2017","","IEEE","IEEE Conferences"
"Encoding-Decoding-Based control and filtering of networked systems: insights, developments and opportunities","Z. Wang; L. Wang; S. Liu; G. Wei","Department of Computer Science, Brunel University London, Uxbridge, Middlesex, UB8 3PH, United Kingdom","IEEE/CAA Journal of Automatica Sinica","20171220","2018","5","1","3","18","In order to make the information transmission more efficient and reliable in a digital communication channel with limited capacity, various encoding-decoding techniques have been proposed and widely applied in many branches of the signal processing including digital communications, data compression, information encryption, etc. Recently, due to its promising application potentials in the networked systems (NSs), the analysis and synthesis issues of the NSs under various encoding-decoding schemes have stirred some research attention. However, because of the network-enhanced complexity caused by the limited network resources, it poses new challenges to the design of suitable encoding-decoding procedures to meet certain control or filtering performance for the NSs. In this survey paper, our aim is to present a comprehensive review of the encoding-decoding-based control and filtering problems for different types of NSs. First, some basic introduction with respect to the coding-decoding mechanism is presented in terms of its engineering insights, specific properties and theoretical formulations. Then, the recent representative research progress in the design of the encoding-decoding protocols for various control and filtering problems is discussed. Some possible further research topics are finally outlined for the encoding-decoding-based NSs.","2329-9266;23299266","","10.1109/JAS.2017.7510727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8232586","","Bandwidth;Data communication;Decoding;Distortion;Encoding;System performance","decoding;encoding;filtering theory","data compression;digital communication channel;digital communications;encoding-decoding techniques;filtering problems;information encryption;information transmission;network-enhanced complexity","","","","","","","","Jan. 2018","","IEEE","IEEE Journals & Magazines"
"Domain-specific sentiment analysis approaches for code-mixed social network data","A. Pravalika; V. Oza; N. P. Meghana; S. S. Kamath","Department of Information Technology, National Institute of Technology Karnataka, Surathkal, Mangalore - 575025, India","2017 8th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","20171214","2017","","","1","6","Sentiment Analysis is one of the prominent research fields in Natural Language Processing because of its widespread real-world applications. Customer preferences, options and experiences can be analyzed through social media, reviews, blogs and other online social networking site data. However, due to increasing informal usage of local languages in social media platforms, multi-lingual or code-mixed data is fast becoming a common occurrence. Mixed code is generated when users use more than a single language in social network comments. Such data presents a significant challenge for applications using sentiment analysis and is yet to be fully explored by researchers. Existing sentiment analysis methods applied to monolingual social data are not suitable for code-mixed data due to the inconsistency in the grammatical structure in these sentences. In this paper, a novel method focused on performing effective sentiment analysis of bilingual sentences written in Hindi and English is proposed, that takes into account linguistic code switching and the grammatical transitions between the two considered languages. Experimental evaluation using real-world, code-mixed datasets obtained from Facebook showed that the proposed approach achieved very good accuracy and was also efficient performance-wise.","","Electronic:978-1-5090-3038-5; POD:978-1-5090-3039-2","10.1109/ICCCNT.2017.8204074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8204074","Code-mixed data;Machine Learning;Sentiment analysis;grammatical transitions;lexical analysis","Data mining;Dictionaries;Facebook;Motion pictures;Pragmatics;Sentiment analysis","natural language processing;security of data;sentiment analysis;social networking (online);text analysis","English;Facebook;Natural Language Processing;account linguistic code switching;code-mixed data;code-mixed datasets;code-mixed social network data;domain-specific sentiment analysis;increasing informal usage;local languages;mixed code;monolingual social data;online social networking site data;sentiment analysis methods;social media platforms;social network comments","","","","","","","","3-5 July 2017","","IEEE","IEEE Conferences"
"Flipping introductory programming classes using spinoza and agile pedagogy","F. A. Deeb; T. Hickey","Computer Science Department, Brandeis University, Waltham, MA 02453, USA","2017 IEEE Frontiers in Education Conference (FIE)","20171214","2017","","","1","9","In this paper we present a new approach to flipping large introductory programming classes that we call the Solve-Then-Debug approach. This is a Computer Supported Agile Teaching methodology in which students solve problems using a web-based IDE we created, Spinoza, and then start reviewing failed attempts by their peers to classify the errors and comment on them. The classification and comment information is then made available as a hint to those students still trying to solve the problem when they encounter a similar error. Spinoza provides a wide variety of visualizations and dashboards that allow the instructor to closely monitor the progress of the students in this activity and to pivot to another phase of the activity at the appropriate time. It also has features that allow the instructor to easily detect and intervene with students who have failed to demonstrate mastery of the skills and concepts covered in that lesson. Spinoza builds on the ideas behind several other recent systems and this paper demonstrates that the Solve-Then-Debug approach can successfully keep all students actively engaged in learning coding skills even when there is a large range of skills in the class.","","Electronic:978-1-5090-5920-1; POD:978-1-5090-4920-2; USB:978-1-5090-5919-5","10.1109/FIE.2017.8190519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8190519","","Debugging;Problem-solving;Programming;STEM;Tools","Internet;computer aided instruction;computer science education;program debugging;programming environments;teaching","Computer Supported Agile Teaching methodology;Solve-Then-Debug approach;agile pedagogy;introductory programming classes;spinoza;web-based IDE","","","","","","","","18-21 Oct. 2017","","IEEE","IEEE Conferences"
"A first look at mobile Ad-Blocking apps","M. Ikram; M. A. Kaafar","Data61, CSIRO","2017 IEEE 16th International Symposium on Network Computing and Applications (NCA)","20171211","2017","","","1","8","Online advertisers, third party trackers and analytics services are constantly tracking user activities as they access web services through their web browsers or mobile apps. While, web browser plugins disabling and blocking Ads (often associated tracking/analytics scripts), e.g. AdBlock Plus[3] have been well studied and are relatively well understood, an emerging new category of apps in the tracking mobile eco-system, referred as the mobile Ad-Blocking apps, received very little to no attention. With the recent significant increase of the number of mobile Ad-Blockers and the exponential growth of mobile Ad-Blocking apps' popularity, this paper aims to fill in the gap and study this new category of players in the mobile ad/tracking eco-system. This paper presents the first study of Android Ad-Blocking apps (or Ad-Blockers), analysing 97 Ad-Blocking mobile apps extracted from a corpus of more than 1.5 million Android apps on Google Play. While the main (declared) purpose of the apps is to block advertisements and mobile tracking services, our data analysis revealed the paradoxical presence of third-party tracking libraries and permissions to access sensitive resources on users' mobile devices, as well as the existence of embedded malware code within some mobile Ad-Blockers. We also analysed user reviews and found that even though a fraction of users raised concerns about the privacy and the actual performance of the mobile Ad-Blocking apps, most of the apps still attract a relatively high rating.","","Electronic:978-1-5386-1465-5; POD:978-1-5386-1466-2; USB:978-1-5386-1464-8","10.1109/NCA.2017.8171376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8171376","","Androids;Browsers;Google;Humanoid robots;Libraries;Mobile communication;Tools","Android (operating system);Internet;Web services;Web sites;advertising data processing;data analysis;data privacy;invasive software;mobile computing;online front-ends","AdBlock Plus;Ads disabling;Android Ad-Blocking apps;Google Play;Web browser;Web services;analytics scripts;analytics services;associated tracking;mobile Ad-Blockers;mobile Ad-Blocking apps;mobile apps;mobile tracking services;online adverstisers;third party trackers;third-party tracking libraries;tracking mobile eco-system","","","","","","","","Oct. 30 2017-Nov. 1 2017","","IEEE","IEEE Conferences"
"Let's work together: Improving block-based environments by supporting synchronous collaboration","J. Tsan; F. J. Rodr√≠guez; K. E. Boyer; C. Lynch","Department of Computer Science, North Carolina State University, Raleigh, NC 27606","2017 IEEE Blocks and Beyond Workshop (B&B)","20171130","2017","","","53","56","Block-based programming environments are a good way to teach beginners how to code, in part because they eliminate syntax errors and provide visual feedback. However, many of the existing environments do not explicitly support synchronous collaboration. Collaboration is a critical component of computer science practice and CS education. We therefore argue that features to support collaboration could significantly enhance existing and new block-based programming environments. We review existing block-based programming environments, suggest design ideas for supporting synchronous collaboration, and evaluate environments that currently support some of these features.","","Electronic:978-1-5386-2480-7; POD:978-1-5386-2481-4","10.1109/BLOCKS.2017.8120411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8120411","","Collaboration;Navigation;Programming environments;Programming profession","computer science education;groupware;programming environments;teaching","block-based programming environments;computer science practice;synchronous collaboration;syntax errors;visual feedback","","","","","","","","9-10 Oct. 2017","","IEEE","IEEE Conferences"
"IntPTI: Automatic integer error repair with proper-type inference","X. Cheng; M. Zhou; X. Song; M. Gu; J. Sun","School of Software, TNLIST, KLISS, Tsinghua University, China","2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)","20171123","2017","","","996","1001","Integer errors in C/C++ are caused by arithmetic operations yielding results which are unrepresentable in certain type. They can lead to serious safety and security issues. Due to the complicated semantics of C/C++ integers, integer errors are widely harbored in real-world programs and it is error-prone to repair them even for experts. An automatic tool is desired to 1) automatically generate fixes which assist developers to correct the buggy code, and 2) provide sufficient hints to help developers review the generated fixes and better understand integer types in C/C++. In this paper, we present a tool IntPTI that implements the desired functionalities for C programs. IntPTI infers appropriate types for variables and expressions to eliminate representation issues, and then utilizes the derived types with fix patterns codified from the successful human-written patches. IntPTI provides a user-friendly web interface which allows users to review and manage the fixes. We evaluate IntPTI on 7 real-world projects and the results show its competitive repair accuracy and its scalability on large code bases. The demo video for IntPTI is available at: https://youtu.be/9Tgd4A_FgZM.","","Electronic:978-1-5386-2684-9; POD:978-1-5386-3976-4","10.1109/ASE.2017.8115718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115718","fix pattern;integer error;type inference","Computer bugs;Maintenance engineering;Runtime;Scalability;Security;Semantics;Tools","Internet;human computer interaction;inference mechanisms;probability;program debugging;program diagnostics;security of data;user interfaces","C/C++ integers;appropriate types;arithmetic operations;automatic integer error repair;automatic tool;competitive repair accuracy;complicated semantics;derived types;desired functionalities;fix patterns;generated fixes;integer errors;integer types;proper-type inference;security issues;tool IntPTI","","","","","","","","Oct. 30 2017-Nov. 3 2017","","IEEE","IEEE Conferences"
"Artifact Evaluation: Is It a Real Incentive?","B. R. Childers; P. K. Chrysanthis","Sch. of Comput. & Inf., Univ. of Pittsburgh, Pittsburgh, PA, USA","2017 IEEE 13th International Conference on e-Science (e-Science)","20171116","2017","","","488","489","It is well accepted that we learn hard lessons when implementing and re-evaluating systems, yet it is also acknowledged that science faces a crisis in reproducibility. Experimental computer science is far from immune, although it should be easier for CS than other sciences, given the emphasis on experimental artifacts, such as source code, data sets, workflows, parameters, etc. The data management community pioneered methods at ACM SIGMOD 2007 and 2008 to encourage and incentivize authors to improve their software development and experimental practices. Now, after 10 years, the broader CS community has started to adopt Artifact Evaluation (AE) to review artifacts along with papers. In this paper, we examine how AE has incentivized authors, and whether the process is having a measurable impact. Our answer can help guide CS, and more broadly, other computationally-oriented sciences, in encouraging peer-review of software artifacts and developing additional community practices for incentives.","","Electronic:978-1-5386-2686-3; POD:978-1-5386-2687-0","10.1109/eScience.2017.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109184","","Conferences;Electronic mail;Market research;Measurement;Open Access;Robustness;Software","Unified Modeling Language;software engineering","AE;UML;artifact evaluation;computationally-oriented sciences;computer science;data management community;peer-review;real incentive;reevaluating systems;software artifacts;software development","","","","","","","","24-27 Oct. 2017","","IEEE","IEEE Conferences"
"Practical Evaluation of Static Analysis Tools for Cryptography: Benchmarking Method and Case Study","A. Braga; R. Dahab; N. Antunes; N. Laranjeiro; M. Vieira","Inst. of Comput., State Univ. of Campinas, Campinas, Brazil","2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)","20171116","2017","","","170","181","The incorrect use of cryptography is a common source of critical software vulnerabilities. As developers lack knowledge in applied cryptography and support from experts is scarce, this situation is frequently addressed by adopting static code analysis tools to automatically detect cryptography misuse during coding and reviews, even if the effectiveness of such tools is far from being well understood. This paper proposes a method for benchmarking static code analysis tools for the detection of cryptography misuse, and evaluates the method in a case study, with the goal of selecting the most adequate tools for specific development contexts. Our method classifies cryptography misuse in nine categories recognized by developers (weak cryptography, poor key management, bad randomness, etc.) and provides the workload, metrics and procedure needed for a fair assessment and comparison of tools. We found that all evaluated tools together detected only 35% of cryptography misuses in our tests. Furthermore, none of the evaluated tools detected insecure elliptic curves, weak parameters in key agreement, and most insecure configurations for RSA and ECDSA. This suggests cryptography misuse is underestimated by tool builders. Despite that, we show that it is possible to benefit from an adequate tool selection during the development of cryptographic software.","","Electronic:978-1-5386-0941-5; POD:978-1-5386-0942-2","10.1109/ISSRE.2017.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109084","benchmarking;cryptography;software security;static analysis tools","Benchmark testing;Elliptic curve cryptography;Encoding;Measurement;Software;Tools","cryptography;program diagnostics","ECDSA;RSA;adequate tool selection;critical software vulnerabilities;cryptographic software development;cryptography misuse;static analysis tools;static code analysis tools","","","","","","","","23-26 Oct. 2017","","IEEE","IEEE Conferences"
"Sparse Coded Handcrafted and Deep Features for Colon Capsule Video Summarization","A. Mohammed; S. Yildirim; M. Pedersen; √ò. Hovde; F. Cheikh","Dept. of Comput. Sci., Norwegian Univ. of Sci. & Technol., Gjovik, Norway","2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS)","20171113","2017","","","728","733","Capsule endoscopy, which uses a wireless camera to take images of the digestive track, is emerging as an alternative to traditional wired colonoscopy. A single examination produces a sequence of approximately 50,000 frames. These sequences are manually reviewed, which is time consuming and typically takes about 45-90 minutes and requires the undivided concentration of the reviewer. In this paper, we propose a novel capsule video summarization framework using sparse coding and dictionary learning in feature space. Video frames are clustered into superframes based on power spectral density, and cluster representative frames are used for video summarization. Handcrafted and deep features that are extracted for representative frames are sparse coded using a learned dictionary. Sparse coded features are later used for training SVM classifier. The proposed method was compared with state-of-the-art methods based on sensitivity and specificity. The achieved results show that our proposed framework provides robust capsule video summarization without losing informative segments.","","Electronic:978-1-5386-1710-6; POD:978-1-5386-1711-3","10.1109/CBMS.2017.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104288","Dictionary learning;KSVD;Random forest;capsule endoscopy;deep features;informative frame.","Cameras;Colon;Dictionaries;Feature extraction;Histograms;Image color analysis;Machine learning","biomedical optical imaging;endoscopes;feature extraction;learning (artificial intelligence);medical image processing;support vector machines;video signal processing","SVM classifier;capsule endoscopy;capsule video summarization framework;cluster representative frames;colon capsule video summarization;deep features;dictionary learning;digestive track images;feature extraction;power spectral density;sparse coded handcrafted;sparse coding;time 45.0 min to 90.0 min;video frames;wired colonoscopy;wireless camera","","","","","","","","22-24 June 2017","","IEEE","IEEE Conferences"
"Anarchy or Order on the Streets: Review Based Characterization of Location Based Mobile Games","P. Alavesa; M. Pakanen; H. Kukka; M. Pouke; T. Ojala","University of Oulu, Oulu, Finland","Proceedings of the Annual Symposium on Computer-Human Interaction in Play","20171107","2017","","","101","113","<p>Location based mobile games have traditionally relied on implicit codes of conduct, legal ordinances, common social norms, or community emergent rules. However, these games are becoming increasingly popular and enforcing these implicit or explicit restrictions has become difficult. In this paper, we present a critical and systematic review of both commercial and non-commercial location based mobile games. We list selected characteristics of the games and highlight their connection to the affordances and restrictions on urban game arenas. We also demonstrate the feasibility of our characterization by applying it to two recent location based mobile games, Pokemon GO [53] and Street Art Gangs [4].</p>","","Electronic:978-1-4503-4898-0","10.1145/3116595.3116614","","","game design;location based mobile games;pervasive games","","","","","","","","","","","15-18 Oct. 2017","","ACM","ACM Conferences"
"Simulation software engineering: experiences and challenges","S. Wagner; D. Pfl√ºger; M. Mehl","University of Stuttgart, Germany","Proceedings of the 3rd International Workshop on Software Engineering for High Performance Computing in Computational Science and Engineering","20171107","2015","","","1","4","<p>Using software for large-scale simulations has become an important research method in many disciplines. With increasingly complex simulations, simulation software becomes a valuable assest. Yet, the quality of many simulation codes is worrying. In this paper, we want to collect and structure the challenges for a systematic simulation software engineering as a reference and the basis for further research. We describe our own experiences with developing simulation software and collaborating with non-computer-scientists. We complement our experienced challenges with a brief literature review. We structured the challenges for simulation software engineering into six areas: motivation and recognition; education and training; developer turnover; software length of life; verification, validation and debugging; and efficiency vs. maintainability. Overcoming these challenges needs efforts from research agencies, scientific computing researchers as well as software engineering researchers.</p>","","Electronic:978-1-4503-4012-0","10.1145/2830168.2830171","","","simulation software;software engineering","","","","","1","","","","","","15-15 Nov. 2015","","ACM","ACM Conferences"
"Security Smells in Android","M. Ghafari; P. Gadient; O. Nierstrasz","Software Composition Group, Univ. of Bern, Bern, Switzerland","2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation (SCAM)","20171102","2017","","","121","130","The ubiquity of smartphones, and their very broad capabilities and usage, make the security of these devices tremendously important. Unfortunately, despite all progress in security and privacy mechanisms, vulnerabilities continue to proliferate.,,Research has shown that many vulnerabilities are due to insecure programming practices. However, each study has often dealt with a specific issue, making the results less actionable for practitioners. To promote secure programming practices, we have reviewed related research, and identified avoidable vulnerabilities in Android-run devices and the security code smells that indicate their presence. In particular, we explain the vulnerabilities, their corresponding smells, and we discuss how they could be eliminated or mitigated during development. Moreover, we develop a lightweight static analysis tool and discuss the extent to which it successfully detects several vulnerabilities in about 46 000 apps hosted by the official Android market.","","Electronic:978-1-5386-3238-3; POD:978-1-5386-3239-0","10.1109/SCAM.2017.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090145","Android;Programming;Security;Smell","Androids;Cryptography;Humanoid robots;Libraries;Smart phones;Tools","Android (operating system);data privacy;invasive software;mobile computing;program diagnostics;programming;smart phones;source code (software)","Android;privacy mechanisms;secure programming practices;security code;security code smells;security mechanisms;smartphones;static analysis tool","","","","","","","","17-18 Sept. 2017","","IEEE","IEEE Conferences"
"Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review","Y. Li; S. Schulze; G. Saake","Otto-von-Guericke Universit&#228;t Magdeburg, Germany","Proceedings of the 21st International Systems and Software Product Line Conference - Volume A","20171009","2017","","","133","142","<p>Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering & machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.</p>","","","10.1145/3106195.3106207","","","Feature Identification;Natural Language Documents;Reverse Engineering;Software Product Lines;Systematic Literature Review;Variability Extraction","","","","","","","","","","","25-29 Sept. 2017","","ACM","ACM Conferences"
"Scalable Video Summarization: A Comparative Study","A. Abozeid; H. Farouk; K. ElDahshan","Dept. of Mathematics, Computer Science Division, Al-Azhar University, Cairo, Egypt","Proceedings of the International Conference on Compute and Data Analysis","20171009","2017","","","215","219","<p>The amount of videos over the internet and media storage systems has dramatically increased. This poses challenges in video content understanding and management. A video is a complex and resource consuming media. In addition, efficient use of video data requires the data to be understood and accessed without having to watch it entirely. For those reasons, video summarization (VS) has been a hot topic of recent researches. VS is the process of creating a compact representation that can provide the user with concise information about the video content. VS helps in efficient storage, quick browsing, and retrieval of video data maintaining its main features. In video codec and streaming contexts, Scalable Video Coding (SVC) enables dynamic adaptation based on network conditions and device capabilities. This paper reviews the recent work on scalable video summarization (SVS) and discusses its role in current research directions.</p>","","","10.1145/3093241.3093287","","","Scalable video coding;Video summarization;video content;video processing","","","","","","","","","","","19-23 May 2017","","ACM","ACM Conferences"
"Android apps and user feedback: a dataset for software evolution and quality improvement","G. Grano; A. Di Sorbo; F. Mercaldo; C. A. Visaggio; G. Canfora; S. Panichella","University of Zurich, Switzerland","Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics","20170914","2017","","","8","11","<p> Nowadays, Android represents the most popular mobile platform with a market share of around 80%. Previous research showed that data contained in user reviews and code change history of mobile apps represent a rich source of information for reducing software maintenance and development effort, increasing customers' satisfaction. Stemming from this observation, we present in this paper a large dataset of Android applications belonging to 23 different apps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews and more than 450,000 user feedback (extracted with specific text mining approaches). Furthermore, for each app version in our dataset, we employed the Paprika tool and developed several Python scripts to detect 8 different code smells and compute 22 code quality indicators. The paper discusses the potential usefulness of the dataset for future research in the field. </p>","","Electronic:978-1-4503-5158-4","10.1145/3121264.3121266","","","App Reviews;Mobile Applications;Software Maintenance and Evolution;Software Quality","","","","","","","","","","","5-5 Sept. 2017","","ACM","ACM Conferences"
"Towards triaging code-smell candidates via runtime scenarios and method-call dependencies","T. Haendler; S. Sobernig; M. Strembeck","WU Vienna, Austria","Proceedings of the XP2017 Scientific Workshops","20170914","2017","","","1","9","<p>Managing technical debt includes the detection and assessment of debt at the code and design levels (such as bad smells). Existing approaches and tools for smell detection primarily use static program data for decision support. While a static analysis allows for identifying smell candidates without executing and instrumenting the system, such approaches also come with the risk of missing candidates or of producing false positives. Moreover, smell candidates might result from a deliberate design decision (e.g., of applying a particular design pattern). Such risks and the general ambivalence of smell detection require a manual design and/or code inspection for reviewing all alleged smells.</p> <p>In this paper, we propose an approach to obtain tailorable design documentation for object-oriented systems based on runtime tests. In particular, the approach supports a tool-supported triaging of code-smell candidates. We use runtime scenario tests to extract execution traces. Based on these execution traces, different (automatically derived) model perspectives on method-call dependencies (e.g., dependency structure matrices, DSMs; UML2 sequence diagrams) are then used as decision support for assessing smell candidates. Our approach is implemented as part of the KaleidoScope tool which is publicly available for download.</p>","","","10.1145/3120459.3120468","","","code smell;decision support;dependency structure matrix;design documentation;execution trace;scenario-based testing;software behavior;technical debt;unified modeling language (UML2)","","","","","","","","","","","22-26 May 2017","","ACM","ACM Conferences"
"Using MDA to Automate the Integration of Virtual Platforms for System-Level Simulation","D. Perillo; M. D. Natale","Elettron. SpA, Rome, Italy","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","20170911","2017","1","","268","277","This paper presents the work performed at (removed for blind review) to automate the integration of virtual systems development (VSD) and simulation in its embedded software development process. The approach is based on a combination of metamodels, model transformations and design patterns, the SysML standard and the use of the open source Eclipse framework. The purpose is to derive all the design refinements, including the production code and the code used for simulation and verification from a single set of SysML models. Stereotypes and model transformations are defined to allow the integration of automatically generated interfaces and manually produced code implementing virtual platforms for the simulation of HW/SW heterogeneous systems on the SIMICS platform.","0730-3157;07303157","Electronic:978-1-5386-0367-3; POD:978-1-5386-0368-0","10.1109/COMPSAC.2017.259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029618","","Hardware;Load modeling;Software;Standards;Tools;Unified modeling language","formal specification;hardware-software codesign;multiprocessing systems;software engineering","HW-SW heterogeneous systems;HW/SW heterogeneous systems;MDA;SIMICS platform;SysML models;SysML standard;VSD;automate the integration;design patterns;embedded software development process;open source Eclipse framework;production code;single set;system-level simulation;virtual platforms;virtual system development","","","","","","","","4-8 July 2017","","IEEE","IEEE Conferences"
"A survey of rate control in HEVC and SHVC video encoding","A. A. Ramanand; I. Ahmad; V. Swaminathan","University of Texas at Arlington, USA","2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","20170907","2017","","","145","150","Rate control in video coding has proven to be a prolific research area because it is the heart of video coding. This paper presents an overview of rate control techniques in HEVC. The paper introduces and reviews some of the rate control algorithms for prior video coding standards. Next, the paper presents the basic features of HEVC that drive the need for new rate control techniques. The paper also describes R-D model based taxonomy of various algorithms including the classification criteria. Another classification categorizes the rate control algorithms according to their basic principle and mechanisms. The paper includes a section giving an overview of the scalable extension of HEVC namely SHVC while highlighting some of the possible challenges in SHVC rate control design. Finally, the paper identifies some of the conspicuous unresolved research issues in HEVC rate control and outlines possible future directions.","","Electronic:978-1-5386-0560-8; POD:978-1-5386-0561-5; USB:978-1-5386-0559-2","10.1109/ICMEW.2017.8026268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8026268","HEVC;Quantization Parameter (QP);Rate Distortion Optimization (RDO);SHVC","Bit rate;Complexity theory;Indexes;Optimization;Rate-distortion;Silicon;Sun","telecommunication control;video coding","HEVC;R-D model based taxonomy;SHVC;rate control techniques;video encoding","","","","","","","","10-14 July 2017","","IEEE","IEEE Conferences"
"Investigating Spheres of Influence in Communication Design: Comparing the Citation Practices of Research Faculty and Industry Leaders","B. Lauren; S. Pigg; L. Brentnell; H. Fooksman; K. Mapes","Michigan State University, East Lansing, MI","Proceedings of the 34th ACM International Conference on the Design of Communication","20170907","2016","","","1","9","<p>Divides among academic knowledge and practitioner needs have been well documented in technical communication and related fields. However, the recent popularity of practitioner publications indicates a timely moment to reassess the relationships among academic and nonacademic influencers in emerging communication design knowledge. In this paper, we introduce initial findings from the first phase of an ongoing research project that analyzes citation practices in communication design publications. First, we created a sample of over 1500 citations from 30 recent peer-reviewed journal articles that treat emerging communication design themes. Next, we coded authors or author teams from the 30 original articles and the 1500 cited publications to identify whether each author or author team was comprised of academic research faculty, nonacademics, or both. Based on this analysis, we show that academic citations comprise just over 60 percent of the total number in our sample. While this is a clear majority, non-academic citations do frequently influence the shape of peer-reviewed work on emerging communication design themes. Based on our findings, we argue for extending citation analysis to a larger sample of peer-reviewed journal articles and different publications types (i.e., proceedings, books, trade magazines, blogs, social media conversations) in order to test our findings. We furthermore call for social network analysis (SNA) that can reveal more complex interconnections among academic researchers and industry or independent thought leaders around topics such as user experience, content management and strategy, and project management.</p>","","","10.1145/2987592.2987602","","","Citation Analysis;Content Strategy;Empirical Research;Social Networks;User Experience","","","","","","","","","","","23-24 Sept. 2016","","ACM","ACM Conferences"
"The Socket Store: An app model for the application-network interaction","C. Liaskos; A. Tsioliaridou; S. Ioannidis","Foundation for Research and Technology - Hellas (FORTH), Greece","2017 IEEE Symposium on Computers and Communications (ISCC)","20170904","2017","","","365","370","A developer of mobile or desktop applications is responsible for implementing the network logic of his software. Nonetheless: i) Developers are not network specialists, while pressure for emphasis on the visible application parts places the network logic out of the coding focus. Moreover, computer networks undergo evolution at paces that developers may not follow. ii) From the network resource provider point of view, marketing novel services and involving a broad audience is also challenge for the same reason. Moreover, the objectives of end-user networking logic are neither clear nor uniform. This constitutes the central optimization of network resources an additional challenge. As a solution to these problems, we propose the Socket Store. The Store is a marketplace containing end-user network logic in modular form. The Store modules act as intelligent mediators between the end-user and the network resources. Each module has a clear, specialized objective, such as connecting two clients over the Internet while avoiding transit networks suspicious for eavesdropping. The Store is populated and peer-reviewed by network specialists, whose motive is the visibility, practical applicability and monetization potential of their work. A developer first purchases access to a given socket module. Subsequently, he incorporates it to his applications under development, obtaining state-of-the-art performance with trivial coding burden. A full Store prototype is implemented and a critical data streaming module is evaluated as a driving case.","","Electronic:978-1-5386-1629-1; POD:978-1-5386-1630-7; USB:978-1-5386-1628-4","10.1109/ISCC.2017.8024557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024557","intelligent network logic;network-application interaction;store","Companies;Computational modeling;Computer networks;Encoding;Libraries;Sockets;Software","Internet;mobile computing","Internet;app model;application-network interaction;broad audience;central optimization;coding focus;computer networks;critical data streaming module;desktop applications;eavesdropping;end-user network logic;end-user networking logic;intelligent mediators;marketplace;mobile applications;network resource provider;network resources;network specialists;peer-reviewed;practical applicability;socket module;socket store;store modules;transit networks;trivial coding;visible application parts","","","","","","","","3-6 July 2017","","IEEE","IEEE Conferences"
"Business Documentation Derivation from Aspect-driven Enterprise Information Systems","K. Cemus; T. Cerny","Faculty of Electrical Engineering, Czech Technical University in Prague, Technicka, Praha, CZ","Proceedings of the International Conference on Research in Adaptive and Convergent Systems","20170831","2016","","","153","158","<p>Business documentation provides important overview of the system. It lists implemented business operations and their preconditions and post-conditions, as well as the architecture of the domain model. Such information is valuable for domain experts, who are able to review it, validate the conditions, and help to address errors in early phases of development. However, it is challenging to derive this documentation from existing systems as the business rules are tangled into source code.</p> <p>In this paper, we introduce a novel technique to business documentation derivation from aspect-driven enterprise information systems. These systems isolate business rules in a single focal point and we demonstrate its transformation into business documentation. In the combination with code inspection, we deliver reliable and easy to generate documentation significantly simplifying involvement of domain experts into development. Furthermore, we show the transformation of business rules into a formal language verifiable by a checker to validate the feasibility of business operations.</p>","","","10.1145/2987386.2987402","","","Aspect-Oriented Programming;Business Documentation;Enterprise Information Systems;Model-Driven Development","","","","","","","","","","","11-14 Oct. 2016","","ACM","ACM Conferences"
"Surveying Security Practice Adherence in Software Development","P. Morrison; B. H. Smith; L. Williams","Department of Computer Science, North Carolina State University, Raleigh, North Carolina","Proceedings of the Hot Topics in Science of Security: Symposium and Bootcamp","20170831","2017","","","85","94","<p>Software development teams are increasingly incorporating security practices in to their software development processes. However, little empirical evidence exists on the costs and benefits associated with the application of security practices. Balancing the trade off between the costs in time, effort, and complexity of applying security practices and the benefit of an appropriate level of security in delivered software requires measuring security practice benefits and costs. The goal of this research is to support researcher investigations of software development security practice adherence by building and validating a set of security practices and adherence measures through literature review and survey data analysis. We extracted 16 software development security practices from a review of the literature, and established a set of adherence measures based on technology acceptance theory. We built a survey around the 13 most common practices and our adherence measures. We surveyed 11 security-focused open source projects to collect empirical data as a test of our theorizing about practice adherence. In our collected survey data, each of the 13 security practices we identified was used daily by at least one survey participant. Tracking vulnerabilities and applying secure coding standards are the practices most often applied daily. In our data, Ease of use, Effectiveness, and Training, measured via Likert items, did not always show the expected theoretical relationship with practice use. In our data, Training is positively correlated with practice use, while Effectiveness and Ease of use vary in their correlations with practice use on a practice by practice basis.</p>","","","10.1145/3055305.3055312","","","","","","","","","","","","","","4-5 April 2017","","ACM","ACM Conferences"
"Genetic improvement of computational biology software","W. B. Langdon; K. Zile","University College London, Gower Street, UK","Proceedings of the Genetic and Evolutionary Computation Conference Companion","20170831","2017","","","1657","1660","<p>There is a cultural divide between computer scientists and biologists that needs to be addressed. The two disciplines used to be quite unrelated but many new research areas have arisen from their synergy. We selectively review two multi-disciplinary problems: dealing with contamination in sequencing data repositories and improving software using biology inspired evolutionary computing. Through several examples, we show that ideas from biology may result in optimised code and provide surprising improvements that overcome challenges in speed and quality trade-offs. On the other hand, development of computational methods is essential for maintaining contamination free databases. Computer scientists and biologists must always be sceptical of each others data, just as they would be of their own.</p>","","","10.1145/3067695.3082540","","","1KGP;1k geneomes;<i>in silico</i> contamination;DNA sequences;GGGP;GI;GP;NCBI GEO;NGS;SBSE;big data cleanup;bioinformatics;data cleansing;genechip;genetic improvement;genetic programming;hitch-hiking genes;identification and correction of mislabelled genes;microarray;molecular biology;next generation sequencing;search based software engineering;software engineering","","","","","","","","","","","15-19 July 2017","","ACM","ACM Conferences"
"Refining interprocedural change-impact analysis using equivalence relations","A. Gyori; S. K. Lahiri; N. Partush","University of Illinois at Urbana-Champaign, USA","Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis","20170831","2017","","","318","328","<p> Change-impact analysis (CIA) is the task of determining the set of program elements impacted by a program change. Precise CIA has great potential to avoid expensive testing and code reviews for (parts of) changes that are refactorings (semantics-preserving). However most statement-level CIA techniques suffer from imprecision as they do not incorporate the semantics of the change. </p> <p> We formalize change impact in terms of the trace semantics of two program versions. We show how to leverage equivalence relations to make dataflow-based CIA aware of the change semantics, thereby improving precision in the presence of semantics-preserving changes. We propose an anytime algorithm that applies costly equivalence-relation inference incrementally to refine the set of impacted statements. We implemented a prototype and evaluated it on 322 real-world changes from open-source projects and benchmark programs used by prior research. The evaluation results show an average 35% improvement in the number of impacted statements compared to prior dataflow-based techniques. </p>","","","10.1145/3092703.3092719","","","Static analysis;program verification;software change","","","","","","","","","","","10-14 July 2017","","ACM","ACM Conferences"
"Compressing deep neural networks for efficient visual inference","S. Ge; Z. Luo; S. Zhao; X. Jin; X. Y. Zhang","Beijing Key Laboratory of IOT information Security, Institute of Information Engineering, Chinese Academy of Sciences","2017 IEEE International Conference on Multimedia and Expo (ICME)","20170831","2017","","","667","672","The deployments of deep neural network models on mobile or embedded devices have been challenged due to two main reasons: 1) the large model size for storage, and 2) the large memory bandwidth for inference. To address these issues, this paper develops a deep neural network compression framework to reduce the resource usage for efficient visual inference. By reviewing the trained deep model, we propose a hybrid model compression algorithm via four major modules. Approximation module reduces the number of weights in each fully connected layer with low rank approximation. Then, quantization module analyzes weight distribution in each layer and represents them with low precision fixed point, which reduces the bits for storing each weight. After that, pruning module suppresses small weights to further reduce the number of parameters. Finally, coding module joint optimizes the representation and encoding of the sparse structure of the pruned weights with relative index by Huffman coding. Beyond the compression of model size, we propose an adaptive fixed point memory allocation algorithm to reduce memory footprint in inference. The proposed framework, along with the model compression and memory allocation algorithms, can provide 20-30x compression rate with negligible accuracy loss. We conduct an evaluation on two representative models, AlexNet and VGG-16, for object recognition and face verification tasks, which demonstrate the effectiveness of our proposed compression framework.","","Electronic:978-1-5090-6067-2; POD:978-1-5090-6068-9; USB:978-1-5090-6066-5","10.1109/ICME.2017.8019465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019465","Deep neural networks;face verification;model compression;object recognition;visual inference","Adaptation models;Bandwidth;Computational modeling;Face;Neural networks;Quantization (signal);Visualization","approximation theory;data compression;face recognition;image coding;neural nets;object recognition","AlexNet model;Huffman coding;VGG-16 model;approximation module;coding module;deep neural network compression framework;face verification task;hybrid model compression algorithm;low rank approximation;memory bandwidth;object recognition task;pruning module;quantization module;visual inference","","","","","","","","10-14 July 2017","","IEEE","IEEE Conferences"
"Risk-based attack surface approximation: how much data is enough?","C. Theisen; K. Herzig; B. Murphy; L. Williams","Comput. Sci., NC State Univ., Raleigh, NC, USA","2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)","20170824","2017","","","273","282","Proactive security reviews and test efforts are a necessary component of the software development lifecycle. Resource limitations often preclude reviewing the entire code base. Making informed decisions on what code to review can improve a team's ability to find and remove vulnerabilities. Risk-based attack surface approximation (RASA) is a technique that uses crash dump stack traces to predict what code may contain exploitable vulnerabilities. The goal of this research is to help software development teams prioritize security efforts by the efficient development of a risk-based attack surface approximation. We explore the use of RASA using Mozilla Firefox and Microsoft Windows stack traces from crash dumps. We create RASA at the file level for Firefox, in which the 15.8% of the files that were part of the approximation contained 73.6% of the vulnerabilities seen for the product. We also explore the effect of random sampling of crashes on the approximation, as it may be impractical for organizations to store and process every crash received. We find that 10-fold random sampling of crashes at a rate of 10% resulted in 3% less vulnerabilities identified than using the entire set of stack traces for Mozilla Firefox. Sampling crashes in Windows 8.1 at a rate of 40% resulted in insignificant differences in vulnerability and file coverage as compared to a rate of 100%.","","Electronic:978-1-5386-2717-4; POD:978-1-5386-2718-1","10.1109/ICSE-SEIP.2017.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965451","attack surface;prediction models;stack traces","Computer bugs;Measurement;Security;Software systems","security of data;software engineering","Microsoft Windows;Mozilla Firefox;RASA;proactive security reviews;random sampling;risk-based attack surface approximation;software development lifecycle;test efforts","","","","","","","","20-28 May 2017","","IEEE","IEEE Conferences"
"Review of Research on Student-Facing Learning Analytics Dashboards and Educational Recommender Systems","R. Bodily; K. Verbert","Instructional Psychology and Technology Department, Brigham Young University, 150 MCKB, Provo, UT","IEEE Transactions on Learning Technologies","20171214","2017","10","4","405","418","This article is a comprehensive literature review of student-facing learning analytics reporting systems that track learning analytics data and report it directly to students. This literature review builds on four previously conducted literature reviews in similar domains. Out of the 945 articles retrieved from databases and journals, 93 articles were included in the analysis. Articles were coded based on the following five categories: functionality, data sources, design analysis, student perceptions, and measured effects. Based on this review, we need research on learning analytics reporting systems that targets the design and development process of reporting systems, not only the final products. This design and development process includes needs analyses, visual design analyses, information selection justifications, and student perception surveys. In addition, experiments to determine the effect of these systems on student behavior, achievement, and skills are needed to add to the small existing body of evidence. Furthermore, experimental studies should include usability tests and methodologies to examine student use of these systems, as these factors may affect experimental findings. Finally, observational study methods, such as propensity score matching, should be used to increase student access to these systems but still rigorously measure experimental effects.","1939-1382;19391382","","10.1109/TLT.2017.2740172","KU Leuven Research Council; 10.13039/501100003130 - Research Foundation Flanders (FWO); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010828","Data mining;adaptive and intelligent educational systems;data and knowledge visualization;homework support systems;literature review;self-assessment technologies","Computer science;Data mining;Databases;Learning systems;Recommender systems;Visualization","computer aided instruction;educational courses;recommender systems;student experiments","Educational Recommender Systems;data sources;design analysis;learning analytics data;student access;student behavior;student perception;student perceptions;student-facing learning analytics;visual design analyses","","","","","","","20170815","Oct.-Dec. 1 2017","","IEEE","IEEE Journals & Magazines"
"Resource Management in Non-Orthogonal Multiple Access Networks for 5G and Beyond","L. Song; Y. Li; Z. Ding; H. V. Poor","Univ. of York, York, UK","IEEE Network","20170728","2017","31","4","8","14","Non-orthogonal multiple access techniques have been proposed recently for 5G wireless systems and beyond to improve access efficiency by allowing many users to share the same spectrum. Due to the strong co-channel interference among mobile users introduced by NOMA, it poses significant challenges for system design and resource management. This article reviews resource management issues in NOMA systems. The main taxonomy of NOMA is presented by focusing on the two main categories of resource reuse: power-domain and code-domain NOMA. Then a novel radio resource management framework is proposed based on game-theoretic models for uplink and downlink transmissions. Finally, potential applications and open research directions in the area of resource management for NOMA are provided.","0890-8044;08908044","","10.1109/MNET.2017.1600287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7994888","","5G mobile communication;Downlink;Games;Heterogeneous networks;Interchannel interference;Microcell networks;NOMA;Power control;Resource management;System analysis and design;Uplink","5G mobile communication;cochannel interference;game theory","5G wireless systems;access efficiency;co-channel interference;code-domain NOMA;game-theoretic models;nonorthogonal multiple access networks;novel radio resource management framework;power-domain NOMA","","1","","","","","","July-August 2017","","IEEE","IEEE Journals & Magazines"
"Network Coding in Relay-Based Device-to-Device Communications","J. Huang; H. Gharavi; H. Yan; C. C. Xing","","IEEE Network","20170728","2017","31","4","102","107","Device-to-device (D2D) communications has been realized as an effective means to improve network throughput, reduce transmission latency, and extend cellular coverage in 5G systems. Network coding is a well established technique known for its capability to reduce the number of retransmissions. In this article, we review stateof- the-art network coding in relay-based D2D communications, in terms of application scenarios and network coding techniques. We then apply two representative network coding techniques to dual-hop D2D communications and present an efficient relay node selecting mechanism as a case study. We also outline potential future research directions, according to the current research challenges. Our intention is to provide researchers and practitioners with a comprehensive overview of the current research status in this area and hope that this article may motivate more researchers to participate in developing network coding techniques for different relay-based D2D communications scenarios.","0890-8044;08908044","","10.1109/MNET.2017.1700063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7994922","","Data communication;Device-to-device communication;Encoding;Network coding;Relays;Servers;Throughput","5G mobile communication;cellular radio;network coding;relay networks (telecommunication);synchronisation","5G systems;cellular coverage;device-to-device communications;dual-hop D2D communications;network coding;network throughput;relay node selecting mechanism;relay-based D2D communications;transmission latency","","","","","","","","July-August 2017","","IEEE","IEEE Journals & Magazines"
"Automatic Analysis of Facial Actions: A Survey","B. Martinez; M. F. Valstar; B. Jiang; M. Pantic","School of Computer Science, University of Nottingham, Nottingham, Nottinghamshire United Kingdom of Great Britain and Northern Ireland NG8 1BB (e-mail: brais.martinez@nottingham.ac.uk)","IEEE Transactions on Affective Computing","","2017","Early Access","Early Access","1","1","As one of the most comprehensive and objective ways to describe facial expressions, the Facial Action Coding System (FACS) has recently received significant attention. Over the past 30 years, extensive research has been conducted by psychologists and neuroscientists on various aspects of facial expression analysis using FACS. Automating FACS coding would make this research faster and more widely applicable, opening up new avenues to understanding how we communicate through facial expressions. Such an automated process can also potentially increase the reliability, precision and temporal resolution of coding. This paper provides a comprehensive survey of research into machine analysis of facial actions. We systematically review all components of such systems: pre-processing, feature extraction and machine coding of facial actions. In addition, existing FACS-coded facial expression databases are summarised. Finally, challenges that have to be addressed to make automatic facial action analysis applicable in real-life situations are extensively discussed. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the future of machine recognition of facial actions: what are the challenges and opportunities that researchers in the field face.","1949-3045;19493045","","10.1109/TAFFC.2017.2731763","European Union Horizon 2020; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990582","Action Unit analysis;facial expression recognition;survey","Computer science;Databases;Encoding;Face;Face recognition;Feature extraction;Psychology","","","","1","","","","","20170725","","","IEEE","IEEE Early Access Articles"
"Challenges for Static Analysis of Java Reflection - Literature Review and Empirical Study","D. Landman; A. Serebrenik; J. J. Vinju","Centrum Wiskunde & Inf., Amsterdam, Netherlands","2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)","20170720","2017","","","507","518","The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.","","Electronic:978-1-5386-3868-2; POD:978-1-5386-3869-9","10.1109/ICSE.2017.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985689","Empirical Study;Java;Reflection;Static Analysis;Systematic Literature Review","Bibliographies;Grammar;Java;Semantics;Software;Systematics;Tools","Java;application program interfaces;computational linguistics;program diagnostics;public domain software;software tools;source code (software);trees (mathematics)","Java Reflection API;Java projects;Java systems;abstract syntax trees;code idioms;collected descriptive statistics;collections semantics;dynamic proxies;literature review;nonexceptional exceptions;programmatic filtering meta objects;real-world Java code analysis;reflection code analysis;reflective Java code;software behavior;source code;static analysis tool","","","","","","","","20-28 May 2017","","IEEE","IEEE Conferences"
"To Type or Not to Type: Quantifying Detectable Bugs in JavaScript","Z. Gao; C. Bird; E. T. Barr","Univ. Coll. London, London, UK","2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)","20170720","2017","","","758","769","JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error. Evaluating static type systems against public bugs, which have survived testing and review, is conservative: it understates their effectiveness at detecting bugs during private development, not to mention their other benefits such as facilitating code search/completion and serving as documentation. Despite this uneven playing field, our central finding is that both static type systems find an important percentage of public bugs: both Flow 0.30 and TypeScript 2.0 successfully detect 15%!.","","Electronic:978-1-5386-3868-2; POD:978-1-5386-3869-9","10.1109/ICSE.2017.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985711","Flow;JavaScript;TypeScript;mining software repositories;static type systems","Computer bugs;Documentation;Facebook;History;Measurement uncertainty;Software;Surgery","Java;program debugging","JavaScript;TypeScript;Web domain;bug detection;static type systems","","","","","","","","20-28 May 2017","","IEEE","IEEE Conferences"
"Recommending and Localizing Change Requests for Mobile Apps Based on User Reviews","F. Palomba; P. Salza; A. Ciurumelea; S. Panichella; H. Gall; F. Ferrucci; A. De Lucia","Delft Univ. of Technol., Delft, Netherlands","2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)","20170720","2017","","","106","117","Researchers have proposed several approaches to extract information from user reviews useful for maintaining and evolving mobile apps. However, most of them just perform automatic classification of user reviews according to specific keywords (e.g., bugs, features). Moreover, they do not provide any support for linking user feedback to the source code components to be changed, thus requiring a manual, time-consuming, and error-prone task. In this paper, we introduce CHANGEADVISOR, a novel approach that analyzes the structure, semantics, and sentiments of sentences contained in user reviews to extract useful (user) feedback from maintenance perspectives and recommend to developers changes to software artifacts. It relies on natural language processing and clustering algorithms to group user reviews around similar user needs and suggestions for change. Then, it involves textual based heuristics to determine the code artifacts that need to be maintained according to the recommended software changes. The quantitative and qualitative studies carried out on 44,683 user reviews of 10 open source mobile apps and their original developers showed a high accuracy of CHANGEADVISOR in (i) clustering similar user change requests and (ii) identifying the code components impacted by the suggested changes. Moreover, the obtained results show that ChangeAdvisor is more accurate than a baseline approach for linking user feedback clusters to the source code in terms of both precision (+47%) and recall (+38%).","","Electronic:978-1-5386-3868-2; POD:978-1-5386-3869-9","10.1109/ICSE.2017.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985654","Impact Analysis;Mining User Reviews;Mobile Apps;Natural Language Processing","Computer bugs;Feature extraction;Joining processes;Maintenance engineering;Mobile communication;Software;Tools","mobile computing;natural language processing;source code (software)","CHANGEADVISOR;mobile apps;source code components;textual based heuristics;user feedback","","","","","","","","20-28 May 2017","","IEEE","IEEE Conferences"
"A systematic review on the use of Definition of Done on agile software development projects","A. Silva; T. Ara√∫jo; J. Nunes; M. Perkusich; E. Dilorenzo; H. Almeida; A. Perkusich","Federal Institute of Para&#237;ba, Monteiro, Para&#237;ba, Brazil","Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering","20170711","2017","","","364","373","<p>Background: Definition of Done (DoD) is a Scrum practice that consists of a simple list of criteria that adds verifiable or demonstrable value to the product. It is one of the most popular agile practices and assures a balance between short-term delivery of features and long-term product quality, but little is known of its actual use in Agile teams.</p> <p>Objective: To identify possible gaps in the literature and define a starting point to define DoD for practitioners through the identification and synthesis of the DoD criteria used in agile projects as presented in the scientific literature.</p> <p>Method: We applied a Systematic Literature Review of studies published up to (and including) 2016 through database search and backward and forward snowballing.</p> <p>Results: In total, we evaluated 2326 papers, of which 8 included DoD criteria used in agile projects. We identified that some studies presented up to 4 levels of DoD, which include story, sprint, release or project. We identified 62 done criteria, which are related to software verification and validation, deploy, code inspection, test process quality, regulatory compliance, software architecture design, process management, configuration management and non-functional requirements.</p> <p>Conclusion: The main implication for research is a need for more and better empirical studies documenting and evaluating the use of the DoD in agile software development. For the industry, the review provides a map of how DoD is currently being used in the industry and can be used as a starting point to define or compare with their own DoD definition.</p>","","Electronic:978-1-4503-4804-1","10.1145/3084226.3084262","","","Agile Software Development;Definition of Done;Systematic Literature Review","","","","","","","","","","","15-16 June 2017","","ACM","ACM Conferences"
"CSEPM - A Continuous Software Engineering Process Metamodel","S. Krusche; B. Bruegge","Tech. Univ. Munchen, Munich, Germany","2017 IEEE/ACM 3rd International Workshop on Rapid Continuous Software Engineering (RCoSE)","20170707","2017","","","2","8","Software engineers have to cope with uncertainties and changing requirements. Agile methods provide flexibility towards changes and the emergence of continuous delivery has made regular feedback loops possible. The abilities to maintain high code quality through reviews, to regularly release software, and to collect and prioritize user feedback, are necessary for continuous software engineering (CSE). However, there exists no software process metamodel that handles the continuous character of software engineering. In this paper, we describe an empirical process metamodel for continuous software engineering called CSEPM, which treats development activities as parallel running workflows and allows tailoring and customization. CSEPM includes static aspects that describe the relations between specific CSE concepts including reviews, releases, and feedback. It also describes the dynamic aspect of CSE, how development workflows are activated through change events. We show how CSEPM allows to instantiate linear, iterative, agile and continuous process models and how it enables tailoring and customization.","","Electronic:978-1-5386-0428-1; POD:978-1-5386-0429-8","10.1109/RCoSE.2017.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967945","Agile Methods;Change;Event;Feedback;Process Model;Release;Review;Workflow","Adaptation models;Concrete;Process control;Software;Software engineering;Tools;Unified modeling language","software engineering","CSEPM;continuous software engineering process metamodel;high code quality","","","","","","","","22-22 May 2017","","IEEE","IEEE Conferences"
"How Does Contributors' Involvement Influence the Build Status of an Open-Source Software Project?","M. Rebou√ßas; R. O. Santos; G. Pinto; F. Castor","Fed. Univ. of Pernambuco, Recife, Brazil","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","20170703","2017","","","475","478","The recent introduction of the pull-based development model promoted agile development practices such as Code Reviews and Continuous Integration (CI). CI, in particular, is currently a standard development practice in open-source software (OSS) projects. Although it is well-known that OSS contributors have different involvements (e.g., while some developers drive the project, there is a long tail of peripheral developers), little is known about how the contributor's degree of participation can influence the build status of an OSS project. Through TravisTorrent's dataset, we compare the success rates of builds made by casual and non-casual contributors and what factors on their contributions may influence the build result. Our results suggest that there is no representative difference between their build success (they are similar in 85% of the analyzed projects), meaning that being a casual contributor is not a strong indicator for creating failing builds. Also, factors like the size of their contributions and the number of project configurations (jobs) have the potential of impacting the build success.","","Electronic:978-1-5386-1544-7; POD:978-1-5386-1545-4","10.1109/MSR.2017.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962400","Casual Contributors;Continuous Integration;Open-Source Development","Buildings;Data mining;Documentation;Electronic mail;Open source software;Tools","public domain software;software engineering;software prototyping","CI;OSS project;TravisTorrent dataset;agile development practices;continuous integration;open-source software project;pull-based development model;standard development practice","","","","","","","","20-21 May 2017","","IEEE","IEEE Conferences"
"Product Line Architecture Recovery: An Approach Proposal (Extended Abstract)","C. Lima","","2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)","20170703","2017","","","481","482","The Product Line Architecture (PLA) is an important asset for the success of Software Product Line (SPL) projects. Due to the complexity of managing the architectural variability, maintain the PLA up-to-date and synchronized with the project source code is a hard problem. The systematic use of Software Architecture Recovery (SAR) techniques enables the PLA recovery and keeps the PLA aligned with the development. In this context, we present our initial proposal that consists of an approach to recover PLAs based on the use of (bottom-up) SAR techniques. We performed some studies (such as surveys, literature reviews, and exploratory studies) to investigate the relationship between SAR and PLA to identify gaps and define the research area state-of-the-art. The combination of SAR and PLA is an important strategy to address some issues of PLA design. We identified that few studies address architectural variability, PLA variability traceability, and empirical evaluation such as experiments, surveys, mixed-methods, and so on.","","Electronic:978-1-5386-1589-8; POD:978-1-5386-1590-4","10.1109/ICSE-C.2017.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965394","Product Line Architecture;Software Product Line;Software architecture","Computer architecture;Guidelines;Programmable logic arrays;Proposals;Software architecture;Software product lines;Tools","software architecture;software product lines;source code (software)","PLA design;PLA recovery;PLA variability traceability;SAR techniques;SPL projects;architectural variability;empirical evaluation;product line architecture recovery;software architecture recovery;software product line projects","","","","","","","","20-28 May 2017","","IEEE","IEEE Conferences"
"Sentiment analysis on social media using morphological sentence pattern model","Y. Han; K. K. Kim","Department of Computer and Information Sciences, Towson University, Towson, USA","2017 IEEE 15th International Conference on Software Engineering Research, Management and Applications (SERA)","20170703","2017","","","79","84","Social media became popular than ever as people are willing to share their emotions and opinions or to participate in social networking. Accordingly, the understanding of social media usage became important. The sentiment analysis is emerged as one of useful methods to analyze emotional stats expressed in textual data including social media data. However, this method still presents some limitations, particularly with an accuracy issue. For example, our previous sentiment analysis used a probability model and needed to adopt human-coded train-sets to maintain an acceptable accuracy level (89%). To overcome and improve this weakness, we propose an automated sentiment analysis in this paper by using the morphological sentence pattern model. We found that this new approach presented in this paper allowed us to achieve a higher level of accuracy (91.2%). The movie reviews were used for this analysis from IMDb, Rotten Tomatoes, Metacritic, YouTube and Twitter.","","Electronic:978-1-5090-5756-6; POD:978-1-5090-5757-3","10.1109/SERA.2017.7965710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965710","aspect based approach;morphological patterns;natural language processing;sentiment analysis;social media","Analytical models;Motion pictures;Sentiment analysis;Tools;Twitter;YouTube","probability;sentiment analysis;social networking (online);text analysis","IMDb;Metacritic;Rotten Tomatoes;Twitter;YouTube;acceptable accuracy level;automated sentiment analysis;emotional state analysis;human-coded train-sets;morphological sentence pattern model;probability model;social media data;social media usage;social networking;textual data","","","","","","","","7-9 June 2017","","IEEE","IEEE Conferences"
"Developer-Related Factors in Change Prediction: An Empirical Assessment","G. Catolino; F. Palomba; A. De Lucia; F. Ferrucci; A. Zaidman","Univ. of Salerno, Salerno, Italy","2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)","20170629","2017","","","186","195","Predicting the areas of the source code having a higher likelihood to change in the future is a crucial activity to allow developers to plan preventive maintenance operations such as refactoring or peer-code reviews. In the past the research community was active in devising change prediction models based on structural metrics extracted from the source code. More recently, Elish et al. showed how evolution metrics can be more efficient for predicting change-prone classes. In this paper, we aim at making a further step ahead by investigating the role of different developer-related factors, which are able to capture the complexity of the development process under different perspectives, in the context of change prediction. We also compared such models with existing change-prediction models based on evolution and code metrics. Our findings reveal the capabilities of developer-based metrics in identifying classes of a software system more likely to be changed in the future. Moreover, we observed interesting complementarities among the experimented prediction models, that may possibly lead to the definition of new combined models exploiting developer-related factors as well as product and evolution metrics.","","Electronic:978-1-5386-0535-6; POD:978-1-5386-0536-3","10.1109/ICPC.2017.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961516","Change prediction;Empirical Studies;Mining Software Repositories","Complexity theory;Context;Maintenance engineering;Measurement;Object oriented modeling;Predictive models;Unified modeling language","preventive maintenance;software maintenance;software metrics;source code (software)","change prediction models;change-prone classes;code metrics;developer-related factors;development process;evolution metrics;preventive maintenance operations;software system;source code;structural metrics","","","","","","","","22-23 May 2017","","IEEE","IEEE Conferences"
"Comprehending Studies on Program Comprehension","I. Schr√∂ter; J. Kr√ºger; J. Siegmund; T. Leich","Otto-von-Guericke-Univ. Magdeburg, Magdeburg, Germany","2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)","20170629","2017","","","308","311","Program comprehension is an important aspect of developing and maintaining software, as programmers spend most of their time comprehending source code. Thus, it is the focus of many studies and experiments to evaluate approaches and techniques that aim to improve program comprehension. As the amount of corresponding work increases, the question arises how researchers address program comprehension. To answer this question, we conducted a literature review of papers published at the International Conference on Program Comprehension, the major venue for research on program comprehension. In this article, we i) present preliminary results of the literature review and ii) derive further research directions. The results indicate the necessity for a more detailed analysis of program comprehension and empirical research.","","Electronic:978-1-5386-0535-6; POD:978-1-5386-0536-3","10.1109/ICPC.2017.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961527","Empirical Research;Study Comprehension;Systematic Review","Bibliographies;Context;Documentation;Guidelines;Software;Terminology;Testing","software maintenance;source code (software)","program comprehension;software development;software maintenance;source code","","","","","","","","22-23 May 2017","","IEEE","IEEE Conferences"
"A study for extended regular expression-based testing","P. Liu; J. Ai; Z. J. Xu","College of Information and Computer, Shanghai Business School, Shanghai 201400, China","2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)","20170629","2017","","","821","826","Software testing has become an essential activity to guarantee software quality. To reduce the overall cost of software testing, model-based testing has been widely studied in the past two decades and Finite State Machine (FSM) is used to build the model of software behaviors. However, due to the inadequacy of the modeling ability of FSM, FSM-based testing cannot be taken as a test oracle to solve all issues in software testing. To improve the modeling capability of the model, a few researchers have proposed using Extended Regular Expressions (ERE) to model software behaviors. This paper reviews the method of the ERE-based testing and presents six modeling rules to convert program codes to the ERE model. Then, a case is adopted to illustrate the process of generating executable paths from the ERE model and the method of designing test cases by those paths. Compared with the traditional graphic traversal method of constructing executable paths from the program, ERE not only has robust modeling capability to describe more types of software behaviors than FSM, but also can be used to construct effective executable paths to detect program errors.","","Electronic:978-1-5090-5507-4; POD:978-1-5090-5508-1","10.1109/ICIS.2017.7960106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960106","ERE-based testing;extended regular expression;model-based testing;software testing","Algorithm design and analysis;Business;Mathematical model;Software;Software testing;Switches","formal languages;program testing","ERE-based testing;cost reduction;executable paths generation;extended regular expression-based testing;model-based testing;program codes;program errors detection;software behaviors;software quality;software testing","","","","","","","","24-26 May 2017","","IEEE","IEEE Conferences"
"An empirical study on the application of mutation testing for a safety-critical industrial software system","R. Ramler; T. Wetzlmaier; C. Klammer","Software Competence Center Hagenberg GmbH, Hagenberg, Austria","Proceedings of the Symposium on Applied Computing","20170626","2017","","","1401","1408","<p><i>Background:</i> Testing is an essential activity in safety-critical software development, following high standards in terms of code coverage. Mutation testing allows assessing the effectiveness of testing and helps to further improve test cases. However, mutation testing is not widely practiced due to scalability problems when applied to real-world systems. <i>Objective:</i> The objective of the study is to investigate the applicability and usefulness of mutation testing for improving the quality of unit testing in context of safety-critical software systems. <i>Method:</i> A case study has been conducted together with an engineering company developing safety-critical systems. Mutation analysis has been applied to the studied system under test (60,000 LOC of C code) producing 75,043 mutants of which 27,158 survived test execution. A sample of 200 live mutants has been reviewed by the engineers, who also improved the existing unit test suite based on their findings. <i>Findings:</i> The reviewed sample contained 24+ equivalent mutants and 12+ duplicated mutants. It revealed a weak spot in the testing approach and provided valuable guidance to improve the existing unit test suite. Two new faults were found in the code when improving the tests. Test execution against the mutants required over 4,000 hours computing time. The overall effort was about half a person year.</p>","","Electronic:978-1-4503-4486-9","10.1145/3019612.3019830","","","IEC 61508;coverage;mutation analysis;mutation testing;safety-critical system;unit testing;verification","","","","","","","","","","","3-7 April 2017","","ACM","ACM Conferences"
"CSEPM: a continuous software engineering process metamodel","S. Krusche; B. Bruegge","Technische Universit&#228;t M&#252;nchen, Munich, Germany","Proceedings of the 3rd International Workshop on Rapid Continuous Software Engineering","20170626","2017","","","2","8","<p>Software engineers have to cope with uncertainties and changing requirements. Agile methods provide flexibility towards changes and the emergence of continuous delivery has made regular feedback loops possible. The abilities to maintain high code quality through reviews, to regularly release software, and to collect and prioritize user feedback, are necessary for continuous software engineering (CSE). However, there exists no software process metamodel that handles the continuous character of software engineering.</p> <p>In this paper, we describe an empirical process metamodel for continuous software engineering called CSEPM, which treats development activities as parallel running workflows and allows tailoring and customization. CSEPM includes static aspects that describe the relations between specific CSE concepts including reviews, releases, and feedback. It also describes the dynamic aspect of CSE, how development workflows are activated through change events. We show how CSEPM allows to instantiate linear, iterative, agile and continuous process models and how it enables tailoring and customization.</p>","","","10.1109/RCoSE.2017..6","","","agile methods;change;event;feedback;process model;release;review;workflow","","","","","","","","","","","20-28 May 2017","","ACM","ACM Conferences"
"What We Say vs. What They Do: A Comparison of Middle-School Coding Camps in the CS Education Literature and Mainstream Coding Camps (Abstract Only)","A. DeWitt; J. Fay; M. Goldman; E. Nicolson; L. Oyolu; L. Resch; J. M. Salda√±a; S. Sounalath; T. Williams; K. Yetter; E. Zak; N. Brown; S. A. Rebelsky","Grinnell College, Grinnell, IA, USA","Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education","20170511","2017","","","707","707","<p>In attempts to broaden participation in computing, the computer science education community has developed a wide variety of outreach activities to encourage students of different ages to learn computational thinking techniques and to develop an interest in computer science. In their recent surveys of the CSed literature, Decker, McGill, and Settle identify over eighty papers on K-12 outreach activities, of which approximately forty address middle-school coding camps. However, summer coding camps are offered by a much wider variety of organizations than computer science educators committed to diversifying the field. Some are offered by organizations committed to diversity, such as Black Girls Code and Girls Who Code. Others are offered by universities for recruitment, and necessarily to support diversification. Still others are offered by for-profit entities. What are the relationships between the two models of camp? Do the ideas that appear in the research literature filter out to the more mainstream camps, or do the more mainstream camps provide a very different model of computer science? In this project, we reviewed both the computer science education literature (52 sources representing 45 camps) and summer code camps identified on the World-Wide Web (480 different camps). In this poster, we report on common approaches and themes that others may choose to adapt or adopt. We also explore significant differences between the research-centered camps and the mainstream camps in approach, language, and apparent outreach goals.</p>","","","10.1145/3017680.3022434","","","code camp;diversity;gender;middle school;outreach;race","","","","","","","","","","","8-11 March 2017","","ACM","ACM Conferences"
"Using and Customizing Open-Source Runestone Ebooks for Computer Science Classes (Abstract Only)","B. Miller; P. Resnick; B. Ericson","Luther College, Decorah, IA, USA","Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education","20170511","2017","","","741","741","<p>Runestone is an open-source ebook platform designed to create and publish interactive computer science textbooks (See http://runestoneinteractive.org/). Runestone textbooks support programming within the browser, code visualizations, and a wide variety of practice activities, from multiple choice and fill-in-the-blank questions to Parsons Problems (drag-and-drop mixed-up code). The presenters have several years of experience developing and using ebooks for CS1, AP CS A, AP CSP, data structures, and web programming. Several studies have demonstrated good usability and positive learning and attitude impacts on students using these ebooks. Runestone ebooks can be customized to meet the needs of individual courses and teachers. The goal of this workshop is to help computer science teachers use and modify Runestone ebooks. The hands-on session will start by leading participants through use of the ebooks as if they were students. Participants will next create their own custom course of any existing ebook in the library and will use the instructor's dashboard to review student activity, modify the course, and grade students. Finally, participants will create their own assignments using Runestone's active learning components, which serves as a starting point for authoring their own content in Runestone. Laptop Required.</p>","","","10.1145/3017680.3017814","","","AP CS;CS1;CS2;ebooks;electronic books;interactive learning;online learning;parsons problems","","","","","","","","","","","8-11 March 2017","","ACM","ACM Conferences"
"The role of anxiety when learning to program: a systematic review of the literature","K. Nolan; S. Bergin","Maynooth University Ireland","Proceedings of the 16th Koli Calling International Conference on Computing Education Research","20170508","2016","","","61","70","<p>The World Health Organisation assert that the number one global health issue for young people is their mental health. For students mental well-being is associated with effective learning and their ability to navigate through college, having the resilience to cope effectively with the challenges and stressors that are a part of everyday student life. The <i>My World Survey</i> , the first of its kind on youth mental health in Ireland, found that for any given 100 students, irrespective of discipline, 40 suffer from depression and 38 from anxiety. In Ireland, Computer Science is only available as a third level course. This means that students often have no prior formal education in Computer Science to build on and so have to learn it for the first time in third level. Computer Science in third level has a high number of contact hours. This is to ensure that the large amount of theoretical and practical work is covered. Alongside the large number of contact hours, there is a considerable need for independent study in order to comprehend the fundamental concepts. The sheer volume of work that novice programmers have to complete is a contributing factor to the students stress. At our institution a large number of Computer Science students register for university counselling services. Learning to program is notoriously difficult with high attrition and failure rates. Learning typically takes place in a lab environment where in-experienced programmers will begin to type (""code"") shortly after being presented with a problem rather than spending time designing a solution. Thus the lab becomes active and busy from the onset, making struggling students cripplingly perceive their peers know more. Further, novice programmers use the compiler to constantly monitor their progress and generating syntax errors can be perceived as negative feedback. Such an environment can create or compound anxiety and stress. Over the past decade, our team has conducted extensive research on le- rning to program and specifically the factors that influence success. In this paper we present a systematic literature review investigating the type of anxiety Computer Science students experience throughout a Computer Science course. The work is novel, valuable, and very timely. The objective of the review is to determine and collate the current state-of-the art on how anxiety affects students when learning to program. The approach used is systematic, in that a structured search of electronic resources has been conducted and the results are presented and quantitatively analysed. A detailed discussion on the findings is provided and important implications and recommendations for the teaching and learning of programming are described.</p>","","Electronic:978-1-4503-4770-9","10.1145/2999541.2999557","","","anxiety;computer science;learning;programming;systematic review","","","","","","","","","","","24-27 Nov. 2016","","ACM","ACM Conferences"
"FLINTS: a tool for architectural-level modeling of features in software systems","J. Buckley; J. Rosik; S. Herold; A. Wasala; G. Botterweck; C. Exton","Lero/CSIS, University of Limerick, Limerick, Ireland","Proccedings of the 10th European Conference on Software Architecture Workshops","20170424","2016","","","1","7","<p>A functional perspective of software systems, at the architectural level allows developers to maintain a consistent understanding of the relationships between different functionalities of their system as it evolves, and allows them to analyze the system at a functional-chunk level rather than at the traditional, structural levels more typically presented by IDEs.</p> <p>This paper describes the derivation, implementation and evaluation of a prototype tool built to obtain this functional perspective from existing systems. The tool supports developers as they first attempt to locate specific functionalities in the source code. This support is based on preliminary design principles identified by observing experienced software developers in-vivo, as they performed this task manually. After the code associated with several such functionalities is located in the code, a graphical view allows the developer to assess the source code dependencies between the identified features and with the rest of the system. This helps developers understand the inter-functional interfaces and can be reviewed over time, as features are added and removed, to ensure on-going consistency between the architect's perspective of the features in the system and the code-base.</p>","","","10.1145/2993412.3003390","","","consistency, tool;feature location;feature modeling","","","","","","","","","","","Nov. 28 2016-Dec. 2 2016","","ACM","ACM Conferences"
"Software engineer education support system ALECSS utilizing DevOps tools","M. Ohtsuki; K. Ohta; T. Kakeshita","Saga University, Saga, Japan","Proceedings of the 18th International Conference on Information Integration and Web-based Applications and Services","20170424","2016","","","209","213","<p>Various types of DevOps tools are widely used for software development in order to ensure software quality and quick delivery of the software. Typical examples of such DevOps tools are continuous integration tool Jenkins, version control tool Git, unit test tool JUnit, coding style checker Checkstyle and static code analysis tool FindBugs. In this paper, we propose an education support system ALECSS to train software developers by integrating several DevOps tools explained above. The system automatically checks the programs submitted by the student teams and provides feedbacks generated by the DevOps tools to the students. The feedbacks are valuable to learn various techniques for high quality software development and to support evaluation by the teacher. We also develop various scripts for output checking and Git working status checking. These scripts use exercise contents and student's information in checking and sometimes need to generate typical results from templates for comparing them with the students' answers. Such scripts are also integrated to ALECSS. We evaluate ALECSS by comparing the messages generated by Checkstyle and FindBugs with the review comments produced the student teams. We found that the automatically generated messages and the review comments are greatly differ so that both are important for effective education.</p>","","","10.1145/3011141.3011200","","","DevOps tools;cooperative software development;e-leaning;education support system;software quality;web-based system","","","","","","","","","","","28-30 Nov. 2016","","ACM","ACM Conferences"
"Static Vulnerability Analysis for Secure Mobile Platforms","D. Kalyanasundaram; M. D'Souza","Samsung R&D Institute India - Bangalore, Bagmane Constellation Business Park, Bengaluru, Karnataka, India","Proceedings of the 10th Innovations in Software Engineering Conference","20170424","2017","","","195","201","<p>Security certification of mobile platforms is usually a lengthy process. Apart from internal development and testing, it requires hiring a consultancy for guidance and certification. Consultants start with code reviews to identify potential vulnerabilities and subsequently conduct independent tests in their laboratory. Typically consultancy is expensive. We propose a technique to statically analyze the secure mobile platform source code to identify some of the potential vulnerabilities typically detected by the consultants. We have devised a set of patterns in code that are typically flagged by consultants as vulnerabilities. The presence of each such pattern is detected by custom developed checkers. A prototype implementation tool called SVAT (Static Vulnerability Analysis Tool) containing custom checkers has been developed by extending the Clang static analysis tool. SVAT is a collection of custom checkers developed to detect the vulnerabilities in TEE. We illustrate the working of our tool on OP-TEE, an open source implementation of TEE (Trusted Execution Environment), a typical environment that runs in mobile platforms to assure security. SVAT is currently not publicly available.</p>","","","10.1145/3021460.3021484","","","AST (Abstract Syntax Tree);CFG (Control Flow Graph);Common Criteria (CC);Global Platform (GP);JIL (Joint Interpretation Library);Java Card OpenPlatform (JCOP);Secure OS;Static analysis;TEE (Trusted Execution Environment);Target Of Evaluation (TOE);Vulnerability;eSE (embedded Secure Element)","","","","","","","","","","","5-7 Feb. 2017","","ACM","ACM Conferences"
"A survey of resources for introducing coding into schools","F. J. Garc√≠a-Pe√±alvo; A. M. Rees; J. Hughes; I. Jormanainen; T. Toivonen; J. Vermeersch","University of Salamanca, Spain","Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality","20170424","2016","","","19","26","<p>Within TACCLE 3 -- Coding European Union Erasmus+ KA2 Programme project, a review and evaluation of a set of resources that can contribute to teaching programming to younger children has made. This paper presents a survey of this review including the most outstanding products in order to help teachers to introduce programming in pre-university studies.</p>","","","10.1145/3012430.3012491","","","TACCLE 3;coding;computational thinking;resources","","","","","","","","","","","2-4 Nov. 2016","","ACM","ACM Conferences"
"Process modeling for simulation: observations and open issues","G. Wagner; M. Seck; F. McKenzie","Brandenburg University of Technology, GERMANY","Proceedings of the 2016 Winter Simulation Conference","20170424","2016","","","1072","1083","<p>We review the state of the art of process modeling for discrete event simulation, make a number of observations and identify a number of issues that have to be tackled for promoting the use of process modeling in simulation. Process models are of particular interest in model-based simulation engineering approaches where the executable simulation model (code) is obtained with the help of textual or visual models. We present an illustrative example of model-based simulation development.</p>","","","","","","","","","","","","","","","","","11-14 Dec. 2016","","ACM","ACM Conferences"
"Evaluation of feature descriptors for cancerous tissue recognition","P. Stanitsas; A. Cherian; Xinyan Li; A. Truskinovsky; V. Morellas; N. Papanikolopoulos","Department of Computer Science and Engineering, University of Minnesota, USA","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","1490","1495","Computer-Aided Diagnosis (CAD) has witnessed a rapid growth over the past decade, providing a variety of automated tools for the analysis of medical images. In surgical pathology, such tools enhance the diagnosing capabilities of pathologists by allowing them to review and diagnose a larger number of cases daily. Geared towards developing such tools, the main goal of this paper is to identify useful computer vision based feature descriptors for recognizing cancerous tissues in histopathologic images. To this end, we use images of Hematoxylin & Eosin-stained microscopic sections of breast and prostate carcinomas, and myometrial leiomyosarcomas, and provide an exhaustive evaluation of several state of the art feature representations for this task. Among the various image descriptors that we chose to compare, including representations based on convolutional neural networks, Fisher vectors, and sparse codes, we found that working with covariance based descriptors shows superior performance on all three types of cancer considered. While covariance descriptors are known to be effective for texture recognition, it is the first time that they are demonstrated to be useful for the proposed task and evaluated against deep learning models. Capitalizing on Region Covariance Descriptors (RCDs), we derive a powerful image descriptor for cancerous tissue recognition termed, Covariance Kernel Descriptor (CKD), which consistently outperformed all the considered image representations. Our experiments show that using CKD lead to 92.83%, 91.51%, and 98.10% classification accuracy for the recognition of breast carcinomas, prostate carcinomas, and myometrial leiomyosarcomas, respectively.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899848","","Cancer;Covariance matrices;Feature extraction;Geometry;Histograms;Image color analysis;Symmetric matrices","cancer;computer vision;feature extraction;image representation;image texture;medical image processing;neural nets;object recognition","CAD;CKD;RCD;breast carcinomas;cancerous tissue recognition;computer vision;computer-aided diagnosis;convolutional neural networks;covariance based descriptors;covariance kernel descriptor;feature descriptors;feature representation;image descriptors;myometrial leiomyosarcomas;pathologists;prostate carcinomas;region covariance descriptors;texture recognition","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conferences"
"A Survey of Dictionary Learning Algorithms for Face Recognition","Y. Xu; Z. Li; J. Yang; D. Zhang","Bio-Computing Research Centre, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China","IEEE Access","20170602","2017","5","","8502","8514","During the past several years, as one of the most successful applications of sparse coding and dictionary learning, dictionary-based face recognition has received significant attention. Although some surveys of sparse coding and dictionary learning have been reported, there is no specialized survey concerning dictionary learning algorithms for face recognition. This paper provides a survey of dictionary learning algorithms for face recognition. To provide a comprehensive overview, we not only categorize existing dictionary learning algorithms for face recognition but also present details of each category. Since the number of atoms has an important impact on classification performance, we also review the algorithms for selecting the number of atoms. Specifically, we select six typical dictionary learning algorithms with different numbers of atoms to perform experiments on face databases. In summary, this paper provides a broad view of dictionary learning algorithms for face recognition and advances study in this field. It is very useful for readers to understand the profiles of this subject and to grasp the theoretical rationales and potentials as well as their applicability to different cases of face recognition.","","","10.1109/ACCESS.2017.2695239","Foundation for Young Talents in Higher Education of Guangdong; Open Fund Project of Fujian Provincial Key Laboratory of Information Processing and Intelligent Control, Minjiang University; Shenzhen Council for Scientific and Technological Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7903603","Dictionary learning;face recognition;sparse coding","Classification algorithms;Dictionaries;Encoding;Face;Face recognition;Image coding;Training","face recognition;image classification;learning (artificial intelligence);visual databases","classification performance;dictionary learning algorithm;face databases;face recognition","","4","","","","","20170418","2017","","IEEE","IEEE Journals & Magazines"
"I Know What You Coded Last Summer: Mining Candidate Expertise from GitHub Repositories","R. Saxena; N. Pedanekar","TCS Research, Pune, India","Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing","20170417","2017","","","299","302","<p>Resumes and social recommendations are often high-level indicators of a candidate's technical skillset. In this paper, we present a method to create a more detailed technology skill profile of a candidate based on her code repository contributions. For this purpose, we annotate user contributions to GitHub code repositories with technology tags found in Stack Overflow questions and answers (Q&A) in an unsupervised manner. We also present SkillMap, a visual representation of candidate skill profile, for quick review and comparison with other candidate profiles. We create SkillMaps for 66 Java programmers and present a preliminary qualitative assessment though manual analysis and interviews of technical recruiters.</p>","","","10.1145/3022198.3026354","","","github;programming skills;stack overflow;tagging;talent acquisition","","","","","","","","","","","Feb. 25 2017-March 1 2017","","ACM","ACM Conferences"
"Trends and issues in student-facing learning analytics reporting systems research","R. Bodily; K. Verbert","Brigham Young University","Proceedings of the Seventh International Learning Analytics & Knowledge Conference","20170417","2017","","","309","318","<p>We conducted a literature review on systems that track learning analytics data (e.g., resource use, time spent, assessment data, etc.) and provide a report back to students in the form of visualizations, feedback, or recommendations. This review included a rigorous article search process; 945 articles were identified in the initial search. After filtering out articles that did not meet the inclusion criteria, 94 articles were included in the final analysis. Articles were coded on five categories chosen based on previous work done in this area: functionality, data sources, design analysis, perceived effects, and actual effects. The purpose of this review is to identify trends in the current student-facing learning analytics reporting system literature and provide recommendations for learning analytics researchers and practitioners for future work.</p>","","","10.1145/3027385.3027403","","","educational recommender systems;learning analytics;learning analytics dashboards;literature review;student-facing systems","","","","","","","","","","","13-17 March 2017","","ACM","ACM Conferences"
"S-HOT: Scalable High-Order Tucker Decomposition","J. Oh; K. Shin; E. E. Papalexakis; C. Faloutsos; H. Yu","POSTECH, Pohang, South Korea","Proceedings of the Tenth ACM International Conference on Web Search and Data Mining","20170417","2017","","","761","770","<p>Multi-aspect data appear frequently in many web-related applications. For example, product reviews are quadruplets of (user, product, keyword, timestamp). How can we analyze such web-scale multi-aspect data? Can we analyze them on an off-the-shelf workstation with limited amount of memory?</p> <p>Tucker decomposition has been widely used for discovering patterns in relationships among entities in multi-aspect data, naturally expressed as high-order tensors. However, existing algorithms for Tucker decomposition have limited scalability, and especially, fail to decompose high-order tensors since they explicitly materialize intermediate data, whose size rapidly grows as the order increases (‚â• 4). We call this problem M-Bottleneck (""Materialization Bottleneck"").</p> <p>To avoid M-Bottleneck, we propose S-HOT, a scalable high-order tucker decomposition method that employs the on-the-fly computation to minimize the materialized intermediate data. Moreover, S-HOT is designed for handling disk-resident tensors, too large to fit in memory, without loading them all in memory at once. We provide theoretical analysis on the amount of memory space and the number of scans of data required by S-HOT. In our experiments, S-HOT showed better scalability not only with the order but also with the dimensionality and the rank than baseline methods. In particular, S-HOT decomposed tensors 1000√ó larger than baseline methods in terms dimensionality. S- HOT also successfully analyzed real-world tensors that are both large-scale and high-order on an off-the-shelf workstation with limited amount of memory, while baseline methods failed. The source code of S-HOT is publicly available at http://dm.postech.ac.kr/shot to encourage reproducibility.</p>","","","10.1145/3018661.3018721","","","","","","","","","","","","","","6-10 Feb. 2017","","ACM","ACM Conferences"
"Coding for Distributed Fog Computing","S. Li; M. A. Maddah-Ali; A. S. Avestimehr","","IEEE Communications Magazine","20170414","2017","55","4","34","40","Redundancy is abundant in fog networks (i.e., many computing and storage points) and grows linearly with network size. We demonstrate the transformational role of coding in fog computing for leveraging such redundancy to substantially reduce the bandwidth consumption and latency of computing. In particular, we discuss two recently proposed coding concepts, minimum bandwidth codes and minimum latency codes, and illustrate their impacts on fog computing. We also review a unified coding framework that includes the above two coding techniques as special cases, and enables a trade-off between computation latency and communication load to optimize system performance. At the end, we will discuss several open problems and future research directions.","0163-6804;01636804","","10.1109/MCOM.2017.1600894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7901473","","Bandwidth;Computer architecture;Edge computing;Encoding;Redundancy;Time factors","distributed processing;redundancy;software architecture","distributed fog computing;fog architecture;fog networks;redundancy;system performance;unified coding framework","","2","","","","","","April 2017","","IEEE","IEEE Journals & Magazines"
"Effectiveness of code contribution: from patch-based to pull-request-based tools","J. Zhu; M. Zhou; A. Mockus","Peking University, China","Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering","20170413","2016","","","871","882","<p> Code contributions in Free/Libre and Open Source Software projects are controlled to maintain high-quality of software. Alternatives to patch-based code contribution tools such as mailing lists and issue trackers have been developed with the pull request systems being the most visible and widely available on GitHub. Is the code contribution process more effective with pull request systems? To answer that, we quantify the effectiveness via the rates contributions are accepted and ignored, via the time until the first response and final resolution and via the numbers of contributions. To control for the latent variables, our study includes a project that migrated from an issue tracker to the GitHub pull request system and a comparison between projects using mailing lists and pull request systems. Our results show pull request systems to be associated with reduced review times and larger numbers of contributions. However, not all the comparisons indicate substantially better accept or ignore rates in pull request systems. These variations may be most simply explained by the differences in contribution practices the projects employ and may be less affected by the type of tool. Our results clarify the importance of understanding the role of tools in effective management of the broad network of potential contributors and may lead to strategies and practices making the code contribution more satisfying and efficient from both contributors' and maintainers' perspectives. </p>","","","10.1145/2950290.2950364","","","Code contribution;FLOSS;effectiveness;issue tracker;mailing list;pull request","","","","","","","","","","","13-18 Nov. 2016","","ACM","ACM Conferences"
"Accessibility and Usability Evaluation of Rich Internet Applications","R. P. M. Fortes; H. L. Antonelli; A. de Lima Salgado","University of Sao Paulo, Sao Carlos, Brazil","Proceedings of the 22nd Brazilian Symposium on Multimedia and the Web","20170413","2016","","","7","8","<p>Popularity of Internet applications has reached significant scales. In consequence, a wide diversity of solutions has been created based on Web features. Rich Internet Application (RIA) is a relevant term adopted for technological advances in software developed for the Web, which refers to Web applications aimed to provide users with a desktop similar experience. RIAs usually have wider capabilities in comparison with traditional hypertext applications, specially regarding to the interactive elements of their interfaces. New possibilities that have emerged from RIA are essential to support relevant aspects of Web 2.0, such as participation and collaboration. As among other applications, developing accessible and usable RIAs is a valuable and fundamental aspect for development teams, since these new interaction features available on the Web are not always accessible for people with disabilities or reduced mobility. For this reason, this mini-course aims to present the main concepts usually used to evaluate accessibility and usability of RIA; it is an overview of perspectives about practices and theoretical references, from Standards for Quality up to the implementation resources of Web applications. The approach of this mini-course covered practices on main RIA coding techniques, and methods of usability and accessibility evaluation as Guidelines Review and Heuristic Evaluation. Moreover, this mini-course was developed aiming newcomers and professionals that want to specialize their skills on the development and evaluation of usable and accessible RIAs.</p>","","","10.1145/2976796.2988221","","","RIA;accessibility;evaluation;usability","","","","","","","","","","","8-11 Nov. 2016","","ACM","ACM Conferences"
"CALAPPA: a toolchain for mining Android applications","V. Avdiienko; K. Kuznetsov; P. Calciati; J. C. Caiza Rom√°n; A. Gorla; A. Zeller","Saarland University, Germany","Proceedings of the International Workshop on App Market Analytics","20170413","2016","","","22","25","<p> Software engineering researchers and practitioners working on the Android ecosystem frequently have to do the same tasks over and over: retrieve data from the Google Play store to analyze it, decompile the Dalvik bytecode to understand the behavior of the app, and analyze applications metadata and user reviews. In this paper we present CALAPPA, a highly reusable and customizable toolchain that allows researchers to easily run common analysis tasks on large Android application datasets. CALAPPA includes components to retrieve the data from different Android stores, and comes with a predefined, but extensible, set of modules that can analyze apps metadata and code. </p>","","Electronic:978-1-4503-4398-5","10.1145/2993259.2993262","","","Android Analysis;Android app mining;App market analysis","","","","","","","","","","","14-14 Nov. 2016","","ACM","ACM Conferences"
"Performances of Multitones for Ultra-Wideband Software-Defined Radar","J. Le Kernec; O. Romain","School of Engineering in the Systems, Power and Energy Group, University of Glasgow, Glasgow, U.K.","IEEE Access","20170515","2017","5","","6570","6588","From the literature review, it is apparent that there is a gap in quantifying the performances of multitone waveforms specifically for radar applications and experimental results not commonly found. This paper focuses on the radar performance analysis of multitones with P3 phase-codes in simulation and experimentally to determine the effect of hardware on radar performances. For this purpose, a software-defined radar (SDR) approach has been used, including a digital core with hardware in-the-loop controlled by MATLAB and an analog front end that uses bandpass sampling and a reference channel. The proposed radar setup with processing algorithm has been evaluated in terms of processing time, showing that a real-time implementation on the latest field programmable gate array chipsets is feasible. This approach is flexible and entirely arbitrary waveforms can be generated with an instantaneous bandwidth up to 800 MHz. All the experimental results presented in Section IV are beyond the state of the art and bring novel insight into the impairment of SDR.","","","10.1109/ACCESS.2017.2693300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7896580","Chirp;OFDM;performance evaluation;quantization;ultra wideband radar","Hardware;OFDM;Radar applications;Synthetic aperture radar;Ultra wideband radar","OFDM modulation;field programmable gate arrays;phase coding;radar signal processing","MATLAB;P3 phase-codes;SDR approach;field programmable gate array chipsets;hardware in-the-loop;multitone waveforms;radar performance analysis;ultra-wideband software-defined radar","","","","","","","20170412","2017","","IEEE","IEEE Journals & Magazines"
"Guiding Dynamic Symbolic Execution toward Unverified Program Executions","M. Christakis; P. M√ºller; V. W√ºstholz","Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland","2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)","20170403","2016","","","144","155","Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.","","Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3","10.1145/2884781.2884843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886899","dynamic symbolic execution;partial verification;program verification;static analysis;testing","Aerospace electronics;Conferences;Instruments;Performance analysis;Redundancy;Software engineering;Testing","program testing","Clousot;Pex tool;arithmetic overflow;automatic test case generation;code instrumentation;dynamic symbolic execution;program error detection;program execution;unverified program execution","","","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"The Sky Is Not the Limit: Multitasking Across GitHub Projects","B. Vasilescu; K. Blincoe; Q. Xuan; C. Casalnuovo; D. Damian; P. Devanbu; V. Filkov","Dept. Comput. Sci., Univ. of California, Davis, Davis, CA, USA","2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)","20170403","2016","","","994","1005","Software development has always inherently required multitasking: developers switch between coding, reviewing, testing, designing, and meeting with colleagues. The advent of software ecosystems like GitHub has enabled something new: the ability to easily switch between projects. Developers also have social incentives to contribute to many projects; prolific contributors gain social recognition and (eventually) economic rewards. Multitasking, however, comes at a cognitive cost: frequent context-switches can lead to distraction, sub-standard work, and even greater stress. In this paper, we gather ecosystem-level data on a group of programmers working on a large collection of projects. We develop models and methods for measuring the rate and breadth of a developers' context-switching behavior, and we study how context-switching affects their productivity. We also survey developers to understand the reasons for and perceptions of multitasking. We find that the most common reason for multitasking is interrelationships and dependencies between projects. Notably, we find that the rate of switching and breadth (number of projects) of a developer's work matter. Developers who work on many projects have higher productivity if they focus on few projects per day. Developers that switch projects too much during the course of a day have lower productivity as they work on more projects overall. Despite these findings, developers perceptions of the benefits of multitasking are varied.","","Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3","10.1145/2884781.2884875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886974","GitHub;Multitasking;productivity","Context;Encoding;Interrupters;Multitasking;Productivity;Software;Switches","project management;software development management","GitHub projects;context-switching behavior;ecosystem-level data gathering;multitasking;software development","","4","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"An Empirical Study of Blindness and Program Comprehension","A. Armaly","Dept. of Comput. Sci. & Eng., Univ. of Notre Dame, Notre Dame, IN, USA","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","683","685","Blind programmers typically use a screen reader when reading code whereas sighted programmers are able to skim the code with their eyes. This difference has the potential to impact the generalizability of software engineering studies and approaches. We present a summary of a paper which will soon be under review at TSE that investigates how code comprehension of blind programmers differs from that of sighted programmers. Put briefly, we found no statistically-significant differences between the areas of code that the blind programmers found to be important and the areas of code that the sighted programmers found to be important.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883372","","Blindness;Computer languages;Conferences;Measurement;Programming;Software engineering;Software maintenance","computer aided instruction;handicapped aids;software engineering","blind programmers;blindness;program comprehension;reading code;screen reader;software engineering studies","","","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"Computing counter-examples for privilege protection losses using security models","M. A. Laverdi√®re; E. Merlo","TCS Innovation Labs, Tata Consultancy Services & &#x00C9;cole Polytechnique de Montr&#x00E9;al, Canada","2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20170323","2017","","","240","249","Role-Based Access Control (RBAC) is commonly used in web applications to protect information and restrict operations. Code changes may affect the security of the application and need to be validated, in order to avoid security vulnerabilities, which is a major undertaking. A statement suffers from privilege protection loss in a release pair when it was definitely protected on all execution paths in the previous release and is now reachable by some execution paths with an inferior privilege protection. Because the code change and the resulting privilege protection loss may be distant (e.g. in different functions or files), developers may find it difficult to diagnose and correct the issue. We use Pattern Traversal Flow Analysis (PTFA) to statically analyze code-derived formal models. Our analysis automatically computes counter-examples of definite protection properties and privilege protection losses. We computed privilege protections and their changes for 147 release pairs of WordPress. We computed counter-examples for a total of 14,116 privilege protection losses we found spread in 31 release pairs.We present the distribution of counter-examples' lengths, as well as their spread across function and file boundaries. Our results show that counter-examples are typically short and localized. The median example spans 88 statements, crosses a single function boundary, and is contained in the same file. The 90<sup>th</sup> centile example measures 174 statements and spans 3 function boundaries over 3 files. We believe that the privilege protection counter-examples' characteristics would be helpful to focus developers' attention for security reviews. These counter-examples are also a first step toward explanations.","","Electronic:978-1-5090-5501-2; POD:978-1-5090-5502-9","10.1109/SANER.2017.7884625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884625","Access Control;Evolution;Model Checking;Software Maintenance;Static Analysis","Access control;Automata;Computational modeling;Maintenance engineering;Model checking","security of data","PTFA;WordPress;code-derived formal models;pattern traversal flow analysis;privilege protection losses;security models","","","","","","","","20-24 Feb. 2017","","IEEE","IEEE Conferences"
"Discovering Important Source Code Terms","P. Rodeghero","Dept. of Comput. Sci. & Eng., Univ. of Notre Dame, Notre Dame, IN, USA","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","671","673","Terms in source code have become extremely important in Software Engineering research. These ``important' terms are typically used as input to research tools. Therefore, the quality of the output of these tools will depend on the quality of the term extraction technique. Currently, there is no definitive best technique for predicting the importance of terms during program comprehension. In my work, I perform a literature review of several techniques. I then propose a unified importance prediction model based on a machine learning algorithm. I evaluate my model in a field study involving professional programmers, as well as a standard 10-fold synthetic study. I found that my model predicts the top quartile of most-important source code terms with approximately 50% precision and recall, outperforming tf/idf and other popular techniques. Furthermore, I found that, during actual program comprehension tasks, the predictions from my model help programmers equivalent to a real set of most-important terms.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883368","importance;machine learning;source code terms","Conferences;Gaze tracking;Machine learning algorithms;Measurement;Predictive models;Software;Software engineering","learning (artificial intelligence);reverse engineering;software engineering;source code (software)","machine learning algorithm;professional programmers;program comprehension;software engineering research;source code;standard 10-fold synthetic study;term extraction;unified importance prediction model","","","","","","","","14-22 May 2016","","IEEE","IEEE Conferences"
"Nmag Micromagnetic Simulation Tool ‚Äî Software Engineering Lessons Learned","H. Fangohr; M. Albert; M. Franchin","Univ. of Southampton, Southampton, UK","2016 IEEE/ACM International Workshop on Software Engineering for Science (SE4Science)","20170316","2016","","","1","7","We review design and development decisions and their impact for the open source code Nmag from a software engineering in computational science point of view. We summarise lessons learned and recommendations for future computational science projects. Key lessons include that encapsulating the simulation functionality in a library of a general purpose language, here Python, provides great flexibility in using the software. The choice of Python for the top-level user interface was very well received by users from the science and engineering community. The from-source installation in which required external libraries and dependencies are compiled from a tarball was remarkably robust. In places, the code is a lot more ambitious than necessary, which introduces unnecessary complexity and reduces main- tainability. Tests distributed with the package are useful, although more unit tests and continuous integration would have been desirable. The detailed documentation, together with a tutorial for the usage of the system, was perceived as one of its main strengths by the community.","","Electronic:978-1-4503-4167-7; POD:978-1-5090-2206-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7878375","Computational Science Software Engineering;Finite Elements;Nmag;Python","Computational modeling;Libraries;Magnetization;Mathematical model;Micromagnetics;Software;Software engineering","digital simulation;graphical user interfaces;micromagnetics;physics computing;public domain software;software libraries;software packages","Nmag micromagnetic simulation tool;Python;external libraries;general purpose language library;open source code;software engineering;software package;tarball;top-level user interface;unit tests","","","","","","","","16-16 May 2016","","IEEE","IEEE Conferences"
"Auditing Code for Security Vulnerabilities with CodeSonar","D. Vitek","","2016 IEEE Cybersecurity Development (SecDev)","20170206","2016","","","154","154","Summary form only given. This tutorial will give a security-oriented introduction to CodeSonar, its program model, and its analyses. Attendees will be given access to a live CodeSonar instance, and will be shown how to explore a code base and review analysis results. Finally, a taste of how to extend CodeSonar will be given.given. CodeSonar is a static analysis tool for finding programming errors and security vulnerabilities in C, C++, and Java source code as well as x86, x86_64, and ARM machine code programs.","","Electronic:978-1-5090-5589-0; POD:978-1-5090-5590-6","10.1109/SecDev.2016.042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839811","","Analytical models;C++ languages;Computer security;Java;Programming;Tutorials","C++ language;Java;program diagnostics;security of data","C;C++;CodeSonar;Java source code;security vulnerabilities;security-oriented introduction;static analysis tool","","","","","","","","3-4 Nov. 2016","","IEEE","IEEE Conferences"
"Word embeddings for Arabic sentiment analysis","A. A. Altowayan; L. Tao","Computer Science Department, Pace University, New York, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3820","3825","Manual feature extraction is a challenging and time consuming task, especially in a Morphologically Rich Language (MRL) such as Arabic. In this paper, we rely on word embeddings as the main source of features for opinion mining in Arabic text such as tweets, consumer reviews, and news articles. First, we compile a large Arabic corpus from various sources to learn word representations. Second, we train and generate word vectors (embeddings) from the corpus. Third, we use the embeddings in our feature representation for training several binary classifiers to detect subjectivity and sentiment in both Standard Arabic and Dialectal Arabic. We compare our results with other methods in literature; our approach - with no hand-crafted features - achieves a slightly better accuracy than the top hand-crafted methods. To reproduce our results and for further work, we publish the data and code used in our experiments<sup>1</sup>.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841054","Arabic sentiment;word embeddings","Feature extraction;Manuals;Semantics;Sentiment analysis;Standards;Training","data mining;feature extraction;pattern classification;sentiment analysis","Arabic sentiment analysis;Arabic text;MRL;binary classifiers;dialectal Arabic;feature extraction;morphologically rich language;opinion mining;standard Arabic;word embeddings","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Empirical evaluations of preprocessing parameters' impact on predictive coding's effectiveness","R. Chhatwal; N. Huber-Fliflet; R. Keeling; J. Zhang; H. Zhao","Legal AT&T Services, Inc., Washington, D.C. USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1394","1401","Predictive coding, once used in only a small fraction of legal and business matters, is now widely deployed to quickly cull through increasingly vast amounts of data and reduce the need for costly and inefficient human document review. Previously, the sole front-end input used to create a predictive model was the exemplar documents (training data) chosen by subject-matter experts. Many predictive coding tools require users to rely on static preprocessing parameters and a single machine learning algorithm to develop the predictive model. Little research has been published discussing the impact preprocessing parameters and learning algorithms have on the effectiveness of the technology. A deeper dive into the generation of a predictive model shows that the settings and algorithm can have a strong effect on the accuracy and efficacy of a predictive coding tool. Understanding how these input parameters affect the output will empower legal teams with the information they need to implement predictive coding as efficiently and effectively as possible. This paper outlines different preprocessing parameters and algorithms as applied to multiple real-world data sets to understand the influence of various approaches.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840747","e-discovery;ediscovery;electronic discovery;predictive coding;technology assisted review","Electronic mail;Law;Machine learning algorithms;Predictive coding;Predictive models;Training","business data processing;information management;learning (artificial intelligence)","business matters;empirical evaluations;exemplar documents;human document review;information management;legal matters;predictive coding effectiveness;predictive coding tools;preprocessing parameters;single machine learning algorithm;static preprocessing parameters;subject matter experts","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"Application of Non-Orthogonal Multiple Access in LTE and 5G Networks","Z. Ding; Y. Liu; J. Choi; Q. Sun; M. Elkashlan; C. L. I; H. V. Poor","","IEEE Communications Magazine","20170203","2017","55","2","185","191","As the latest member of the multiple access family, non-orthogonal multiple access (NOMA) has been recently proposed for 3GPP LTE and is envisioned to be an essential component of 5G mobile networks. The key feature of NOMA is to serve multiple users at the same time/frequency/ code, but with different power levels, which yields a significant spectral efficiency gain over conventional orthogonal MA. The article provides a systematic treatment of this newly emerging technology, from its combination with MIMO technologies to cooperative NOMA, as well as the interplay between NOMA and cognitive radio. This article also reviews the state of the art in the standardization activities concerning the implementation of NOMA in LTE and 5G networks.","0163-6804;01636804","","10.1109/MCOM.2017.1500657CM","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7842433","","5G mobile communication;Long Term Evolution;MIMO communication;NOMA;Signal to noise ratio;Throughput","5G mobile communication;Long Term Evolution","3GPP LTE;5G mobile networks;LTE;MIMO technologies;NOMA;cognitive radio;non orthogonal multiple access application;time-frequency-code","","52","","","","","","February 2017","","IEEE","IEEE Journals & Magazines"
"Vulnerabilities and their surrounding ethical questions: a code of ethics for the private sector","A. D. Gregorio","Zeronomicon Milan, Italy","2016 International Conference on Cyber Conflict (CyCon U.S.)","20170202","2016","","","1","4","Zero-day vulnerabilities - weaknesses in software that are unknown to the parties who can mitigate their specific negative effects - are gaining a prominent role in the modern-day intelligence, national-security, and law-enforcement operations. At the same time, the lack of transparency and accountability in their trade and adoption, their possible overexploitation or abuse, the latent conflict of interests by entities handling them, and their potential double effect may pose societal risks or lead to the breach of human rights. If left unaddressed, these usage-related challenges call into question the legitimacy of zero-day vulnerabilities as enablers of national security and law enforcement operations and erode the benefits that their proportionate use have for the judiciary, defence, and intelligence purposes. This work explores what the private sector involved in the trade of zero-day vulnerabilities can do to ensure the respect human rights and the benign and societally beneficial use of those capabilities. After reviewing what can go wrong in the acquisition of zero-day vulnerabilities, the article contributes the first code of ethics focused on the trade of vulnerability information, where the author sets forth six principles and eight corresponding ethical standards aimed respectively at guiding and regulating the conduct of this business.","","Electronic:978-1-5090-5258-5; POD:978-1-5090-6172-3","10.1109/CYCONUS.2016.7836615","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836615","Computer Security;Ethics;Law enforcement;National security;Software","Business;Ethics;Law;National security;Software;Standards","law;national security;security of data","human rights;law-enforcement operations;modern-day intelligence;national-security;vulnerability information;zero-day vulnerabilities","","","","","","","","21-23 Oct. 2016","","IEEE","IEEE Conferences"
"Understanding Code Smells in Android Applications","U. A. Mannan; I. Ahmed; R. A. M. Almurshed; D. Dig; C. Jensen","Oregon State Univ., Corvallis, OR, USA","2016 IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft)","20170126","2016","","","225","236","Code smells are associated with poor coding practices that cause long-term maintainability problems and mask bugs. Despite mobile being a fast growing software sector, code smells in mobile applications have been understudied. We do not know how code smells in mobile applications compare to those in desktop applications, and how code smells are affecting the design of mobile applications. Without such knowledge, application developers, tool builders, and researchers cannot improve the practice and state of the art of mobile development.We first reviewed the literature on code smells in Android applications and found that there is a significant gap between the most studied code smells in literature and most frequently occurring code smells in real world applications. Inspired by this finding, we conducted a large scale empirical study to compare the type, density, and distribution of code smells in mobile vs. desktop applications. We analyze an open-source corpus of 500 Android applications (total of 6.7M LOC) and 750 desktop Java applications (total of 16M LOC), and compare 14,553 instances of code smells in Android applications to 117,557 instances of code smells in desktop applications. We find that, despite mobile applications having different structure and workflow than desktop applications, the variety and density of code smells is similar. However, the distribution of code smells is different - some code smells occur more frequently in mobile applications. We also found that different categories of Android applications have different code smell distributions. We highlight several implications of our study for application developers, tool builders, and researchers.","","Electronic:978-1-4503-4178-3; POD:978-1-5090-2233-5","10.1109/MobileSoft.2016.048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832986","Android;Code Smell;Java;literature review","Androids;Humanoid robots;Java;Mobile applications;Mobile communication;Smart phones;Software engineering","Android (operating system);mobile computing;program debugging;public domain software;software maintenance;source code (software)","Android applications;code smells;desktop applications;maintainability problems;mask bugs;mobile applications;mobile development;open-source corpus;software sector","","","","","","","","16-17 May 2016","","IEEE","IEEE Conferences"
"Helping Mobile Software Code Reviewers: A Study of Bug Repair and Refactoring Patterns","Z. Chen","","2016 IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft)","20170126","2016","","","34","35","Mobile Developers commonly spend a significant amount of time and effort on conducting code reviews on newly introduced and domain-specific practices, such as platform-specific feature addition, quality of service anti-pattern refactorings, and battery-related bug fixes. To address these problems, we conducted a large empirical study over the software change history of 318 open source projects and investigated platform-dependent code changes from open source projects. Our analysis focuses on what types of changes mobile application developers typically make and how they perceive, recall, and communicate changed and affected code. Our study required the development of an automated strategy to examine open source repositories and categorize platform-related refactoring edits, bug repairs, and API updates, mining 1,961,990 commit changes. Our findings call for the need to develop a new recommendation system aimed at efficiently identifying required changes such as bug fixes and refactorings during mobile application code reviews.","","Electronic:978-1-4503-4178-3; POD:978-1-5090-2233-5","10.1109/MobileSoft.2016.026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832964","mobile application development;refactoring;software repair","Cloning;Computer bugs;History;Maintenance engineering;Mobile applications;Mobile communication;Software","mobile computing;public domain software;software maintenance;software reviews;source code (software)","bug repair;mobile application developer;mobile software code reviewer;open source repository;recommendation system;refactoring pattern","","","","","","","","16-17 May 2016","","IEEE","IEEE Conferences"
"Web Systems Quality Evolution","A. Rio; F. B. e. Abreu","ISCTE, ISTAR, Inst. Univ. de Lisboa, Lisbon, Portugal","2016 10th International Conference on the Quality of Information and Communications Technology (QUATIC)","20170116","2016","","","248","253","Software evolution is a well-established research area, but not in the area of web systems/applications. Web projects are normally more complex than other software development projects because they have both server and client code, encompass a variety of programming languages, and are multidisciplinary. We aim to produce a catalog of web smells to help avoiding the problems in web development code before they happen, thus saving time and reducing cost. By means of longitudinal studies we plan to analyze the impact of these web smells in web systems maintainability and reliability. This will require developing a tool to detect the proposed web smells. For validation sake, we will also use surveys among web systems developers and peer reviewing in academic fora.","","CD:978-1-5090-3580-9; Electronic:978-1-5090-3581-6; POD:978-1-5090-3582-3","10.1109/QUATIC.2016.060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814557","irregular time series;longitudinal studies;software evolution;software quality;web code smells;web engineering","Computer bugs;Computer languages;Measurement;Reliability;Servers;Software quality","Internet;project management;software maintenance;software quality;software reliability","Web applications;Web development code;Web projects;Web smells;Web systems quality evolution;client code;programming languages;server code;software development projects;software evolution;system maintainability;system reliability","","","","","","","","6-9 Sept. 2016","","IEEE","IEEE Conferences"
"Maintenance Effort Estimation for Open Source Software: A Systematic Literature Review","H. Wu; L. Shi; C. Chen; Q. Wang; B. Boehm","Lab. for Internet Software Technol., Inst. of Software, Beijing, China","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20170116","2016","","","32","43","Open Source Software (OSS) is distributed and maintained collaboratively by developers all over the world. However, frequent personnel turnover and lack of organizational management makes it difficult to capture the actual development effort. Various OSS maintenance effort estimation approaches have been developed to provide a way to understand and estimate development effort. The goal of this study is to identify the current state of art of the existing maintenance effort estimation approaches for OSS. We performed a systematic literature review on the relevant studies published in the period between 2000-2015 by both automatic and manual searches from different sources. We derived a set of keywords from the research questions and established selection criteria to carefully choose the papers to evaluate. 29 out of 3,312 papers were selected based on a well designed selection process. Our results show that the commonly used OSS maintenance effort estimation methods are actual effort estimation and maintenance activity time prediction, the most commonly used metrics and factors for actual effort estimation are source code measurements and people related metrics, the most commonly mentioned activity for maintenance activity time prediction is bug fixing. Accuracy measures and cross validation is used for validating the estimation models. Based on the above findings, we identified the issues in evaluation methods for actual maintenance effort estimations and the needs for quantitative OSS maintenance effort inference from size-related metrics. Meanwhile, we highlighted individual contribution and performance measurement as a novel and promising research area.","","Electronic:978-1-5090-3806-0; POD:978-1-5090-3807-7","10.1109/ICSME.2016.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816452","","Data mining;Estimation;Maintenance engineering;Measurement;Planning;Protocols;Software","program debugging;public domain software;software maintenance;software metrics","OSS;bug fixing;maintenance activity time prediction;maintenance effort estimation;open source software;people related metrics;source code measurements","","1","","","","","","2-7 Oct. 2016","","IEEE","IEEE Conferences"
"A Proposal on Requirements for COSMIC FSM Automation from Source Code","A. Tarhan; B. √ñzkan; G. C. ƒ∞√ß√∂z","Comput. Eng. Dept., Hacettepe Univ., Ankara, Turkey","2016 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM-MENSURA)","20170109","2016","","","195","200","Automation of functional size measurement (FSM) process has increasingly gained importance since manual measurement is time-consuming, costly, and sometimes errorprone. There exist studies that automate measurement from different software artifacts such as requirements specifications, design models, and software code. In this study we review and compare four studies that we have carried out in recent years to automate COSMIC FSM from software code. Based on the comparison and the lessons learned, we derive an operational scenario for automated FSM from software code and propose a set of requirements that need to be considered in automation. We think the proposal will be helpful not only for our future studies and also for future work of interested researchers.","","Electronic:978-1-5090-4147-3; POD:978-1-5090-4148-0","10.1109/IWSM-Mensura.2016.038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809610","COSMIC;automation;function points;functional size measurement;requirements;source code","Automation;Manuals;Phase measurement;Size measurement;Software;Software measurement;Time measurement","program compilers;source code (software)","COSMIC FSM automation;FSM process;automated FSM;functional size measurement;software artifacts;software code;source code","","","","","","","","5-7 Oct. 2016","","IEEE","IEEE Conferences"
"EARec: Leveraging Expertise and Authority for Pull-Request Reviewer Recommendation in GitHub","H. Ying; L. Chen; T. Liang; J. Wu","Zhejiang Univ., Hangzhou, China","2016 IEEE/ACM 3rd International Workshop on CrowdSourcing in Software Engineering (CSI-SE)","20170109","2016","","","29","35","Pull-Request (PR) is a primary way of code contribution from developers to improve quality of software projects in GitHub. For a popular GitHub project, tens of PR are submitted daily, while only a small number of developers, i.e core developers, have the grant to judge whether to merge these changes into the main branches or not. Due to the time-consumption of PR review and the diversity of PR aspects, it is becoming a big challenge for core developers to quickly discover the useful PR. Currently, recommending appropriate reviewers (developers) for incoming PR to quickly collect meaningful comments, is treated as an effective and crowdsourced way to help core developers to make decisions and thus accelerate project development. In this paper, we propose a reviewer recommendation approach (EARec) which simultaneously considers developer expertise and authority. Specifically, we first construct a graph of incoming PR and possible reviewers, and then take advantage of text similarity of PR and social relations of reviewers to find the appropriate reviewers. The experimental analysis on MSR Mining Challenge Datasetfootnote{http://ghtorrent.org/msr14.html} provides good evaluation for our approach in terms of precision and recall.","","Electronic:978-1-4503-4158-5; POD:978-1-5090-2201-4","10.1109/CSI-SE.2016.013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809395","","Acceleration;Collaboration;Control systems;Data mining;Large scale integration;Semantics;Software","crowdsourcing;recommender systems;software engineering","EARec;GitHub project;MSR mining challenge datasetfootnote;PR aspects;PR graph;PR review;code contribution;core developers;crowdsourcing;project development;pull-request reviewer recommendation;software project quality","","","","","","","","16-16 May 2016","","IEEE","IEEE Conferences"
"Model-Driven Development in Robotics Domain: A Systematic Literature Review","T. Heineck; E. Gon√ßalves; A. Sousa; M. Oliveira; J. Castro","Inst. Fed. Catarinense, Blumenau, Brazil","2016 X Brazilian Symposium on Software Components, Architectures and Reuse (SBCARS)","20161219","2016","","","151","160","Robots are complex agents composed of various sensors and actuators that work together with software to meet specific requirements. The subset of robots that has the ability to interact among them and even with people, through gestures or speaking, is known as Social Robots. Model-Driven Development is a promising paradigm because it promotes the reuse of components and quick code generation with quality. ModelDriven Development has been widely used in the context of Robotics in order to reduce complexity, reduce development effort and reuse of software. Due to these facts, it becomes pertinent the development of a systematic literature review to compile these results. In this paper we investigate how MDD techniques have helped the field of Robotics, therefore a systematic literature review was conducted seeking to identify approaches and their main technical features, as well as the types of specific requirements, behavioral and social issues. We came to conclusion that the existing approaches provide many interesting capabilities, typically by using the component-based development paradigm seeking a higher level of software reuse and facilitating the implementation of systems.","","Electronic:978-1-5090-5086-4; POD:978-1-5090-5087-1","10.1109/SBCARS.2016.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789849","model-driven development;requirements engineering;robotics;software engineering;systematic literature review","Bibliographies;Computer architecture;Context modeling;Robot sensing systems;Software;Systematics","control engineering computing;human-robot interaction;object-oriented programming;program compilers;software reusability","MDD techniques;code generation;component reuse;component-based development paradigm;model-driven development;robotics domain;software complexity reduction;software development effort reduction;software reuse reduction","","1","","","","","","19-20 Sept. 2016","","IEEE","IEEE Conferences"
"Towards the Characterization of Monitor Smells in Adaptive Systems","M. A. Serikawa; A. d. S. Landi; B. R. Siqueira; R. S. Costa; F. C. Ferrari; R. Menotti; V. V. d. Camargo","Dept. of Comput., Fed. Univ. of Sao Carlos, Sa&#x0308;o Carlos, Brazil","2016 X Brazilian Symposium on Software Components, Architectures and Reuse (SBCARS)","20161219","2016","","","51","60","Adaptive Systems (ASs) can adapt themselves to achanging environment or new user needs. Monitors are essential in AS, being responsible for collecting and processing data from environment. There exist different kinds of monitors with distinct characteristics. Based on a literature review, we have noticed that Monitors are usually designed and implemented in an inadequate way: i) making them obscure in the source-code, ii) compelling all of them to have the same polling rate and also iii) predetermining the execution order among them. This leads to maintenance, evolution and performance problems. Besides, based on our observations, this erroneous way monitors are implemented follows a pattern and it is a recurrent practice. Therefore, we believe it can be classified as Monitor Smells of Adaptive Systems. In this paper we present two architectural smells we have identified: the Obscure Monitor and the Oppressed Monitors. The first smell occurs when the monitors are not evident in the source-code. The second smell occurs when monitors are compelled to have the same poling rate and an immutable execution order at runtime. The presence of these smells compromises the reusability, evolvability and maintainability. We have also conducted an exploratory study by comparing the impact of maintenance tasks in the original version of an AS called PhoneAdapter with a refactored version, in which the smells were removed. The results indicate the maintenance is facilitated in the version without the smells.","","Electronic:978-1-5090-5086-4; POD:978-1-5090-5087-1","10.1109/SBCARS.2016.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789839","Adaptive System;Architectural Smells;Monitoring","Adaptive systems;Bluetooth;Broadcasting;Global Positioning System;Maintenance engineering;Monitoring;Sensors","software maintenance;software reusability;source code (software)","PhoneAdapter;architectural smells;data collection;data processing;monitor smells of adaptive system characterization;obscure monitor;oppressed monitors;polling rate;software evolvability;software maintenance;software reusability;source-code","","1","","","","","","19-20 Sept. 2016","","IEEE","IEEE Conferences"
"Survey of Approaches for Handling Static Analysis Alarms","T. Muske; A. Serebrenik","Tata Res. Dev. & Design, Centre Tata Consultancy Services, Pune, India","2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation (SCAM)","20161215","2016","","","157","166","Static analysis tools have showcased their importance and usefulness in automated detection of code anomalies and defects. However, the large number of alarms reported and cost incurred in their manual inspections have been the major concerns with the usage of static analysis tools. Existing studies addressing these concerns differ greatly in their approaches to handle the alarms, varying from automatic postprocessing of alarms, supporting the tool-users during manual inspections of the alarms, to designing of light-weight static analysis tools. A comprehensive study of approaches for handling alarms is, however, not found. In this paper, we review 79 alarms handling studies collected through a systematic literature search and classify the approaches proposed into seven categories. The literature search is performed by combining the keywords-based database search and snowballing. Our review is intended to provide an overview of various alarms handling approaches, their merits and shortcomings, and different techniques used in their implementations. Our findings include that the categorized alarms handling approaches are complementary and they can be combined together in different ways. The categorized approaches and techniques employed in them can help the designers and developers of static analysis tools to make informed choices.","","Electronic:978-1-5090-3848-0; POD:978-1-5090-3849-7","10.1109/SCAM.2016.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7781809","False Positives Elimination;Handling of Alarms;Ranking of Alarms;Snowballing;Static Analysis Alarms","Data mining;Databases;Google;Inspection;Manuals;Software;Systematics","program diagnostics;query processing;software tools","automated code anomalies detection;automated code defects detection;keywords-based database search;snowballing;static analysis alarms handling;static analysis tools;systematic literature search","","3","","","","","","2-3 Oct. 2016","","IEEE","IEEE Conferences"
"A systematic mapping study on assessing computational thinking abilities","A. L. S. O. de Araujo; W. L. Andrade; D. D. Serey Guerrero","Software Practices Laboratory, Federal University of Campina Grande, Departament of Exact Sciences, Federal University of Para&#x00ED;ba, Brazil","2016 IEEE Frontiers in Education Conference (FIE)","20161201","2016","","","1","9","Several initiatives have been created to promote Computational Thinking (CT) abilities in students. There are multiple approaches of assessing CT and wide abilities and skills involved. However, the evidence on how to assess CT has not yet been systematically grouped or reviewed. The goal of our study is to identify and classify approaches to promote CT and the different ways of assessing CT abilities. To achieve this goal, a systematic mapping study was planned and executed. The results reveal that: (i) programming courses are the most common pedagogical approaches to promote CT for K-12 students; (ii) multiple skills are involved in CT, but solving problems, algorithms, and abstraction are most common abilities assessed; and (iii) codes and multi-choice questionnaires are the most common artifacts for assessing CT abilities. This study points out to the fact that there are open questions for exploring and developing new researches for promoting and assessing CT abilities.","","Electronic:978-1-5090-1790-4; POD:978-1-5090-1791-1","10.1109/FIE.2016.7757678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7757678","","Context;Education;Libraries;Programming profession;Systematics;Vehicles","computer science education;educational courses","K-12 students;computational thinking;programming courses;systematic mapping study","","","","","","","","12-15 Oct. 2016","","IEEE","IEEE Conferences"
"Modular architecture for code and metadata sharing","T. Tauber; B. C. d. S. Oliveira","University of Hong Kong, China","Proceedings of the 15th International Conference on Modularity","20161111","2016","","","106","117","<p> Every fragment of code we write has dependencies and associated metadata. Code dependencies range from local references and standard library definitions to external third party libraries. Metadata spans from within source code files (hierarchical names and code comments) to external files and database servers (package-level dependency configurations, build and test results, code reviews etc.). This scattered storage and non-uniform access limits our programming environments in their functionality and extensibility. In this paper, we propose a modular system architecture, Haknam, better suited for code and related metadata sharing. Haknam precisely tracks code interdependencies, allows flexible naming and querying of code references, and collects code fragments and their related metadata as messages in a distributed log-centric pipeline. We argue that this setting brings considerable advantages. In particular, we focus on modular development of tools and services that can assist in programming-related tasks. Every new functionality can be simply added by creating and processing messages from the distributed pipeline. </p>","","","10.1145/2889443.2889455","","","Haknam;Haskell;code slice;event log;evidence;fragmented code distribution;tags","","","","","","","","","","","14-17 March 2016","","ACM","ACM Conferences"
"Integrating software project resources using source code identifiers","L. Inozemtseva; S. Subramanian; R. Holmes","University of Waterloo, Canada","Companion Proceedings of the 36th International Conference on Software Engineering","20161111","2014","","","400","403","<p> Source code identifiers such as classes, methods, and fields appear in many different contexts. For instance, a developer performing a task using the android.app.Activity class could consult various project resources including the class's source file, API documentation, issue tracker, mailing list discussions, code reviews, or questions on Stack Overflow. </p> <p> These information sources are logically connected by the source code elements they describe, but are generally decoupled from each other. This has historically been tolerated by developers, since there was no obvious way to easily navigate between the data sources. However, it is now common for these sources to have web-based front ends that provide a standard mechanism (the browser) for viewing and interacting with the data they contain. Augmenting these front ends with hyperlinks and search would make development easier by allowing developers to quickly navigate between disparate sources of information about the same code element. </p> <p> In this paper, we propose a method of automatically linking disparate information repositories with an emphasis on high precision. We also propose a method of augmenting web-based front ends with these links to make it easier for developers to quickly gain a comprehensive view of the source code elements they are investigating. Research challenges include identifying source code tokens in the midst of natural language text and incomplete code fragments, dynamically augmenting the web views of the data repositories, and supporting novel composition of the link data to provide comprehensive views for specific source code elements. </p>","","","10.1145/2591062.2591108","","","Newton;Traceability;semantic links","","","","","1","","","","","","May 31 2014-June 7 2014","","ACM","ACM Conferences"
"Leveraging informal documentation to summarize classes and methods in context","L. Guerrouj; D. Bourque; P. C. Rigby","Concordia University, Montr&#233;al, QC, Canada","Proceedings of the 37th International Conference on Software Engineering","20161111","2015","2","","639","642","<p>Critical information related to a software developer's current task is trapped in technical developer discussions, bug reports, code reviews, and other software artefacts. Much of this information pertains to the proper use of code elements (<i>e.g.</i>, methods and classes) that capture vital problem domain knowledge. To understand the purpose of these code elements, software developers must either access documentation and online posts and understand the source code or peruse a substantial amount of text. In this paper, we use the context that surrounds code elements in StackOverflow posts to summarize the use and purpose of code elements. To provide focus to our investigation, we consider the generation of summaries for library identifiers discussed in StackOverflow. Our automatic summarization approach was evaluated on a sample of 100 randomly-selected library identifiers with respect to a benchmark of summaries provided by two annotators. The results show that the approach attains an R-precision of 54%, which is appropriate given the diverse ways in which code elements can be used.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Will they like this?: evaluating code contributions with language models","V. J. Hellendoorn; P. T. Devanbu; A. Bacchelli","Delft University of Technology, The Netherlands","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","157","167","<p>Popular open-source software projects receive and review contributions from a diverse array of developers, many of whom have little to no prior involvement with the project. A recent survey reported that reviewers consider conformance to the project's code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively evaluate the existence and effects of this phenomenon. To this aim we use language models, which were shown to accurately capture stylistic aspects of code. We find that rejected changesets do contain code significantly less similar to the project than accepted ones; furthermore, the less similar changesets are more likely to be subject to thorough review. Armed with these results we further investigate whether new contributors learn to conform to the project style and find that experience is positively correlated with conformance to the project's code style.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"On the alignment of source code quality perspectives through experimentation: an industrial case","T. V. Ribeiro; G. H. Travassos","Universidade Federal do Rio de Janeiro, Rio de Janeiro, Brazil","Proceedings of the Third International Workshop on Conducting Empirical Studies in Industry","20161111","2015","","","26","33","<p>Alignment is a key factor for success in many software development projects. Aligned teams are capable of bringing collaboration and positive results to companies; whereas misalignment among developers can make a conflicted environment and even lead the project to failure. OBJECTIVE. To assist developers in an embedded software development company in their conceptual alignment regarding source code quality. METHOD. In the organizational context, plan and perform a series of studies such as surveys, systematic literature review (SLR), qualitative data analysis and focus group to support the identification of conceptual misalignments among developers and establish common terminology and guidance concerning source code quality. RESULTS. The results from a survey conducted in one company showed a conceptual misalignment among developers regarding the source code quality that was triggering continuous rework during software evolution activities. Through an SLR and a qualitative analysis of code snippets, a set of evidence-based coding guidelines for readability and understandability of source code were formulated. These guidelines were evaluated and used as an instrument for aligning source code perspectives during a focus group, showing their feasibility and adequacy to the company's context. CONCLUSIONS. The use of all contextual information observed -- e.g. teams' locations, software development context, and time constraints -- along with the information gathered during the industry-academia collaboration was particularly important to help us appropriately chose research methods to be used, and formulate evidence-based coding guidelines that matched the company's needs and expectations. Further evaluations have to be carried out to ensure the quality impact of some guidelines proposed before using them all over the company.</p>","","","","","","code quality;conceptual alignment;experimental studies;industry-academia collaboration","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Twice the Bits, Twice the Trouble: Vulnerabilities Induced by Migrating to 64-Bit Platforms","C. Wressnegger; F. Yamaguchi; A. Maier; K. Rieck","TU Braunschweig, Braunschweig, Germany","Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security","20161111","2016","","","541","552","<p>Subtle flaws in integer computations are a prime source for exploitable vulnerabilities in system code. Unfortunately, even code shown to be secure on one platform can be vulnerable on another, making the migration of code a notable security challenge. In this paper, we provide the first study on how code that works as expected on 32-bit platforms can become vulnerable on 64-bit platforms. To this end, we systematically review the effects of data model changes between platforms. We find that the larger width of integer types and the increased amount of addressable memory introduce previously non-existent vulnerabilities that often lie dormant in program code. We empirically evaluate the prevalence of these flaws on the source code of Debian stable (""Jessie"") and 200 popular open-source projects hosted on GitHub. Moreover, we discuss 64-bit migration vulnerabilities that have been discovered as part of our study, including vulnerabilities in Chromium, the Boost C++ Libraries, libarchive, the Linux Kernel, and zlib.</p>","","","10.1145/2976749.2978403","","","data models;integer-based vulnerabilities;software security","","","","","","","","","","","24-28 Oct. 2016","","ACM","ACM Conferences"
"NeedFeed: taming change notifications by modeling code relevance","R. Padhye; S. Mani; V. S. Sinha","IBM Research India, New Delhi, India","Proceedings of the 29th ACM/IEEE international conference on Automated software engineering","20161111","2014","","","665","676","<p>Most software development tools allow developers to subscribe to notifications about code checked-in by their team members in order to review changes to artifacts that they are responsible for. However, past user studies have indicated that this mechanism is counter-productive, as developers spend a significant amount of effort sifting through such feeds looking for items that are relevant to them. We present NeedFeed, a system that models code relevance by mining a project's software repository and highlights changes that a developer may need to review. We evaluate several techniques to model code relevance, from a naive TOUCH-based approach to generic HISTORY-based classifiers using temporal code metrics at file and method-level granularities, which are then improved by building developer-specific models using TEXT-based features from commit messages. NeedFeed reduces notification clutter by more than 90%, on average, with the best strategy giving an average precision and recall of more than 75%.</p>","","","10.1145/2642937.2642985","","","collaborative software development;mining software repositories;version control","","","","","3","","","","","","15-19 Sept. 2014","","ACM","ACM Conferences"
"Risk-based attack surface approximation: poster","C. Theisen; L. Williams","North Carolina State University, Raleigh, NC","Proceedings of the Symposium and Bootcamp on the Science of Security","20161111","2016","","","121","123","<p>Proactive security review and test efforts are a necessary component of the software development lifecycle. Since resource limitations often preclude reviewing, testing and fortifying the entire code base, prioritizing what code to review/test can improve a team's ability to find and remove more vulnerabilities that are reachable by an attacker. One way that professionals perform this prioritization is the identification of the attack surface of software systems. However, identifying the attack surface of a software system is non-trivial. <i>The goal of this poster is to present the concept of a risk-based attack surface approximation based on <b>crash dump stack traces</b> for the prioritization of security code rework efforts</i>. For this poster, we will present results from previous efforts in the attack surface approximation space, including studies on its effectiveness in approximating security relevant code for Windows and Firefox. We will also discuss future research directions for attack surface approximation, including discovery of additional metrics from stack traces and determining how many stack traces are required for a good approximation.</p>","","","10.1145/2898375.2898388","","","attack surface;crash dumps;metrics;security;stack traces","","","","","","","","","","","19-21 April 2016","","ACM","ACM Conferences"
"Panel: Future Directions of Block-based Programming","N. C. C. Brown; J. M√∂nig; A. Bau; D. Weintrop","University of Kent, Canterbury, United Kingdom","Proceedings of the 47th ACM Technical Symposium on Computing Science Education","20161111","2016","","","315","316","<p>Blocks-based programming is becoming the way that learners are being introduced to programming and computer science. Led by the popularity of tools like Scratch, Alice, and Code.org's Hour of Code activities, many new programming environments and initiatives are employing the blocks-based modality. This trend can be seen in the growing number of classroom computer science curricula incorporating blocksbased environments into their materials. Despite this rise in use, many open questions remain surrounding blocks-based programming. In this panel, we discuss the current state of blocks-based programming environments, review what we know about learning with blocks-based tools, and look to the future, discussing what form next-generation blocks-based, or blocks-inspired, programming environments might take. Research looking at blocks-based programming is revealing that modality matters: that the representations used to present programming concepts affect learners' conceptual understanding [6], programming practices [3], and perceptions of programming and computer science [5]. This panel brings together leading designers and researchers looking to advance graphical, blocks-based programming through new, innovate designs. The panel will open with a review of current research literature on learning with blocks-based programming and then continue with presentations of three recently designed blocks-based programming environments (Greenfoot 3, GP, Pencil Code), each of which look to push the boundaries of the approach in diffierent directions. These short presentations will frame the discussion of pertinent questions facing designers and educators who use blocksbased programming environments.</p>","","","10.1145/2839509.2844661","","","blocks-based editing","","","","","1","","","","","","2-5 March 2016","","ACM","ACM Conferences"
"Automated attack surface approximation","C. Theisen","North Carolina State University, USA","Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering","20161111","2015","","","1063","1065","<p> While software systems are being developed and released to consumers more rapidly than ever, security remains an important issue for developers. Shorter development cycles means less time for these critical security testing and review efforts. The attack surface of a system is the sum of all paths for untrusted data into and out of a system. Code that lies on the attack surface therefore contains code with actual exploitable vulnerabilities. However, identifying code that lies on the attack surface requires the same contested security resources from the secure testing efforts themselves. My research proposes an automated technique to approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces user crashes represent activity that puts the system under stress, and is therefore indicative of potential security vulnerabilities. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Mozilla Firefox, the attack surface approximation selected 8.4% of files and contained 72.1% of known vulnerabilities. A similar trial was performed on the Windows 8 product. </p>","","","10.1145/2786805.2807563","","","Stack traces;attack surface;crash dumps","","","","","","","","","","","Aug. 30 2015-Sept. 4 2015","","ACM","ACM Conferences"
"Learning to rank relevant files for bug reports using domain knowledge","X. Ye; R. Bunescu; C. Liu","Ohio University, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","20161111","2014","","","689","699","<p> When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity. This paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code files into methods, API descriptions of library components used in the code, the bug-fixing history, and the code change history. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluated our system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the newly introduced learning-to-rank approach significantly outperforms two recent state-of-the-art methods in recommending relevant files for bug reports. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70% of the bug reports in the Eclipse Platform and Tomcat projects. </p>","","","10.1145/2635868.2635874","","","bug reports;learning to rank;software maintenance","","","","","18","","","","","","16-21 Nov. 2014","","ACM","ACM Conferences"
"Milware: Identification and Implications of State Authored Malicious Software","T. Herr; E. Armbrust","The George Washington University","Proceedings of the 2015 New Security Paradigms Workshop","20161111","2015","","","29","43","<p>The pervasive development and deployment of malicious software by states presents a new challenge for the information security and policy communities because of the resource advantage and legal status of governments. The difference between state and non-state authored code is typically described in vague terms of sophistication, contributing to the inaccurate confirmation bias of many that states simply `do it better'. This paper attempts to determine if state authored code is demonstrably different from that written by non-state actors and if so, how. To do so, we examine a collection of malware samples which, through existing analytic techniques, have been attributed to a mix of state and non-state actors. Reviewing technical information available in the public domain for each sample, reverse-engineering a sub-set, we determine that there is a set of criteria by which state authored code can be differentiated from the conventional malware of non-state groups. This MAlicious Software Sophistication or MASS index relies on a set of characteristics which describe the behavior and construction of malware including the severity of exploits and customization of the payload. In addition to highlighting these particular differences, the paper discusses several policy implications which arise from identifying a separate class of state-authored code. This is an interdisciplinary effort and pilot project based on a limited dataset however the conclusions drawn have important ramifications for both the information security and relevant policymaking communities.</p>","","","10.1145/2841113.2841116","","","","","","","","1","","","","","","8-11 Sept. 2015","","ACM","ACM Conferences"
"Collaborative Verification of Information Flow for a High-Assurance App Store","M. D. Ernst; R. Just; S. Millstein; W. Dietl; S. Pernsteiner; F. Roesner; K. Koscher; P. B. Barros; R. Bhoraskar; S. Han; P. Vines; E. X. Wu","University of Washington, Seattle, USA","Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security","20161111","2014","","","1092","1104","<p>Current app stores distribute some malware to unsuspecting users, even though the app approval process may be costly and time-consuming. High-integrity app stores must provide stronger guarantees that their apps are not malicious. We propose a verification model for use in such app stores to guarantee that the apps are free of malicious information flows. In our model, the software vendor and the app store auditor collaborate -- each does tasks that are easy for her/him, reducing overall verification cost. The software vendor provides a behavioral specification of information flow (at a finer granularity than used by current app stores) and source code annotated with information-flow type qualifiers. A flow-sensitive, context-sensitive information-flow type system checks the information flow type qualifiers in the source code and proves that only information flows in the specification can occur at run time. The app store auditor uses the vendor-provided source code to manually verify declassifications.</p> <p>We have implemented the information-flow type system for Android apps written in Java, and we evaluated both its effectiveness at detecting information-flow violations and its usability in practice. In an adversarial Red Team evaluation, we analyzed 72 apps (576,000 LOC) for malware. The 57 Trojans among these had been written specifically to defeat a malware analysis such as ours. Nonetheless, our information-flow type system was effective: it detected 96% of malware whose malicious behavior was related to information flow and 82% of all malware. In addition to the adversarial evaluation, we evaluated the practicality of using the collaborative model. The programmer annotation burden is low: 6 annotations per 100 LOC. Every sound analysis requires a human to review potential false alarms, and in our experiments, this took 30 minutes per 1,000 LOC for an auditor unfamiliar with the app.</p>","","","10.1145/2660267.2660343","","","android security;collaborative verification;information flow;static analysis","","","","","14","","","","","","3-7 Nov. 2014","","ACM","ACM Conferences"
"Mitigating Access Control Vulnerabilities through Interactive Static Analysis","J. Zhu; B. Chu; H. Lipford; T. Thomas","University of North Carolina at Charlotte, Charlotte, NC, USA","Proceedings of the 20th ACM Symposium on Access Control Models and Technologies","20161111","2015","","","199","209","<p>Access control vulnerabilities due to programming errors have consistently ranked amongst top software vulnerabilities. Previous research efforts have concentrated on using automatic program analysis techniques to detect access control vulnerabilities in applications. We report a comparative study of six open source PHP applications, and find that implicit assumptions of previous research techniques can significantly limit their effectiveness. We propose a more effective hybrid approach to mitigate access control vulnerabilities. Developers are reminded in-situ of potential access control vulnerabilities, where self-review of code can help them discover mistakes. Additionally, developers are prompted for application-specific access control knowledge, providing samples of code that could be thought of as static analysis by example. These examples are turned into code patterns that can be used in performing static analysis to detect additional access control vulnerabilities and alert the developer to take corrective actions. Our evaluation of six open source applications detected 20 zero-day access control vulnerabilities in addition to finding all access control vulnerabilities detected in previous works.</p>","","","10.1145/2752952.2752976","","","access control vulnerability;secure programming;static analysis","","","","","3","","","","","","1-3 June 2015","","ACM","ACM Conferences"
"SOMETHINGit: a prototyping library for live and sound improvisation","T. Oda; K. Nakakoji; Y. Yamamoto","Key Technology Laboratory, Software Research Associates, Inc. Tokyo, Japan","Proceedings of the 1st International Workshop on Live Programming","20161111","2013","","","11","14","<p>Live programming can be considered an interaction with incomplete code. Dynamic languages embrace the similar style of programming, such as pair programming and prototyping in a review session. Static languages require a certain degree of completeness of code, such as type safety and namespace resolution. SOMETHINGit is a Smalltalk library that combines dynamic Smalltalk and static Haskell and VDM-SL. SOMETHINGit enables programmers to write incomplete but yet partially mathematically sound programs by five levels of bridging mechanisms.</p>","","","","","","live programming;prototyping;sketch;smalltalk","","","","","","","","","","","19-19 May 2013","","ACM","ACM Conferences"
"Approximating attack surfaces with stack traces","C. Theisen; K. Herzig; P. Morrison; B. Murphy; L. Williams","NCSU, Raleigh, North Carolina","Proceedings of the 37th International Conference on Software Engineering","20161111","2015","2","","199","208","<p>Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its <i>attack surface</i>, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is <i>to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis.</i> In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.</p>","","","","","","attack surface;models;reliability;security;stack traces;testing;vulnerability","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Defining Concepts, Practices, and Standards for K-12 CS (Abstract Only)","P. Yongpradit; D. Seehorn; T. Pirmann; I. Lee; B. Twarek","code.org, seattle, WA, USA","Proceedings of the 47th ACM Technical Symposium on Computing Science Education","20161111","2016","","","704","705","<p>Our community has been seeing explosive interest and growth in K-12 computer science education. With this, a common question from states and school districts arises: What should students learn in a K-12 computer science pathway? We in the community are routinely asked to provide input on what is critical for students to learn at various grade levels.</p> <p>Code.org, CSTA, ACM, and lead states have kicked off the development of a framework for states and districts creating a K-12 pathway in computer science. This effort, called Framing K-12 CS, is not about developing standards (that will be left up to the CSTA/states to do), but rather a more high-level framework that can guide states or districts who want to then design their own curriculum, standards, assessments, or teacher certification programs. The framework would identify the core themes, practices, and (within each theme) learning statements that detail CS literacy when exiting certain grade spans (ex: exit competencies at grades 2, 5, 8, and 12). The CSTA and Code.org are aligning the development of this framework and the current CSTA standards revision. K-12 practitioners, researchers, states, districts, industry, and non-profit organizations have been involved throughout the development.</p> <p>This BOF will allow the CS education community to get an internal community preview of the framework and standards before the larger public review period later in the Spring, and discuss its relationship to the CSTA standards. The BOF will include the opportunity to give input and ask questions.</p>","","","10.1145/2839509.2850494","","","concepts;education;k-12;practices;standards;teachers","","","","","","","","","","","2-5 March 2016","","ACM","ACM Conferences"
"Compiler-Driven Software Speculation for Thread-Level Parallelism","P. Yiapanis; G. Brown; M. Luj√°n","University of Manchester, Oxford Road","ACM Transactions on Programming Languages and Systems (TOPLAS)","20161111","2016","38","2","1","45","<p>Current parallelizing compilers can tackle applications exercising regular access patterns on arrays or affine indices, where data dependencies can be expressed in a linear form. Unfortunately, there are cases that independence between statements of code cannot be guaranteed and thus the compiler conservatively produces sequential code. Programs that involve extensive pointer use, irregular access patterns, and loops with unknown number of iterations are examples of such cases. This limits the extraction of parallelism in cases where dependencies are rarely or never triggered at runtime. Speculative parallelism refers to methods employed during program execution that aim to produce a valid parallel execution schedule for programs immune to static parallelization. The motivation for this article is to review recent developments in the area of compiler-driven software speculation for thread-level parallelism and how they came about. The article is divided into two parts. In the first part the fundamentals of speculative parallelization for thread-level parallelism are explained along with a design choice categorization for implementing such systems. Design choices include the ways speculative data is handled, how data dependence violations are detected and resolved, how the correct data are made visible to other threads, or how speculative threads are scheduled. The second part is structured around those design choices providing the advances and trends in the literature with reference to key developments in the area. Although the focus of the article is in software speculative parallelization, a section is dedicated for providing the interested reader with pointers and references for exploring similar topics such as hardware thread-level speculation, transactional memory, and automatic parallelization.</p>","0164-0925;01640925","","10.1145/2821505","","","Thread-level speculation;automatic parallelization;multicore processors;runtime parallelization;speculative parallelization","","","","","","","","","","","January 2016","","ACM","ACM Journals & Magazines"
"On Synergies between Diversity and Task Decomposition in Constructing Complex Systems with GP","J. P. C. Bonson; S. Kelly; A. R. McIntyre; M. I. Heywood","Dalhousie University, Halifax, NS, Canada","Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion","20161111","2016","","","969","976","<p>Complexity in genetic programming is unfortunately often associated with undesirable properties such as code bloat. In this work, we review developments in which complex systems are promoted through: 1) the evolution of teams of programs, and then 2) the context specific reuse of previously evolved code. To do so, two classes of diversity are identified: intra-team diversity and inter-team diversity. Intra-team diversity promotes task decomposition/cooperative coevolution between multiple programs, i.e. teams of programs. A fundamental requirement is that programs can learn context. Inter-team diversity is promoted through maintaining model and task diversity during evolution. The combination of both result in the ability to identify teams of programs and associate them with specific contexts, and then organize teams of programs hierarchically so solve multiple tasks. Finally, the concept of cumulative population wide performance is used to illustrate how inter model diversity in particular introduces useful biases into the types of solutions evolved.</p>","","","10.1145/2908961.2931655","","","coevolution;diversity maintenance;genetic programming;task decomposition","","","","","","","","","","","20-24 July 2016","","ACM","ACM Conferences"
"MODESTO: Data-centric Analytic Optimization of Complex Stencil Programs on Heterogeneous Architectures","T. Gysi; T. Grosser; T. Hoefler","ETH, Zurich, Switzerland","Proceedings of the 29th ACM on International Conference on Supercomputing","20161111","2015","","","177","186","<p>Code transformations, such as loop tiling and loop fusion, are of key importance for the efficient implementation of stencil computations. However, their direct application to a large code base is costly and severely impacts program maintainability. While recently introduced domain-specific languages facilitate the application of such transformations, they typically still require manual tuning or auto-tuning techniques to select the transformations that yield optimal performance. In this paper, we introduce MODESTO, a model-driven stencil optimization framework, that for a stencil program suggests program transformations optimized for a given target architecture. Initially, we review and categorize data locality transformations for stencil programs and introduce a stencil algebra that allows the expression and enumeration of different stencil program implementation variants. Combining this algebra with a compile-time performance model, we show how to automatically tune stencil programs. We use our framework to model the STELLA library and optimize kernels used by the COSMO atmospheric model on multi-core and hybrid CPU-GPU architectures. Compared to naive and expert-tuned variants, the automatically tuned kernels attain a 2.0-3.1x and a 1.0-1.8x speedup respectively.</p>","","","10.1145/2751205.2751223","","","fusion;heterogeneous systems;performance model;stencil;tiling","","","","","6","","","","","","8-11 June 2015","","ACM","ACM Conferences"
"Software-security patterns: degree of maturity","M. Bunke","Universit&#228;t Bremen, Bremen, Germany","Proceedings of the 20th European Conference on Pattern Languages of Programs","20161111","2015","","","1","17","<p>Since Gamma et al. published their <i>design patterns</i>, patterns are very popular in the area of software engineering. They provide best practice to handle recurring problems during the software development phase. Three years later, security patterns appeared and provided solutions for security problems. Besides the name analogy, design and security patterns should be very similar except for the security factor. In research and industry software engineering with design patterns is widespread. However, some researchers suspect that security-pattern engineering is made difficult by some issues such as terminology or description form. Detecting adoption problems can help to improve security patterns in future. Thus, they can promote the awareness of security especially in the software maintenance phase where many programmers first give attention to security problems. Therefore, we will compare design and security patterns to find indicators for negative impact on security pattern engineering in software development. We address this issue on inspecting the aspects classification, description form, provided code examples, and usage in the software life-cycle. We determine the degree of maturity of software-security patterns by comparing them to the well-explored design patterns. To achieve the objective, we inspect the pattern terminology and conducted a study on the used description forms including provided UML diagrams and code examples. Moreover, a literature review is conducted to compare their state of research w.r.t. the software life-cycle. The maturity degree of security patterns compared to common design patterns differs and we depict further research opportunities on security patterns.</p>","","","10.1145/2855321.2855364","","","design patterns;patterns;security patterns","","","","","1","","","","","","8-12 July 2015","","ACM","ACM Conferences"
"App store mining and analysis","A. Al-Subaihin; A. Finkelstein; M. Harman; Y. Jia; W. Martin; F. Sarro; Y. Zhang","University College London, UK","Proceedings of the 3rd International Workshop on Software Development Lifecycle for Mobile","20161111","2015","","","1","2","<p> App stores are not merely disrupting traditional software deployment practice, but also offer considerable potential benefit to scientific research. Software engineering researchers have never had available, a more rich, wide and varied source of information about software products. There is some source code availability, supporting scientific investigation as it does with more traditional open source systems. However, what is important and different about app stores, is the other data available. Researchers can access user perceptions, expressed in rating and review data. Information is also available on app popularity (typically expressed as the number or rank of downloads). For more traditional applications, this data would simply be too commercially sensitive for public release. Pricing information is also partially available, though at the time of writing, this is sadly submerging beneath a more opaque layer of in-app purchasing. This talk will review research trends in the nascent field of App Store Analysis, presenting results from the UCL app Analysis Group (UCLappA) and others, and will give some directions for future work. </p>","","","10.1145/2804345.2804346","","","App stores;Mining Software Repositories","","","","","1","","","","","","31-31 Aug. 2015","","ACM","ACM Conferences"
"SEWordSim: software-specific word similarity database","Y. Tian; D. Lo; J. Lawall","Singapore Management University, Singapore","Companion Proceedings of the 36th International Conference on Software Engineering","20161111","2014","","","568","571","<p> Measuring the similarity of words is important in accurately representing and comparing documents, and thus improves the results of many natural language processing (NLP) tasks. The NLP community has proposed various measurements based on WordNet, a lexical database that contains relationships between many pairs of words. Recently, a number of techniques have been proposed to address software engineering issues such as code search and fault localization that require understanding natural language documents, and a measure of word similarity could improve their results. However, WordNet only contains information about words senses in general-purpose conversation, which often differ from word senses in a software-engineering context, and the software-specific word similarity resources that have been developed rely on data sources containing only a limited range of words and word uses. </p> <p> In recent work, we have proposed a word similarity resource based on information collected automatically from StackOverflow. We have found that the results of this resource are given scores on a 3-point Likert scale that are over 50% higher than the results of a resource based on WordNet. In this demo paper, we review our data collection methodology and propose a Java API to make the resulting word similarity resource useful in practice. </p> <p> The SEWordSim database and related information can be found at http://goo.gl/BVEAs8. Demo video is available at http://goo.gl/dyNwyb. </p>","","","10.1145/2591062.2591071","","","Database;SEWordSim;Word Similarity","","","","","7","","","","","","May 31 2014-June 7 2014","","ACM","ACM Conferences"
"Birds of a Feather: Mapping Alice Curriculum to Standards (Abstract Only)","D. Slater; W. Dann; S. Cooper","Carnegie Mellon University, Pittsburgh, PA, USA","Proceedings of the 46th ACM Technical Symposium on Computer Science Education","20161111","2015","","","692","692","<p>This session is for anyone currently using Alice 2 and / or Alice 3, or those exploring the possibility of using Alice in their curriculum. The session will focus on a mapping of an Alice curriculum and instructional materials to CSTA, code.org and ACM curriculum standards and guidelines for a wide range of age/grade levels. The discussion leaders and attendees will share teaching strategies, tips, and techniques for implementing these standards. This is an opportunity to share assignments and pointers to web sites to access instructional materials, such as syllabi, student projects, exams, and other resources with experienced Alice instructors and those new to Alice.</p> <p>Goals for this session include: <ul><li>Review a course mapping of the Alice curriculum to well-known introductory curricula and pedagogy learning standards.</li> <li>Discuss learning standards and objectives encountered in the use of Alice in different environments and educational contexts.</li> <li>Allow educators using Alice to share exemplary assignments and teaching strategies.</li> <li>Provide an opportunity for those thinking about using Alice to ask questions as they determine how it may be most useful in their particular situation.</li> <li>Foster the development of community by allowing Alice teachers to connect and become resources for each other throughout the year.</li> <li>Allow educators to interact with Alice team members to learn more about the latest features of Alice 3 and Alice 2.</li></ul></p>","","","10.1145/2676723.2691838","","","alice;curriculum standards;instructional materials;pedagogy","","","","","","","","","","","4-7 March 2015","","ACM","ACM Conferences"
"Verified Secure Implementations for the HTTPS Ecosystem: Invited Talk","C. Fournet","Microsoft Research, Cambridge, United Kingdom","Proceedings of the 2016 ACM Workshop on Programming Languages and Analysis for Security","20161111","2016","","","89","89","<p>The HTTPS ecosystem, including the SSL/TLS protocol, the X.509 public-key infrastructure, and their cryptographic libraries, is the standardized foundation of Internet Security. Despite 20 years of progress and extensions, however, its practical security remains controversial, as witnessed by recent efforts to improve its design and implementations, as well as recent disclosures of attacks against its deployments. The Everest project is a collaboration between Microsoft Research, INRIA, and the community at large that aims at modelling, programming, and verifying the main HTTPS components with strong machine-checked security guarantees, down to core system and cryptographic assumptions. Although HTTPS involves a relatively small amount of code, it requires efficient low-level programming and intricate proofs of functional correctness and security. To this end, we are also improving our verifications tools (F*, Dafny, Lean, Z3) and developing new ones. In my talk, I will present our project, review our experience with miTLS, a verified reference implementation of TLS coded in F*, and describe current work towards verified, secure, efficient HTTPS.</p>","","","10.1145/2993600.2996279","","","HTTPs;TLS;Z3;cryptography;dafny;fstar;lean;protocol;security;verification","","","","","","","","","","","24-24 Oct. 2016","","ACM","ACM Conferences"
"Heterogeneous computing: what does it mean for compiler research?","N. Rubin","Nvidia Research, Santa Clara, CA, USA","ACM SIGPLAN Notices","20161111","2014","49","8","315","316","<p>The current trend in computer architecture is to increase the number of cores, to create specialized types of cores within a single machine, and to network such machines together in very fluid web/cloud computing arrangements. Compilers have traditionally focused on optimizations to code that improve performance, but is that the right target to speed up real applications? Consider loading a web page (like starting GMAIL) the page is transferred to the client, any JavaScript is compiled, the JavaScript executes, and the page gets displayed. The classic compiler model (which was first developed in the late 50's) was a great fit for single core machines but has fallen behind architecture, and language. For example how do you compile a single program for a machine that has both a CPU and a graphics coprocessor (a GPU) with a very different programming and memory model? Together with the changes in architecture there have been changes in programming languages. Dynamic languages are used more, static languages are used less. How does this effect compiler research? In this talk, I'll review a number of traditional compiler research challenges that have (or will) become burning issues and will describe some new problems areas that were not considered in the past. For example language specifica-tions are large complex technical documents that are difficult for non-experts to follow. Application programmers are often not willing to read these documents; can a compiler bridge the gap?</p>","0362-1340;03621340","","10.1145/2692916.2558891","","","compilers;processor architectures;software architectures","","","","","","","","","","","August 2014","","ACM","ACM Journals & Magazines"
"Batch Model for Batched Timestamps Data Analysis with Application to the SSA Disability Program","Q. Yue; A. Yuan; X. Che; M. Huynh; C. Zhou","Clinical Center/ National Institues of Health, Bethesda, MD, USA","Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","20161111","2016","","","343","352","<p>The Office of Disability Adjudication and Review (ODAR) is responsible for holding hearings, issuing decisions, and reviewing appeals as part of the Social Security Administration's disability determining process. In order to control and process cases, the ODAR has established a Case Processing and Management System (CPMS) to record management information since December 2003. The CPMS provides a detailed case status history for each case. Due to the large number of appeal requests and limited resources, the number of pending claims at ODAR was over one million cases by March 31, 2015. Our National Institutes of Health (NIH) team collaborated with SSA and developed a Case Status Change Model (CSCM) project to meet the ODAR's urgent need of reducing backlogs and improve hearings and appeals process. One of the key issues in our CSCM project is to estimate the expected service time and its variation for each case status code. The challenge is that the system's recorded job departure times may not be the true job finished times. As the CPMS timestamps data of case status codes showed apparent batch patterns, we proposed a batch model and applied the constrained least squares method to estimate the mean service times and the variances. We also proposed a batch search algorithm to determine the optimal batch partition, as no batch partition was given in the real data. Simulation studies were conducted to evaluate the performance of the proposed methods. Finally, we applied the method to analyze a real CPMS data from ODAR/SSA.</p>","","","10.1145/2939672.2939706","","","batch information matrix;batch model;batched timestamps data;case processing and management system;constrained least squares estimation;disability determining process;service time","","","","","","","","","","","13-17 Aug. 2016","","ACM","ACM Conferences"
"Forking and coordination in multi-platform development: a case study","A. N. Duc; A. Mockus; R. Hackbarth; J. Palframan","Norwegian University of Science and Technology, Trondheim, Norway and Avaya Labs Research, Basking Ridge, NJ","Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","20161111","2014","","","1","10","<p>[Context] With the proliferation of desktop and mobile platforms the development and maintenance of identical or similar applications on multiple platforms is urgently needed. [Goal] We study a software product deployed to more than 25 software/hardware combinations over 10 years to understand multi-platform development practices. [Method] We use semi structured interviews, project wikis, VCSs and issue tracking systems to understand and quantify these practices. [Results] We find the projects using MR cloning, MR review meeting, cross platform coordinator's role as three primary means of coordination. We find that forking code temporarily relieves the coordination needs and is driven by divergent schedule, market needs, and organizational policy. Based on our qualitative findings we propose quantitative measures of coordination, redundant work, and parallel development. [Conclusions] A model of coordination intensity suggests that it is related to the amount of paralel and redundant work. We hope that this work will provide a basis for quantitative understanding of issues faced in multi-platform software development.</p>","","","10.1145/2652524.2652546","","","coordination;empirical study;fork;multiple platform development","","","","","2","","","","","","18-19 Sept. 2014","","ACM","ACM Conferences"
"A Descriptive Categorized Typology of Requisite Skills for Business Intelligence Professionals","T. de Jager; I. Brown","Department of Information Systems, University of Cape Town, South Africa","Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists","20161111","2016","","","1","10","<p>Business Intelligence (BI) is regarded by executives as a critical practice to adopt and invest in. The purpose of this research is to develop a categorized typology of skills required by BI professionals. A review of extant literature resulted in the identification of twenty three skills. The research aimed to validate these skills, and add additional skills to this typology based on the experiences of BI professionals within industry. These experiences were captured through interviews. Skills were then categorized by identifying commonalities across them. No additional skills were identified by the interviewed participants. A categorized typology of skills was developed which grouped the initial twenty three skills into seven higher order categories. The seven categories of skills were identified as: (1) Prepare data for subject matter expert (SME), analyst or other external party for further analysis; (2) Apply simulation modelling, statistical techniques and provide business insight; (3) Manage stakeholders and project and operational tasks; (4) Develop strategic long term BI roadmap that links to corporate strategy; (5) Understand business processes in order to effectively extract user requirements; (6) Design and code sustainable solutions; (7) Absorb and distribute knowledge.</p>","","","10.1145/2987491.2987521","","","Analytics;Business Intelligence;IS Profession;IT Skills;Typology","","","","","","","","","","","26-28 Sept. 2016","","ACM","ACM Conferences"
"ALETHEIA: Improving the Usability of Static Security Analysis","O. Tripp; S. Guarnieri; M. Pistoia; A. Aravkin","IBM T.J. Watson Research Center, Yorktown Heights, NY, USA","Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security","20161111","2014","","","762","774","<p>The scale and complexity of modern software systems complicate manual security auditing. Automated analysis tools are gradually becoming a necessity. Specifically, static security analyses carry the promise of efficiently verifying large code bases. Yet, a critical usability barrier, hindering the adoption of static security analysis by developers, is the excess of false reports. Current tools do not offer the user any direct means of customizing or cleansing the report. The user is thus left to review hundreds, if not thousands, of potential warnings, and classify them as either actionable or spurious. This is both burdensome and error prone, leaving developers disenchanted by static security checkers.</p> <p>We address this challenge by introducing a general technique to refine the output of static security checkers. The key idea is to apply statistical learning to the warnings output by the analysis based on user feedback on a small set of warnings. This leads to an interactive solution, whereby the user classifies a small fragment of the issues reported by the analysis, and the learning algorithm then classifies the remaining warnings automatically. An important aspect of our solution is that it is user centric. The user can express different classification policies, ranging from strong bias toward elimination of false warnings to strong bias toward preservation of true warnings, which our filtering system then executes.</p> <p>We have implemented our approach as the Aletheia tool. Our evaluation of Aletheia on a diversified set of nearly 4,000 client-side JavaScript benchmarks, extracted from 675 popular Web sites, is highly encouraging. As an example, based only on 200 classified warnings, and with a policy biased toward preservation of true warnings, Aletheia is able to boost precision by a threefold factor (x 2.868), while reducing recall by a negligible factor (x 1.006). Other policies are enforced with a similarly high level of efficacy.</p>","","","10.1145/2660267.2660339","","","classification;false alarms;information-flow security;machine learning;static analysis;usable security","","","","","1","","","","","","3-7 Nov. 2014","","ACM","ACM Conferences"
"Where do experts look while doing 3D image segmentation","A. Sanandaji; C. Grimm; R. West; M. Parola","Oregon State University, Meghan Kajihara, University of North Texas","Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research & Applications","20161111","2016","","","171","174","<p>3D image segmentation is a fundamental process in many scientific and medical applications. Automatic algorithms do exist, but there are many use cases where these algorithms fail. The gold standard is still manual segmentation or review. Unfortunately, even for an expert this is laborious, time consuming, and prone to errors. Existing 3D segmentation tools do not currently take into account human mental models and low-level perception tasks. Our goal is to improve the quality and efficiency of manual segmentation and review by analyzing how experts perform segmentation. As a preliminary step we conducted a field study with 8 segmentation experts, recording video and eye tracking data. We developed a novel coding scheme to analyze this data and verified that it successfully covers and quantifies the low-level actions, tasks and behaviors of experts during 3D image segmentation.</p>","","","10.1145/2857491.2857538","","","3D image segmentation;coding scheme;perception","","","","","","","","","","","14-17 March 2016","","ACM","ACM Conferences"
"Evaluating awareness information in distributed collaborative editing by software-engineers","J. Schenk","Freie Universit&#228;t Berlin, Berlin, Germany","Proceedings of the First International Workshop on User Evaluation for Software Engineering Researchers","20161111","2012","","","35","38","<p>In co-located collaborative software development activities like pair programming, side-by-side programming, code reviews or code walkthroughs, the individuals automatically gain a fine granular mutual understanding of where in the shared workspace the other participants are, what they are doing and what their levels of interest are. These points of so called awareness information are critical for an efficient and smooth collaboration but cannot be obtained via the natural mechanisms in virtual teams. Application sharing and groupware for collaborative editing are widely used for collaborative tasks in distributed software development but considered from the awareness and flexibility aspect they are far off the co-located setting. To better support virtual team collaboration by improving tools for distributed software development it is neccesary to evaluate awareness and its impacts to certain collaborative situations. Awareness itself is an invisible phenomenon and due to its intangible nature cannot be easily observed or measured. Thus we recorded virtual teams using Saros, a groupware for distributed collaborative party programming, respectively VNC and now analyse these videos using the grounded theory methodology. This approach for evaluating awareness leads to various problems concerning the recording setup and time exposure for analysis.</p>","","","","","","CSCW;application sharing;collaborative editing;distributed software development;groupware;workspace awareness","","","","","","","","","","","5-5 June 2012","","ACM","ACM Conferences"
"Error mining: bug detection through comparison with large code databases","A. Breckel","University of Ulm, Ulm, Germany","Proceedings of the 9th IEEE Working Conference on Mining Software Repositories","20161111","2012","","","175","178","<p>Bugs are hard to find. Static analysis tools are capable of systematically detecting predefined sets of errors, but extending them to find new error types requires a deep understanding of the underlying programming language. Manual reviews on the other hand, while being able to reveal more individual errors, require much more time. We present a new approach to automatically detect bugs through comparison with a large code database. The source file is analyzed for similar but slightly different code fragments in the database. Frequent occurrences of common differences indicate a potential bug that can be fixed by applying the modification back to the original source file.</p> <p>In this paper, we give an overview of the resulting algorithm and some important implementation details. We further evaluate the circumstances under which good detection rates can be achieved. The results demonstrate that consistently high detection rates of up to 50% are possible for certain error types across different programming languages.</p>","","","","","","code databases;code similarity;comparison-based bug detection;static analysis","","","","","","","","","","","2-3 June 2012","","ACM","ACM Conferences"
"The Impact of Different Teaching Approaches and Languages on Student Learning of Introductory Programming Concepts","W. M. Kunkle; R. B. Allen","Penn State Harrisburg, Middletown, PA","ACM Transactions on Computing Education (TOCE)","20161111","2016","16","1","1","26","<p>Learning to program, especially in the object-oriented paradigm, is a difficult undertaking for many students. As a result, computing educators have tried a variety of instructional methods to assist beginning programmers. These include developing approaches geared specifically toward novices and experimenting with different introductory programming languages. However, determining the effectiveness of these interventions poses a problem. The research presented here developed an instrument to assess student learning of fundamental and object-oriented programming concepts, then used that instrument to investigate the impact of different teaching approaches and languages on university students‚Äô ability to learn those concepts. Extensive data analysis showed that the instrument performed well overall. Reliability of the assessment tool was statistically satisfactory and content validity was supported by intrinsic characteristics, question response analysis, and expert review. Preliminary support for construct validity was provided through exploratory factor analysis. Three components that at least partly represented the construct ‚Äúunderstanding of fundamental programming concepts‚Äù were identified: methods and functions, mathematical and logical expressions, and control structures. Analysis revealed significant differences in student performance based on instructional language and approach. The analyses showed differences on the overall score and questions involving assignment, mathematical and logical expressions, and code completion. Instructional language and approach did not appear to affect student performance on questions addressing object-oriented concepts.</p>","","","10.1145/2785807","","","Education","","","","","1","","","","","","February 2016","","ACM","ACM Journals & Magazines"
"Expressive Genetic Programming: Concepts and Applications","L. Spector; N. F. McPhee","Hampshire College, Amherst, MA, USA","Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion","20161111","2016","","","589","608","<p>The language in which evolving programs are expressed can have significant impacts on the dynamics and problem-solving capabilities of a genetic programming system. In GP these impacts are driven by far more than the absolute computational power of the languages used; just because a computation is theoretically possible in a language, it doesn't mean it's readily discoverable or leveraged by evolution. Highly expressive languages can facilitate the evolution of programs for any computable function using, as appropriate, multiple data types, evolved subroutines, evolved control structures, evolved data structures, and evolved modular program and data architectures. In some cases expressive languages can even support the evolution of programs that express methods for their own reproduction and variation (and hence for the evolution of their offspring).</p> <p>This tutorial will present a range of approaches that have been taken for evolving programs in expressive programming languages. We will then provide a detailed introduction to the Push programming language, which was designed specifically for expressiveness in genetic programming systems. Push programs are syntactically unconstrained but can nonetheless make use of multiple data types and express arbitrary control structures, potentially supporting the evolution of complex, modular programs in a particularly simple and flexible way.</p> <p>Interleaved with our description of the Push language will be demonstrations of the use of analytical tools such as graph databases and program diff/merge tools to explore ways in which evolved Push programs are actually taking advantage of the language's expressive features. We will illustrate, for example, the effective use of multiple types and type-appropriate functions, the evolution and modification of code blocks and looping/recursive constructs, and the ability of Push programs to handle multiple, potentially related tasks.</p> <p>We will conclude wi- h a brief review of over a decade of Push-based research, including the production of human-competitive results, along with recent enhancements to Push that are intended to support the evolution of complex and robust software systems.</p>","","","10.1145/2908961.2926988","","","evolvability;genetic programming;push programming language","","","","","","","","","","","20-24 July 2016","","ACM","ACM Conferences"
"Towards clone-and-own support: locating relevant methods in legacy products","R. Lape√±a; M. Ballarin; C. Cetina","Universidad San Jorge, Zaragoza, Spain","Proceedings of the 20th International Systems and Software Product Line Conference","20161111","2016","","","194","203","<p>Clone-and-Own (CAO) is a common practice in families of software products consisting of reusing code from methods in legacy products in new developments. In industrial scenarios, CAO consumes high amounts of time and effort without guaranteeing good results. We propose a novel approach, Computer Assisted CAO (CACAO), that given the natural language requirements of a new product, and the legacy products from that family, ranks the legacy methods in the family for each of the new product requirements according to their relevancy to the new development. We evaluated our approach in the industrial domain of train control software. Without CACAO, software engineers tasked with the development of a new product had to manually review a total of 2200 methods in the family. Results show that CACAO can reduce the number of methods to be reviewed, and guide software engineers towards the identification of relevant legacy methods to be reused in the new product.</p>","","","10.1145/2934466.2934485","","","clone and own;families of software products;software reuse","","","","","","","","","","","16-23 Sept. 2016","","ACM","ACM Conferences"
"A Search-based Training Algorithm for Cost-aware Defect Prediction","A. Panichella; C. V. Alexandru; S. Panichella; A. Bacchelli; H. C. Gall","Delft University of Technology, Delft, Netherlands","Proceedings of the Genetic and Evolutionary Computation Conference 2016","20161111","2016","","","1077","1084","<p>Research has yielded approaches to predict future defects in software artifacts based on historical information, thus assisting companies in effectively allocating limited development resources and developers in reviewing each others' code changes. Developers are unlikely to devote the same effort to inspect each software artifact predicted to contain defects, since the effort varies with the artifacts' size (cost) and the number of defects it exhibits (effectiveness). We propose to use Genetic Algorithms (GAs) for training prediction models to maximize their cost-effectiveness. We evaluate the approach on two well-known models, Regression Tree and Generalized Linear Model, and predict defects between multiple releases of six open source projects. Our results show that regression models trained by GAs significantly outperform their traditional counterparts, improving the cost-effectiveness by up to 240%. Often the top 10% of predicted lines of code contain up to twice as many defects.</p>","","","10.1145/2908812.2908938","","","defect prediction;genetic algorithm;machine learning","","","","","1","","","","","","20-24 July 2016","","ACM","ACM Conferences"
"Differences in the Learning Principles Dominating Student-Student vs. Student-Instructor Interactions while Working on Programming Tasks","A. Gaspar; J. Torsella; N. Honken; S. Sohoni; C. Arnold","University of South Florida, Tampa, FL, USA","Proceedings of the 47th ACM Technical Symposium on Computing Science Education","20161111","2016","","","255","260","<p>Peer learning principles have been successfully applied to novice programmers. Pedagogies such as Pair Programming, Peer Testing, Peer review of code or tests, or, more generally Peer Instruction, have repeatedly demonstrated their effectiveness in improving both individual performance and retention rates. This paper proposes to supplement the existing literature by investigating how students interact with one another during collaborative programming tasks. More specifically, we are interested in comparing the learning principles used during student-student interactions with those used during student-instructor or student-teaching assistant dialogs.</p> <p>Students in online and face to face courses, who worked collaboratively on programming assignments, were surveyed to gain an understanding of the frequency with which they engaged in specific activities. These that are representative of the learning principles that have been supported by research to promote learning.</p> <p>Results suggest that some learning principles, may be absent from student-student interactions. We discuss how the success of collaborative programming pedagogies put into question the role of these principles and whether they may contribute to further improve peer-based approaches.</p>","","","10.1145/2839509.2844627","","","learning principles;novice programmers;peer learning","","","","","","","","","","","2-5 March 2016","","ACM","ACM Conferences"
"Image Tag Assignment, Refinement and Retrieval","X. Li; T. Uricchio; L. Ballan; M. Bertini; C. G. M. Snoek; A. Del Bimbo","Renmin University of China, Beijing, China","Proceedings of the 23rd ACM international conference on Multimedia","20161111","2015","","","1325","1326","<p>This tutorial focuses on challenges and solutions for content-based image annotation and retrieval in the context of online image sharing and tagging. We present a unified review on three closely linked problems, i.e., tag assignment, tag refinement, and tag-based image retrieval. We introduce a taxonomy to structure the growing literature, understand the ingredients of the main works, clarify their connections and difference, and recognize their merits and limitations. Moreover, we present an open-source testbed, with training sets of varying sizes and three test datasets, to evaluate methods of varied learning complexity. A selected set of eleven representative works have been implemented and evaluated. During the tutorial we provide a practice session for hands on experience with the methods, software and datasets. For repeatable experiments all data and code are online at http://www.micc.unifi.it/tagsurvey</p>","","","10.1145/2733373.2807419","","","content-based image retrieval;social tagging;tag assignment;tag refinement;tag relevance;tag retrieval","","","","","","","","","","","26-30 Oct. 2015","","ACM","ACM Conferences"
"Should your 8-year-old learn coding?","C. Duncan; T. Bell; S. Tanimoto","University of Canterbury, Christchurch, New Zealand","Proceedings of the 9th Workshop in Primary and Secondary Computing Education","20161111","2014","","","60","69","<p>There has been considerable interest in teaching ""coding"" to primary school aged students, and many creative ""Initial Learning Environments"" (ILEs) have been released to encourage this. Announcements and commentaries about such developments can polarise opinions, with some calling for widespread teaching of coding, while others see it as too soon to have students learning industry-specific skills. It is not always clear what is meant by teaching coding (which is often used as a synonym for programming), and what the benefits and costs of this are. Here we explore the meaning and potential impact of learning coding/programming for younger students. We collect the arguments for and against learning coding at a young age, and review the initiatives that have been developed to achieve this (including new languages, school curricula, and teaching resources). This leads to a set of criteria around the value of teaching young people to code, to inform curriculum designers, teachers and parents. The age at which coding should be taught can depend on many factors, including the learning tools used, context, teacher training and confidence, culture, specific skills taught, how engaging an ILE is, how much it lets students explore concepts for themselves, and whether opportunities exist to continue learning after an early introduction.</p>","","","10.1145/2670757.2670774","","","coding;programming;young students","","","","","6","","","","","","5-7 Nov. 2014","","ACM","ACM Conferences"
"Open source-style collaborative development practices in commercial projects using GitHub","E. Kalliamvakou; D. Damian; K. Blincoe; L. Singer; D. M. German","University of Victoria","Proceedings of the 37th International Conference on Software Engineering","20161111","2015","1","","574","585","<p>Researchers are currently drawn to study projects hosted on GitHub due to its popularity, ease of obtaining data, and its distinctive built-in social features. GitHub has been found to create a transparent development environment, which together with a pull request-based workflow, provides a lightweight mechanism for committing, reviewing and managing code changes. These features impact how GitHub is used and the benefits it provides to teams' development and collaboration. While most of the evidence we have is from GitHub's use in open source software (<scp>oss</scp>) projects, GitHub is also used in an increasing number of commercial projects. It is unknown how GitHub supports these projects given that GitHub's workflow model does not intuitively fit the commercial development way of working. In this paper, we report findings from an online survey and interviews with GitHub users on how GitHub is used for collaboration in commercial projects. We found that many commercial projects adopted practices that are more typical of <scp>oss</scp> projects including reduced communication, more independent work, and self-organization. We discuss how GitHub's transparency and popular workflow can promote open collaboration, allowing organizations to increase code reuse and promote knowledge sharing across their teams.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Investigating the Model-Driven Development for Systems-of-Systems","V. V. Graciano Neto; M. Guessi; L. B. R. Oliveira; F. Oquendo; E. Y. Nakagawa","Federal University of Goi&#225;s, Goi&#226;nia - Brazil, University of S&#227;o Paulo - USP, S&#227;o Carlos - Brazil","Proceedings of the 2014 European Conference on Software Architecture Workshops","20161111","2014","","","1","8","<p>Software-intensive systems have become increasingly large and complex and new techniques and methodologies are necessary to deal with such complexity. Model-Driven Development (MDD) has been used to deal with complex scenarios, since software models, despite details, facilitate the visualization of the whole. Moreover, MDD has been widely recognized as a way to assure quality, reducing time and effort, and making possible the automatic transformation of models to generate source code. In this direction, software-intensive Systems-of-Systems (SoS) is a class of software systems that have emerged over the iminence of large systems which have a high-level of complexity. Considering the success of MDD in other areas, we decided to investigate how MDD has been used in the context of SoS. This paper presents results of a Systematic Literature Review conducted to scrutinize and bring to light the state of the art in the field of MDD for SoS. Besides that, we discuss future research directions and perspectives, aiming at contributing to the development of SoS.</p>","","","10.1145/2642803.2642825","","","Model-Driven Development;Software Generation;System-of-Systems","","","","","","","","","","","25-29 Aug. 2014","","ACM","ACM Conferences"
"Applying Reinforcement Theory to Implementing a Retargeting Advertising in the Electronic Commerce Website","K. C. Yang; C. H. Huang; C. W. Tsai","Associate Professor, Department of Information Management, Hwa Hsia University of Technology, No.111, Gongzhuan Rd., Zhonghe Dist., New Taipei City 235, Taiwan, (886)2-8941-5100","Proceedings of the 17th International Conference on Electronic Commerce 2015","20161111","2015","","","1","5","<p>This study implements a retargeting advertising e-commerce website. In digital marketing, retargeting advertising is a new technique to explore the product information for visitors. We review the retargeting advertising technique and explain how it works. Based on Reinforcement Theory, this study designs the retargeting advertising to persuade potential customers to go back the c-commerce website and to complete the shopping. We provide JavaScript code for practitioners' reference.</p>","","","10.1145/2781562.2781571","","","ASP.Net;E-commerce Website;Reinforcement Theory;Retargeting Advertising","","","","","","","","","","","3-5 Aug. 2015","","ACM","ACM Conferences"
"Evaluating Mobile Apps with A/B and Quasi A/B Tests","Y. Xu; N. Chen","LinkedIn Corporation, Mountain View, CA, USA","Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","20161111","2016","","","313","322","<p>We have seen an explosive growth of mobile usage, particularly on mobile apps. It is more important than ever to be able to properly evaluate mobile app release. A/B testing is a standard framework to evaluate new ideas. We have seen much of its applications in the online world across the industry [9,10,12]. Running A/B tests on mobile apps turns out to be quite different, and much of it is attributed to the fact that we cannot ship code easily to mobile apps other than going through a lengthy build, review and release process. Mobile infrastructure and user behavior differences also contribute to how A/B tests are conducted differently on mobile apps, which will be discussed in details in this paper. In addition to measuring features individually in the new app version through randomized A/B tests, we have a unique opportunity to evaluate the mobile app as a whole using the quasi-experimental framework [21]. Not all features can be A/B tested due to infrastructure changes and wholistic product redesign. We propose and establish quasi-experimental techniques for measuring impact from mobile app release, with results shared from a recent major app launch at LinkedIn.</p>","","","10.1145/2939672.2939703","","","A/B testing;causal inference;mobile;quasi-experiments","","","","","","","","","","","13-17 Aug. 2016","","ACM","ACM Conferences"
"When designing usability questionnaires, does it hurt to be positive?","J. Sauro; J. R. Lewis","Oracle Corporation, Denver, Colorado, USA","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","20161111","2011","","","2215","2224","<p>When designing questionnaires there is a tradition of including items with both positive and negative wording to minimize acquiescence and extreme response biases. Two disadvantages of this approach are respondents accidentally agreeing with negative items (mistakes) and researchers forgetting to reverse the scales (miscoding).</p> <p>The original System Usability Scale (SUS) and an all positively worded version were administered in two experiments (n=161 and n=213) across eleven websites. There was no evidence for differences in the response biases between the different versions. A review of 27 SUS datasets found 3 (11%) were miscoded by researchers and 21 out of 158 questionnaires (13%) contained mistakes from users.</p> <p>We found no evidence that the purported advantages of including negative and positive items in usability questionnaires outweigh the disadvantages of mistakes and miscoding. It is recommended that researchers using the standard SUS verify the proper coding of scores and include procedural steps to ensure error-free completion of the SUS by users.</p> <p>Researchers can use the all positive version with confidence because respondents are less likely to make mistakes when responding, researchers are less likely to make errors in coding, and the scores will be similar to the standard SUS.</p>","","","10.1145/1978942.1979266","","","acquiescent bias;satisfaction measures;standardized questionnaires;system usability scale (sus);usability evaluation","","","","","6","","","","","","5-10 May 2012","","ACM","ACM Conferences"
"New Frontiers of Large Scale Multimedia Information Retrieval","S. F. Chang","Columbia University, New York, NY, USA","Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval","20161111","2016","","","5","5","<p>Multimedia information retrieval aims to automatically extract useful information from large collection of images, videos, and combinations with other data like text and speech. As reported in recent news, it's now possible to search information over millions or more of products with just an example image on the mobile phone. Intelligent apps are being deployed by major companies to automatically generate keywords or even captions of an image at a sophistication level that could not be imagined before. In this talk, I will review core technologies involved and discuss challenges and opportunities ahead. First, to address the complexity bottleneck when scaling up the data size, I will present extremely compact hash codes and deep learning image classification models that can reduce complexity by orders of magnitude while preserving approximate accuracy. Second, to support easy extension of recognition systems to new domains, instead of relying on fixed image categories, we introduce a new paradigm to automatically discover unique multimodal concepts and structures using large amounts of multimedia data available. Last, to support emerging applications beyond basic image categorization, I will discuss on-going efforts in understanding how images are used in expressing sentiments and emotions in online social media and how languages/cultures may influence such online multimedia communication.</p>","","","10.1145/2911996.2930063","","","","","","","","","","","","","","6-9 June 2016","","ACM","ACM Conferences"
"Using targeted symbolic execution for reducing false-positives in dataflow analysis","S. Arzt; S. Rasthofer; R. Hahn; E. Bodden","TU Darmstadt, Germany","Proceedings of the 4th ACM SIGPLAN International Workshop on State Of the Art in Program Analysis","20161111","2015","","","1","6","<p> Static data flow analysis is an indispensable tool for finding potentially malicious data leaks in software programs. Programs, nowadays often consisting of millions of lines of code, have grown much too large to allow for a complete manual inspection. Nevertheless, security experts need to judge whether an application is trustworthy or not, developers need to find bugs, and quality experts need to assess the maturity of software products. Thus, analysts take advantage of automated data flow analysis tools to find candidates for suspicious leaks which are then further investigated. While much progress has been made in the area with a broad variety of static data flow analysis tools proposed in academia and being offered commercially, the number of false alarms raised by these tools is still a concern. Many of the false alarms are reported because the analysis tool detects data flows along paths which are not realizable at runtime, e.g., due to contradictory conditions on the path. Still, every single report is a potential issue and must be reviewed by an expert which is labor-intensive and costly. In this work, we therefore propose TASMAN, a post-analysis based on symbolic execution that removes such false data leaks along unrealizable paths from the result set. Thus, it greatly improves the usefulness of the result presented to the human analyst. In our experiments on DroidBench examples, TASMAN reduces the number of false positives by about 80% without pruning any true positives. Additionally, TASMAN also identified false positives in real-world examples which we confirmed by hand. With an average execution time of 5.4 seconds per alleged leak to be checked on large real-world applications, TASMAN is fast enough for practical use. </p>","","","10.1145/2771284.2771285","","","Data Flow Analysis;False Positives;Precision;Symbolic Execution;TASMAN","","","","","1","","","","","","14-14 June 2015","","ACM","ACM Conferences"
"Author retrospective for PYRROS: static task scheduling and code generation for message passing multiprocessors","T. Yang; A. Gerasoulis","University of California at Santa Barbara, Santa Barbara, CA, USA","ACM International Conference on Supercomputing 25th Anniversary Volume","20161111","2014","","","18","20","<p>Given a program with annotated task parallelism represented as a directed acyclic graph (DAG), the PYRROS project was focused on fast DAG scheduling, code generation and runtime execution on distributed memory architectures. PYRROS scheduling goes through several processing stages including clustering of tasks, cluster mapping, and task execution ordering. Since the publication of the PYRROS project, there have been new advancements in the area of DAG scheduling algorithms, the use of DAG scheduling for irregular and large-scale computation, and software system development with annotated task parallelism on modern parallel and cloud architectures. This retrospective describes our experience from this project and the follow-up work, and reviews representative papers related to DAG scheduling published in the last decade.</p>","","","10.1145/2591635.2591647","","","dag;parallel processing;scheduling;task graph","","","","","","","","","","","10-13 June 2014","","ACM","ACM Conferences"
"Towards an architectural design of a guideline-driven EMR system: a contextual inquiry of Malawi","Y. J. Msosa; M. Densmore; C. M. Keet","University of Cape Town, Cape Town, South Africa","Proceedings of the Seventh International Conference on Information and Communication Technologies and Development","20161111","2015","","","1","4","<p>Computerised clinical practice guidelines are a key component of effective clinical decision support systems, especially in low-resource regions such as Malawi. To address shortages in staffing and budgets for training, the practice of <i>task-shifting</i>, the clinical practice guidelines (CPGs) enable health workers with limited training to provide a standardised level of care. However, CPGs are tradition-ally paper-based, with only a few CPGs having been computerised for Malawi's national electronic health record system. These CPGs have been hard-coded into the system, necessitating significant additional work to add support for future and revised CPGs. We further investigate CPG computerisation challenges in order to understand the motivations for the current computerised CPGs implementation. We use semi-structured interviews, code reviews, and observations in Malawi. Most significantly, we extend existing understanding of software engineering principles to the context of low-resource environments, noting that the tensions between conflicting stakeholder requirements, deadline and deliverable expectations, and good software engineering often result in systems that are harder to maintain, further exacerbating potential problems with longevity of ICTD deployments. We further suggest that a component-based approach in conjunction with communities of open source developers might help alleviate this problem by providing more scalable and robust CPG support.</p>","","","10.1145/2737856.2737899","","","ICT4D;clinical decision support;clinical practice guidelines;protocols","","","","","","","","","","","15-18 May 2015","","ACM","ACM Conferences"
"Supporting CS10K: A New Computer Science Methods Course for Mathematics Education Students","R. Flatland; D. Lim; J. Matthews; S. Vandenberg","Siena College, Loudonville, NY, USA","Proceedings of the 46th ACM Technical Symposium on Computer Science Education","20161111","2015","","","302","307","<p>We describe a new methods of teaching computer science (CS) course tailored for mathematics education majors but also applicable to others interested in teaching CS. Goals of the course are enhancing their ability and confidence in developing and offering CS courses at high schools and starting CS courses at high schools that do not offer them. The course involves a combination of reading, programming, lesson/unit plan development, code reviews, and discussion of the various paradigms for introducing CS at the secondary level. Results indicate the course enhances the students' confidence, ability, and preparation for teaching CS in high schools.</p>","","","10.1145/2676723.2677274","","","advance placement computer science;cs principles;cs10k;methods course;snap!","","","","","","","","","","","4-7 March 2015","","ACM","ACM Conferences"
"Tracing back the history of commits in low-tech reviewing environments: a case study of the Linux kernel","Y. Jiang; B. Adams; F. Khomh; D. M. German","MCIS, Polytechnique, Montr&#233;al, Canada","Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","20161111","2014","","","1","10","<p><u>Context</u>: During software maintenance, people typically go back to the original reviews of a patch to understand the actual design rationale and potential risks of the code. Whereas modern web-based reviewing environments like gerrit make this process relatively easy, the low-tech, mailing-list based reviewing environments of many open source systems make linking a commit back to its reviews and earlier versions far from trivial, since (1) a commit has no physical link with any reviewing email, (2) the discussed patches are not always fully identical to the accepted commits and (3) some discussions last across multiple email threads, each of which containing potentially multiple versions of the same patch.</p> <p><u>Goal</u>: To support maintainers in reconstructing the reviewing history of kernel patches, and studying (for the first time) the characteristics of the recovered reviewing histories.</p> <p><u>Method</u>: This paper performs a comparative empirical study on the Linux kernel mailing lists of 3 email-to-email and email-to-commit linking techniques based on checksums, common patch lines and clone detection.</p> <p><u>Results</u>: Around 25% of the patches had an (until now) hidden reviewing history of more than four weeks, and patches with multiple versions typically are larger and have a higher acceptance rate than patches with just one version.</p> <p><u>Conclusion</u>: The plus-minus-line-based technique is the best approach for linking patch emails to commits, while it needs to be combined with the checksum-based technique for linking different patch versions.</p>","","","10.1145/2652524.2652542","","","clone detection;linux kernel;low-tech reviewing environment;mailing list;open source;review;software engineering;traceability","","","","","3","","","","","","18-19 Sept. 2014","","ACM","ACM Conferences"
"Software analytics for digital games: keynote abstract","T. Zimmermann","Microsoft Research, Redmond, WA","Proceedings of the Fourth International Workshop on Games and Software Engineering","20161111","2015","","","1","2","<p>Software and its development generates an inordinate amount of data. Development activities such as check-ins, work items, bug reports, code reviews, and test executions are recorded in software repositories. User interactions that reflect how customers experience software are recorded in telemetry data, run-time traces, and log files and helps to track application and feature usage and expose performance and reliability. Software analytics takes this data and turns it into actionable insight to better inform decisions related to software.</p> <p>In this talk, I will summarize our efforts in the area of software analytics with a special focus on digital games. I will present several examples of games studies, which we have worked on at Microsoft Research such as how players are engaged in Project Gotham Racing, how skill develops over time in Halo Reach and Forza Motorsports, and the initial experience of game play. I will also point out important differences between games development and traditional software development. The work presented in this talk has been done by Nachi Nagappan, myself, and many others who have visited our group over the past years.</p>","","","","","","digital games;software analytics","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Systematic analysis of qualitative data in security","H. Hibshi","Carnegie Mellon University, Pittsburgh","Proceedings of the Symposium and Bootcamp on the Science of Security","20161111","2016","","","52","52","<p>This tutorial will introduce participants to Grounded Theory, which is a qualitative framework to discover new theory from an empirical analysis of data. This form of analysis is particularly useful when analyzing text, audio or video artifacts that lack structure, but contain rich descriptions. We will frame Grounded Theory in the context of qualitative methods and case studies, which complement quantitative methods, such as controlled experiments and simulations. We will contrast the approaches developed by Glaser and Strauss, and introduce coding theory - the most prominent qualitative method for performing analysis to discover Grounded Theory. Topics include coding frames, first- and second-cycle coding, and saturation. We will use examples from security interview scripts to teach participants: developing a coding frame, coding a source document to discover relationships in the data, developing heuristics to resolve ambiguities between codes, and performing second-cycle coding to discover relationships within categories. Then, participants will learn how to discover theory from coded data. Participants will further learn about inter-rater reliability statistics, including Cohen's and Fleiss' Kappa, Krippendorf's Alpha, and Vanbelle's Index. Finally, we will review how to present Grounded Theory results in publications, including how to describe the methodology, report observations, and describe threats to validity.</p>","","","10.1145/2898375.2898387","","","grounded theory;qualitative;security analysis","","","","","","","","","","","19-21 April 2016","","ACM","ACM Conferences"
"A Multi-Paradigm Approach to Teaching Students Embedded Systems Design using FPGAs and CPLDs","D. C. Dyer; Y. L. Aung","School of Engineering, University of Warwick, Coventry, Warwickshire, CV3 4AL","Proceedings of the FPGA World Conference 2014","20161111","2014","","","1","8","<p>To create optimal embedded electronic systems, it is essential to ensure all implementation options are considered, and students of electronics and computer engineering must be educated in hardware, software and firmware.</p> <p>We begin by reviewing in an educational context various implementation techniques. These include commercial microcontrollers, custom instruction set architectures (ISA), Field Programmable Gate Arrays (FPGAs) for `soft-core' processors and dedicated digital `engines', as well as Complex Programmable Logic Devices (CPLDs) for interface management.</p> <p>Thereafter, we describe our work to create a platform that incorporates the above but is extended to include software development and tools. Regarding ISAs, we use an FPGA configured with a soft-core ARM Cortex-M1 32-bit processor but also introduce a custom hybrid RISC/CISC 12-bit processor called VIP. This helps students explore and compare multiple soft-core implementation issues. Furthermore, unlike most proprietary platforms, we can provide students with the HDL code of all our peripherals and interfaces. Especially those for the address and data lines used communicate with devices on our associated custom Teaching Auxiliary Board (TAB); which itself uses a CPLD programmed to provide features such as bus handshake, protocol conversions, timers, interrupts and simulation of `slow memory locations'.</p> <p>We believe that our holistic approach provides exceptional learning opportunities to show how implementations may be partitioned across FPGAs and CPLDs acting as dedicated programmed logic or programmable soft-core processors.</p>","","","10.1145/2674095.2674099","","","ADC;CPLD;DAC;Embedded;FPGA;interfacing;microcontroller;soft-core","","","","","","","","","","","9-11 Sept. 2014","","ACM","ACM Conferences"
"A large-scale survey on the effects of selected development practices on software correctness","M. Stavnycha; H. Yin; T. R√∂mer","University of Tartu, Estonia","Proceedings of the 2015 International Conference on Software and System Process","20161111","2015","","","117","121","<p> Releasing software with required quality is important for software producers in order to be successful. Therefore, understanding which development practices affect software quality has always been of high interest. However, little empirical evidence has been reported on this matter. The research presented in this paper aims at analyzing the effects of nine pre-selected development practices on the quality aspect correctness of software releases. To this end we collected data from software developers worldwide, using an online survey. 1006 valid responses were received and analyzed with the help of statistical methods. We found that four of the nine development practices show statistically significant effects on the correctness of released software, namely solving technical debt, test coverage, code reviews, and monitoring and fixing software quality problems. Our results indicate that using development practices specifically focusing on improving software quality shows a positive effect on the level of correctness of released software. Development practices that are more organizational in nature didn‚Äôt show similarly clear effects. </p>","","","10.1145/2785592.2785617","","","Development practices;Software correctness;Survey study","","","","","","","","","","","24-26 Aug. 2015","","ACM","ACM Conferences"
"Hi-Lo tech games: crafting, coding and collaboration of augmented board games by high school youth","Y. Kafai; V. Vasudevan","University of Pennsylvania, Philadelphia, PA","Proceedings of the 14th International Conference on Interaction Design and Children","20161111","2015","","","130","139","<p>Most research on game making activities for learning has focused on programming screen-based designs. Only recently has research begun to include the design of tangible interfaces; connecting on-screen programming with hands-on crafting. In this paper, we examine the potential of a workshop that combines the high and low of technology with game design in which teams of high school youth crafted, coded and collaborated on their own augmented board games to highlight intersections between learning programming and making, and creating across digital and tangible modalities. We focused our analysis of students' projects, interactions, and reflections on how young designers conceptualized the integration of screen and board game elements, realized computational concepts and practices in their board game designs and augmentations, and reflected on their game design experience connecting crafting and coding. In the discussion, we review how the expansion of game making activities can create new opportunities for interaction design and research.</p>","","","10.1145/2771839.2771853","","","MaKey MaKey;board games;game design;maker activities;scratch","","","","","","","","","","","21-24 June 2015","","ACM","ACM Conferences"
"App store mining and analysis: MSR for app stores","M. Harman; Y. Jia; Y. Zhang","University College London, Malet Place, London, UK","Proceedings of the 9th IEEE Working Conference on Mining Software Repositories","20161111","2012","","","108","111","<p>This paper introduces app store mining and analysis as a form of software repository mining. Unlike other software repositories traditionally used in MSR work, app stores usually do not provide source code. However, they do provide a wealth of other information in the form of pricing and customer reviews. Therefore, we use data mining to extract feature information, which we then combine with more readily available information to analyse apps' technical, customer and business aspects. We applied our approach to the 32,108 non-zero priced apps available in the Blackberry app store in September 2011. Our results show that there is a strong correlation between customer rating and the rank of app downloads, though perhaps surprisingly, there is no correlation between price and downloads, nor between price and rating. More importantly, we show that these correlation findings carry over to (and are even occasionally enhanced within) the space of data mined app features, providing evidence that our 'App store MSR' approach can be valuable to app developers.</p>","","","","","","","","","","","","","","","","","2-3 June 2012","","ACM","ACM Conferences"
"From ""nobody cares"" to ""way to go!"": A Design Framework for Social Sharing in Personal Informatics","D. A. Epstein; B. H. Jacobson; E. Bales; D. W. McDonald; S. A. Munson","University of Washington, Seattle, WA, USA","Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing","20161111","2015","","","1622","1636","<p>Many research applications and popular commercial applications include features for sharing personally collected data with others in social awareness streams. Prior work has identified several barriers to use as well as discrepancies between designer goals and how these features are used in practice. We develop a framework for designing and evaluating these features based on an extensive review of prior literature. We demonstrate the value of this framework by analyzing physical activity sharing on Twitter, coding 4,771 tweets and their responses and gathering 444 reactions from 97 potential tweet recipients, learning that specific user-generated content leads to more responses and is better received by the post audience. We conclude by extending our findings to other sharing problems and discussing the value of our design framework.</p>","","","10.1145/2675133.2675135","","","health;personal informatics;self-tracking;social awareness streams;social network sites;social sharing","","","","","","","","","","","14-18 March 2015","","ACM","ACM Conferences"
"The ATB Framework: Quantifying and Classifying Epistemic Strategies in Tangible Problem-Solving Tasks","A. Esteves; S. Bakker; A. N. Antle; A. May; J. Warren; I. Oakley","University of Madeira, Funchal, Portugal","Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction","20161111","2015","","","13","20","<p>In task performance, pragmatic actions refer to behaviors that make direct progress, while epistemic actions involve altering the world so that cognitive processes are faster, more reliable or less taxing. Epistemic actions are frequently presented as a beneficial consequence of interacting with tangible systems. However, we currently lack tools to measure epistemic behaviors, making substantiating such claims highly challenging. This paper addresses this problem by presenting ATB, a video-coding framework that enables the identification and measurement of different epistemic actions during problem-solving tasks. The framework was developed through a systematic literature review of 78 papers, and analyzed through a study involving a jigsaw puzzle -- a classical spatial problem -- involving 60 participants. In order to assess the framework's value as a metric, we analyze the study with respect to its reliability, validity and predictive power. The broadly supportive results lead us to conclude that the ATB framework enables the use of observed epistemic behaviors as a performance metric for tangible systems. We believe that the development of metrics focused explicitly on the properties of tangible interaction are currently required to gain insight into the genuine and unique benefits of tangible interaction. The ATB framework is a step towards this goal.</p>","","","10.1145/2677199.2680546","","","epistemic actions;tangible interaction;video-coding","","","","","","","","","","","15-19 Jan. 2015","","ACM","ACM Conferences"
"Quality questions need quality code: classifying code fragments on stack overflow","M. Duijn; A. Kuƒçera; A. Bacchelli","Delft University of Technology, The Netherlands","Proceedings of the 12th Working Conference on Mining Software Repositories","20161111","2015","","","410","413","<p>Stack Overflow (SO) is a question and answers (Q&A) web platform on software development that is gaining in popularity. With increasing popularity often comes a very unwelcome side effect: A decrease in the average quality of a post. To keep Q&A websites like SO useful it is vital that this side effect is countered. Previous research proved to be reasonably successful in using properties of questions to help identify low quality questions to be later reviewed and improved.</p> <p>We present an approach to improve the classification of high and low quality questions based on a novel source of information: the analysis of the code fragments in SO questions. We show that we get similar performance to classification based on a wider set of metrics thus potentially reaching a better overall classification.</p>","","","","","","","","","","","","","","","","","16-24 May 2015","","ACM","ACM Conferences"
"Eliciting Tacit Expertise in 3D Volume Segmentation","R. West; M. Kajihara; M. Parola; K. Hays; L. Hillard; A. Carlew; J. Deutsch; B. Lane; M. Holloway; B. John; A. Sanandaji; C. Grimm","Univ. of North Texas","Proceedings of the 9th International Symposium on Visual Information Communication and Interaction","20161111","2016","","","59","66","<p>The output of 3D volume segmentation is crucial to a wide range of endeavors. Producing accurate segmentations often proves to be both inefficient and challenging, in part due to lack of imaging data quality (contrast and resolution), and because of ambiguity in the data that can only be resolved with higher-level knowledge of the structure and the context wherein it resides. Automatic and semi-automatic approaches are improving, but in many cases still fail or require substantial manual clean-up or intervention. Expert manual segmentation and review is therefore still the gold standard for many applications. Unfortunately, existing tools (both custom-made and commercial) are often designed based on the underlying algorithm, not the best method for expressing higher-level intention. Our goal is to analyze manual (or semi-automatic) segmentation to gain a better understanding of both low-level (perceptual tasks and actions) and high-level decision making. This can be used to produce segmentation tools that are more accurate, efficient, and easier to use. Questioning or observation alone is insufficient to capture this information, so we utilize a hybrid capture protocol that blends observation, surveys, and eye tracking. We then developed, and validated, data coding schemes capable of discerning low-level actions and overall task structures.</p>","","","10.1145/2968220.2968235","","","3D volume segmentation;conceptual framework","","","","","","","","","","","24-26 Sept. 2016","","ACM","ACM Conferences"
"Coding, Designing, and Logistics: How Modes Affect Equity in Computer Science Education (Abstract Only)","D. A. McClintock; N. Shah","Michigan State University, East Lansing, MI, USA","Proceedings of the 47th ACM Technical Symposium on Computing Science Education","20161111","2016","","","686","686","<p>This poster delves into the interactions of pair programming students to observe the equality or inequality across three modes of discussion. These three modes are coding, designing and logistics. Coding being when students are discussing code, designing being the discussion of aesthetic aspects, and logistics being the discussion of saving or manipulating files. The students whose interactions are reviewed and assessed in this poster are participants in a three-week elementary computer science program taking place in Northern California in the years 2009, 2012, and 2013. The students' ages range between ten and eleven years old. This poster will compare and contrast specific interactions and examples to highlight how modes can present an interaction as equitable, inequitable, or both. The poster will also present tables to aid in the explanation of how one mode in an interaction can be equitable while the other two modes are not.</p>","","","10.1145/2839509.2850534","","","equality;modes;pair programming","","","","","","","","","","","2-5 March 2016","","ACM","ACM Conferences"
"Towards a theory of conceptual design for software","D. Jackson","Massachusetts Institute of Technology, USA","2015 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward!)","20161111","2015","","","282","296","<p> Concepts are the building blocks of software systems. They are not just subjective mental constructs, but are objective features of a system's design: increments of functionality that were consciously introduced by a designer to serve particular purposes. This essay argues for viewing the design of software in terms of concepts, with their invention (or adoption) and refinement as the central activity of software design. A family of products can be characterized by arranging concepts in a dependence graph from which coherent concept subsets can be extracted. Just as bugs can be found in the code of a function prior to testing by reviewing the programmer's argument for its correctness, so flaws can be found in a software design by reviewing an argument by the designer. This argument consists of providing, for each concept, a single compelling purpose, and demonstrating how the concept fulfills the purpose with an archetypal scenario called an 'operational principle'. Some simple conditions (primarily in the relationship between concepts and their purposes) can then be applied to reveal flaws in the conceptual design. </p>","","Electronic:978-1-4503-3688-8","10.1145/2814228.2814248","","","Concepts;conceptual design;purposes;usability","","","","","","","","","","","25-30 Oct. 2015","","ACM","ACM Conferences"
"Deep Learning of Behaviors for Security","G. Cybenko","Dartmouth College, Etna, NH, USA","Proceedings of the 2015 ACM International Workshop on International Workshop on Security and Privacy Analytics","20161111","2015","","","1","1","<p>Deep learning has generated much research and commercialization interest recently. In a way, it is the third incarnation of neural networks as pattern classifiers, using insightful algorithms and architectures that act as unsupervised auto-encoders which learn hierarchies of features in a dataset. After a short review of that work, we will discuss computational approaches for deep learning of behaviors as opposed to just static patterns. Our approach is based on structured non-negative matrix factorizations of matrices that encode observation frequencies of behaviors. Example security applications and covert channel detection and coding will be presented.</p>","","","10.1145/2713579.2713592","","","behaviors.;machines learning;security","","","","","","","","","","","4-4 March 2015","","ACM","ACM Conferences"
"Exploration of Participation in Student Software Engineering Teams","L. Marshall; V. Pieterse; L. Thompson; D. M. Venter","University of Pretoria, Pretoria, South Africa","ACM Transactions on Computing Education (TOCE)","20161111","2016","16","2","1","38","<p>Employers require software engineers to work in teams when developing software systems. It is therefore important for graduates to have experienced teamwork before they enter the job market.</p> <p>We describe an experiential learning exercise that we designed to teach the software engineering process in conjunction with teamwork skills. The underlying teaching strategy applied in the exercise maximises risks in order to provide maximal experiential learning opportunities. The students are expected to work in fairly large, yet short-lived, instructor-assigned teams to complete software engineering tasks. After undergoing the exercise our students form self-selected teams for their capstone projects. In this article, we determine and report on the influence the teaching exercise had on the formation of teams for the capstone project. By analysing data provided by the students through regular peer reviews we gain insight into the team dynamics as well as to what extent the members contributed to the team effort.</p> <p>We develop and present a graphical model of a capstone project team which highlights participation of individuals during the teaching exercise. The participatory history of the members is visualised using segmented concentric rings. We consider how this visualisation can aid the identification of capstone project teams that are at risk. In our experience the composition of the team and the behaviour of other members in the team may have a marked impact on the behaviour of each individual in the team. We established a team classification in order to model information about teams. We use a statistical clustering method to classify teams. For this we use team profiles that are based on the participatory levels of its members. The team types that emerge from the clustering are used to derive migration models. When we consider migration, we build spring models to visualise the teams through which individuals migrate. We colour code the teams to character- se them according to the team types that were identified during the cluster classification of the teams. Owing to the complexity of the resulting model, only migrations for capstone team members who have worked together during the exercise or for solitary capstone team members are modelled. These models support the identification of areas of interest that warrant further investigation.</p> <p>To conclude, we present our observations from the analysis of team compositions, team types, and team migrations and provide directions for future work and collaborations.</p>","","","10.1145/2791396","","","Software engineering;soft skills;teaching teamwork;team formation;team management","","","","","","","","","","","March 2016","","ACM","ACM Journals & Magazines"
"Poster: Measuring Developers' Sentiments in the Android Open Source Project","T. Ahmed","BUET, Dhaka, Bangladesh","Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services Companion","20161111","2016","","","7","7","<p>Sentiments have a high impact on productivity, task quality, task synchronization and job satisfaction of a highly collaborative activity where a group of people communicates through different communication channels (i.e., code reviews, bug comments, mailing list). In this research, we try to investigate how we can predict the sentiments of Android developers accurately and how those emotions can potentially influence communication/ collaborations between a communicating pair and ultimately influence project outcome.</p>","","","10.1145/2938559.2948838","","","android;sentiment analysis;software development","","","","","","","","","","","25-30 June 2016","","ACM","ACM Conferences"
"Energy-Efficient Networking Solutions in Cloud-Based Environments: A Systematic Literature Review","F. A. Moghaddam; P. Lago; P. Grosso","University of Amsterdam and VU University Amsterdam, Amsterdam, The Netherlands","ACM Computing Surveys (CSUR)","20161111","2015","47","4","1","32","<p>The energy consumed by data centers hosting cloud services is increasing enormously. This brings the need to reduce energy consumption of different components in data centers. In this work, we focus on energy efficiency of the networking component. However, how different networking solutions impact energy consumption is still an open question. We investigate the state of the art in energy-efficient networking solutions in cloud-based environments. We follow a systematic literature review method to select primary studies. We create a metamodel based on the codes extracted from our primary studies using the Coding analytical method. Our findings show three abstraction levels of the proposed networking solutions to achieve energy efficiency in cloud-based environments: Strategy, Solution, and Technology. We study the historical trends in the investigated solutions and conclude that the emerging and most widely adopted one is the <i>Decision framework</i>.</p>","0360-0300;03600300","","10.1145/2764464","","","Energy efficiency;cloud;networking;systematic literature review","","","","","6","","","","","","July 2015","","ACM","ACM Journals & Magazines"
"Towards a Systematic Review of Automated Feedback Generation for Programming Exercises","H. Keuning; J. Jeuring; B. Heeren","Open University, Heerlen, Netherlands","Proceedings of the 2016 ACM Conference on Innovation and Technology in Computer Science Education","20161111","2016","","","41","46","<p>Formative feedback, aimed at helping students to improve their work, is an important factor in learning. Many tools that offer programming exercises provide automated feedback on student solutions. We are performing a systematic literature review to find out what kind of feedback is provided, which techniques are used to generate the feedback, how adaptable the feedback is, and how these tools are evaluated. We have designed a labelling to classify the tools, and use Narciss' feedback content categories to classify feedback messages. We report on the results of the first iteration of our search in which we coded 69 tools. We have found that tools do not often give feedback on fixing problems and taking a next step, and that teachers cannot easily adapt tools to their own needs.</p>","","","10.1145/2899415.2899422","","","automated feedback;learning programming;programming tools;systematic literature review","","","","","2","","","","","","11-13 July 2016","","ACM","ACM Conferences"
"Wrangler: interactive visual specification of data transformation scripts","S. Kandel; A. Paepcke; J. Hellerstein; J. Heer","Stanford University, Stanford, California, USA","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","20161111","2011","","","3363","3372","<p>Though data analysis tools continue to improve, analysts still expend an inordinate amount of time and effort manipulating data and assessing data quality issues. Such ""data wrangling"" regularly involves reformatting data values or layout, correcting erroneous or missing values, and integrating multiple data sources. These transforms are often difficult to specify and difficult to reuse across analysis tasks, teams, and tools. In response, we introduce <i>Wrangler</i>, an interactive system for creating data transformations. Wrangler combines direct manipulation of visualized data with automatic inference of relevant transforms, enabling analysts to iteratively explore the space of applicable operations and preview their effects. Wrangler leverages semantic data types (e.g., geographic locations, dates, classification codes) to aid validation and type conversion. Interactive histories support review, refinement, and annotation of transformation scripts. User study results show that Wrangler significantly reduces specification time and promotes the use of robust, auditable transforms instead of manual editing.</p>","","","10.1145/1978942.1979444","","","data analysis;data cleaning;transformation;visualization;wrangler","","","","","22","1","","","","","5-10 May 2012","","ACM","ACM Conferences"
"Active files as a measure of software maintainability","L. Schulte; H. Sajnani; J. Czerwonka","Northeastern University, USA","Companion Proceedings of the 36th International Conference on Software Engineering","20161111","2014","","","34","43","<p> In this paper, we explore the set of source files which are changed unusually often. We define these files as active files. Although discovery of active files relies only on version history and defect classification, the simple concept of active files can deliver key insights into software development activities. Active files can help focus code reviews, implement targeted testing, show areas for potential merge conflicts and identify areas that are central for program comprehension. </p> <p> In an empirical study of six large software systems within Microsoft ranging from products to services, we found that active files constitute only between 2-8% of the total system size, contribute 20-40% of system file changes, and are responsible for 60-90% of all defects. Not only this, but we establish that the majority, 65-95%, of the active files are architectural hub files which change due to feature addition as opposed to fixing defects. </p>","","","10.1145/2591062.2591176","","","active file;maintainability;risk;software metrics;source file change;technical debt","","","","","1","","","","","","May 31 2014-June 7 2014","","ACM","ACM Conferences"
"Establishing A Standard For Underwater Digital Acoustic Communications and Networks","D. Green","Chief Scientist, Teledyne Benthos, Inc., NIAG SG190 Vice Chairman, North America, 49 Edgerton Drive, North Falmouth, MA 02556 USA","Proceedings of the 10th International Conference on Underwater Networks & Systems","20161111","2015","","","1","5","<p>This paper provides an overview of a NATO open-access protocol for underwater acoustic communications intended to facilitate interoperability among deployed marine assets. NATO has established an advisory committee with a mandate to review the technical specification and the considered applications as this scheme makes its way to a NATO Standardization Agreement (STANAG). The purpose of this paper is to acquaint the community with the technology and to use the international collaboration as an example of the process required to establish a Standard.</p> <p>Underwater (UW) communication capabilities are currently manufacturer-specific, with no international standards for digital coding protocols. Different manufacturers provide different UW communication capabilities, generally using proprietary digital coding technologies. Presently, there exists no general interoperable capability for digital UW communication between assets using modems from different manufacturers.</p> <p>An interoperability capability is essential as maritime operators seek to integrate an increasingly heterogeneous mix of maritime assets. Many autonomous Underwater Vehicles, surface craft, fixed- and rotary-wing aircraft, moored ocean sensing systems, gateway buoys and floats depend on underwater acoustic communications for command, control and information sharing. There is currently no existing capability for these to communicate UW with each other unless they carry matching equipment from the same manufacturer. There also are no existing means to discover other communicating assets to permit the formation of ad-hoc networks. Among the several manufacturers of UW digital modems, only a very few claim to be able to communicate with systems produced by other manufacturers. The establishment of an UW digital communications standard therefore has wide application in both military and civilian contexts.</p> <p>The requirement is for communication interoperability between UW military and civilian- maritime assets to facilitate digital UW communication and, additionally, the ad-hoc networked coordination and sharing of information among underwater sensors, surface ships, submarines, AUVs, gateway buoys and sensor networks, covering the transmission and reception of many types of information and providing a communications service that compliments that offered by existing systems.</p>","","","10.1145/2831296.2831333","","","Acoustic communications;frequency hopping;interoperability;modem standards","","","","","","","","","","","22-24 Oct. 2015","","ACM","ACM Conferences"
"User Reviews of Gamepad Controllers: A Source of User Requirements and User Experience","B. Merdenyan; H. Petrie","University of York, York, United Kingdom","Proceedings of the 2015 Annual Symposium on Computer-Human Interaction in Play","20161111","2015","","","643","648","<p>The development of the digital games industry has motivated game console makers to provide better gamepads for gamers. As gamepads provide the interaction between digital games and gamers, it is important to understand gamers' requirements for these devices. This study used content analysis to investigate whether existing gamepads satisfy gamers' requirements and provide good game experience, with a view to informing new designs. A content analysis of user reviews of four different game consoles was conducted. An emergent coding scheme with 11 categories was developed. 'Comfort' was the most frequently mentioned category, accounting for nearly 25% of all comments in the reviews. 'Material Quality' and 'Responsiveness' yielded the most negative comments. Implications for design improvements are discussed.</p>","","","10.1145/2793107.2810332","","","gamepad controllers;gamepad design;user experience;user requirements","","","","","","","","","","","5-7 Oct. 2015","","ACM","ACM Conferences"
"New Horizons in the Assessment of Computer Science at School and Beyond: Leveraging on the ViVA Platform","D. Giordano; F. Maiorana; A. P. Csizmadia; S. Marsden; C. Riedesel; S. Mishra; L. Vinikienƒó","University of Catania, Catania, Italy","Proceedings of the 2015 ITiCSE on Working Group Reports","20161111","2015","","","117","147","<p>A revolution is taking place across Europe and worldwide in how we teach our children about computing, in primary and secondary school. Out goes ICT and how to use Microsoft Office; in comes coding and computer science. Assessment has a crucial role to play in this revolution. If teachers use low-quality assessment instruments we will end-up teaching the wrong subject; and viceversa. This paper reviews the state of the field, and makes concrete, achievable proposals for developing shared, high quality assessments for computer science. Central to this proposal is the collaborative platform VIVA (the Vilnius collaboratively coded and Validated computer science questions/tasks for Assess- ment). Two requirements are key to VIVA: 1) support for multiple competency frameworks, so that the contributors can meta-tag resources with respect to the framework they are most familiar with; and 2) support for crowdsourcing the validation of each question/task and its mapping to competencies. The use of a taxonomy of questions/tasks type that has been mapped to computational thinking concepts and to a competency framework is proposed. Some seed questions are already available in the online platform prototype, and various supporters have granted permission to use large questions banks. The design requirements of a full implementation of the VIVA platform for a modern and effective approach to assessment including support for digital badges, are outlined; and some preliminary results from a survey administered to the initial contributors to VIVA are presented.</p>","","","10.1145/2858796.2858801","","","activities;assessment;competency frameworks;computational thinking;crowdsourcing;learning progression;lifelong learning;metadata;open badges;questions;repositories","","","","","1","","","","","","4-8 July 2015","","ACM","ACM Conferences"
"The WEAR Scale: Developing a Measure of the Social Acceptability of a Wearable Device","N. Kelly; S. Gilbert","Iowa State University, Ames, IA, USA","Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems","20161111","2016","","","2864","2871","<p>The factors affecting the social acceptability of wearable devices are not well understood, yet they have a strong influence on whether a new wearable succeeds or fails. Factors uniquely affecting wearable acceptability as compared to other technology include manners, moral codes, the symbolic communication of dress, habits of dress, fashion, context of use, form, and aesthetics. This paper describes the development of the WEarable Acceptability Range (WEAR Scale), designed to predict acceptance of a particular wearable. First, the construct ""social acceptability of a wearable"" was defined using literature and an interview study. Second, the WEAR Scale's item pool was composed, and reviewed by experts. Third, the resulting scale was administered to sample respondents along with validation measures. The data will be evaluated for reliability and validity, and the scale's length will be adjusted, culminating in a validated WEAR Scale useful to both industry and academia.</p>","","","10.1145/2851581.2892331","","","scale development;scale validation;social acceptance;wearable device","","","","","","","","","","","7-12 May 2016","","ACM","ACM Conferences"
"From identification of parallelizability to derivation of parallelizable codes","A. Morihata","University of Tokyo, Japan","Proceedings of the 5th International Workshop on Functional High-Performance Computing","20161111","2016","","","1","1","<p> Although now parallel computing is very common, current parallel programming methods tend to be domain-specific (specializing in certain program patterns such as nested loops) and/or manual (programmers need to specify independent tasks). This situation poses a serious difficulty in developing efficient parallel programs. We often need to manually transform codes written in usual programming patterns to ones in a parallelizable form. We hope to have a solid foundation to streamline this transformation. This talk first reviews necessity of a method of systematically deriving parallelizable codes and then introduces an ongoing work on extending lambda calculus for the purpose. The distinguished feature of the new calculus is a special construct that enable evaluation with incomplete information, which is useful to express important parallel computation patterns such as reductions (aggregations). We then investigate derivations of parallelizable codes as transformations on the calculus. </p>","","","10.1145/2975991.2984053","","","Lambda Calculus;Parallel Programming;Program Transformation","","","","","","","","","","","22-22 Sept. 2016","","ACM","ACM Conferences"
"Remote Data Auditing in Cloud Computing Environments: A Survey, Taxonomy, and Open Issues","M. Sookhak; A. Gani; H. Talebian; A. Akhunzada; S. U. Khan; R. Buyya; A. Y. Zomaya","University of Malaya, Kuala Lumpur, Malaysia","ACM Computing Surveys (CSUR)","20161111","2015","47","4","1","34","<p>Cloud computing has emerged as a long-dreamt vision of the utility computing paradigm that provides reliable and resilient infrastructure for users to remotely store data and use on-demand applications and services. Currently, many individuals and organizations mitigate the burden of local data storage and reduce the maintenance cost by outsourcing data to the cloud. However, the outsourced data is not always trustworthy due to the loss of physical control and possession over the data. As a result, many scholars have concentrated on relieving the security threats of the outsourced data by designing the Remote Data Auditing (RDA) technique as a new concept to enable public auditability for the stored data in the cloud. The RDA is a useful technique to check the reliability and integrity of data outsourced to a single or distributed servers. This is because all of the RDA techniques for single cloud servers are unable to support data recovery; such techniques are complemented with redundant storage mechanisms. The article also reviews techniques of remote data auditing more comprehensively in the domain of the distributed clouds in conjunction with the presentation of classifying ongoing developments within this specified area. The thematic taxonomy of the distributed storage auditing is presented based on significant parameters, such as scheme nature, security pattern, objective functions, auditing mode, update mode, cryptography model, and dynamic data structure. The more recent remote auditing approaches, which have not gained considerable attention in distributed cloud environments, are also critically analyzed and further categorized into three different classes, namely, replication based, erasure coding based, and network coding based, to present a taxonomy. This survey also aims to investigate similarities and differences of such a framework on the basis of the thematic taxonomy to diagnose significant and explore major outstanding issues.</p>","0360-0300;03600300","","10.1145/2764465","","","Remote data auditing;cloud computing;erasure coding;network coding;replication","","","","","6","1","","","","","July 2015","","ACM","ACM Journals & Magazines"
"Impact of routing protocols on the quality of transmission for video streaming","S. Maamar; T. hafnaoui","LaSTIC, Computer Science, Batna 2, University, Algeria 213","Proceedings of the International Conference on Internet of things and Cloud Computing","20161111","2016","","","1","7","<p>Mobile ad hoc networks are characterized by infrastructure less, dynamic topology, without any centralized administration, and limited resources witch make more difficult the stream of multimedia applications over wireless networks. Providing Quality of Service for video streaming is an important challenge. In this paper, firstly, we have reviewed many issues and different coding techniques for video streaming over MANET and secondly we propose to study two routing protocols (AODV and DSDV) to evaluate witch of them can improve QoS for real-time multimedia applications. Results show that AODV protocol performs better than DSDV in terms of throughput and network load with high mobility, but roles are reversed in terms of bit rate, loss rate and network load for large-scale networks.</p>","","","10.1145/2896387.2906359","","","Coding techniques;Mobile Ad hoc Network;Multimedia;QoS;Routing protocol;Video streaming","","","","","","","","","","","22-23 March 2016","","ACM","ACM Conferences"
"Automatic Contract Insertion with CCBot","S. A. Carr; F. Logozzo; M. Payer","Purdue University, West Lafayette, IN","IEEE Transactions on Software Engineering","20170811","2017","43","8","701","714","Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.","0098-5589;00985589","","10.1109/TSE.2016.2625248","10.13039/100000001 - NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7736073","Contract-based verification;assertions;automated patching;class invariants","C# languages;Computer bugs;Contracts;Instruments;Reactive power;Runtime;Semantics","C# language;program compilers;program diagnostics;program verification;software fault tolerance","CCBot;CodeContracts Bot;automatic code transformation;automatic contract insertion;contract-based static analysis;contract-based verification;fault detection;file paths;null pointer dereferences;object invariants;open-source C# implementation;out-of-bounds array accesses;static analysis tools;static contract checker","","","","","","","20161104","Aug. 1 2017","","IEEE","IEEE Journals & Magazines"
"Software Architects in Large-Scale Distributed Projects: An Ericsson Case Study","R. Britto; D. Smite; L. O. Damm","Blekinge Institute of Technology","IEEE Software","20161028","2016","33","6","48","55","Software architects are key assets for successful development projects. However, not much research has investigated the challenges they face in large-scale distributed projects. So, researchers investigated how architects at Ericsson were organized, their roles and responsibilities, and the effort they spent guarding and governing a large-scale legacy product developed by teams at multiple locations. Despite recent trends such as microservices and agile development, Ericsson had to follow a more centralized approach to deal with the challenges of scale, distribution, and monolithic architecture of a legacy software product. So, the architectural decisions were centralized to a team of architects. The team extensively used code reviews to not only check the code's state but also reveal defects that could turn into maintainability problems. The study results also suggest that the effort architects spend designing architecture, guarding its integrity and evolvability, and mentoring development teams is directly related to team maturity. In addition, significant investment is needed whenever new teams and locations are onboarded.","0740-7459;07407459","","10.1109/MS.2016.146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7725230","global software engineering;large-scale software development;software architects;software development;software engineering","Atmospheric measurements;Complexity theory;Computer architecture;Mentoring;Particle measurements;Product development;Software architecture","distributed processing;personnel;software architecture;software development management;software maintenance","Ericsson;agile development trend;large-scale distributed projects;legacy software product;microservices trend;software architects","","2","","","","","","Nov.-Dec. 2016","","IEEE","IEEE Journals & Magazines"
"Recommending relevant classes for bug reports using multi-objective search","R. Almhana; W. Mkaouer; M. Kessentini; A. Ouni","Computer and Information Science Department, University of Michigan, Dearborn, MI, USA","2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)","20161006","2016","","","286","295","Developers may follow a tedious process to find the cause of a bug based on code reviews and reproducing the abnormal behavior. In this paper, we propose an automated approach to finding and ranking potential classes with the respect to the probability of containing a bug based on a bug report description. Our approach finds a good balance between minimizing the number of recommended classes and maximizing the relevance of the proposed solution using a multi-objective optimization algorithm. The relevance of the recommended classes (solution) is estimated based on the use of the history of changes and bug-fixing, and the lexical similarity between the bug report description and the API documentation. We evaluated our system on 6 open source Java projects, using the version of the project before fixing the bug of many bug reports. The experimental results show that the search-based approach significantly outperforms three state-of-the-art methods in recommending relevant files for bug reports. In particular, our multi-objective approach is able to successfully locate the true buggy methods within the top 10 recommendations for over 87% of the bug reports.","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582766","Search-based software engineering;bug reports;multi-objective optimization;software maintenance","Computer bugs;Documentation;History;Optimization;Search problems;Software;Software engineering","Java;application program interfaces;estimation theory;optimisation;probability;program debugging;public domain software;search problems;software maintenance","API documentation;bug report description;class recommendation;multiobjective optimization algorithm;multiobjective search;open source Java project;probability;relevance estimation;software maintenance","","","","","","","","3-7 Sept. 2016","","IEEE","IEEE Conferences"
"Thing as a Service Interoperability: Review and Framework Proposal","D. Androcec; N. Vrcek","Fac. of Organ. & Inf., Univ. of Zagreb, Varazdin, Croatia","2016 IEEE 4th International Conference on Future Internet of Things and Cloud (FiCloud)","20160926","2016","","","309","316","Internet of things (IoT) is global network of interconnected entities aimed at automating our lives. However, a number of IoT devices and systems have been created in parallel and there is no universal coding language and communication protocol. For this reason, IoT interoperability is a complex research and practical problem. In this work, we review existing work about IoT interoperability and IoT ontologies. Next, we propose a framework for things as a service interoperability that includes composition of different sensors and actuators at service level, and their integration with existing cloud services. The proposed framework consists of four main layers: virtual sensor, service, semantic, and interoperability layer. We plan to use the proposed framework to achieve service-level interoperability of smart things that will be semantically annotated in order to describe their functional and non-functional properties (with special emphasis on security and privacy).","","","10.1109/FiCloud.2016.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575879","Internet of things;interoperability;semantic thing;thing as a service","Context;Intelligent sensors;Internet of things;Interoperability;Ontologies;Semantics","Internet of Things;actuators;cloud computing;open systems;protocols;sensors","Internet of things;IoT devices;IoT interoperability;IoT ontologies;IoT systems;Thing as a Service interoperability;actuators;cloud services;communication protocol;functional properties;interconnected entities;interoperability layer;nonfunctional properties;semantic layer;service layer;service-level interoperability;smart things;universal coding language;virtual sensor layer","","","","","","","","22-24 Aug. 2016","","IEEE","IEEE Conferences"
"An Overview of Physical Layer Security in Wireless Communication Systems With CSIT Uncertainty","A. Hyadi; Z. Rezki; M. S. Alouini","Division of Computer, Electrical, and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, SaudiArabia","IEEE Access","20170520","2016","4","","6121","6132","The concept of physical layer security builds on the pivotal idea of turning the channel's imperfections, such as noise and fading, into a source of security. This is established through appropriately designed coding techniques and signal processing strategies. In this vein, it has been shown that fading channels can enhance the transmission of confidential information and that a secure communication can be achieved even when the channel to the eavesdropper is better than the main channel. However, to fully benefit from what fading has to offer, the knowledge of the channel state information at the transmitter (CSIT) is of primordial importance. In practical wireless communication systems, CSIT is usually obtained, prior to data transmission, through CSI feedback sent by the receivers. The channel links over which this feedback information is sent can be either noisy, rate-limited, or delayed, leading to CSIT uncertainty. In this paper, we present a comprehensive review of recent and ongoing research works on physical layer security with CSIT uncertainty. We focus on both information theoretic and signal processing approaches to the topic when the uncertainty concerns the channel to the wiretapper or the channel to the legitimate receiver. Moreover, we present a classification of the research works based on the considered channel uncertainty. Mainly, we distinguish between the cases when the uncertainty comes from an estimation error of the CSIT, from a CSI feedback link with limited capacity, or from an outdated CSI.","","","10.1109/ACCESS.2016.2612585","10.13039/501100004052 - CRG 2 Grant from the Office of Sponsored Research at the King Abdullah University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572890","Physical layer security;channel state information;estimation error;fading channels;outdated CSI;rate-limited feedback","Channel state information;Error analysis;Fading channels;Feedback;Network security;Physical layer;Receivers;Uncertainty","data communication;fading channels;radio links;radio receivers;radio transmitters;signal processing;telecommunication security","CSI feedback link;CSIT uncertainty;channel imperfections;channel state information at the transmitter;channel uncertainty;confidential information transmission;data transmission;fading channels;information theoretic approach;physical layer security;receivers;secure communication;signal processing strategies;wireless communication systems;wiretapper","","3","","","","","20160921","2016","","IEEE","IEEE Journals & Magazines"
"Automatic Fix for C Integer Errors by Precision Improvement","X. Cheng; M. Zhou; X. Song; M. Gu; J. Sun","Key Lab. for Inf. Syst. Security, MoE, China","2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)","20160825","2016","1","","2","11","Integer errors in C program may lead to serious failures and vulnerabilities. They are harbored in a wide range of programs including mature software such as Linux kernel. Code reviewing is laborious and cannot guarantee reliable fixes for errors. Addressing potential errors in the development phase is error-prone even for experts and essentially hinders developing efficiency. In this paper we propose a novel approach to automate fix for C integer errors. Our approach directly replaces original C integers with dynamic-precision integers to fix potential errors without detecting them in advance. Many errors can be fixed by precision improvement without changing the design of application. We implement a tool CIntFix to automatically fix C integer errors. CIntFix succeeds in fixing all 5414 programs in NIST's Juliet test suite from 7 weakness categories. Meanwhile, on Juliet test suite and SPEC CINT2000 benchmarks, CIntFix processes C source code at the rate of 0.157s/KLOC and the fixed programs have 18.0% slowdown on average. The results show that CIntFix is capable to fix integer errors in real-world C programs.","","Electronic:978-1-4673-8845-0; POD:978-1-4673-8846-7","10.1109/COMPSAC.2016.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551988","code transformation;integer error;tolerability","Encoding;Kernel;Security;Semantics;Software reliability","C language;program testing;software fault tolerance;source code (software)","C integer error detection;C program;C source code;CIntFix;NIST Juliet test suite;SPEC CINT2000 benchmarks;automatic fix;dynamic-precision integers;precision improvement","","","","","","","","10-14 June 2016","","IEEE","IEEE Conferences"
"Technology and the Arts: Educational Encounters of the Third Kind","A. Kelliher","Virginia Tech","IEEE MultiMedia","20160805","2016","23","3","8","11","This review of integrative technology-arts education discusses STEM versus STEAM; the push for teaching kids to code; and how various tools, programs, and initiatives are introducing a new generation of students to a 21st century liberal arts education.","1070-986X;1070986X","","10.1109/MMUL.2016.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7535124","STEAM;STEM;coding;education;history of computing;multimedia;programing;scientific computing;software engineering;technology-arts education","Education;Education courses;Engineering education;Learning systems;STEM;Scientific computing;Software engineering","STEM;art;computer aided instruction;teaching;technology","STEAM;STEM;integrative technology-arts education;liberal arts education;teaching","","","","","","","","July-Sept. 2016","","IEEE","IEEE Journals & Magazines"
"An Evaluation of Design Rule Spaces as Risk Containers","A. Leigh; M. Wermelinger; A. Zisman","Comput. & Commun. Dept., Open Univ., Milton Keynes, UK","2016 13th Working IEEE/IFIP Conference on Software Architecture (WICSA)","20160721","2016","","","295","298","It is well understood that software development can be a risky enterprise and industrial projects often overrun budget and schedule. Effective risk management is, therefore, vital for a successful project outcome. Design Rule Spaces (DRSpaces) have been used by other researchers to understand why implemented software is error-prone [1, 2]. This industrial case study evaluates whether such spaces are durable, meaningful, and isolating risk containers. DRSpaces were created from UML class diagrams of architectural design artefacts. In our study, object orientated metrics were calculated from the UML diagrams, and compared to the error-proneness of the DRSpace implementation, to determine whether architectural coupling translated into implementation difficulties. A correlation between architectural coupling and error-proneness of DRSpaces was observed in the case study. Software developers were asked to identify DRSpaces they found difficult to implement, in order to understand which factors, other than architectural coupling, were also important. The qualitative results show agreement between the code areas developers found difficult to implement and the error-prone DRSpaces. However, the results also show that architectural coupling is just one risk factor of many. The case study suggests that architectural DRSpaces can be used to facilitate a targeted risk review prior to implementation and manage risk.","","Electronic:978-1-5090-2131-4; POD:978-1-5090-2563-3","10.1109/WICSA.2016.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516850","analysis;architecture;risk;software","Computer architecture;Computer bugs;Containers;Couplings;Measurement;Software;Unified modeling language","Unified Modeling Language;object-oriented methods;project management;risk management;software architecture;software development management","DRSpace implementation;UML class diagrams;UML diagrams;architectural DRSpaces;architectural coupling;architectural design artefacts;design rule space evaluation;industrial projects;object orientated metrics;risk container isolation;risk containers;risk management;software development","","1","","","","","","5-8 April 2016","","IEEE","IEEE Conferences"
"Uniform Resource Identifier Encoded by Base62x","Z. Liu; H. Liu; J. Hardy; L. Liu","People's Daily Online, Inst. of People's Daily Online, Beijing, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","20160721","2015","","","1087","1093","This paper proposes a new Uniform Resource Identifier (URI) encoding scheme based on Base62x. URI is one of the base components of the Internet. URI is usually encoded in numerous applications for easy access. Current methods include encoding the URI with a percentage symbol followed by hexadecimal code (%hex) or the use of Base64. Deficiencies of existing methods include problems that ""%hex"" increases the length of encoded strings almost threefold and Base64 introduces error-prone symbols. Furthermore, to resolve the above issues, many tailored URI encoding schemes have been introduced for specific purposes, which can give rise to confusion during future development. With the literature reviews on existing approaches, this paper introduces a new general scheme of encoding URI by Base62x, which provides 64-bit representation using only standard characters (a-z, A-Z, 0-9). The scheme will bring a cost-effective Internet for the future.","","Electronic:978-1-4673-7211-4; POD:978-1-4673-7212-1","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518379","Base 16;Base62x;Characters Encoding;URI;URL;URLEncode","Computer languages;Encoding;Internet;Protocols;Standards;Uniform resource locators;World Wide Web","Internet;encoding","Base62x;Internet;URI encoding scheme;error-prone symbols;hexadecimal code;percentage symbol;standard characters;uniform resource identifier encoding scheme","","","","","","","","10-14 Aug. 2015","","IEEE","IEEE Conferences"
"Protocols and Mechanisms to Recover Failed Packets in Wireless Networks: History and Evolution","S. A. Khan; M. Moosa; F. Naeem; M. H. Alizai; J. M. Kim","Department of Electrical, Electronics, and Computer Engineering, Embedded Ubiquitous Computing Systems Laboratory, University of Ulsan, Ulsan, South Korea","IEEE Access","20170520","2016","4","","4207","4224","The emergence of multihop wireless networks and the increase in low-latency demands of error tolerant applications, such as voice over internet protocol, have triggered the development of new protocols and mechanisms for recovering failed packets. For example, recovering partially corrupt packets instead of retransmission has emerged as an effective way to improve key network performance metrics, such as goodput, latency, and energy consumption. A number of similar and interesting solutions have been proposed recently to either reconstruct or process corrupt packets on wireless networks. The proliferation of multimedia services on 3G and long term evolution networks, and the stringent quality of service requirements for these applications have given birth to robust codes and new error tolerant mechanisms for packet delivery. Despite years of active research in the field, we lack a comprehensive survey that summarizes recent developments in this area and highlights avenues with potential for future growth. This survey tries to fill in this void by providing a comprehensive review of the evolution of this field and underscoring areas for future research.","","","10.1109/ACCESS.2016.2593605","10.13039/501100003725 - National Research Foundation of Korea within the Ministry of Science, ICT and Future Planning through the Leading Human Resource Training Program of Regional Neo Industry; 10.13039/501100007053 - Korea Institute of Energy Technology Evaluation and Planning within the Ministry of Trade, Industry, and Energy, Republic of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7517301","3G;LTE;Wireless networks;error tolerant;internet of things;packet recovery","3G mobile communication;Error tolerance;Internet of things;Long Term Evolution;Multi-hop communications;Network performance;Wireless networks","3G mobile communication;Internet telephony;Long Term Evolution;fault tolerance;multimedia communication;quality of service;radio networks;telecommunication network reliability","3G;error tolerant applications;long term evolution networks;multihop wireless networks;multimedia services;quality of service;voice over Internet protocol","","3","","","","","20160720","2016","","IEEE","IEEE Journals & Magazines"
"Ethics for Big Data and Analytics","D. E. O'Leary","University of Southern California","IEEE Intelligent Systems","20160718","2016","31","4","81","84","This article investigates some issues associated with Big Data and analytics (Big Data) and ethics, examining how Big Data ethics are different than computer ethics and other more general ethical frameworks. In so doing, the author briefly investigates some of the previous research in ""computer ethics,"" reviews some codes of ethics, analyzes the application of two frameworks on ethics to Big Data, and provides a definition of Big Data ethics. Ultimately, the author argues that in order to get sufficient specificity, a code of ethics for Big Data is necessary.","1541-1672;15411672","","10.1109/MIS.2016.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515114","Internet of Things;analytics;big data;code of conduct;code of ethics;ethics;intelligent systems","Big data;Data analytics;Ethics;Internet of things","Big Data;ethical aspects","Big Data analytics;Big Data ethics;computer ethics;ethical codes","","2","","15","","","","July-Aug. 2016","","IEEE","IEEE Journals & Magazines"
"FeedBaG: An interaction tracker for Visual Studio","S. Amann; S. Proksch; S. Nadi","Software Technology Group, Technische Universit&#228;t Darmstadt, Germany","2016 IEEE 24th International Conference on Program Comprehension (ICPC)","20160707","2016","","","1","3","Integrated Development Environments (IDEs) provide a convenient standalone solution that supports developers during various phases of software development. In order to provide better support for developers within such IDEs, we need to understand how developers use them. To infer useful conclusions, such information should be gathered for different types of IDEs, for different programming languages, and in different development settings. In this paper, we present FEEDBAG, an extension for Visual Studio that tracks developers' interactions with the IDE. FEEDBAG generates a rich stream of interaction events and provides means for developers to review and submit the data to a server. We recently used the tool in a study, recording more than 6,300 hours of work time. Future studies with different user groups are needed to explore and compare IDE-usage aspects, like code-comprehension assistance, in detail. Therefore, we publish FEEDBAG and encourage other researchers to use it as well.","","Electronic:978-1-5090-1428-6; POD:978-1-5090-1429-3","10.1109/ICPC.2016.7503741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503741","","Context;Generators;Instruments;Navigation;Servers;Software;Visualization","programming environments","FeedBaG;IDE;code-comprehension assistance;integrated development environments;interaction tracker;software development;visual studio","","1","","","","","","16-17 May 2016","","IEEE","IEEE Conferences"
"JET Program for Closing Gaps to Fusion Energy","X. Litaudon","EUROfusion Consortium, JET, Culham Science Centre, Abingdon, U.K.","IEEE Transactions on Plasma Science","20160909","2016","44","9","1481","1488","The European program foresees that a pure tritium (T-T) and an extended deuterium-tritium (D-T) experimental campaign will be carried on Joint European Torus (JET). The realization of the JET T-T and D-T experimental campaigns is an important contribution to fill major physics and technological gaps for the development of fusion energy. The JET-ITER-like wall experiment provides an ideal test bed to study in an integrated way the interplay between the plasma surface interactions and the plasma operation with the ITER plasma-facing materials (beryllium wall and tungsten divertor). Recent and significant progresses of the ITER scenarios development on JET are reviewed together with their extrapolation by assuming a 50%-50% D-T plasma mixture. The JET performance has been recovered at a plasma current up to 2.5 MA for both the ITER baseline and the hybrid scenarios. These two scenarios will be pursued at higher fusion performance by increasing the applied powers and/or plasma current in the coming experimental campaign in the conditions compatible with the W-divertor. The D-T experimental campaign will provide a unique opportunity to benchmark the ITER relevant 14-MeV neutron detection calibration procedures, neutronic codes for calculating the neutron flux, and machine activation, and to investigate the radiation damage of ITER functional materials. In view of the full tritium and D-T experimental campaigns, the JET tritium plant is being upgraded with an increase in the T-fuelling capability using different gas injection modules, with an improved T-accountancy and a new water detritiation system to fully close the T-cycle at JET.","0093-3813;00933813","","10.1109/TPS.2016.2572158","EUROfusion Consortium and has received funding from the Euratom Research and Training Programme 2014-2018; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506213","Fusion energy;ITER plasma scenarios;neutronics;tokamak;tritium technology","Europe;Heating;Impurities;Neutrons;Physics;Plasma temperature","Tokamak devices;fusion reactor divertors;plasma boundary layers;plasma toroidal confinement","European program;ITER baseline;ITER functional materials;ITER plasma-facing materials;ITER scenarios development;JET D-T experimental campaign;JET T-T experimental campaign;JET program;JET-ITER-like wall experiment;Joint European Torus;W-divertor;extended deuterium-tritium experimental campaign;fusion energy;neutron detection calibration procedures;neutron flux;neutronic codes;plasma operation;plasma surface interactions;pure tritium experimental campaign;radiation damage;water detritiation system","","2","","","","","20160707","Sept. 2016","","IEEE","IEEE Journals & Magazines"
"Code-modulated embedded test for phased arrays","K. Greene; V. Chauhan; B. Floyd","Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695","2016 IEEE 34th VLSI Test Symposium (VTS)","20160526","2016","","","1","4","Millimeter wave (mm-wave) design has become the forefront for enabling multi-Gb/s wireless communications due to the abundance of available bandwidth at frequencies above 24 GHz. At these frequencies, phased arrays are used to meet link budgets by combining phase-adjusted responses of multiple antennas to form a high-gain, directive beam which is electrically steerable. Current requirements point to array sizes ranging from 8-32 elements, each of which must be measured and calibrated in terms of RF output power and phase to obtain the desired array performance. This paper will first review phased-array topologies and calibration requirements. We will then present a code modulated technique for manufacturing test of the array which uses only digital code modulators per element and a single global mm-wave squaring circuit in the form of a power detector. This approach allows measurement of full array performance with a single detector using minimum additional built-in-test hardware. Behavioral models indicate that this method can estimate the phase response within 1 degree and an output power within 0.2 dB for each individual element using global array measurements.","","Electronic:978-1-4673-8454-4; POD:978-1-4673-8455-1; USB:978-1-4673-8453-7","10.1109/VTS.2016.7477274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477274","","Calibration;Multiaccess communication;Phase measurement;Phase shifters;Phased arrays;Power generation","antenna phased arrays;built-in self test;calibration;detector circuits;millimetre wave antenna arrays;modulation coding;radiocommunication;telecommunication industry","RF output power;behavioral models;built-in-test hardware;code modulated technique;code-modulated embedded test;digital code modulators;directive beam;global array measurements;link budgets;manufacturing test;millimeter wave design;mm-wave design;mm-wave squaring circuit;phase response;phase-adjusted responses;phased-array topologies;power detector;wireless communications","","","","8","","","","25-27 April 2016","","IEEE","IEEE Conferences"
"The Impact of Human Discussions on Just-in-Time Quality Assurance: An Empirical Study on OpenStack and Eclipse","P. Tourani; B. Adams","Polytech. Montreal, Montreal, ON, Canada","2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20160523","2016","1","","189","200","In order to spot defect-introducing code changes during review before they are integrated into a project's version control system, a variety of defect prediction models have been designed. Most of these models focus exclusively on source code properties, like the number of added or deleted lines, or developer-related measures like experience. However, a code change is only the outcome of a much longer process, involving discussions on an issue report and review discussions on (different versions of) a patch. % Ignoring the characteristics of these activities during prediction is unfortunate, since Similar to how body language implicitly can reveal a person's real feelings, the length, intensity or positivity of these discussions can provide important additional clues about how risky a particular patch is or how confident developers and reviewers are about the patch. In this paper, we build logistic regression models to study the impact of the characteristics of issue and review discussions on the defect-proneness of a patch. Comparison of these models to conventional source code-based models shows that issue and review metrics combined improve precision and recall of the explanatory models up to 10%. Review time and issue discussion lag are amongst the most important metrics, having a positive (i.e., increasing) relation with defect-proneness.","","Electronic:978-1-5090-1855-0; POD:978-1-5090-1856-7","10.1109/SANER.2016.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476642","Human Discussion Metrics;Just-In-Time Quality Assurance","Biological system modeling;Control systems;Data models;Databases;Measurement;Predictive models;Software","configuration management;just-in-time;project management;quality assurance;regression analysis;software fault tolerance;software quality;software reviews;source code (software)","Eclipse;OpenStack;defect prediction models;defect-introducing code changes;defect-proneness;developer-related measures;human discussions;issue discussion lag;just-in-time quality assurance;logistic regression models;person real feelings;precision;project version control system;recall;review metrics;review time;source code properties;source code-based models","","1","","45","","","","14-18 March 2016","","IEEE","IEEE Conferences"
"Automated Static Analysis of Unit Test Code","R. Ramler; M. Moser; J. Pichler","Software Analytics &Evolution, Software Competence Center Hagenberg, Hagenberg, Austria","2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20160523","2016","2","","25","28","Automated unit tests are an essential software quality assurance measure that is widely used in practice. In many projects, thus, large volumes of test code have co-evolved with the production code throughout development. Like any other code, test code too may contain faults, affecting the effectiveness, reliability and usefulness of the tests. Furthermore, throughout the software system's ongoing development and maintenance phase, the test code too has to be constantly adapted and maintained. To support detecting problems in test code and improving its quality, we implemented 42 static checks for analyzing JUnit tests. These checks encompass best practices for writing unit tests, common issues observed in using xUnit frameworks, and our experiences collected from several years of providing trainings and reviews of test code for industry and in teaching. The checks can be run using the open source analysis tool PMD. In addition to a description of the implemented checks and their rationale, we demonstrate the applicability of using static analysis for test code by analyzing the unit tests of the open source project JFreeChart.","","Electronic:978-1-5090-1855-0; POD:978-1-5090-1856-7","10.1109/SANER.2016.102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476757","static analysis;test code;unit testing","Encoding;Maintenance engineering;Production;Software quality;Testing;Writing","program testing;quality assurance;software quality","automated static analysis;automated unit tests;open source analysis tool PMD;open source project JFreeChart;production code;software quality assurance measure;software system;teaching;unit test code;xUnit frameworks","","","","8","","","","14-18 March 2016","","IEEE","IEEE Conferences"
"Mining Unstructured Data in Software Repositories: Current and Future Trends","G. Bavota","Free Univ. of Bozen-Bolzano, Bolzano, Italy","2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20160523","2016","5","","1","12","The amount of unstructured data available to software engineering researchers in versioning systems, issue trackers, achieved communications, and many other repositories is continuously growing over time. The mining of such data represents an unprecedented opportunity for researchers to investigate new research questions and to build a new generation of recommender systems supporting development and maintenance activities. This paper describes works on the application of Mining Unstructured Data (MUD) in software engineering. The paper briefly reviews the types of unstructured data available to researchers providing pointers to basic mining techniques to exploit them. Then, an overview of the existing applications of MUD in software engineering is provided with a specific focus on textual data present in software repositories and code components. The paper also discusses perils the ""miner"" should avoid while mining unstructured data and lists possible future trends for the field.","","Electronic:978-1-5090-1855-0; POD:978-1-5090-1856-7","10.1109/SANER.2016.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476768","future trends;mining unstructured data","Data mining;Matrix decomposition;Multiuser detection;Natural language processing;Pattern matching;Software;Software engineering","data mining;recommender systems;software engineering","mining unstructured data;recommender systems;software engineering researchers;software repositories","","","","88","","","","14-18 March 2016","","IEEE","IEEE Conferences"
"How do GitHub Users Feel with Pull-Based Development?","Y. Saito; K. Fujiwara; H. Igaki; N. Yoshida; H. Iida","Grad. Sch. of Inf. Sci., Nara Inst. of Sci. & Technol., Ikoma, Japan","2016 7th International Workshop on Empirical Software Engineering in Practice (IWESEP)","20160505","2016","","","7","11","Modern OSS projects have adopted Git to manage versions of their source code and GitHub for hosting their Git repositories. GitHub provides a characteristic feature notably pull request, and many projects adopt pull-based development model by using it. This development model offers an opportunity to review the source code before merging it into the mainstream. Getting acceptance of the pull request, any developer should strictly follow the flow of pull-based development. To follow the flow, she/he needs to use correctly Git commands. However, using these commands is complicated and requires further knowledge of them. In this paper, we conducted a large-scale survey of 1552 developers on GitHub to investigate the difficulty of the Git from the aspect of the pull request. The result shows developers struggle with 'git rebase -i' command tends not to perform re-ordering, compressing and dividing their commits.","","Electronic:978-1-5090-1851-2; POD:978-1-5090-1852-9","10.1109/IWESEP.2016.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7464545","","Conferences;Control systems;Electronic mail;History;Information science;Merging;Software","configuration management;distributed programming;public domain software","Git command;Git repositories;GitHub users;OSS project;pull request;pull-based development model;source code;version management","","1","","14","","","","13-13 March 2016","","IEEE","IEEE Conferences"
"Application for Decision-Making in Transportation Logistics Function: Supply Chain Colombian Green Coffee","J. T. D√≠az; J. D. V. Cubillos; H. Bolivar","Ind. Eng. Program, Catholic Univ. of Colombia, Bogota, Colombia","2016 Second International Symposium on Stochastic Models in Reliability Engineering, Life Science and Operations Management (SMRLO)","20160314","2016","","","575","582","The following article presents a review on the state of the art five basic criteria that a decision maker in the Supply Chain (SC) should consider in selecting carriers for the supply channel with suppliers and for channel distribution with distributors: opportunity, flexibility, cost savings, efficiency / productivity and technology. These are validated to a case study of SC Colombian green coffee, through the technical outranking multicriteria decision (ELECTRE), which seeks Carriers Selection (CS) under criteria. One of the contributions of this research is the methodology for decision making from building criteria from the literature to the application of ELECTRE technique. The second contribution to the decision maker in any SC is that he could take the code developed in Matlab and run at any time as making support organizational, logistical or production decisions, for any problem of selection, classification and prioritization.","","Electronic:978-1-4673-9941-8; POD:978-1-4673-9942-5","10.1109/SMRLO.2016.100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433175","ELECTRE;Multicriteria Decision Techniques;Supply Chain Management;green coffee;outranking;supply chain","Containers;Decision making;Green products;Logistics;Packaging;Transportation","decision making;goods distribution;logistics;supply chain management;transportation","CS;ELECTRE;Matlab;SC;carriers selection;channel distribution;decision-making application;supply chain colombian green coffee;supply channel;technical outranking multicriteria decision;transportation logistics function","","","","21","","","","15-18 Feb. 2016","","IEEE","IEEE Conferences"
"Using Hardware Transactional Memory to Enable Speculative Trace Optimization","J. Salamanca; J. N. Amaral; G. Araujo","Inst. of Comput., UNICAMP, Campinas, Brazil","2015 International Symposium on Computer Architecture and High Performance Computing Workshop (SBAC-PADW)","20160303","2015","","","1","6","This paper describes a novel speculation technique for the optimization, and simultaneous execution, of multiple alternative traces of hot code regions. This technique, called Speculative Trace Optimization (STO), enumerates, optimizes, and speculatively executes traces of hot loops. It requires hardware support that can be provided in a similar fashion as that available in Hardware Transactional Memory (HTM) systems. This paper discusses the necessary features to support STO, namely multi-versioning, lazy conflict resolution, eager conflict detection, and transaction synchronization. A review of existing HTM architectures - Intel TSX, IBM BG/Q, and IBM POWER8 - shows that none of them have all the features required to implement STO. However, this work demonstrates that STO can be implemented on top of existing HTM architectures through the addition of privatization and pause/resume code. The evaluation of a prototype STO implementation, on top of Intel TSX, using benchmarks from Parboil, Media Bench, and SPEC2006, indicates that STO can yield whole-program speedups of up to 9%. This initial result is promising given that the prototype has significant overhead caused by the code that compensates for TSX absent features. An analysis, included in the paper, suggests that HTM mechanisms have the potential to considerably improve trace performance provided that they efficiently implement the suggested features.","","Electronic:978-1-4673-8621-0; POD:978-1-4673-8622-7","10.1109/SBAC-PADW.2015.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423112","Hardware Transactional Memory;Speculative Execution;Speculative Optimization;TSX;Trace Optimization","Benchmark testing;Computer architecture;Electronic mail;Hardware;Optimization;Privatization;Prototypes","parallel programming;privatisation;synchronisation;transaction processing","HTM architectures;HTM mechanisms;HTM systems;IBM BG/Q;IBM POWER8;Intel TSX;MediaBench;Parboil;SPEC2006;eager conflict detection;hardware support;hardware transactional memory;hardware transactional memory systems;hot code regions;lazy conflict resolution;multiversioning;pause-resume code;privatization;prototype STO implementation;speculation technique;speculative trace optimization;transaction synchronization","","","","13","","","","18-21 Oct. 2015","","IEEE","IEEE Conferences"
"Hadoop-Based Distributed Computing Algorithms for Healthcare and Clinic Data Processing","J. Ni; Y. Chen; J. Sha; M. Zhang","Big Data & Inf. Res. Lab., Shanghai Sanda Univ., Shanghai, China","2015 Eighth International Conference on Internet Computing for Science and Engineering (ICICSE)","20160303","2015","","","188","193","There exit a huge demand on utilizing big data technology to process healthcare related patients data for healthcare information extraction and medical knowledge discovery. In this paper, we briefly review the demands and application potentials using big data technology with an emphasis on common challenges. After briefly addressing the Hadoop/MapReduce code components and modules, we use a simple clinic data to demonstrate how to map and reduce on small dataset with illustrated workflow. We give simple scenario of using other MapReduce calculation modules for counting and classification. This serves as a basic step into future utilization of big data to healthcare domain.","","Electronic:978-1-5090-0454-6; POD:978-1-5090-0455-3","10.1109/ICICSE.2015.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422478","Hadoop;Healthcare information systems;MapReduce;big data;healthcare and clinic data","Big data;Computers;Data mining;Distributed databases;Medical services","Big Data;data mining;health care;medical information systems;parallel processing","Big data technology;Hadoop code component;Hadoop code module;MapReduce code component;MapReduce code module;clinic data processing;hadoop-based distributed computing algorithm;healthcare information extraction;medical knowledge discovery","","","","8","","","","6-8 Nov. 2015","","IEEE","IEEE Conferences"
"The HtComp Research Project: An Overview","A. Cilardo","DIETI, Univ. of Naples Federico II, Naples, Italy","2015 10th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC)","20160303","2015","","","510","514","This contribution reviews the main results of the HtComp project, a two-year research programme aiming at facilitating the integration of FPGA-based accelerators into general-purpose computing. The project covered the automated generation of HDL code from parallel applications written in traditional high-level software languages, as well as the customization of the processing, memory, and on-chip interconnect subsystems tailored on the application requirements. The ultimate outcome of the research was the introduction of methods and tools allowing software developers, particularly from the HPC domain, to access hardware-accelerated platforms incurring significantly reduced design complexity and overheads.","","CD-ROM:978-1-4673-8317-2; Electronic:978-1-4673-9473-4; POD:978-1-4673-9474-1","10.1109/3PGCIC.2015.135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424619","","Computer architecture;Hardware;Instruction sets;Programming;Registers;Zinc","field programmable gate arrays;parallel processing","FPGA-based accelerators;HDL code;HPC domain;HtComp research project;general-purpose computing;hardware-accelerated platforms;high-level software languages;on-chip interconnect subsystems;parallel applications","","","","24","","","","4-6 Nov. 2015","","IEEE","IEEE Conferences"
"Code Clarity","G. J. Holzmann","NASA/JPL","IEEE Software","20160226","2016","33","2","22","25","Naming conventions affect the readability of your code and the ease with which you can find your way around when you're reviewing that code. Naming conventions aren't meant to help the compiler. A compiler has no trouble distinguishing names, no matter how long, short, or obscure they are. But to us humans, they can matter a great deal.","0740-7459;07407459","","10.1109/MS.2016.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420491","C language;Linux;function names;identifier names;programming;software development;software engineering","Encoding;Linux;Market research;Software reliability;Standards;White spaces","Unix;program compilers","Unix code clarity;code readability;compiler","","1","","1","","","","Mar.-Apr. 2016","","IEEE","IEEE Journals & Magazines"
"Developing, Verifying, and Maintaining High-Quality Automated Test Scripts","V. Garousi; M. Felderer","Hacettepe University","IEEE Software","20160425","2016","33","3","68","75","With the increasing importance, size, and complexity of automated test suites, the need exists for suitable methods and tools to develop, assess the quality of, and maintain test code (scripts) in parallel with regular production (application) code. A recent review paper called this subarea of software testing software test code engineering (STCE). This article summarizes STCE tools, techniques, and guidelines. It also presents specific quantitative examples in this area based on experience in projects and raises important issues practitioners and researchers must address to further advance this field.","0740-7459;07407459","","10.1109/MS.2016.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412621","software engineering;software test code engineering;software testing;test automation;test code;test scripts","Maintenance engineering;Production processes;Quality assessment;Software engineering;Software testing;Test automation","program testing;program verification;software maintenance","STCE;automated test script development;automated test script maintenance;automated test script verification;software test code engineering;software testing","","3","","18","","","20160218","May-June 2016","","IEEE","IEEE Journals & Magazines"
"Turbo Automatic Speech Recognition","S. Receveur; R. Wei√ü; T. Fingscheidt","Institute for Communications Technology, Technische Universit&#228;t Braunschweig, Braunschweig, Germany","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20170520","2016","24","5","846","862","Performance of automatic speech recognition (ASR) systems can significantly be improved by integrating further sources of information such as additional modalities, or acoustic channels, or acoustic models. Given the arising problem of information fusion, striking parallels to problems in digital communications are exhibited, where the discovery of the turbo codes by Berrou et al. was a groundbreaking innovation. In this paper, we show ways how to successfully apply the turbo principle to the domain of ASR and thereby provide solutions to the abovementioned information fusion problem. The contribution of our work is fourfold: First, we review the turbo decoding forward-backward algorithm (FBA), giving detailed insights into turbo ASR, and providing a new interpretation and formulation of the so-called extrinsic information being passed between the recognizers. Second, we present a real-time capable turbo-decoding Viterbi algorithm suitable for practical information fusion and recognition tasks. Then we present simulation results for a multimodal example of information fusion. Finally, we prove the suitability of both our turbo FBA and turbo Viterbi algorithm also for a single-channel multimodel recognition task obtained by using two acoustic feature extraction methods. On a small vocabulary task (challenging, since spelling is included), our proposed turbo ASR approach outperforms even the best reference system on average over all SNR conditions and investigated noise types by a relative word error rate (WER) reduction of 22.4% (audio-visual task) and 18.2% (audio-only task), respectively.","2329-9290;23299290","","10.1109/TASLP.2016.2520364","5th International Workshop on Spoken Dialog Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406701","Speech recognition;hidden Markov models;iterative decoding;multimedia systems;robustness","Acoustics;Convolutional codes;Decoding;Digital communication;Hidden Markov models;Iterative decoding;Speech","Viterbi decoding;audio-visual systems;feature extraction;speech recognition;turbo codes","SNR conditions;acoustic feature extraction method;audio-only task relative WER reduction;audio-visual task relative word error rate reduction;digital communication;groundbreaking innovation;information fusion problem;information recognition;information sources;single-channel multimodel recognition task;small vocabulary task;striking parallels;turbo ASR system performance;turbo automatic speech recognition performance;turbo codes;turbo decoding FBA;turbo decoding forward-backward algorithm;turbo principle;turbo-decoding Viterbi algorithm","","","","64","","","20160212","May 2016","","IEEE","IEEE Journals & Magazines"
"Does Diversity of Papers Affect Their Citations? Evidence from American Physical Society Journals","M. K. Enduri; I. V. Reddy; S. Jolad","Indian Inst. of Technol. Gandhinagar, Chandkheda, India","2015 11th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","20160208","2015","","","505","511","In this work, we study the correlation between interdisciplinarity of papers within physical sciences and their citations by using meta data of articles published in American Physical Society's Physical Review journals between 1985 to 2012. We use the Weitzman diversity index to measure the diversity of papers and authors, exploiting the hierarchical structure of PACS (Physics and Astronomy Classification Scheme) codes. We find that the fraction of authors with high diversity is increasing with time, where as the fraction of least diversity are decreasing, and moderate diversity authors have higher tendency to switch over to other diversity groups. The diversity index of papers is correlated with the citations they received in a given time period from their publication year. Papers with lower and higher end of diversity index receive lesser citations than the moderate diversity papers.","","Electronic:978-1-4673-9721-6; POD:978-1-4673-9722-3","10.1109/SITIS.2015.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7400609","Citation;Diversity;Interdisciplinarity;PACS codes","Cultural differences;Extraterrestrial measurements;Indexes;Market research;Physics;Picture archiving and communication systems","citation analysis;meta data;publishing","American Physical Society Journals;PACS codes;Physics and Astronomy Classification Scheme codes;Weitzman diversity index;citations;meta data;moderate diversity papers;publication year","","","","26","","","","23-27 Nov. 2015","","IEEE","IEEE Conferences"
"SQL Injection: A sample review","S. Mukherjee; P. Sen; S. Bora; C. Pradhan","School of Computer Engineering, KIIT University, Bhubaneswar, India","2015 6th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","20160201","2015","","","1","7","In today's world, SQL Injection is a serious security threat over the Internet for the various dynamic web applications residing over the internet. These Web applications conduct many vital processes in various web-based businesses. As the use of internet for various online services is rising, so is the security threats present in the web increasing. There is a universal need present for all dynamic web applications and this universal need is the need to store, retrieve or manipulate information from a database. Most of systems which manage the databases and its requirements such as MySQL Server and PostgreSQL use SQL as their language. Flexibility of SQL makes it a powerful language. It allows its users to ask what he/she wants without leaking any information about how the data will be fetched. However the vast use of SQL based databases has made it the center of attention of hackers. They take advantage of the poorly coded Web applications to attack the databases. They introduce an apparent SQL query, through an unauthorized user input, into the legitimate query statement. In this paper, we have tried to present a comprehensive review of all the different types of SQL injection attacks present, as well as detection of such attacks and preventive measure used. We have highlighted their individual strengths and weaknesses. Such a classification would help other researchers to choose the right technique for further studies.","","CD-ROM:978-1-4799-7983-7; Electronic:978-1-4799-7984-4; POD:978-1-4799-7985-1","10.1109/ICCCNT.2015.7395166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395166","Detection;Information security;Prevention;SQL injection;Web application vulnerability","Databases;Decision support systems;Irrigation;Lead;Servers","Internet;SQL;pattern classification;query processing;relational databases;security of data","Internet;SQL based database;SQL injection attack;SQL query;Web application;attack classification;security threat","","","","19","","","","13-15 July 2015","","IEEE","IEEE Conferences"
"CaptainTeach: multi-stage, in-flow peer review for programming assignments","J. G. Politz; D. Patterson; S. Krishnamurthi; K. Fisler","Brown University, Providence, RI, USA","Proceedings of the 2014 conference on Innovation & technology in computer science education","20160129","2014","","","267","272","<p>Computing educators have used peer review in various ways in courses at many levels. Few of these efforts have applied peer review to multiple deliverables (such as specifications, tests, and code) within the same programming problem, or to assignments that are still in progress (as opposed to completed). This paper describes CaptainTeach, a programming environment enhanced with peer-review capabilities at multiple stages within assignments in progress. Multi-stage, in-flow peer review raises many logistical and pedagogical issues. This paper describes CaptainTeach and our experience using it in two undergraduate courses (one first-year and one upper-level); our analysis emphasizes issues that arise from the conjunction of multiple stages and in-flow reviewing, rather than peer review in general.</p>","","","10.1145/2591708.2591738","","","learning environments;peer review;testing","","","","","1","","","","","","21-25 June 2014","","ACM","ACM Conferences"
"The future of accelerator programming: abstraction, performance or can we have both?","K. Rocki; M. Burtscher; R. Suda","IBM Research, San Jose, CA and University of Tokyo, Bunkyo-ku, Tokyo, Japan","Proceedings of the 29th Annual ACM Symposium on Applied Computing","20160129","2014","","","886","895","<p>In a perfect world, code would only be written once and would run on different devices with high efficiency. To a degree, that used to be the case in the era of frequency scaling on a single core. However, due to power limitations, parallel programming has become necessary to obtain performance gains. But parallel architectures differ substantially from each other, often require specialized knowledge to exploit them, and typically necessitate reimplementation and fine tuning of programs. These slow tasks frequently result in situations where most of the time is spent reimplementing old rather than writing new code.</p> <p>The goal of our research is to find programming techniques that increase productivity, maintain high performance, and provide abstraction to free the programmer from these unnecessary and time-consuming tasks. However, such techniques usually come at the cost of substantial performance degradation. This paper investigates current approaches to portable accelerator programming, seeking to answer whether they make it possible to combine high efficiency with sufficient algorithm abstraction. It discusses OpenCL as a potential solution and presents three approaches of writing portable code: GPU-centric, CPU-centric, and combined. By applying the three approaches to a real-world program, we show that it is at least sometimes possible to run exactly the same code on many different devices with minimal performance degradation using parameterization. The main contributions of this paper are an extensive review of the current state-of-the-art and our original approach of addressing the stated problem with the <i>copious-parallelism</i> technique.</p>","","","10.1145/2554850.2555029","","","","","","","","1","","","","","","24-28 March 2014","","ACM","ACM Conferences"
"An historical examination of open source releases and their vulnerabilities","N. Edwards; L. Chen","Hewlett-Packard Laboratories, Bristol, United Kingdom","Proceedings of the 2012 ACM conference on Computer and communications security","20160129","2012","","","183","194","<p>This paper examines historical releases of Sendmail, Postfix, Apache httpd and OpenSSL by using static source code analysis and the entry-rate in the Common Vulnerabilities and Exposures dictionary (CVE) for a release, which we take as a measure of the rate of discovery of exploitable bugs. We show that the change in number and density of issues reported by the source code analyzer is indicative of the change in rate of discovery of exploitable bugs for new releases --- formally we demonstrate a statistically significant correlation of moderate strength. The strength of the correlation is an artifact of other factors such as the degree of scrutiny: the number of security analysts investigating the software. This also demonstrates that static source code analysis can be used to make some assessment of risk even when constraints do not permit human review of the issues identified by the analysis.</p> <p>We find only a weak correlation between absolute values measured by the source code analyzer and rate of discovery of exploitable bugs, so in general it is unsafe to use absolute values of number of issues or issue densities to compare different applications or software. Our results demonstrate that software quality, as measured by the number of issues, issue density or number of exploitable bugs, does not always improve with each new release. However, generally the rate of discovery of exploitable bugs begins to drop three to five years after the initial release.</p>","","","10.1145/2382196.2382218","","","open source software;risk analysis;static analysis","","","","","3","","","","","","16-18 Oct. 2012","","ACM","ACM Conferences"
"The Prickly Pear Archive: a portable hypermedia for scholarly publication","","","Proceedings of the 1st Conference of the Extreme Science and Engineering Discovery Environment","20160129","2012","","","1","8","<p>An <i>executable paper</i> is a hypermedia for publishing, reviewing, and reading scholarly papers which include a complete HPC software development or scientific code. A <i>hypermedia</i> is an integrated interface to multimedia including text, figures, video, and executables, on a subject of interest. Results within the executable paper include numeric output, graphs, charts, tables, equations and the underlying codes which generated such results. These results are dynamically regenerated and included in the paper upon recompilation and re-execution of the code. This enables a scientifically enriched environment which functions not only as a journal but as a laboratory in itself, in which readers and reviewers may interact with and validate the results.</p> <p>The Prickly Pear Archive (PPA) is such a system [2]. One distinguishing feature of the PPA is the inclusion of an underlying component-based simulation framework, Cactus [8], which simplifies the process of composing, compiling, and executing simulation codes. Code creation is simplified using common bits of infrastructure; each paper augments to the functionality of the framework. New distinguishing features include the (1) portability and (2) reproducibility of the archive, which allow researchers to move and re-create the software environment in which the simulation code was created. Further, the (3) Piraha parser is now used to match complex multi-line expressions inside parameter and LaTEX files. Finally, (4) an altogether new web interface has been created. The new interface options closely mirror the directory structure within the paper itself, which gives the reader a transparent view of the paper. Thus, once accustomed to reading from the archive, assembling a paper package becomes a straightforward and intuitive process.</p> <p>A PPA production system hosted on HPC resources (e.g. an XSEDE machine) unifies the computational scientific process with the publication process. A researcher may use t- e production archive to test simulations; and upon arriving at a scientifically meaningful result, the user may then incorporate the result in an executable paper on the very same resource the simulation was conducted. Housed within a virtual machine, the PPA allows multiple accounts within the same production archive, enabling users across campuses to bridge their efforts in developing scientific codes.</p>","","","10.1145/2335755.2335840","","","","","","","","","","","","","","16-20 July 2012","","ACM","ACM Conferences"
"Using peer review to teach software testing","J. Smith; J. Tessler; E. Kramer; C. Lin","University of Texas at Austin, Austin, TX, USA","Proceedings of the ninth annual international conference on International computing education research","20160129","2012","","","93","98","<p>This paper explains how peer review can be used to teach software testing, an important skill that is typically not carefully taught in most programming courses. The goals of such peer review are (1) to frame testing as a fun and competitive activity, (2) to allow students to learn from each other, (3) to demonstrate the importance of testing by uncovering latent bugs in the students' code, and (4) to provide a mechanism for evaluating testing skills. This paper explains how we added peer review to an honors data structure course without significantly reducing its heavy programming load. We evaluate our intervention by summarizing surveys of student attitudes taken throughout the course.</p>","","","10.1145/2361276.2361295","","","education;peer review;software testing","","","","","1","","","","","","9-11 Sept. 2012","","ACM","ACM Conferences"
"What influences dwell time during source code reading?: analysis of element type and frequency as factors","T. Busjahn; R. Bednarik; C. Schulte","Freie Universit&#228;t Berlin","Proceedings of the Symposium on Eye Tracking Research and Applications","20160129","2014","","","335","338","<p>While knowledge about reading behavior in natural-language text is abundant, little is known about the visual attention distribution when reading source code of computer programs. Yet, this knowledge is important for teaching programming skills as well as designing IDEs and programming languages. We conducted a study in which 15 programmers with various expertise read short source codes and recorded their eye movements. In order to study attention distribution on code elements, we introduced the following procedure: First we (pre)-processed the eye movement data using log-transformation. Taking into account the word lengths, we then analyzed the time spent on different lexical elements. It shows that most attention is oriented towards understanding of identifiers, operators, keywords and literals, relatively little reading time is spent on separators. We further inspected the attention on keywords and provide a description of the gaze on these primary building blocks for any formal language. The analysis indicates that approaches from research on natural-language text reading can be applied to source code as well, however not without review.</p>","","","10.1145/2578153.2578211","","","code reading;eye tracking;program comprehension","","","","","2","","","","","","26-28 March 2014","","ACM","ACM Conferences"
"Explorations of studio-based learning in computing education","C. Hundhausen","Washington State University, Pullman, WA","Proceedings of the Seventeenth Western Canadian Conference on Computing Education","20160129","2012","","","1","1","<p>While the demand for professionals in software engineering continues to grow, the output of postsecondary degree programs in computing has declined by 50% over the past seven years, and retention rates have remained below 30%. Moreover, with their traditional emphasis on individual problem-solving and skeptical stance toward student collaboration, these degree programs may not be adequately preparing students for careers in the computing profession, in which ""soft skills"" such as collaboration, teamwork, and communication are becoming increasingly important. How can computing educators respond to this state of affairs? One intriguing possibility is to adapt the studio-based learning (SBL) model--the centerpiece of architecture and fine arts education for over a century--for computing education. Rooted in social constructivist learning theory, the SBL model consists of two key components: the design studio, a shared physical space in which students work on solutions to assigned problems; and design crits, in which students present their evolving solutions to peers and experts for critical review. Over the past decade, my colleagues and I have been empirically exploring adaptations of SBL design crits in computing education. In this talk, I will begin by presenting two strands of this work: (a) pedagogical code reviews--an adaptation of the code inspection commonly used in the software industry; and (b) the use of prototype walkthroughs--a pedagogical version of early prototyping studies commonly used in the user-centered design process. Both of these strands of research provide evidence that SBL design crits can fruitfully be used in computing education in order to develop critical thinking and analysis skills, to effect positive attitudinal shifts, and to provide rich opportunities for participation in authentic practices. However, because of the logistical constraints of computing education, the design studio component of SBL proves challenging to adapt. My tal- will conclude with a futuristic vision that leverages contemporary trends in online social networking in order to create ""virtual"" design studios that bring together communities of computing students, mentors, instructors, and industry professionals in unprecedented ways..</p>","","","10.1145/2247569.2247570","","","","","","","","","","","","","","4-5 May 2012","","ACM","ACM Conferences"
"LASE: an example-based program transformation tool for locating and applying systematic edits","J. Jacobellis; N. Meng; M. Kim","University of Texas at Austin, USA","Proceedings of the 2013 International Conference on Software Engineering","20160129","2013","","","1319","1322","<p> Adding features and fixing bugs in software often require systematic edits which are similar, but not identical, changes to many code locations. Finding all edit locations and editing them correctly is tedious and error-prone. In this paper, we demonstrate an Eclipse plug-in called LASE that (1) creates context-aware edit scripts from two or more examples, and uses these scripts to (2) automatically identify edit locations and (3) transform the code. In LASE, users can view syntactic edit operations and corresponding context for each input example. They can also choose a different subset of the examples to adjust the abstraction level of inferred edits. When LASE locates target methods matching the inferred edit context and suggests customized edits, users can review and correct LASEs edit suggestion. These features can reduce developers burden in repetitively applying similar edits to different methods. The tools video demonstration is available at https://www.youtube.com/ watch?v=npDqMVP2e9Q. </p>","","","","","","","","","","","","","","","","","18-26 May 2013","","ACM","ACM Conferences"
"Parallel verlet neighbor list algorithm for GPU-optimized MD simulations","T. J. Lipscomb; A. Zou; S. S. Cho","Wake Forest University, Winston-Salem, NC","Proceedings of the ACM Conference on Bioinformatics, Computational Biology and Biomedicine","20160129","2012","","","321","328","<p>Understanding protein and RNA biomolecular folding and assembly processes have important applications because misfolding is associated with diseases like Alzheimer's and Parkinson's. However, simulating biologically relevant biomolecules on timescales that correspond to biological functions is an extraordinary challenge due to bottlenecks that are mainly involved in force calculations. We briefly review the molecular dynamics (MD) algorithm and highlight the main bottlenecks, which involve the calculation of the forces that interact between its substituent particles. We then present new GPU-specific performance optimization techniques for MD simulations, including 1) a parallel Verlet Neighbor List algorithm that is readily implemented using the CUDPP library and 2) a bitwise shift type compression algorithm that decreases data transfer with GPUs. We also evaluate the single vs. double precision implementation of our MD simulation code using well-established biophysical metrics, and we observe negligible differences. The GPU performance optimizations are applied to coarse-grained MD simulations of the ribosome, a protein-RNA molecular machine for protein synthesis composed of 10,219 residues and nucleotides. We observe a size-dependent speedup of 30x of the GPU-optimized MD simulation code on a single GPU over the single core CPU-optimized approach for the full 70s ribosome when all optimizations are taken into account.</p>","","","10.1145/2382936.2382977","","","CUDPP;CURAND;coarse-grained MD simulations;energy drift;floating point analysis","","","","","1","","","","","","7-10 Oct. 2012","","ACM","ACM Conferences"
"Instructor-centric source code plagiarism detection and plagiarism corpus","J. Y. H. Poon; K. Sugiyama; Y. F. Tan; M. Y. Kan","National University of Singapore, Singapore, Singapore","Proceedings of the 17th ACM annual conference on Innovation and technology in computer science education","20160129","2012","","","122","127","<p>Existing source code plagiarism systems focus on the problem of identifying plagiarism between pairs of submissions. The task of detection, while essential, is only a small part of managing plagiarism in an instructional setting. Holistic plagiarism detection and management requires coordination and sharing of assignment similarity -- elevating plagiarism detection from pairwise similarity to cluster-based similarity; from a single assignment to a sequence of assignments in the same course, and even among instructors of different courses.</p> <p>To address these shortcomings, we have developed Student Submissions Integrity Diagnosis (SSID), an open-source system that provides holistic plagiarism detection in an instructor-centric way. SSID's visuals show overviews of plagiarism clusters throughout all assignments in a course as well as highlighting most-similar submissions on any specific student. SSID supports plagiarism detection workflows; <i>e.g.</i>, allowing student assistants to flag suspicious assignments for later review and confirmation by an instructor with proper authority. Evidence is automatically entered into SSID's logs and shared among instructors.</p> <p>We have additionally collected a source code plagiarism corpus, which we employ to identify and correct shortcomings of previous plagiarism detection engines and to optimize parameter tuning for SSID deployment. Since its deployment, SSID's workflow enhancements have made plagiarism detection in our faculty less tedious and more successful.</p>","","","10.1145/2325296.2325328","","","corpus studies;plagiarism assessment;plagiarism detection;programming;similarity;user interface","","","","","","","","","","","3-5 July 2012","","ACM","ACM Conferences"
"Acoustic telemetry for the last decade and challenges for the next one","A. B. Baggeroer","Massachusetts Institute of Technology, Cambridge MA","Proceedings of the Eighth ACM International Conference on Underwater Networks and Systems","20160129","2013","","","1","1","<p>The review article [Kilfoyle and Baggeroer, <i>IEEE Journal of Oceanic Engineering</i>, 25(1), 4-27 (2000)] surveyed the literature for underwater acoustic communication up to January 2000. This presentation attempts to update the literature review since then. Some themes for the next decade are also suggested. The emphasis is on the physical layer since there is ample room for performance improvements at this layer in the context of acoustic communication networks. Important components for the physical layer in acoustic communication systems include 1) models of the underwater acoustic channel, 2) transversal and time reversal equalizer, as well as equalizer training, 3) diversity, direct spread spectrum, OFDM, MIMO coding, vectors sensors, stealth, and 4) some perspectives on networks. A review of the last decades and perspectives for the next on these topics are considered.</p>","","","10.1145/2532378.2541578","","","","","","","","","","","","","","11-13 Nov. 2013","","ACM","ACM Conferences"
"An industrial study on the risk of software changes","E. Shihab; A. E. Hassan; B. Adams; Z. M. Jiang","Queen's University, Canada","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","20160129","2012","","","1","11","<p>Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., <i>changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing</i>. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67%, and a precision improvement of 87% (using developer specific models) and 37% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects.</p>","","","10.1145/2393596.2393670","","","bug inducing changes;change metrics;change risk;code metrics","","","","","15","","","","","","11-16 Nov. 2012","","ACM","ACM Conferences"
"A mid-career review of teaching computer science I","A. N. Kumar","Ramapo College of New Jersey, Mahwah, NJ, USA","Proceeding of the 44th ACM technical symposium on Computer science education","20160129","2013","","","531","536","<p>A mid-career review is presented, of how the teaching of Computer Science I has changed for this instructor over the last two decades. The content of the course has evolved to include algorithm development and program design. Assessment in the course has gone online and moved away from testing how clever the student is, to how much the student has learned in the course. Professional practices are now covered that help students understand and incorporate preferred practices of the discipline. Changes incorporated into the pedagogy include going from using anthropomorphic and ad-hoc to discipline-specific and consistent vocabulary, and from writing code in the class like an experienced programmer to writing it to suit a beginning learner. It is hoped that this review will help new Computer Science I instructors avoid some misconceptions with which this instructor started out.</p>","","","10.1145/2445196.2445352","","","assessment;computer science I;pedagogy;problem-solving;professional practices;program design","","","","","","","","","","","6-9 March 2013","","ACM","ACM Conferences"
"Automated architectural evaluation of web information systems","F. Pinto; U. Kulesza; E. Guerra; J. M. J√∫nior; L. Silva","Federal University of Rio Grande do Norte / Federal Institute of Education, Science and Technology of Rio Grande do Norte, Natal, Brazil","Proceedings of the 19th Brazilian symposium on Multimedia and the web","20160129","2013","","","225","232","<p>Traditional scenario-based architectural analysis methods rely on manual review-based evaluation that requires advanced skills from architects and developers. They are usually applied when the architecture is under development, but before its implementation has begun. The system implementation is one additional and fundamental element that should be used and considered during the software architecture evaluation. In this paper, we propose an approach to add information, which ideally should come from traditional evaluation methods, about scenarios and quality attributes to the source code of web-based systems using metadata. The main aim is to enable the automatic architecture analysis by producing a report with information about scenarios, quality attributes and source code assets, such as: (i) the potential tradeoff points among quality attributes, (ii) the execution time for scenarios and if it has failed or not. Up to now, the approach has been applied mainly to web-based systems, but it can be adapted to other software domains. The paper also presents the tool used to perform static and dynamic analysis, and the results of its application to an e-commerce web system and an enterprise information web system.</p>","","","10.1145/2526188.2526193","","","architecture;automatic evaluation;quality attributes;scenario;scenario-based methods;web systems","","","","","","","","","","","5-8 Nov. 2013","","ACM","ACM Conferences"
"Modern version control: creating an efficient development ecosystem","N. Bertino","Santa Clara University School of Law, Santa Clara, CA, USA","Proceedings of the 40th annual ACM SIGUCCS conference on User services","20160129","2012","","","219","222","<p>In 2011, Santa Clara University School of Law Technology and Academic Computing (LTAC) identified that its version control system could greatly benefit from the use of modern source control management software. Source code for high value projects, such as the Santa Clara Law website, were previously held in a Subversion (SVN) repository in a client-server model, providing version control and redundancy. Because of the resource footprint associated with SVN, only projects with high importance could be setup with version control. As more web-based applications were introduced, the need for a more efficient revision control system arose. Git, a highly efficient decentralized version control system (DVCS), was selected after evaluating similar technologies. This change transformed the entire development process, making the development cycle more streamlined and with greater flexibility. In the early use of Git, LTAC also discovered its use as a deployment tool, increasing redundancy on servers and reducing overhead usually associated with revision control. It also serves as the vital link between LTAC's issue tracking system, Redmine, and the development team. The introduction of Redmine has helped LTAC monitor website issues, manage projects, and continually review changes to the code base. LTAC has created a development ecosystem that provides redundancy and accountability using open source products that carry no cost. Git has significant performance gains over SVN, making its integration and use less frustrating and distracting for developers. Redmine gives developers and customers the opportunity to organize, track, and resolve issues. The flexibility of the technology used means that any project, from a content management system to a one-off script, can benefit from source control without large costs or long deployment times.</p>","","","10.1145/2382456.2382510","","","GIT;automated deployment;decentralized version control;issue tracking;redmine;source control","","","","","","1","","","","","15-19 Oct. 2012","","ACM","ACM Conferences"
"Towards the assessment of semantic similarity analysis of protein data: main approaches and issues","P. H. Guzzi; M. Mina","University of Catanzaro","ACM SIGBioinformatics Record","20160129","2012","2","3","17","18","<p>Bioinformatics approaches to the study of proteins yield to the introduction of different methodologies and related tools for the analysis of different types of data related to proteins, ranging from primary, secondary and tertiary structures to interaction data [1], not to mention functional knowledge.</p> <p>One of the most advanced tools for encoding and representing functional knowledge in a formal way is the Gene Ontology (GO) [2,3]. It is composed of three ontologies, named Biological Process (BP), Molecular Function (MF) and Cellular Component (CC). Each ontology consists of a set of terms (GO terms) representing different functions, biological processes and cellular components within the cell. GO terms are connected each other to form a hierarchical graph. Terms representing similar functions are close to each other within this graph.</p> <p>Biological molecules are associated with GO terms that represent their functions, biological roles and localization. This process, usually referred to as annotation process, can be performed under the supervision of an expert or in a fully automated way. Obviously, computationally inferred annotations, commonly known as Electronically Inferred Annotation (IEA), are not as reliable as experimentally determined annotations. For this reason every annotation is labeled with an Evidence Code (EC) that keeps track of the type of process used to produce the annotation itself. Considering the release of annotations of April, 2010, about the 98% of all the annotations is an IEA annotation [4].</p> <p>The term annotation corpus is commonly used to identify all the annotations involving a set of proteins or genes, usually referring the whole proteomes and genomes (i.e. the annotation corpus of yeast). For lack of space we do not further describe the Gene Ontology. A comprehensive review has been provided by du Plessis et al. [4] and by Guzzi et al. [5].</p> <p>The availability of well formalized functional data enabled the use - f computational methods to analyse genes and proteins from the functional point of view. For example, a set of algorithms, known as functional enrichment algorithms, have been developed to determine the statistical significance of the presence (or the absence) of a GO Term in a set of gene products. A detailed review of these algorithms can be found in [4].</p> <p>An interesting problem is how to express quantitatively the relationships between GO terms. Several measures, referred to as (term) semantic similarity (SS) measures, has been introduced in the last decade. Given two or more GO terms, they try to quantify the similarity of the functional aspects represented by the terms within the cell. Exploiting annotation corpora, semantic similarity measures have been further extended to the evaluation of the similarity of genes and proteins on the basis of their annotations.</p> <p>Many different works have focused on the following tasks: (i) the definition of ad-hoc semantic similarity measures tailored to the characteristics of Gene Ontology; (ii) the definition of measures of comparison of genes and proteins; (iii) the introduction of methodologies for the systematic assessment of semantic similarity measures; (iv) the use of semantic similarity measures in many different contexts and applications. Despite its relevance, the application of semantic similarity for the systematic analysis of protein data is still an open research area. There are, in fact, two main questions that have to be addressed: (i) the systematic assessment of SS with respect to other biological features, i.e. how much an high or a low value of SS is biologically meaningful; (ii) how reliable are the SS themselves, i.e. is there any systematic error or bias in the calculation of SS? Both these problems are relevant for the diffusion of SS measures; while in the first case several approaches have been proposed, confronting SS measures with a pletora of different biological features, only few wo","2159-1210;21591210","","10.1145/2384691.2384694","","","","","","","","1","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"Want to green application software?: mind the target hardware","V. G. Moshnyaga","Fukuoka University, Fukuoka, Japan","Proceedings of the 2013 workshop on Green in/by software engineering","20160129","2013","","","1","2","<p>With the explosive use of Information Technology (IT) in modern life, ""green"" or sustainable solutions capable of minimizing IT impact on the environment become very important. Research on green IT has already produced a variety of approaches aiming at reducing energy usage for data centers and technical equipment, such as computers, projectors, etc. Also methods have been proposed to improve energy efficiency of compilers and software. While focusing on code transformation, the software optimization methods frequently ignore operational features of target hardware and therefore have a limited effect in practice. In this talk we urge for software ""greening"" techniques that increase efficiency of energy reduction schemes of target hardware. The talk starts with a brief review of factors of energy consumption and main hardware schemes used in the state-of-the-art portable battery-operated devices. Then it discusses software optimizations for lowering energy consumption of CPU, memory, HDD, display, and communication. We argue that algorithm and software optimizations capable of improving computational and data efficiency, exploit adaptive, context-aware and quality-aware processing can all contribute to ""green"" applications. Open problems of ""green"" software development are also outlined.</p>","","","10.1145/2451605.2451607","","","adaptive;algorithm;context-aware;efficiency;energy consumption;hardware;low-power;optimization;quality-aware;software","","","","","","","","","","","26-26 March 2013","","ACM","ACM Conferences"
"Object-oriented design in feature-oriented programming","S. Schuster; S. Schulze","TU Braunschweig, Braunschweig, Germany","Proceedings of the 4th International Workshop on Feature-Oriented Software Development","20160129","2012","","","25","28","<p>Object-oriented programming is the state-of-the-art programming paradigm for developing large and complex software systems. To support the development of maintainable and evolvable code, a developer can rely on different mechanisms and concepts such as <i>inheritance</i> and <i>design patterns</i>. Recently, feature-oriented programming (FOP) gained attention, specifically for developing software product lines (SPLs). Although FOP is an own paradigm with dedicated language mechanisms, it partly relies on object-oriented programming. However, only little is known about feature-oriented design and <i>how</i> object-oriented design mechanisms and design principles are used within FOP. In this paper, we want to raise awareness on design patterns in FOP and stimulate discussion on related topics. To this end, we present an exemplary review of using OO design patterns in FOP and limitations thereof from our perspective. Subsequently, we formulate questions that are open and that we think are worth to discuss in the context of feature-oriented design.</p>","","","10.1145/2377816.2377820","","","design pattern;feature-oriented programming","","","","","","","","","","","24-25 Sept. 2012","","ACM","ACM Conferences"
"When food quality control in china meets mobile and wireless technology: interactions, potentials and pitfalls","S. M. Liu","Fudan University, Yangpu District, Shanghai, PRC","Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance","20160129","2013","","","134","143","<p>With the fast economic development and industrial evolution in China, the surge of product quality and safety control issue has become one of the top concerns country wide in recent years. On one hand, citizens, academics and nonprofit organizations urged government to further refine product quality supervision strategies and install new food safety monitoring mechanisms (Qu, 2009). On the other, because of the popularity of wireless sensor systems and mobile data collection and transferring technologies in goods production, transferring and processing worldwide, they quickly gained their popularity among Chinese national quality control administration (Ruiz-Garcia et al, 2009; Jiang, 2008). The purpose of this paper is to conduct a comprehensive review of China's newly installed mobile and wireless quality control system based on the analysis of both first hand and second hand data. Specifically the study focuses on how changes in monitoring and tracing technologies interact with the current quality control and food safety administration, goods production and supply, and inspection systems. The paper concludes that although China's food quality control administration did attempt to bring new technology to enforce quality control code, the current implementation of mobile and wireless monitoring tools still demonstrates a highly reactive nature. In addition, the distribution of mobile and wireless monitoring device is still very uneven when it comes to different types of food. This to certain extent prevents the government from employing the new tools to their full extent to establish a meaningful monitoring system for further food quality crisis prevention.</p>","","","10.1145/2591888.2591910","","","food safety;mobile and wireless technology;monitoring technology","","","","","","","","","","","22-25 Oct. 2013","","ACM","ACM Conferences"
"Statistical quality estimation for general crowdsourcing tasks","Y. Baba; H. Kashima","The University of Tokyo, Tokyo, Japan","Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining","20160129","2013","","","554","562","<p>One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing, which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose an unsupervised statistical quality estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure; multiple workers are first requested to work on the same tasks in the creation stage, and then another set of workers review and grade each artifact in the review stage. We model the ability of each author and the bias of each reviewer, and propose a two-stage probabilistic generative model using the graded response model in the item response theory. Experiments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods, which implies that our method can deliver high quality results with lower costs.</p>","","","10.1145/2487575.2487600","","","crowdsourcing;human computation;quality control","","","","","10","","","","","","11-14 Aug. 2013","","ACM","ACM Conferences"
"Teaching composition quality at scale: human judgment in the age of autograders","J. DeNero; S. Martinis","UC Berkeley, Berkeley, CA, USA","Proceedings of the 45th ACM technical symposium on Computer science education","20160129","2014","","","421","426","<p>We describe an effort to improve the composition quality of student programs: the property that a program can be understood effectively by another person. As a semester-long component of UC Berkeley's first course for majors, CS 61A, we gave students composition guidelines, scores, and qualitative feedback-all generated manually by a course staff of 10 graders for over 700 students. To facilitate this effort, we created a new online tool that allows instructors to provide feedback efficiently at scale. Our system differs from recently developed alternatives in that it is a branch of an industrial tool originally developed for internal code reviews at Google and used extensively by the open-source community. We found that many of the features designed for industrial applications are well-suited for instructional use as well. We extended the system with permissions controls and comment memories tailored for giving educational feedback.</p> <p>Using this tool improved the consistency of the feedback we gave to students, the efficiency of generating that feedback, and our ability to communicate that feedback to students. Emphasizing composition throughout the course improved the composition of our students' code. The quality of student programs improved by a statistically significant margin (p<0.01) over those from a previous semester, measured by a blind comparison of student submissions.</p>","","","10.1145/2538862.2538976","","","introductory programming;web-based feedback","","","","","","","","","","","5-8 March 2014","","ACM","ACM Conferences"
"When open source turns cold on innovation - the challenges of navigating licensing complexities in new research domains","C. Forbes; I. Keivanloo; J. Rilling","Concordia University, Canada","Proceedings of the 34th International Conference on Software Engineering","20160129","2012","","","1447","1448","<p> In this poster, we review the limitations open source licences introduce to the application of Linked Data in Software Engineering. We investigate whether open source licences support special requirements to publish source code as Linked Data on the Internet. </p>","","","","","","","","","","","","","","","","","2-9 June 2012","","ACM","ACM Conferences"
"A survey of appearance models in visual object tracking","X. Li; W. Hu; C. Shen; Z. Zhang; A. Dick; A. V. D. Hengel","NLPR, Institute of Automation, Chinese Academy of Sciences and The University of Adelaide","ACM Transactions on Intelligent Systems and Technology (TIST)","20160129","2013","4","4","1","48","<p>Visual object tracking is a significant computer vision task which can be applied to many domains, such as visual surveillance, human computer interaction, and video compression. Despite extensive research on this topic, it still suffers from difficulties in handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion. Therefore, effective modeling of the 2D appearance of tracked objects is a key issue for the success of a visual tracker. In the literature, researchers have proposed a variety of 2D appearance models.</p> <p>To help readers swiftly learn the recent advances in 2D appearance models for visual object tracking, we contribute this survey, which provides a detailed review of the existing 2D appearance models. In particular, this survey takes a module-based architecture that enables readers to easily grasp the key points of visual object tracking. In this survey, we first decompose the problem of appearance modeling into two different processing stages: visual representation and statistical modeling. Then, different 2D appearance models are categorized and discussed with respect to their composition modules. Finally, we address several issues of interest as well as the remaining challenges for future research on this topic.</p> <p>The contributions of this survey are fourfold. First, we review the literature of visual representations according to their feature-construction mechanisms (i.e., local and global). Second, the existing statistical modeling schemes for tracking-by-detection are reviewed according to their model-construction mechanisms: generative, discriminative, and hybrid generative-discriminative. Third, each type of visual representations or statistical modeling techniques is analyzed and discussed from a theoretical or practical viewpoint. Fourth, the existing benchmark resources (e.g., source codes and video datasets) are examined in this survey.</p>","2157-6904;21576904","","10.1145/2508037.2508039","","","Visual object tracking;appearance model;features;statistical modeling","","","","","101","1","","","","","September 2013","","ACM","ACM Journals & Magazines"
"Understanding software evolution: the maisqual ant data set","B. Baldassari; P. Preux","SQuORING Technologies, France","Proceedings of the 11th Working Conference on Mining Software Repositories","20160129","2014","","","424","427","<p> Software engineering is a maturing discipline which has seen many drastic advances in the last years. However, some studies still point to the lack of rigorous and mathematically grounded methods to raise the field to a new emerging science, with proper and reproducible foundations to build upon. Indeed, mathematicians and statisticians do not necessarily have software engineering knowledge, while software engineers and practitioners do not necessarily have a mathematical background. </p> <p> The Maisqual research project intends to fill the gap between both fields by proposing a controlled and peer-reviewed data set series ready to use and study. These data sets feature metrics from different repositories, from source code to mail activity and configuration management meta data. Metrics are described and commented, and all the steps followed for their extraction and treatment are described with contextual information about the data and its meaning. </p> <p> This article introduces the Apache Ant weekly data set, featuring 636 extracts of the project over 12 years at different levels of artefacts ‚Äì application, files, functions. By associating community and process related information to code extracts, this data set unveils interesting perspectives on the evolution of one of the great success stories of open source. </p>","","","10.1145/2597073.2597136","","","Metrics","","","","","1","","","","","","May 31 2014-June 1 2014","","ACM","ACM Conferences"
"Modelling software engineering research with RSML","H. Jordan; S. Beecham; G. Botterweck","Lero, University of Limerick, Ireland","Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering","20160129","2014","","","1","10","<p><i>Background</i>. In order to understand research on a particular computing topic, practitioners and researchers often need to obtain an overview of its research methods. Current research methods coding schemes either capture insufficient details to support a full critical assessment, or are specialised to a particular research type. <i>Aim</i>. This paper defines and demonstrates RSML, a Research Schema Modelling Language that captures a high level of detail and is applicable to most types of computing research. <i>Method</i>. RSML was designed using concepts from the research methods literature, then refined inductively. An RSML editor was created to assist coders and help reduce coding errors. To demonstrate the feasibility of modelling research with RSML, and to exemplify the summary information that can be derived from a database of RSML encodings, a trial review of 24 articles from one journal was conducted. <i>Results</i>. The review illustrates quantitatively the journal's focus on artifact construction and empiricism. It also reveals that observations are rarely used to inform artifact construction, and purely empirical studies are scarce. <i>Conclusion</i>. RSML can be used to model sophisticated, multifaceted research spanning a wide range of software engineering topics, yielding insights that are not easily captured by current coding schemes.</p>","","","10.1145/2601248.2601295","","","categorisation;checklists;modelling;research methods","","","","","1","","","","","","13-14 May 2014","","ACM","ACM Conferences"
"Automatic generation of dynamic virtual fences as part of BIM-based prevention program for construction safety","A. Hammad; S. Setayeshgar; C. Zhang; Y. Asen","Concordia University, Montreal, Canada","Proceedings of the 2012 Winter Simulation Conference","20160129","2012","","","1","10","<p>The present research aims to investigate a new method for the automatic generation of Dynamic Virtual Fences (DVFs) as part of a BIM-based prevention program for construction safety following the Safety Code of Quebec Provence in Canada. First, the Safety Code is reviewed to identify the information that has spatial aspects that can be represented in BIM. Then, a method is proposed for automatically identifying falling and collision risks and generating DVFs. In this method, workspaces are generated in BIM based on Work Breakdown Structure (WBS) deliverables, the project schedule, the dimensions of equipment, and the geometry of the building. One set of DVFs for collision prevention is generated based on those workspaces. Another set of DVFs is generated where physical barriers are needed for fall prevention. The generated DVFs are used coupled with Real-time Location System (RTLS) tracking of workers and physical fences to check safety requirements and to provide safety warnings.</p>","","","","","","","","","","","","","","","","","9-12 Dec. 2012","","ACM","ACM Conferences"
"Why are industrial agile teams using metrics and how do they use them?","E. Kupiainen; M. V. M√§ntyl√§; J. Itkonen","Aalto University, Finland","Proceedings of the 5th International Workshop on Emerging Trends in Software Metrics","20160129","2014","","","23","29","<p> Agile development methods are increasing in popularity, yet there are limited studies on the reasons and use of metrics in industrial agile development. This paper presents preliminary results from a systematic literature review. Based on our study, metrics and their use are focused to the following areas: Iteration planning, Iteration tracking, Motivating and improving, Identifying process problems, Pre-release quality, Post-release quality and Changes in processes or tools. The findings are mapped against agile principles and it seems that the use of metrics supports the principles with some deviations. Surprisingly, we find little evidence of the use of code metrics. Also, we note that there is a lot of evidence on the use of planning and tracking metrics. Finally, the use of metrics to motivate and enforce process improvements as well as applicable quality metrics can be interesting future research topics. </p>","","","10.1145/2593868.2593873","","","Agile software development;measurement;metrics","","","","","","1","","","","","3-3 June 2014","","ACM","ACM Conferences"
"An exploratory study of the pull-based software development model","G. Gousios; M. Pinzger; A. v. Deursen","Delft University of Technology, Netherlands","Proceedings of the 36th International Conference on Software Engineering","20160129","2014","","","345","355","<p> The advent of distributed version control systems has led to the development of a new paradigm for distributed software development; instead of pushing changes to a central repository, developers pull them from other repositories and merge them locally. Various code hosting sites, notably Github, have tapped on the opportunity to facilitate pull-based development by offering workflow support tools, such as code reviewing systems and integrated issue trackers. In this work, we explore how pull-based software development works, first on the GHTorrent corpus and then on a carefully selected sample of 291 projects. We find that the pull request model offers fast turnaround, increased opportunities for community engagement and decreased time to incorporate contributions. We show that a relatively small number of factors affect both the decision to merge a pull request and the time to process it. We also examine the reasons for pull request rejection and find that technical ones are only a small minority. </p>","","","10.1145/2568225.2568260","","","Pull-based development;distributed software development;empirical software engineering;pull request","","","","","38","","","","","","May 31 2014-June 7 2014","","ACM","ACM Conferences"
"Scaffolding students' learning using test my code","A. Vihavainen; T. Vikberg; M. Luukkainen; M. P√§rtel","University of Helsinki, Helsinki, Finland","Proceedings of the 18th ACM conference on Innovation and technology in computer science education","20160129","2013","","","117","122","<p>As programming is the basis of many CS courses, meaningful activities in supporting students on their journey towards being better programmers is a matter of utmost importance. Programming is not only about learning simple syntax constructs and their applications, but about honing practical problem-solving skills in meaningful contexts. In this article, we describe our current work on an automated assessment system called Test My Code (TMC), which is one of the feedback and support mechanisms that we use in our programming courses. TMC is an assessment service that (1) enables building of scaffolding into programming exercises; (2) retrieves and updates tasks into the students' programming environment as students work on them, and (3) causes no additional overhead to students' programming process. Instructors benefit from TMC as it can be used to perform code reviews, and collect and send feedback even on fully on-line courses.</p>","","","10.1145/2462476.2462501","","","automatic assessment;extreme apprenticeship;programming;situated learning;testing;verification","","","","","5","","","","","","1-3 July 2013","","ACM","ACM Conferences"
"Data parallelism in Haskell","M. M. T. Chakravarty","University of New South Wales, Sydney, Australia","Proceedings of the 2nd ACM SIGPLAN workshop on Functional high-performance computing","20160129","2013","","","97","98","<p>The implicit data parallelism in collective operations on aggregate data structures constitutes an attractive parallel programming model for functional languages. Beginning with our work on integrating nested data parallelism into Haskell, we explored a variety of different approaches to array-centric data parallel programming in Haskell, experimented with a range of code generation and optimisation strategies, and targeted both multicore CPUs and GPUs. In addition to practical tools for parallel programming, the outcomes of this research programme include more widely applicable concepts, such as Haskell's type families and stream fusion.</p> <p>In this talk, I will contrast the different approaches to data parallel programming that we explored. I will discuss their strengths and weaknesses and review what we have learnt in the course of exploring the various options. This includes our experience of implementing these approaches in the Glasgow Haskell Compiler as well the experimental results that we have gathered so far. Finally, I will outline the remaining open challenges and our plans for the future.</p> <p>This talk is based on joint work with Gabriele Keller, Sean Lee, Roman Leshchinskiy, Ben Lippmeier, Trevor L. McDonell, and Simon Peyton Jones.</p>","","","10.1145/2502323.2508151","","","array programming;code optimisation;data parallelism;haskell","","","","","","","","","","","23-23 Sept. 2013","","ACM","ACM Conferences"
"ADAM: External dependency-driven architecture discovery and analysis of quality attributes","D. Ganesan; M. Lindvall","Fraunhofer Center for Experimental Software Engineering, College Park","ACM Transactions on Software Engineering and Methodology (TOSEM)","20160129","2014","23","2","1","51","<p>This article introduces the Architecture Discovery and Analysis Method (ADAM). ADAM supports the discovery of module and runtime views as well as the analysis of quality attributes, such as testability, performance, and maintainability, of software systems. The premise of ADAM is that the implementation constructs, architecture constructs, concerns, and quality attributes are all influenced by the external entities (e.g., libraries, frameworks, COTS software) used by the system under analysis. The analysis uses such external dependencies to identify, classify, and review a minimal set of key source-code files supported by a knowledge base of the external entities. Given the benefits of analyzing external dependencies as a way to discover architectures and potential risks, it is demonstrated that dependencies to external entities are useful not only for architecture discovery but also for analysis of quality attributes. ADAM is evaluated using the NASA's Space Network Access System (SNAS). The results show that this method offers systematic guidelines for discovering the architecture and locating potential risks (e.g., low testability and decreased performance) that are hidden deep inside the system implementation. Some generally applicable lessons for developers and analysts, as well as threats to validity are also discussed.</p>","1049-331X;1049331X","","10.1145/2529998","","","Concerns;external entities;knowledge base;maintainability;module and runtime views;quality;reverse engineering;software architecture;testability","","","","","","","","","","","March 2014","","ACM","ACM Journals & Magazines"
"Heterogeneous computing: what does it mean for compiler research?","N. Rubin","Nvidia Research, Santa Clara, CA, USA","Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming","20160129","2014","","","315","316","<p>The current trend in computer architecture is to increase the number of cores, to create specialized types of cores within a single machine, and to network such machines together in very fluid web/cloud computing arrangements. Compilers have traditionally focused on optimizations to code that improve performance, but is that the right target to speed up real applications? Consider loading a web page (like starting GMAIL) the page is transferred to the client, any JavaScript is compiled, the JavaScript executes, and the page gets displayed. The classic compiler model (which was first developed in the late 50's) was a great fit for single core machines but has fallen behind architecture, and language. For example how do you compile a single program for a machine that has both a CPU and a graphics coprocessor (a GPU) with a very different programming and memory model? Together with the changes in architecture there have been changes in programming languages. Dynamic languages are used more, static languages are used less. How does this effect compiler research? In this talk, I'll review a number of traditional compiler research challenges that have (or will) become burning issues and will describe some new problems areas that were not considered in the past. For example language specifica-tions are large complex technical documents that are difficult for non-experts to follow. Application programmers are often not willing to read these documents; can a compiler bridge the gap?</p>","","","10.1145/2555243.2558891","","","compilers;processor architectures;software architectures","","","","","","","","","","","15-19 Feb. 2014","","ACM","ACM Conferences"
"Discrete event design patterns.","M. Hamri; R. Messouci; C. Frydman","LSIS UMR CNRS 7296, Marseille, France","Proceedings of the 2013 ACM SIGSIM conference on Principles of advanced discrete simulation","20160129","2013","","","349","354","<p>In this paper we highlight techniques from software engineering to design and code the behaviors of object. After a review of behavioral design patterns, we propose the state event design pattern to design basic behaviors described with state machines. In this pattern we objectify events in addition to states. Then, we generalize this pattern to DEVS behaviors. Thus, the DEVS designers may take profit from this technique to design simulations.</p>","","","10.1145/2486092.2486138","","","behavior of object;design patterns;devs;finite state machines;implementation of simulation","","","","","","","","","","","19-22 May 2013","","ACM","ACM Conferences"
"Rebuild processing in RAID5 with emphasis on the supplementary parity augmentation method[37]","A. Thomasian","Thomasian &#38; Associates, Pleasantville, NY","ACM SIGARCH Computer Architecture News","20160129","2012","40","2","18","27","<p>The rotated parity RAID5 disk array tolerates single disk failures by continuing operation by on-demand reconstruction of data blocks of the failed disk, until the systematic reconstruction of the contents of the failed disk is completed by the rebuild process on a spare disk. Supplementary Parity Augmentation (SPA), unlike the pyramid code, which has two parities covering half of the arrays disks each, extends RAID5's P parity with an additional S parity, which covers half of the disks. The extra load with respect to RAID5 of updating the S parity by one half of the disks is compensated by the more efficient on demand reconstructtion and rebuild processing when a disk fails. Although SPA has the same disk space redundancy level as RAID6, unlike RAID6 it can only deal with roughly half of all possible double disk failure cases for eight disks. For rebuild processing SPA reads half of the disks required by RAID5 and this leads to a higher Mean Time to Data Loss than RAID5, since fewer Latent Sector Errors are encountered. We review performance and reliability modeling of RAID5 arrays to provide insights into SPA's performance and reliability, which cannot be gained from numerical results alone. SPA is outperformed by the Intra-Disk Redundancy schemes combined with RAID5, which results in RAID6's reliability and RAID5 performance.</p>","0163-5964;01635964","","10.1145/2234336.2234340","","","","","","","","","1","","","","","May 2012","","ACM","ACM Journals & Magazines"
"Quests to mastery","S. Beth","Common Dreams Ltd, Christchurch, New Zealand","Proceedings of The 8th Australasian Conference on Interactive Entertainment","20160129","2012","","","1","1","<p>A documentary film, US AND THE GAME INDUSTRY (2012) comes to you as curious field research into some works by touted international independent games developers. Three years in the making it focuses on 2D and 3D aesthetic. The games are at the more succinct end of game design scales. Set within minimal systems adherence or within seeming new phases of game asceticism, the film sustains inquiry into design purpose and the principles of dynamic play.</p> <p>There are five developers featured. Their games are deliverable to PC/Mac, Play Station 3, the I-pad or I- phone, the DS and for server supported play. All are high caliber participants in computer science and conceptual thinking, born near the early 1980's. They carry with them expansive formative experience with computer game forms. They attended American universities; Cornell, North Western, Stanford, USC and Parsons; one recently graduated with a Phd at IT, Copenhagen. One American resident is Chinese born. A fifth developer is born and educated in Australia. For this group familiarity with algorithms and decades of temporal lives with computers for play was the norm. These characters as a social group who have not only experienced thousands of hours with rules of play but are players as consumers who have now switched to development. They were challenged by boredom and by opportunity, who, ignoring other explosions such as gamification trends, set off to wield connoisseur status and provide elegant ideas for universal appeal. They, against the tide of 'the market', are almost recluses for months and years repeatedly so as to focus individual attention on the complexity of code, the exquisite natures of flow and games' potentials for human satisfactions.</p> <p>Robin Hunicke is Executive Producer of JOURNEY.(thatgamecompany) and Jenova Chen is creative director. This company's latest game is an adventure game held to the form of a mono myth. Zach Gage <i>(SpellTower)</i> is a concept artist and a relati- e newcomer to games design. <i>SpellTower</i> is a word game trumping word games. Alexander Bruce (Demruth) has been working five years on one adventure experience for the mind, <i>Antichamber</i>. It uses non- ecludiean geometry. Douglas Wilson (Die Gute Fabrik), is producing a swamp opera this year titled MUTAZIONE. Jason Rohrer, developer of short games these last four years is to reveal his next game, THE CASTLE DOCTRINE in the film.</p> <p>These developers and their retinues stand as unique in their investment in one --off game development. Their works represent the edge of critical review. This cluster is a brief representation of people making games sophisticated enough that many are taking pause with them in the ride of life and cultural selection. Their scrutiny and √©lan with dynamism, puzzles, myth and story speak of riches to be tackled and felt.</p> <p>The filmed subjects are presented in interviews, with their work and in dialogues with some peers. Other designers and developers in support are: Richard Lemarchand and Nicholas Fortugno. Also, Austin Wintory, composer for JOURNEY and designers and engineers of the JOURNEY team active during 2009--2012.</p>","","","10.1145/2336727.2336752","","","","","","","","","","","","","","21-22 July 2012","","ACM","ACM Conferences"
"Coding theory for scalable media delivery","M. G. Luby","Qualcomm Technologies, Inc., Berkeley, CA, USA","Proceedings of the 2014 ACM symposium on Principles of distributed computing","20160129","2014","","","153","155","<p>A review of a collaborative body of work focused on coding theory with applications to scalable media delivery.</p>","","","10.1145/2611462.2611515","","","broadcast;coding theory;erasure codes;file delivery;reliability;scalability;streaming","","","","","","","","","","","15-18 July 2014","","ACM","ACM Conferences"
"Feature-based object identification for web automation","C. Herzog; I. Kordomatis; W. Holzinger; R. R. Fayzrakhmanov; B. Kr√ºpl-Sypien","Vienna University of Technology","Proceedings of the 28th Annual ACM Symposium on Applied Computing","20160129","2013","","","742","749","<p>In this paper, we address automatic identification of common functional structures on web pages, a fundamental problem for web automation applications and graphical user interface testing. In contrast to other approaches, we aim to identify relevant patterns without relying on the source code of a web page or keywords, utilizing mostly geometrical and visually perceptible properties. We achieve this by transforming pages into an independent geometrical representation, on top of which we extract a set of features that allows us to employ traditional machine learning techniques for the identification task. We evaluate this approach by analyzing three typical scenarios, reviewing the obtained information retrieval key metrics and estimating the relevance of the chosen features. Our initial results demonstrate the feasibility of the proposed approach.</p>","","","10.1145/2480362.2480504","","","machine learning;web accessibility;web automation;web object identification;web page visual representation","","","","","1","","","","","","18-22 March 2013","","ACM","ACM Conferences"
"Classifying physical strategies in tangible tasks: a video-coding framework for epistemic actions","A. Esteves; S. Bakker; A. N. Antle; A. May; J. Warren; I. Oakley","University of Madeira, Funchal, Madeira, Portugal","CHI '14 Extended Abstracts on Human Factors in Computing Systems","20160129","2014","","","1843","1848","<p>Tangible interaction is a compelling interface paradigm that elegantly merges the fluency of physical manipulation with the flexibility of digital content. However, it is currently challenging to understand the real benefits and advantages of tangible systems. To address this problem, this paper argues that we need new evaluation techniques capable of meaningfully assessing how users perform with tangible, physical objects. Working towards this aim, it presents a video-coding framework that supports the granular identification of epistemic actions (physical actions that are made to simplify cognitive work) during tangible tasks. The framework includes 20 epistemic actions, identified through a systematic literature review of 77 sources. We argue that data generated by applying this process will help us better understand epistemic behavior and, ultimately, lead to the generation of novel, grounded design insights to support physically-grounded cognitive strategies in tangible tasks.</p>","","","10.1145/2559206.2581185","","","epistemic action;tangible interaction;video coding","","","","","","","","","","","April 26 2014-May 1 2014","","ACM","ACM Conferences"
"Simulating software behavior based on UML activity diagram","L. Yu; X. Tang; L. Wang; X. Li","Nanjing University, Nanjing, P.R. China","Proceedings of the 5th Asia-Pacific Symposium on Internetware","20160129","2013","","","1","4","<p>It is encouraged to develop practical approaches to ensure that the software artifacts are created as expected or defect-free as early as possible. In the industry, software analysis and testing techniques are widely used solutions for codes and executables, respectively. However, software design artifacts can only be verified by manual peer review in design phase. In this paper, we propose an approach to automatically simulate the expected software behavior depicted in UML activity diagrams. First, UML activity diagrams are parsed and initialized semantically with a concrete execution. Second, the model is symbolically executed, to collect paths, input variables, and their path conditions. Then, the path conditions are passed to a constraint solver to generate a set of concrete value of possible input variables. Final, the generated concrete input variables are semantically executed on the model to identify the defects as well as to collect the execution path. The model simulation approach reuses the design models, automates the simulation process by using model-based concolic execution, and has the advantage of visibility and observability on model simulation. In addition, we found that the solvable paths represent behavioral scenarios. While simulating the model, input values corresponding to execution paths in the model are generated automatically. They can also be used as test suites to find the inconsistency between the design and implementation. We have also developed a prototype tool to support the above process, and have conducted a trivial case study to demonstrate the applicability of our approach.</p>","","","10.1145/2532443.2532465","","","UML activity diagram;concolic execution;simulation;tool","","","","","","","","","","","23-24 Oct. 2013","","ACM","ACM Conferences"
"Automated software architecture security risk analysis using formalized signatures","M. Almorsy; J. Grundy; A. S. Ibrahim","Swinburne University of Technology, Australia","Proceedings of the 2013 International Conference on Software Engineering","20160129","2013","","","662","671","<p> Reviewing software system architecture to pinpoint potential security flaws before proceeding with system development is a critical milestone in secure software development lifecycles. This includes identifying possible attacks or threat scenarios that target the system and may result in breaching of system security. Additionally we may also assess the strength of the system and its security architecture using well-known security metrics such as system attack surface, Compartmentalization, least-privilege, etc. However, existing efforts are limited to specific, predefined security properties or scenarios that are checked either manually or using limited toolsets. We introduce a new approach to support architecture security analysis using security scenarios and metrics. Our approach is based on formalizing attack scenarios and security metrics signature specification using the Object Constraint Language (OCL). Using formal signatures we analyse a target system to locate signature matches (for attack scenarios), or to take measurements (for security metrics). New scenarios and metrics can be incorporated and calculated provided that a formal signature can be specified. Our approach supports defining security metrics and scenarios at architecture, design, and code levels. We have developed a prototype software system architecture security analysis tool. To the best of our knowledge this is the first extensible architecture security risk analysis tool that supports both metric-based and scenario-based architecture security analysis. We have validated our approach by using it to capture and evaluate signatures from the NIST security principals and attack scenarios defined in the CAPEC database. </p>","","","","","","","","","","","12","","","","","","18-26 May 2013","","ACM","ACM Conferences"
"High level transforms for SIMD and low-level computer vision algorithms","L. Lacassagne; D. Etiemble; A. Hassan Zahraee; A. Dominguez; P. Vezolle","University Paris Sud, Orsay, France","Proceedings of the 2014 Workshop on Programming models for SIMD/Vector processing","20160129","2014","","","49","56","<p>This paper presents a review of algorithmic transforms called High Level Transforms for IBM, Intel and ARM SIMD multicore processors to accelerate the implementation of low level image processing algorithms. We show that these optimizations provide a significant acceleration. A first evaluation of 512-bit SIMD Xeon- Phi is also presented. We focus on the point that the combination of optimizations leading to the best execution time cannot be predicted, and thus, systematic benchmarking is mandatory. Once the best configuration is found for each architecture, a comparison of these performances is presented. The Harris points detection operator is selected as being <i>representative</i> of low level image processing and computer vision algorithms. Being composed of five convolutions, it is more complex than a simple filter and enables more opportunities to combine optimizations. The presented work can scale across a wide range of codes using 2D stencils and convolutions.</p>","","","10.1145/2568058.2568067","","","2d stencil;arm neon;code optimization;high level transforms;ibm altivec;intel sse & xeonphi;low-level computer vision and image processing algorithms;simd","","","","","2","","","","","","16-16 Feb. 2014","","ACM","ACM Conferences"
"Situational awareness: personalizing issue tracking systems","O. Baysal; R. Holmes; M. W. Godfrey","University of Waterloo, Canada","Proceedings of the 2013 International Conference on Software Engineering","20160129","2013","","","1185","1188","<p> Issue tracking systems play a central role in ongoing software development; they are used by developers to support collaborative bug fixing and the implementation of new features, but they are also used by other stakeholders including managers, QA, and end-users for tasks such as project management, communication and discussion, code reviews, and history tracking. Most such systems are designed around the central metaphor of the ""issue"" (bug, defect, ticket, feature, etc.), yet increasingly this model seems ill fitted to the practical needs of growing software projects; for example, our analysis of interviews with 20 Mozilla developers who use Bugzilla heavily revealed that developers face challenges maintaining a global understanding of the issues they are involved with, and that they desire improved support for situational awareness that is difficult to achieve with current issue management systems. In this paper we motivate the need for personalized issue tracking that is centered around the information needs of individual developers together with improved logistical support for the tasks they perform. We also describe an initial approach to implement such a system ‚Äî extending Bugzilla ‚Äî that enhances a developer's situational awareness of their working context by providing views that are tailored to specific tasks they frequently perform; we are actively improving this prototype with input from Mozilla developers. </p>","","","","","","","","","","","3","","","","","","18-26 May 2013","","ACM","ACM Conferences"
"Software analytics = sharing information","T. Zimmermann","Microsoft Research, Redmond, WA","Proceedings of the 9th International Conference on Predictive Models in Software Engineering","20160129","2013","","","1","1","<p>Software and its development generates an inordinate amount of data. Development activities such as check-ins, work items, bug reports, code reviews, and test executions are recorded in software repositories. User interactions that reflect how customers experience software are recorded in telemetry data, run-time traces, and log files and helps to track application and feature usage and expose performance and reliability. Software analytics takes this data and turns it into actionable insight to better inform decisions related to software. To a large extent, <i>software analytics is about what we can learn and share</i> about software. This include our own projects but also projects by others. Looking back at decades of research in empirical software engineering and mining software repositories, software analytics lets us share different things: <i>insights, models, methods, and data</i>. In this talk, I will present successful efforts from Microsoft and academia on software analytics and try to convince you that analytics is all about sharing information.</p>","","","10.1145/2499393.2499405","","","","","","","","","","","","","","9-9 Oct. 2013","","ACM","ACM Conferences"
"Emergent participant interaction","J. Seevinck; E. A. Edmonds; L. Candy","Queensland University of Technology, Kelvin Grove","Proceedings of the 24th Australian Computer-Human Interaction Conference","20160129","2012","","","540","549","<p>Emergence has the potential to effect complex, creative or open-ended interactions and novel game-play. We report on research into an emergent interactive system. This investigates emergent user behaviors and experience through the creation and evaluation of an interactive system. The system is +-<i>NOW</i>, an augmented reality, tangible, interactive art system. The paper briefly describes the qualities of emergence and +-<i>NOW</i> before focusing on its evaluation. This was a qualitative study with 30 participants conducted in context. Data analysis followed Grounded Theory Methods. Coding schemes, induced from data and external literature are presented. Findings show that emergence occurred in over half of the participants. The nature of these emergent behaviors is discussed along with examples from the data. Other findings indicate that participants found interaction with the work satisfactory. Design strategies for facilitating satisfactory experience despite the often unpredictable character of emergence, are briefly reviewed and potential application areas for emergence are discussed.</p>","","","10.1145/2414536.2414619","","","coding scheme;creativity;design;emergence;emergent shape;evaluation;interaction;interactive art;open;predictability","","","","","","","","","","","26-30 Nov. 2012","","ACM","ACM Conferences"
"Foreseen risks for network coding based surveillance applications","V. Talooki; D. E. Lucani; H. Marques; J. Rodriguez","Instituto de Telecomunica&#231;&#245;es, Aveiro - Portugal, Portugal","Proceedings of the 2nd ACM workshop on High performance mobile opportunistic systems","20160129","2013","","","83","88","<p>This survey paper reviews key security weaknesses and attacks of network coding (NC) protocols, especially looking at the implications in surveillance applications. Surveillance is a critical use case, not only due to the stringent constraints or the importance of what is being protected by it, but because these systems are very likely to be attack with potentially serious implications to physical systems. We argue that NC can provide advantages from a performance as well as a security perspective. In particular, we focus on the latter presenting foreseen security risks on systems implementing NC and key countermeasures to curb potential threats.</p>","","","10.1145/2507908.2507915","","","byzantine attacks;corruption attacks;eavesdropping attacks;network coding;state-less and state-aware protocols;surveillance applications;wireless networks","","","","","1","","","","","","3-8 Nov. 2013","","ACM","ACM Conferences"
"Comparison of passive versus active photo capture of built environment features by technology na√Øve Latinos using the SenseCam and Stanford healthy neighborhood discovery tool","J. L. Sheats; S. J. Winter; P. Padilla-Romero; L. Goldman-Rosas; L. A. Grieco; A. C. King","Stanford Prevention Research Center, Palo Alto, CA","Proceedings of the 4th International SenseCam & Pervasive Imaging Conference","20160129","2013","","","8","15","<p>Assessments designed to measure features of the built environment are challenging and have traditionally been conducted by trained researchers. The purpose of this study was to explore and compare both the feasibility and utility of having community residents use two different technological devices to assess their neighborhood built environment features: the Stanford Healthy Neighborhood Discovery Tool (which allows users to thoughtfully take photographs) and the SenseCam (which automatically takes photographs). Consented participants were low income, tech-na√Øve, Latino adolescents aged 11 to 14 years (n=8), and older adults aged 63 to 80 years (n=7) from North Fair Oaks, California. Participants used the devices while on a ""usual"" 45 to 60 minute walk through their neighborhood. Photos from each device were reviewed, coded, categorized into themes, and compared. Perceptual data regarding the use of the SenseCam were available for 15 participants and SenseCam photographs were available for 7 participants. There were 1,678 photos automatically captured by the SenseCam compared to 112 photos taken by participants with the Discovery Tool. Of the original 1,678 SenseCam photos there were 68 in which researchers coded built environment features that were not captured by the community residents using the Discovery Tool. Forty-two (62%) of these photos were of positive features; and 26 (38%) were of negative features. The SenseCam captured a greater number of images with positive features that were not captured by adolescents via the Discovery Tool; as well as a greater number of negative features not captured by the older adults via the Discovery Tool. There were two environmental elements (graffiti, dogs) captured by the Discovery Tool though not the SenseCam. Overall, study participants were receptive to both devices and indicated that they would be interested in using them again for a longer period of time. Older adults reported more positive perceptions about- the SenseCam than adolescents. While the sample was small, study results indicate that the SenseCam may be useful in capturing built environment features that affect physical activity but that community residents don't notice, perhaps because they are habituated to certain conditions in their neighborhoods. The results suggest that this type of habituation may have different valences (positive or negative) for different age groups. Given the impact the built environment has on physical activity, particularly in low-income communities, further research regarding the use of the SenseCam to passively gather built environment data in tech-na√Øve populations is warranted.</p>","","","10.1145/2526667.2526669","","","SenseCam;applications;behavioral science;mHealth;older adults;physical activity;tablet","","","","","","","","","","","18-19 Nov. 2013","","ACM","ACM Conferences"
"Experience report: Formal verification and testing in the development of embedded software","A. Ulrich; A. Votintseva","Siemens AG, Corporate Technology, Munich, Germany","2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)","20160114","2015","","","293","302","The paper summarizes our experiences in applying formal verification using the explicit-state model checker SPIN and combining it with a model-based testing approach to support the validation of embedded software. The discussed example covers a crucial part of the firmware of the fault-tolerant programmable logic controller Siemens SIMATIC S7-400H. The chosen approach is outlined and obstacles that were faced during the project are discussed. The paper advocates why formal verification is not suitable as a standalone method in industrial projects. Rather it must be combined with an appropriate validation method such as testing to maximize the benefits from the combination of both approaches. In this case, formal verification complements code or design model reviews, and testing benefits from the availability of correct formal models provided during verification process.","","CD-ROM:978-1-5090-0405-8; Electronic:978-1-5090-0406-5; POD:978-1-5090-0407-2","10.1109/ISSRE.2015.7381822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381822","","Analytical models;Automation;Embedded software;Fault tolerance;Synchronization;Testing","embedded systems;fault tolerant control;firmware;program testing;program verification;programmable controllers","SPIN;Siemens SIMATIC S7-400H;design model reviews;embedded software validation;explicit-state model checker;fault-tolerant programmable logic controller;firmware;formal testing;formal verification complement code;model-based testing approach","","","","23","","","","2-5 Nov. 2015","","IEEE","IEEE Conferences"
"Investigating Program Behavior Using the Texada LTL Specifications Miner","C. Lemieux; I. Beschastnikh","Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada","2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)","20160107","2015","","","870","875","Temporal specifications, relating program events through time, are useful for tasks ranging from bug detection to program comprehension. Unfortunately, such specifications are often lacking from system descriptions, leading researchers to investigate methods for inferring these specifications from code, execution traces, code comments, and other artifacts. This paper describes Texada, a tool to dynamically mine temporal specifications in LTL from traces of program activity. We review Texada's key features and demonstrate how it can be used to investigate program behavior through two scenarios: validating an implementation that solves the dining philosophers problem and supporting comprehension of a stack implementation. We also detail Texada's other, more advanced, usage options. Texada is an open source tool: https://bitbucket.org/bestchai/texada.","","Electronic:978-1-5090-0025-8; POD:978-1-5090-0026-5","10.1109/ASE.2015.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372082","linear temporal logic;program comprehension;specification mining;texada","Concurrent computing;Data structures;Distance measurement;Java;Runtime","data mining;formal specification;program debugging;public domain software","Texada LTL specification miner;bug detection;code comments;dynamic temporal specification mining;execution traces;open source tool;program activity;program behavior;program comprehension;program events","","1","","19","","","","9-13 Nov. 2015","","IEEE","IEEE Conferences"
"Batch-mode active learning for technology-assisted review","T. K. Saha; M. A. Hasan; C. Burgess; M. A. Habib; J. Johnson","Department of Computer Science, Indiana University Purdue University Indianapolis, IN","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1134","1143","In recent years, technology-assisted review (TAR) has become an increasingly important component of the document review process in litigation discovery. This is fueled largely by dramatic growth in data volumes that may be associated with many matters and investigations. Potential review populations frequently exceed several hundred thousands documents, and document counts in the millions are not uncommon. Budgetary and/or time constraints often make a once traditional linear review of these populations impractical, if not impossible - which made ""predictive coding"" the most discussed TAR approach in recent years. A key challenge in any predictive coding approach is striking the appropriate balance in training the system. The goal is to minimize the time that Subject Matter Experts spend in training the system, while making sure that they perform enough training to achieve acceptable classification performance over the entire review population. Recent research demonstrates that Support Vector Machines (SVM) perform very well in finding a compact, yet effective, training dataset in an iterative fashion using batch-mode active learning. However, this research is limited. Additionally, these efforts have not led to a principled approach for determining the stabilization of the active learning process. In this paper, we propose and compare several batch-mode active learning methods which are integrated within SVM learning algorithm. We also propose methods for determining the stabilization of the active learning method. Experimental results on a set of large-scale, real-life legal document collections validate the superiority of our method over the existing methods for this task.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363867","","Law;Measurement;Predictive coding;Support vector machines;Training;Uncertainty","document handling;learning (artificial intelligence);support vector machines","SVM;batch-mode active learning;document review process;litigation discovery;predictive coding;support vector machine;technology-assisted review","","","","36","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conferences"
"Review of Video and Image Defogging Algorithms and Related Studies on Image Restoration and Enhancement","Y. Xu; J. Wen; L. Fei; Z. Zhang","Bio-Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China","IEEE Access","20170520","2016","4","","165","188","Video and images acquired by a visual system are seriously degraded under hazy and foggy weather, which will affect the detection, tracking, and recognition of targets. Thus, restoring the true scene from such a foggy video or image is of significance. The main goal of this paper was to summarize current video and image defogging algorithms. We first presented a review of the detection and classification method of a foggy image. Then, we summarized existing image defogging algorithms, including image restoration algorithms, image contrast enhancement algorithms, and fusion-based defogging algorithms. We also presented current video defogging algorithms. We summarized objective image quality assessment methods that have been widely used for the comparison of different defogging algorithms, followed by an experimental comparison of various classical image defogging algorithms. Finally, we presented the problems of video and image defogging which need to be further studied. The code of all algorithms will be available at <;uri xlink:href=""http://www.yongxu.org/lunwen.html"" xlink:type=""simple"">http://www.yongxu.org/lunwen.html<;/uri>.","","","10.1109/ACCESS.2015.2511558","Shenzhen Municipal Science and Technology Innovation Council; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365412","Foggy image classification;image defogging;image quality assessment;video defogging","Classification algorithms;Defogging;Feature extraction;Image classification;Image restoration;Meteorology;Visual systems","image enhancement;image fusion;image restoration;object detection;object recognition;target tracking;video signal processing","fusion-based defogging algorithm;image contrast enhancement algorithm;image defogging algorithm;image quality assessment method;image restoration;target detection;target recognition;target tracking;video defogging algorithm;visual system","","7","","141","","","20151224","2016","","IEEE","IEEE Journals & Magazines"
"Semantic zooming of code change history","Y. Yoon; B. A. Myers","Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA 15213, USA","2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","20151217","2015","","","95","99","Previously, we presented our technique for visualizing fine-grained code changes in a timeline view, designed to facilitate reviewing and interacting with the code change history. During user evaluations, it became evident that users often wanted to see the code changes at a higher level of abstraction. Therefore, we developed a novel approach to automatically summarize fine-grained code changes into more conceptual, higher-level changes in real time. Our system provides four collapse levels, which are integrated with the timeline via semantic zooming: raw level (no collapsing), statement level, method level, and type level. Compared to the raw level, the number of code changes shown in the timeline at each level is reduced by 55%, 77%, and 83%, respectively. This implies that the semantic zooming would help users better understand and interact with the history by minimizing the potential information overload.","","Electronic:978-1-4673-7457-6; POD:978-1-4673-7458-3","10.1109/VLHCC.2015.7357203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357203","Azurite;edit collapsing;program comprehension;semantic zooming;software visualization;timeline visualization","","programming environments;user interfaces","code change history;method level;raw level;semantic zooming;statement level;type level","","","","17","","","","18-22 Oct. 2015","","IEEE","IEEE Conferences"
"Taking the Human Out of the Loop: A Review of Bayesian Optimization","B. Shahriari; K. Swersky; Z. Wang; R. P. Adams; N. de Freitas","Univ. of British Columbia, Vancouver, BC, Canada","Proceedings of the IEEE","20151218","2016","104","1","148","175","Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.","0018-9219;00189219","","10.1109/JPROC.2015.2494218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352306","Decision making;decision making;design of experiments;genomic medicine;optimization;response surface methodology;statistical learning","Bayes methods;Big data;Decision making;Design of experiments;Genomes;Linear programming;Optimization;Statistical analysis","Bayes methods;Big Data;optimisation;storage allocation","Bayesian optimization;Big data application;human productivity;large-scale heterogeneous computing;massive complex software system;product quality;storage architecture","","36","","163","","","20151210","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"Surgical Guidance via Multiplexed Molecular Imaging of Fresh Tissues Labeled With SERS-Coded Nanoparticles","Y. Wang; S. Kang; J. D. Doerksen; A. K. Glaser; J. T. C. Liu","Department of Mechanical Engineering, University of Washington, Seattle, WA, USA","IEEE Journal of Selected Topics in Quantum Electronics","20160321","2016","22","4","154","164","The imaging of dysregulated cell-surface receptors (or biomarkers) is a potential means of identifying the presence of cancer with high sensitivity and specificity. However, due to heterogeneities in the expression of protein biomarkers in tumors, molecular imaging technologies should ideally be capable of visualizing a multiplexed panel of cancer biomarkers. Recently, surface-enhanced Raman-scattering (SERS) nanoparticles (NPs) have attracted wide interest due to their potential for sensitive and multiplexed biomarker detection. In this review, we focus on the most recent advances in tumor imaging using SERS-coded NPs. A brief introduction of the structure and optical properties of SERS NPs is provided, followed by a detailed discussion of key imaging issues such as the administration of NPs in tissue (topical versus systemic), the optical configuration and imaging approach of Raman imaging systems, spectral demultiplexing methods for quantifying NP concentrations, and the disambiguation of specific versus nonspecific sources of contrast through ratiometric imaging of targeted and untargeted (control) NP pairs. Finally, future challenges and directions are briefly outlined.","1077-260X;1077260X","","10.1109/JSTQE.2015.2507358","Department of Education GAANN fellowship program; NIH / NCI; NIH/NIBIB; NIH/NIDCR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350220","Biomarkers;Raman spectroscopy;biomedical optical imaging;cancer detection;fiberoptic probes;molecular imaging;nanomedicine;tumors","Biomarkers;Biomedical optical imaging;Cancer;Fluorescence;Optical imaging;Tumors","biomedical optical imaging;cancer;nanocomposites;nanomedicine;nanoparticles;surface enhanced Raman scattering;surgery;tumours","Raman imaging systems;SERS NP optical properties;SERS NP structure;SERS coded NP;SERS coded nanoparticles;biomarker imaging;cancer biomarkers;dysregulated cell surface receptors;molecular imaging technologies;multiplexed biomarker detection;multiplexed molecular imaging;nanoparticle labeled fresh tissues;optical configuration;protein biomarker expression;ratiometric imaging;sensitive biomarker detection;spectral demultiplexing methods;surface enhanced Raman scattering;surgical guidance;tumor imaging","","3","","98","","","20151209","July-Aug. 2016","","IEEE","IEEE Journals & Magazines"
"QoE-Enabled Big Video Streaming for Large-Scale Heterogeneous Clients and Networks in Smart Cities","B. W. Chen; W. Ji; F. Jiang; S. Rho","Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan","IEEE Access","20170520","2016","4","","97","107","The rapid growth of the next-generation communication and networks is bringing video services into more pervasive environments. More and more users access and interact with video content using different devices, such as smart televisions, personal computers, tablets, smartphones, and wearable equipments. Providing heterogeneous Quality of Experience (QoE) that supports a wide variety of multimedia devices is critical to video broadcasting over the next-generation wireless network. This paper reviews practical video broadcasting technologies and examines current requirements ranging from heterogeneous devices to transmission technologies. Meanwhile, various coding methodologies, including QoE modeling, scalable compression efficiency, and flexible transmission, are also discussed. Moreover, this paper presents a typical paradigm as an example for video broadcasting with large-scale heterogeneity support, which enables QoE mapping, joint coding, flexible forward error coding, and cross-layer transmission, as well as optimal and dynamic adaptation to improve the overall receiving quality of heterogeneous devices. Finally, a brief summary of the key ideas and a discussion of interesting open areas are summarized at the end of this paper along with a future recommendation.","","","10.1109/ACCESS.2015.2506648","Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349100","Quality of service;broadcast technology;communications technology;video coding","Broadcasting;Computer architecture;Encoding;Multimedia communication;Next generation networking;Quality of service;Streaming media","data compression;forward error correction;next generation networks;quality of experience;radio networks;smart cities;video coding;video streaming","QoE-enabled big video streaming;coding methodology;cross-layer transmission;flexible forward error coding;large-scale heterogeneous client;large-scale heterogeneous network;multimedia device;next-generation communication;next-generation wireless network;quality of experience;scalable compression efficiency;smart city;video broadcasting technology","","5","","85","","","20151208","2016","","IEEE","IEEE Journals & Magazines"
"Influence of professional demographics on faculty feedback in asynchronous, video-annotated peer review (VAPR)","J. J. Pembridge; Y. Allam; L. K. Davids","Engineering Fundamentals, Embry-Riddle Aeronautical University, Daytona Beach, FL","2015 IEEE Frontiers in Education Conference (FIE)","20151207","2015","","","1","8","The use of an asynchronous video-annotated peer review system (VAPR) presents the opportunity to enhance teaching and diffuse pedagogical practice in higher education. The process involves the video recording of classroom teaching by each faculty member, which is then reviewed and annotated in real-time by faculty peers to identify specific instances of good practices, opportunities for improvement, and opportunities to incorporate research-based teaching approaches. The purpose of this study is to analyze the nature of faculty peer comments generated throughout the peer feedback process. Each peer-generated video annotation text file was coded to characterize the type of feedback given, encompassing characterizations such as: good practice, suggestions; types of teacher knowledge; and general attributes of teaching. The demographics of the peer authors, for corresponding annotations, are also classified based on highest degree, familiarity with the course as an instructor, and familiarity as a student. Findings indicate that a majority of the comments are related to the direct instruction of the course, rather than the content of the instruction. While not statistically significant, familiarity with the course as instructor is positively correlated to the number of comments pertaining to pedagogical content knowledge. These findings support the need for peer-review by faculty familiar with the course to offer context specific suggestions as well as those peers unfamiliar with the course to offer opportunities to diffuse instructional strategies from other contexts.","","Paper:978-1-4799-8454-1","10.1109/FIE.2015.7344392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344392","faculty peer review;professional development","Context;Education;Organizations;Portfolios;Reflection;Reliability;Streaming media","computer aided instruction;demography;educational courses;further education;teaching","VAPR;asynchronous peer review;asynchronous video-annotated peer review system;classroom teaching;course;faculty feedback;higher education;instructional strategies;pedagogical content knowledge;pedagogical practice;peer feedback process;peer-generated video annotation text file;professional demographics;research-based teaching approaches;video recording","","","","39","","","","21-24 Oct. 2015","","IEEE","IEEE Conferences"
"An insider threat activity in a software security course","D. E. Krutz; A. Meneely; S. A. Malachowsky","Rochester Institute of Technology","2015 IEEE Frontiers in Education Conference (FIE)","20151207","2015","","","1","6","Software development teams face a critical threat to the security of their systems: insiders. A malicious insider is a person who violates an authorized level of access in a software system. Unfortunately, when creating software, developers do not typically account for insider threat. Students learning software development are unaware of the impacts of malicious actors and are far too often untrained in prevention methods against them. A few of the defensive mechanisms to protect against insider threats include eliminating system access once an employee leaves an organization, enforcing principle of least privilege, code reviews, and constant monitoring for suspicious activity. At the Department of Software Engineering at the Rochester Institute of Technology, we require a course titled Engineering of Secure Software and have created an activity designed to prepare students for the problem of insider threats. At the beginning of this activity, student teams are given the task of designing a moderately sized secure software system. The goal of this insider is to manipulate the team into creating a flawed system design that would allow attackers to perform malicious activities once the system has been created. When the insider is revealed at the conclusion of the project, students discuss countermeasures regarding the malicious actions the insiders were able to plan or complete, along with methods of prevention that may have been employed by the team to detect the malicious developer. In this paper, we describe the activity along with the results of a survey. We discuss the benefits and challenges of the activity with the goal of giving other instructors the tools they need to conduct this activity at their institution. While many institutions do not offer courses in computer security, this self-contained activity may be used in any computing course to enforce the importance of protecting against insider threats.","","Paper:978-1-4799-8454-1","10.1109/FIE.2015.7344087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344087","Computing Education;Software Engineering;Software Security","Cryptography;Databases;Organizations;Software engineering;Software systems","computer aided instruction;computer science education;educational courses;educational institutions;engineering education;invasive software;safety-critical software","Rochester Institute of Technology;authorized access level violation;computer security;defensive mechanisms;department of software engineering;engineering-of-secure software course;insider threat activity;malicious activities;malicious actors;malicious insider;secure software system design;self-contained activity;software development learning;software development teams;software security course;software system","","1","","18","","","","21-24 Oct. 2015","","IEEE","IEEE Conferences"
"Issues in Trustworthy Software Systems","M. Ezzeddine; H. Akkary","Electr. & Comput. Eng. Dept., American Univ. of Beirut, Beirut, Lebanon","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","1","","1142","1147","Lack of security in computing is most of the timedue to software vulnerabilities. To a broad extent, securitybreaches are not due to problems in cryptography nor in thecommunication medium or in computer hardware and microprocessors. In turn, software vulnerabilities are mainly due to limitations in current state of the art software testing. Unfortunately, no major breakthrough in software testing is expected, neither at this time nor in the near future. To this end, researchers tackled the problem of platform security from a different perspective: dividing the platform into two orthogonal execution environments: trusted execution environment (TEE) and untrusted or rich execution environment (REE). In the latter, a rich execution environment hosting unrestricted commercial off-the-shelf software executes, while in the former a minimalistic highly trusted software stack executes. Execution takes place most of the time in the REE, while only sensitive and security critical operations take place in the TEE. In this paper, we review and discuss the required and highly recommended properties that any code executing in the TEE must preserve, and we show how software, security and machine architects should cooperate for maintaining a secure TEE orthogonal to a rich REE in embedded computing devices.","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345403","Software testing;rich execution environment REE;trusted execution environment TEE;trusted software","Cryptography;Formal verification;Measurement;Software;Software testing","cryptography;program testing;trusted computing","REE;TEE;cryptography;embedded computing devices;rich execution environment;security breach;software testing;software vulnerabilities;trusted execution environment;trustworthy software systems","","","","32","","","","20-22 Aug. 2015","","IEEE","IEEE Conferences"
"Keynote abstracts","Yi Pan","Dept. of Comput. Sci., Georgia State Univ., Atlanta, GA, USA","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","1","","xli","xlvii","Cloud computing has emerged rapidly as a growing paradigm of on-demand access to computing, data and software utilities using a usage-based billing model. Many massive data applications including database applications and web searching should be the ideal applications on cloud platforms. However, many legacy scientific computing codes are written in traditional parallel programming languages such as MPI and OpenMP and cannot be executed on these cloud platforms. With the current cloud programming models, complicated scientific computing algorithms cannot be implemented easily and executed efficiently on many cloud platforms. In this talk, I will give a review of different massively parallel computing platforms and compare various computing domains and programming models on these platforms. In particular, I will point out the shortcomings and limitations of current cloud computing programming models for typical scientific computing algorithms, and propose possible solutions. Current cloud models such as MapReduce or Spark and their variants have succeeded in data-parallel applications such as database operations and web searching; however, they are still not effective for applications with a lot of data dependency such as scientific computing applications. We propose several approaches to solving this problem through extension of current programming models, automatic translation from sequential codes to cloud codes, simple API and framework built on current cloud models and traditional models such as MPI, detection of data and task parallelism, and their efficient scheduling. Some preliminary theoretical and experimental results will also be reported in this talk.","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345256","","","application program interfaces;cloud computing;message passing;parallel processing;scientific information systems;search engines","API;MPI;MapReduce;OpenMP;Spark;Web search;cloud codes;cloud computing programming models;cloud programming models;computing domains;data detection;data utilities;data-parallel applications;database applications;database operations;legacy scientific computing codes;massively parallel computing platforms;on-demand access;parallel programming languages;parallelism task;scheduling;sequential codes;software utilities;usage-based billing model","","","","","","","","20-22 Aug. 2015","","IEEE","IEEE Conferences"
"When code smells twice as much: Metric-based detection of variability-aware code smells","W. Fenske; S. Schulze; D. Meyer; G. Saake","University of Magdeburg, Germany","2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM)","20151123","2015","","","171","180","Code smells are established, widely used characterizations of shortcomings in design and implementation of software systems. As such, they have been subject to intensive research regarding their detection and impact on understandability and changeability of source code. However, current methods do not support highly configurable software systems, that is, systems that can be customized to fit a wide range of requirements or platforms. Such systems commonly owe their configurability to conditional compilation based on C preprocessor annotations (a. k. a. #ifdefs). Since annotations directly interact with the host language (e. g., C), they may have adverse effects on understandability and changeability of source code, referred to as variability-aware code smells. In this paper, we propose a metric-based method that integrates source code and C preprocessor annotations to detect such smells. We evaluate our method for one specific smell on five open-source systems of medium size, thus, demonstrating its general applicability. Moreover, we manually reviewed 100 instances of the smell and provide a qualitative analysis of its potential impact as well as common causes for the occurrence.","","Electronic:978-1-4673-7529-0; POD:978-1-4673-7530-6","10.1109/SCAM.2015.7335413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335413","","Computer bugs;Concrete;Measurement;Open source software;Programming;Scattering;Software systems","C language;program diagnostics;public domain software;source code (software)","C preprocessor annotation;metric-based detection;open-source system;source code;variability-aware code smell","","","","32","","","","27-28 Sept. 2015","","IEEE","IEEE Conferences"
"User reviews matter! Tracking crowdsourced reviews to support evolution of successful apps","F. Palomba; M. Linares-V√°squez; G. Bavota; R. Oliveto; M. Di Penta; D. Poshyvanyk; A. De Lucia","University of Salerno, Fisciano, Italy","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20151123","2015","","","291","300","Nowadays software applications, and especially mobile apps, undergo frequent release updates through app stores. After installing/updating apps, users can post reviews and provide ratings, expressing their level of satisfaction with apps, and possibly pointing out bugs or desired features. In this paper we show-by performing a study on 100 Android apps-how developers addressing user reviews increase their app's success in terms of ratings. Specifically, we devise an approach, named CRISTAL, for tracing informative crowd reviews onto source code changes, and for monitoring the extent to which developers accommodate crowd requests and follow-up user reactions as reflected in their ratings. The results indicate that developers implementing user reviews are rewarded in terms of ratings. This poses the need for specialized recommendation systems aimed at analyzing informative crowd reviews and prioritizing feedback to be satisfied in order to increase the apps success.","","Electronic:978-1-4673-7532-0; USB:978-1-4673-7531-3","10.1109/ICSM.2015.7332475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332475","","Androids;Entropy;Feature extraction;Humanoid robots;Joining processes;Monitoring;Planning","mobile computing;recommender systems;source code (software)","Android apps;CRISTAL;app stores;crowd request;crowdsourced review tracking;informative crowd review tracing;mobile apps;ratings;source code change;specialized recommendation systems;user reaction;user review","","15","","38","","","","Sept. 29 2015-Oct. 1 2015","","IEEE","IEEE Conferences"
"The Importance of Being Isolated: An Empirical Study on Chromium Reviews","S. Datta; D. Bhatt; M. Jain; P. Sarkar; S. Sarkar","Singapore Univ. of Technol. & Design, Singapore, Singapore","2015 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","20151109","2015","","","1","4","As large scale software development has become more collaborative, and software teams more globally distributed, several studies have explored how developer interaction influences software development outcomes. The emphasis so far has been largely on outcomes like defect count, the time to close modification requests etc. In the paper, we examine data from the Chromium project to understand how different aspects of developer discussion relate to the closure time of reviews. On the basis of analyzing reviews discussed by 2000+ developers, our results indicate that quicker closure of reviews owned by a developer relates to higher reception of information and insights from peers. However, we also find evidence that higher engagement in collaboration by a developer is associated with slower closure of the reviews she owns. Within the scope of our study, these results lead us to conclude that peer review of code may have a distinct dynamic that is facilitated by developers working in relative isolation.","1949-3770;19493770","Electronic:978-1-4673-7899-4; POD:978-1-4673-7900-7","10.1109/ESEM.2015.7321215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321215","","Chromium;Collaboration;Data mining;Data models;Electronic mail;Software;Software engineering","software engineering","Chromium project;large scale software development;software teams","","","","22","","","","22-23 Oct. 2015","","IEEE","IEEE Conferences"
"Turbo equalization for single-carrier underwater acoustic communications","Y. R. Zheng; J. Wu; C. Xiao","Missouri University of Science and Technology","IEEE Communications Magazine","20151109","2015","53","11","79","87","Recent research in underwater acoustic communications has taken advantage of MIMO technologies to achieve reliable communication with 10-100 times increase of data rate in comparison to traditional systems. The powerful turbo equalization and FEC coding techniques enable both single-carrier modulation and OFDM systems to combat triply selective UWA channels. This article reviews the time-domain and frequency-domain turbo equalizer schemes for MIMO SCM systems. Low-complexity techniques are presented with both turbo linear equalizers and turbo soft decision feedback equalizers in both the time and frequency domains. Although results are shown specifically for UWA channels, these turbo equalizer techniques are also suitable for terrestrial RF communication systems.","0163-6804;01636804","","10.1109/MCOM.2015.7321975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321975","","Channel estimation;Decision feedback equalizers;Decoding;MIMO;OFDM;Time-domain analysis;Underwater acoustics;Underwater communication","MIMO communication;decision feedback equalisers;forward error correction;frequency-domain analysis;time-domain analysis;underwater acoustic communication","FEC coding techniques;MIMO SCM systems;MIMO technologies;OFDM systems;frequency-domain turbo equalizer schemes;single-carrier modulation;terrestrial RF communication systems;time-domain turbo equalizer schemes;triply selective UWA channels;turbo equalization;turbo linear equalizers;turbo soft decision feedback equalizers;underwater acoustic communications","","3","","10","","","","November 2015","","IEEE","IEEE Journals & Magazines"
"Integrating Open Source Software Repositories on the Web through Linked Data","A. Iqbal; S. Decker","Insight Centre for Data Analytics, Nat. Univ. of Ireland, Galway, Ireland","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","114","121","In this paper, we propose a novel approach to the problem of integrating code forges based on metadata (e.g., programming language, database, intended audience, operating system), similar software projects and software developers. We review the current problems in integrating metadata of different code forges and argue that Semantic Web technologies are suitable for representing and integrating knowledge contained inside these code forges. Further, we show the advantages of interlinking metadata of software projects to other relevant data sources on the Web, which will enable querying more information from the Web. Moreover, we compute the overlapping between code forges based on similar software developers and argue the benefit of interlinking similar software developers and software projects across different code forges through examples.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300963","Data Integration;FLOSS;Linked Data;Semantic Web","Computer languages;Databases;Metadata;Operating systems;Resource description framework;Vocabulary","data integration;meta data;public domain software;semantic Web;software engineering;source code (software)","Linked Data;code forge integration;open source software repository;semantic Web technology;software development;software project metadata","","","","15","","","","13-15 Aug. 2015","","IEEE","IEEE Conferences"
"What Training is Needed by Practicing Engineers Who Create Cyberphysical Systems?","C. Scaffidi","Center for Appl. Syst. & Software, Oregon State Univ. Corvallis, Corvallis, OR, USA","2015 41st Euromicro Conference on Software Engineering and Advanced Applications","20151026","2015","","","298","305","The creation of cyber physical systems requires not only the effective education of future engineers in the classroom, but also ongoing training to support life-long learning by practicing engineers, who will continuously face new technologies and challenges. But what topics exactly should this training cover? To answer this question, an analysis was conducted of an online forum used by programmers who create cyber physical systems with the Lab VIEW development environment. This examination of over 150,000 forum message threads has highlighted crucial gaps in the training of practicing engineers, especially in the areas of hardware-software integration, as well as the design and implementation of user interfaces and algorithms. A complementary review of the available online training materials relevant to these topics reveals the need for more systematic support for learning by practicing system engineers. These results provide empirical support for ongoing efforts to provide more effective education of current and future engineers of cyber physical systems, including through reuse of third-party code and other resources to support learning.","1089-6503;10896503","Electronic:978-1-4673-7585-6; POD:978-1-4673-7586-3","10.1109/SEAA.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7302466","cyberphysical engineering;online education;training","Clustering algorithms;Cyber-physical systems;Hardware;Instruments;Training;User interfaces","computer science education;software engineering;user interfaces;virtual instrumentation","Lab VIEW development environment;cyber physical system;hardware-software integration;practicing engineer;user interface","","2","","17","","","","26-28 Aug. 2015","","IEEE","IEEE Conferences"
"Initial Evidence for Understanding the Relationship between Product Line Architecture and Software Architecture Recovery","C. R. L. Neto; M. P. S. Cardoso; C. v. F. G. Chavez; E. S. d. Almeida","","2015 IX Brazilian Symposium on Components, Architectures and Reuse Software","20151026","2015","","","40","49","Context: Over the years, the interest on software architecture recovery has increased. Due to software product line inherent complexity, the recovery of product line architecture is crucial to alleviate difficulties and enable benefits during the SPL development. Objective: In order to gather data and evidence about the relationship between product line architecture and software architecture recovery, (Method:) we performed a literature survey using some steps of systematic literature review method and an exploratory study. Results: We identified the appearance of more elaborated studies over the years and the majority of solution proposals are used to recover the architecture from legacy systems source code to provide the SPL reference architecture. Conclusion: Most of the studies presented solution proposal combined with case studies. But, only a few of them address empirical evaluation. With the software architecture recovery evolution, more research combining recovery of product line architecture and empirical evaluation still necessary.","","Electronic:978-1-4673-9630-1","10.1109/SBCARS.2015.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306126","Software Architecture;Software Product Lines;Software Reuse;Survey","Computer architecture;Data mining;Market research;Programmable logic arrays;Proposals;Software architecture;Software product lines","software architecture;software maintenance;software product lines;source code (software);system recovery","SPL development;SPL reference architecture;legacy system source code;product line architecture;software architecture recovery;software product line","","","","45","","","","21-22 Sept. 2015","","IEEE","IEEE Conferences"
"VERCE Delivers a Productive E-science Environment for Seismology Research","M. Atkinson; M. Carpen√©; E. Casarotti; S. Claus; R. Filgueira; A. Frank; M. Galea; T. Garth; A. Gem√ºnd; H. Igel; I. Klampanos; A. Krause; L. Krischer; S. H. Leong; F. Magnoni; J. Matser; A. Michelini; A. Rietbrock; H. Schwichtenberg; A. Spinuso; J. P. Vilotte","Univ. of Edinburgh, Edinburgh, UK","2015 IEEE 11th International Conference on e-Science","20151026","2015","","","224","236","The VERCE project has pioneered an e-Infrastructure to support researchers using established simulation codes on high-performance computers in conjunction with multiple sources of observational data. This is accessed and organised via the VERCE science gateway that makes it convenient for seismologists to use these resources from any location via the Internet. Their data handling is made flexible and scalable by two Python libraries, ObsPy and dispel4py and by data services delivered by ORFEUS and EUDAT. Provenance driven tools enable rapid exploration of results and of the relationships between data, which accelerates understanding and method improvement. These powerful facilities are integrated and draw on many other e-Infrastructures. This paper presents the motivation for building such systems, it reviews how solid-Earth scientists can make significant research progress using them and explains the architecture and mechanisms that make their construction and operation achievable. We conclude with a summary of the achievements to date and identify the crucial steps needed to extend the capabilities for seismologists, for solid-Earth scientists and for similar disciplines.","","Electronic:978-1-4673-9325-6; POD:978-1-4673-9326-3","10.1109/eScience.2015.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7304295","Data Science;Data-Intensive;HPC;Metadata and Storage;Science Gateway;Virtual Research Environment;e-Infrastructure;solid-Earth Sciences","Analytical models;Computational modeling;Context;Data models;Earthquakes;Libraries;Logic gates","Internet;data handling;geophysics computing;parallel processing;seismology;simulation","EUDAT;Internet;ORFEUS;ObsPy;Python libraries;VERCE project;data handling;data services;dispel4py;e-infrastructure;high performance computers;productive e-science environment;provenance driven tools;seismology research;simulation codes","","5","","35","","","","Aug. 31 2015-Sept. 4 2015","","IEEE","IEEE Conferences"
"Computing outreach literature review","A. Decker; M. M. McGill; A. Settle","School of Interactive Games & Media and RIT Center for Media, Arts, Games, Interaction and Creativity, Rochester Institute of Technology, Rochester, NY, USA","2015 Research in Equity and Sustained Participation in Engineering, Computing, and Technology (RESPECT)","20151012","2015","","","1","2","The lack of diversity in computing has existed for decades. It has garnered the attention of computing educators and private companies who have implemented a host of outreach and retention programs to draw more diverse students into the field and the workforce. A question that stands out is whether or not these programs are effective in the long term in helping to mitigate the lack of diversity in the field. To determine an answer to this question, the authors undertook a systematic literature review of reported computing outreach activities in relevant journals and conference proceedings for the years 2009‚Äì2014 inclusive. Upon consideration of all relevant articles, 73 articles were coded for information about the type of outreach, target audience, and reported results. Summaries of the findings of the literature review are presented in this poster.","","Electronic:978-1-5090-0151-4; POD:978-1-5090-0152-1","10.1109/RESPECT.2015.7296509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296509","after school programs;broadening participation;diversity issues in computing;gender issues in computing;outreach","","","","","","","","","","","13-14 Aug. 2015","","IEEE","IEEE Conferences"
"Net2plan: an open source network planning tool for bridging the gap between academia and industry","P. Pavon-Marino; J. L. Izquierdo-Zaragoza","Universidad Politecnica de Cartagena","IEEE Network","20151008","2015","29","5","90","96","The plethora of network planning results published in top-ranked journals is a good sign of the success of the network planning research field. Unfortunately, it is often difficult for network carriers and ISPs to reproduce these investigations on their networks. This is partially because of the absence of a software planning tool, meeting the requirements of industry and academia, which can make the adaptation and validation of planning algorithms less time consuming. We describe how a paradigm shift to an open source view of the network planning field emphasizes the power of distributed peer review and transparency to create high-quality software at an accelerated pace and lower cost. Then we present Net2Plan, an open source Java-based software tool. Built on top of a technology-agnostic network representation, it automates the elaboration of performance evaluation tests for userdefined or built-in network design algorithms, network recovery schemes, connection-admission-control systems, or dynamic provisioning algorithms for timevarying traffic. The Net2Plan philosophy enforces code reutilization as an open repository of network planning resources. In this article, a case study in a multilayer IP-over-WDM network is presented to illustrate the potential of Net2Plan. We cover standard CAPEX studies, and more advanced aspects such as a resilience analysis of the network under random independent failures and disaster scenarios, and an energy efficiency assessment of ‚Äúgreen‚Äù schemes that switch off parts of the network during low load periods. All the planning algorithms in this article are publicly available on the Net2Plan website.","0890-8044;08908044","","10.1109/MNET.2015.7293311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293311","","Algorithm design and analysis;IP networks;Industries;Nonhomogeneous media;Optical fiber devices;Optical fiber networks;Telecommunication network planning","IP networks;Java;computer network performance evaluation;public domain software;software tools;telecommunication computing;telecommunication network planning;telecommunication traffic;wavelength division multiplexing","ISP;Net2Plan philosophy;Net2plan;academia;built-in network design algorithm;code reutilization;connection-admission-control system;disaster scenario;distributed peer review;dynamic provisioning algorithm;energy efficiency assessment;high-quality software;industry;multilayer IP-over-WDM network;network carrier;network planning research field;network planning resource;network recovery scheme;open repository;open source Java-based software tool;open source network planning tool;open source view;performance evaluation test;planning algorithm;random independent failure;resilience analysis;software planning tool;standard CAPEX study;technology-agnostic network representation;timevarying traffic;top-ranked journal;userdefined design algorithm","","19","","15","","","","September-October 2015","","IEEE","IEEE Journals & Magazines"
"Mapping Bug Reports to Relevant Files: A Ranking Model, a Fine-Grained Benchmark, and Feature Evaluation","X. Ye; R. Bunescu; C. Liu","School of Electrical Engineering and Computer Science, Ohio University, Athens, OH","IEEE Transactions on Software Engineering","20160414","2016","42","4","379","402","When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and improve productivity. This paper introduces an adaptive ranking approach that leverages project knowledge through functional decomposition of source code, API descriptions of library components, the bug-fixing history, the code change history, and the file dependency graph. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluate the ranking system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the learning-to-rank approach outperforms three recent state-of-the-art methods. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70 percent of the bug reports in the Eclipse Platform and Tomcat projects.","0098-5589;00985589","","10.1109/TSE.2015.2479232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7270328","Bug reports;learning to rank;software maintenance","Benchmark testing;Collaboration;Computational modeling;Computer bugs;History;Software;Standards","","","","1","","72","","","20150916","April 1 2016","","IEEE","IEEE Journals & Magazines"
"State-of-the-art video coding approaches: A survey","M. Xu; Y. Liang; Z. Wang","School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China","2015 IEEE 14th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","20150914","2015","","","284","290","The past few decades have witnessed the explorative growth of video services around humans. However, due to the limited spectrum, video coding has to be developed for efficient video delivery. The core of video coding is compressing video frames by exploiting their redundancy. Aiming at decreasing spatio-temporal redundancy, several video coding standards have been proposed with a hybrid framework during the past two decades. We therefore first survey in this paper the existing standards on video coding. Second, we review some other video coding approaches, which take advantage of state-of-the-art computer vision and machine learning technologies, for lessening both spatio-temporal and perceptual redundancy of images/videos. In retrospect of what has been achieved so far, we finally outlook what the future may hold for video coding.","","CD-ROM:978-1-4673-7289-3; Electronic:978-1-4673-7290-9; POD:978-1-4673-7291-6","10.1109/ICCI-CC.2015.7259399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7259399","Video coding standards;video coding","Computational modeling;Encoding;Image segmentation;Redundancy;Video coding","computer vision;learning (artificial intelligence);redundancy;video coding","computer vision technologies;machine learning technologies;perceptual redundancy;spatio-temporal redundancy;video coding;video frames compression;video services","","","","59","","","","6-8 July 2015","","IEEE","IEEE Conferences"
"Leveraging Informal Documentation to Summarize Classes and Methods in Context","L. Guerrouj; D. Bourque; P. C. Rigby","Dept. of Software Eng., Concordia Univ., Montreal, QC, Canada","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","2","","639","642","Critical information related to a software developer'scurrent task is trapped in technical developer discussions,bug reports, code reviews, and other software artefacts. Muchof this information pertains to the proper use of code elements(e.g., methods and classes) that capture vital problem domainknowledge. To understand the purpose of these code elements, software developers must either access documentation and online posts and understand the source code or peruse a substantial amount of text. In this paper, we use the context that surrounds code elements in StackOverflow posts to summarize the use and purpose of code elements. To provide focus to our investigation, we consider the generation of summaries for library identifiers discussed in StackOverflow. Our automatic summarization approach was evaluated on a sample of 100 randomly-selected library identifiers with respect to a benchmark of summaries provided by two annotators. The results show that the approach attains an R-precision of 54%, which is appropriate given the diverse ways in which code elements can be used.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203032","","Androids;Context;Documentation;Humanoid robots;Java;Libraries;Software","software engineering;source code (software)","StackOverflow;automatic summarization approach;code elements;informal documentation;library identifiers;software artefacts;source code","","0","","22","","","","16-24 May 2015","","IEEE","IEEE Conferences"
"Approximating Attack Surfaces with Stack Traces","C. Theisen; K. Herzig; P. Morrison; B. Murphy; L. Williams","Dept. of Comput. Sci., NCSU, Raleigh, NC, USA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","2","","199","208","Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202964","attack surface;models;reliability;security;stack traces;testing;vulnerability","Approximation methods;Computer crashes;Measurement;Predictive models;Security;Software;Surface treatment","decision making;program diagnostics;project management;software engineering","Windows 8;attack surface approximation;attack surface measurement;decision-making;effort reviewing;security testing;software development;software projects;stack trace analysis;vulnerable code identification","","6","","44","","","","16-24 May 2015","","IEEE","IEEE Conferences"
"Specifications for Web Services Testing: A Systematic Review","E. I. Nabil","Fac. of Comput. Sci. Free, Univ. of Bozen/Bolzano, Bolzano, Italy","2015 IEEE World Congress on Services","20150817","2015","","","152","159","In today's ever increasing demand on loosely coupled systems, software providers need to produce solutions that are flexible enough to be configured at runtime, yet maintaining high quality. One way to address the quality of such systems is through testing. Software testing provides software providers and their clients with techniques to characterize the internal and external quality of their systems, by specifying test cases to check how systems react under different circumstances. In the context of web services the situation changes, software providers become service providers. In other words, they do not provide a physical software to their clients, instead they allow them to invoke remote code from their system. This change affects the way we look at testing itself. In the web service paradigm clients do not own services, instead they invoke them. Different approaches for testing web services have been reported in the literature, with different techniques and using different input information to generate test cases. Considering the topic of web services testing, as a subject for this systematic review is a challenging undertaking, mainly due to the extensive amount of literature in the subject. To narrow it, we have focused on one single particularity that affects dramatically the testing process. That is the specifications used to generate test cases. To this end, in this paper we present a systematic review of the specifications used for testing web services. The outcome of this study answers key questions involved in the advantages and disadvantages of the existing specifications as well as the potential improvements that could lead to more robust web services testing process.","2378-3818;23783818","Electronic:978-1-4673-7275-6; POD:978-1-4673-7276-3","10.1109/SERVICES.2015.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7196519","specification testing;web services testing","Contracts;Data mining;Databases;Systematics;Testing;Web services;XML","Web services;program testing;software quality","Web services testing;external quality;internal quality;software providers;software testing","","2","","32","","","","June 27 2015-July 2 2015","","IEEE","IEEE Conferences"
"Open Source-Style Collaborative Development Practices in Commercial Projects Using GitHub","E. Kalliamvakou; D. Damian; K. Blincoe; L. Singer; D. M. German","","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","1","","574","585","Researchers are currently drawn to study projects hosted on GitHub due to its popularity, ease of obtaining data, and its distinctive built-in social features. GitHub has been found to create a transparent development environment, which together with a pull request-based workflow, provides a lightweight mechanism for committing, reviewing and managing code changes. These features impact how GitHub is used and the benefits it provides to teams' development and collaboration. While most of the evidence we have is from GitHub's use in open source software (OSS) projects, GitHub is also used in an increasing number of commercial projects. It is unknown how GitHub supports these projects given that GitHub's workflow model does not intuitively fit the commercial development way of working. In this paper, we report findings from an online survey and interviews with GitHub users on how GitHub is used for collaboration in commercial projects. We found that many commercial projects adopted practices that are more typical of OSS projects including reduced communication, more independent work, and self-organization. We discuss how GitHub's transparency and popular workflow can promote open collaboration, allowing organizations to increase code reuse and promote knowledge sharing across their teams.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.74","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194607","GitHub;collaboration;commercial projects;coordination;open source;practices;pull requests;workflow","Collaborative software;Encoding;Interviews;Organizations;Software;Writing","computer software;public domain software","GitHub users;OSS projects;built-in social features;code changes;commercial projects;knowledge sharing;online code hosting service;open source software projects;open source-style collaborative development practices;pull request-based workflow;self-organization;transparent development environment","","9","","49","","","","16-24 May 2015","","IEEE","IEEE Conferences"
"Quality Questions Need Quality Code: Classifying Code Fragments on Stack Overflow","M. Duijn; A. Kucera; A. Bacchelli","Delft Univ. of Technol., Delft, Netherlands","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","410","413","Stack Overflow (SO) is a question and answers (Q&A) web platform on software development that is gaining in popularity. With increasing popularity often comes a very unwelcome side effect: A decrease in the average quality of a post. To keep Q&A websites like SO useful it is vital that this side effect is countered. Previous research proved to be reasonably successful in using properties of questions to help identify low quality questions to be later reviewed and improved. We present an approach to improve the classification of high and low quality questions based on a novel source of information: the analysis of the code fragments in SO questions. We show that we get similar performance to classification based on a wider set of metrics thus potentially reaching a better overall classification.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180105","","Accuracy;Algorithm design and analysis;Classification algorithms;Correlation;Decision trees;Java;Measurement","Web sites;pattern classification;question answering (information retrieval)","Q&A Websites;SO questions;code fragment classification;quality code;quality questions;stack overflow","","1","","10","","","","16-17 May 2015","","IEEE","IEEE Conferences"
"On the Alignment of Source Code Quality Perspectives through Experimentation: An Industrial Case","T. V. Ribeiro; G. H. Travassos","Programa de Eng. de Sist. e Comput., Univ. Fed. do Rio de Janeiro, Rio de Janeiro, Brazil","2015 IEEE/ACM 3rd International Workshop on Conducting Empirical Studies in Industry","20150727","2015","","","26","33","CONTEXT. Alignment is a key factor for success in many software development projects. Aligned teams are capable of bringing collaboration and positive results to companies; whereas misalignment among developers can make a conflicted environment and even lead the project to failure. OBJECTIVE. To assist developers in an embedded software development company in their conceptual alignment regarding source code quality. METHOD. In the organizational context, plan and perform a series of studies such as surveys, systematic literature review (SLR), qualitative data analysis and focus group to support the identification of conceptual misalignments among developers and establish common terminology and guidance concerning source code quality. RESULTS. The results from a survey conducted in one company showed a conceptual misalignment among developers regarding the source code quality that was triggering continuous rework during software evolution activities. Through an SLR and a qualitative analysis of code snippets, a set of evidence-based coding guidelines for readability and understandability of source code were formulated. These guidelines were evaluated and used as an instrument for aligning source code perspectives during a focus group, showing their feasibility and adequacy to the company's context. CONCLUSIONS. The use of all contextual information observed - e.g. teams' locations, software development context, and time constraints - along with the information gathered during the industry-academia collaboration was particularly important to help us appropriately chose research methods to be used, and formulate evidence-based coding guidelines that matched the company's needs and expectations. Further evaluations have to be carried out to ensure the quality impact of some guidelines proposed before using them all over the company.","","Electronic:978-1-4673-7028-8; POD:978-1-4673-7029-5","10.1109/CESI.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167423","code quality;conceptual alignment;experimental studies;industry-academia collaboration","Bibliographies;Collaboration;Companies;Context;Encoding;Guidelines;Software","software quality;source code (software)","SLR;code snippets;conceptual misalignment;embedded software development company;evidence-based coding guidelines;industry-academia collaboration;qualitative data analysis;software development projects;software evolution activity;source code quality alignment;systematic literature review","","0","","10","","","","18-18 May 2015","","IEEE","IEEE Conferences"
"A Robust Approach for the Analysis of EMI/EMC Problems With Nonlinear Circuit Loads","J. T. Williams; L. D. Bacon; M. J. Walker; E. C. Zeek","Directed Energy Special Applications, Sandia National Laboratories, Albuquerque, NM, USA","IEEE Transactions on Electromagnetic Compatibility","20150813","2015","57","4","680","687","The analysis of electromagnetic coupling in nonlinear circuit simulations requires a bidirectional, fully consistent approach. Nonlinear responses of semiconductor devices in electronic circuit components can change the impedances seen at circuit nodes, changing the boundary conditions encountered by impressed electromagnetic fields and, thus, changing the characteristics of the energy coupled from these external fields into that circuit. It is important to include the coupling in the circuit simulation self-consistently because this allows us to accurately predict the responses to various EMI/EMC problems of interest. It is also important to predict circuit responses efficiently because that opens the door to statistical applications for the technique being used. In this paper, we review a technique that we have developed called A Thevenin Equivalent Network Approach. This approach is shown to be quite robust in that it is computationally efficient; it can be implemented in a variety of commonly available circuit solving codes; it already includes a few additional techniques required to enhance its implementation in those codes; and it is quite accurate.","0018-9375;00189375","","10.1109/TEMC.2015.2438065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155538","Circuit transient analysis;computational electromagnetics;electromagnetic coupling;nonlinear network analysis","Couplings;Frequency-domain analysis;Impedance;Ports (Computers);Resistors;Time-domain analysis;Transmission line matrix methods","electromagnetic compatibility;electromagnetic interference;statistical analysis","ATHENA;EMC problems;EMI problems;a thevenin equivalent network approach;electronic circuit components;nonlinear circuit simulations;robust approach;semiconductor devices;statistical applications","","2","","12","","","20150713","Aug. 2015","","IEEE","IEEE Journals & Magazines"
"Programming Examples Analysis of Commercial Vocational High School Textbooks in Taiwan","C. F. Chiu","Inst. of Curriculum Instruction & Technol., Nat. Chi Nan Univ., Nantou, Taiwan","2015 International Conference on Learning and Teaching in Computing and Engineering","20150618","2015","","","223","224","This study aims to analyze the programming examples of commercial vocational high school textbooks in Taiwan. Five programming textbooks widely adopted by schools were reviewed to examine the presentation of programming examples' problem-solving steps: problem analysis, solution planning, coding and testing/debugging. It is found that the examples lack abundant guidance of problem-solving steps for students, especially little attention was paid to problem analysis, solution planning and testing/debugging step. Most examples focus on the description of program codes rather than the detail steps of problem-solving.","","Electronic:978-1-4799-9967-5; POD:978-1-4799-9968-2","10.1109/LaTiCE.2015.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7126266","Computer Science Education;Programming;Textbook","Computers;Debugging;Education;Planning;Problem-solving;Programming profession","computer science education;educational courses;program debugging;program testing;vocational training","Taiwan;commercial vocational high school textbooks;problem analysis;program coding;program debugging;program testing;programming example analysis;programming textbooks;solution planning","","0","","11","","","","9-12 April 2015","","IEEE","IEEE Conferences"
"Improving Software Quality as Customers Perceive It","R. Hackbarth; A. Mockus; J. Palframan; R. Sethi","Avaya Labs Research","IEEE Software","20160623","2016","33","4","40","45","A proposed data-driven software quality improvement method has three elements. First, the downstream Customer Quality Metric (CQM) quantifies quality as customers perceive it. On the basis of data collected after systems are deployed, it measures how serious defects affect customers. Second, the upstream Implementation Quality Index (IQI) measures the effectiveness of error removal during development. IQI predicts future customer quality; it has a positive correlation with CQM. Finally, prioritization tools and techniques help focus limited development resources on the riskiest files in the code. This research is based on a multiyear program to improve the quality of delivered systems at Avaya, a global provider of business communication and collaboration systems. Regular reviews with Avaya's R&D Quality Council provided governance for the program.","0740-7459;07407459","","10.1109/MS.2015.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7106410","Customer Quality Metric;Implementation Quality Index;case study;customer perceived quality;data-driven software process improvement;software development;software quality assurance software engineering;software quality method;software risk mitigation","Behavioral science;Computational fluid dynamics;Customer services;Predictive models;Software measurement;Software systems","software metrics;software process improvement;software quality","customer quality metric;data-driven software quality improvement;implementation quality index","","1","","11","","","20150513","July-Aug. 2016","","IEEE","IEEE Journals & Magazines"
"Who Should Review this Pull-Request: Reviewer Recommendation to Expedite Crowd Collaboration","Y. Yu; H. Wang; G. Yin; C. X. Ling","Nat. Lab. for Parallel & Distrib. Process., Nat. Univ. of Defense Technol., Changsha, China","2014 21st Asia-Pacific Software Engineering Conference","20150423","2014","1","","335","342","Github facilitates the pull-request mechanism as an outstanding social coding paradigm by integrating with social media. The review process of pull-requests is a typical crowd sourcing job which needs to solicit opinions of the community. Recommending appropriate reviewers can reduce the time between the submission of a pull-request and the actual review of it. In this paper, we firstly extend the traditional Machine Learning (ML) based approach of bug triaging to reviewer recommendation. Furthermore, we analyze social relations between contributors and reviewers, and propose a novel approach to recommend highly relevant reviewers by mining comment networks (CN) of given projects. Finally, we demonstrate the effectiveness of these two approaches with quantitative evaluations. The results show that CN-based approach achieves a significant improvement over the ML-based approach, and on average it reaches a precision of 78% and 67% for top-1 and top-2 recommendation respectively, and a recall of 77% for top-10 recommendation.","1530-1362;15301362","Electronic:978-1-4799-7426-9; POD:978-1-4799-7427-6","10.1109/APSEC.2014.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091328","Comment Network;Pull-request;Reviewer Recommendation;Social Coding","Communities;Encoding;Mathematical model;Rails;Social network services;Software;Training","data mining;learning (artificial intelligence);program debugging;recommender systems;social networking (online)","CN-based approach;Github;ML-based approach;bug triaging;comment networks mining;crowd collaboration;crowdsourcing job;machine learning based approach;pull-request mechanism;reviewer recommendation;social media","","5","","21","","","","1-4 Dec. 2014","","IEEE","IEEE Conferences"
"Untangling fine-grained code changes","M. Dias; A. Bacchelli; G. Gousios; D. Cassou; S. Ducasse","RMoD Inria Lille-Nord Europe, University of Lille ? CRIStAL, France","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20150409","2015","","","341","350","After working for some time, developers commit their code changes to a version control system. When doing so, they often bundle unrelated changes (e.g., bug fix and refactoring) in a single commit, thus creating a so-called tangled commit. Sharing tangled commits is problematic because it makes review, reversion, and integration of these commits harder and historical analyses of the project less reliable. Researchers have worked at untangling existing commits, i.e., finding which part of a commit relates to which task. In this paper, we contribute to this line of work in two ways: (1) A publicly available dataset of untangled code changes, created with the help of two developers who accurately split their code changes into self contained tasks over a period of four months; (2) a novel approach, EpiceaUntangler, to help developers share untangled commits (aka. atomic commits) by using fine-grained code change information. EpiceaUntangler is based and tested on the publicly available dataset, and further evaluated by deploying it to 7 developers, who used it for 2 weeks. We recorded a median success rate of 91% and average one of 75%, in automatically creating clusters of untangled fine-grained code changes.","1534-5351;15345351","Electronic:978-1-4799-8469-5; POD:978-1-4799-8470-1","10.1109/SANER.2015.7081844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081844","","Clustering algorithms;Java;Machine learning algorithms;Reliability;Software;Testing;Training","software maintenance","EpiceaUntangler;fine-grained code change information;fine-grained code change untangling;version control system","","12","","28","","","","2-6 March 2015","","IEEE","IEEE Conferences"
"Improving the integration process of large software systems","Y. Jiang","MCIS lab, Polytechnique Montreal, Canada","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20150409","2015","","","598","598","Software integration is the software engineering activity where code changes of different developers are combined into a consistent whole. While the advent of distributed version control systems has allowed distributed development to scale up substantially, at the same time the risk of integration conflicts (changes that do not go well together) and the time required to fix them has increased as well. In order to help practitioners deal with this paradox, this thesis aims to understand and improve the integration process of modern software organizations. We took the Linux kernel as our pilot case study, which is supported by a distributed version control system (Git) and low-tech reviewing system (mailing list). So far, we have (1) analyzed how to reconstruct the data of the integration process in a low-tech environment where reviews are stored in emails without explicit link to version control commits, and (2) studied the characteristics of the Linux integration process. We found that the commits developed by more mature developers and impacting less subsystems are more likely to be accepted. As a next step, we plan to build a model quantifying the integration effort.","1534-5351;15345351","Electronic:978-1-4799-8469-5; POD:978-1-4799-8470-1","10.1109/SANER.2015.7081888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081888","","Analytical models;Electronic mail;History;Kernel;Linux;Software engineering","Linux;configuration management;distributed processing;integrated software;operating system kernels;software engineering","Git;Linux integration process;Linux kernel;distributed version control system;large software systems integration process;low-tech reviewing system;mailing list;software code changes;software engineering activity;version control commits","","0","","2","","","","2-6 March 2015","","IEEE","IEEE Conferences"
"Fundamental limits of energy harvesting communications","O. Ozel; K. Tutuncuoglu; S. Ulukus; A. Yener","University of Maryland","IEEE Communications Magazine","20150408","2015","53","4","126","132","Wireless networks composed of energy harvesting devices will introduce several transformative changes in wireless networking as we know it: energy self-sufficient, energy self-sustaining, perpetual operation; reduced use of conventional energy and accompanying carbon footprint; untethered mobility; and an ability to deploy wireless networks in hard-to-reach places such as remote rural areas, within structures, and within the human body. Energy harvesting brings new dimensions to the wireless communication problem in the form of intermittency and randomness of available energy, which necessitates a fresh look at wireless communication protocols at the physical, medium access, and networking layers. Scheduling and optimization aspects of energy harvesting communications in the medium access and networking layers have been relatively wellunderstood and surveyed in the recent paper [1]. This branch of literature takes a physical layer rate-power relationship that is valid in energy harvesting conditions under large-enough batteries and long-enough durations between energy harvests so that information-theoretic asymptotes are achieved, and optimizes the transmit power over time in order to maximize the throughput. Another branch of recent literature aims to understand the fundamental capacity limits, i.e. information-theoretic capacities, of energy harvesting links under smaller scale dynamics, considering energy harvests at the channel use level. This branch necessitates a deeper look at the coding and transmission schemes in the physical layer, and ultimately aims to develop an information theory of energy harvesting communications, akin to Shannon's development of an information theory for average power constrained communications. In this introductory article, we survey recent results in this branch and point to open problems that could be of interest to a broad set of researchers in the fields of communication theory, information theory, signal processing, and netw- rking. In particular, we review capacities of energy harvesting links with infinite-sized, finitesized, and no batteries at the transmitter.","0163-6804;01636804","","10.1109/MCOM.2015.7081085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081085","","Batteries;Channel capacity;Energy harvesting;Transmitters;Wireless networks","energy harvesting;information theory;optimisation;protocols;radiocommunication;telecommunication scheduling","Shannons development;coding schemes;energy harvesting communications optimization;energy harvesting communications scheduling;energy harvesting devices;information theory;information-theoretic asymptotes;transmission schemes;wireless communication protocols;wireless networks","","23","","15","","","","April 2015","","IEEE","IEEE Journals & Magazines"
"Social context, singular focus","D. R. Chesney","Electrical Engineering and Computer Science Department, The University of Michigan, Ann Arbor, MI, USA","2014 IEEE Frontiers in Education Conference (FIE) Proceedings","20150219","2014","","","1","6","In the Computer Science Department at the University of Michigan (UM), we have spent the past five years building social context into several courses in the traditional computer science (CS) curriculum. Specifically, freshman- and senior-level, project-based classes have been designed to both teach significant and appropriate academic content, while also building software games and apps for children with cognitive and/or physical disabilities within the university's associated hospital. Children with disabilities provide the context for these courses, while the content remains representative of a traditional curriculum. The cadence of the course(s) is typically: propose, pitch, form groups, design, build, test, and improve. Each student submits a written proposal for the project. A group of experts read all of the proposals, and choose the `best' for pitching to the other students in the course. Groups are formed, using student preferences, based upon the pitches. The newly formed groups follow an engineering process that includes gathering requirements and designing the software system. Student groups partition the work, and implement three iterations (alpha, beta, final) of their designs. Groups are re-formed, and the newly formed groups test code from each other's groups. Finally, original development groups have the ability to make final improvements to their code. Student groups are offered the opportunity to continue work on promising projects through subsequent independent studies or internships. If the software project is `minimally viable', the groups are encouraged to continue development and work with the university's Center for Entrepreneurship or Office of Technology Transfer toward eventual commercialization. During the past academic year (2013-2014), one of the senior-level courses had the singular focus of developing software apps and games for a 13 year old girl with athetoid cerebral palsy. Her family was actively involved in the course, filt- ring project ideas and offering suggestions for improvement. Some of the most promising apps are currently installed in the family's house for methodical review and eventual improvement. This paper will discuss the general process that is used in this style of course, the specific approach that was used during the 2013-2014 academic year, and offer general suggestions for implementation in disciplines other than computer science.","0190-5848;01905848","Electronic:978-1-4799-3922-0; POD:978-1-4799-3923-7; USB:978-1-4799-3921-3","10.1109/FIE.2014.7044467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044467","collaborative learning;social context;software engineering","Computer science;Context;Educational institutions;Games;Hospitals;Proposals;Software","computer games;computer science;educational courses;handicapped aids","Center for Entrepreneurship;Office of Technology Transfer;University of Michigan;academic content;apps;athetoid cerebral palsy;children with cognitive disabilities;children with physical disabilities;freshman-level;l computer science curriculum;project-based classes;senior-level;social context;software games;software project","","0","","12","","","","22-25 Oct. 2014","","IEEE","IEEE Conferences"
"Efficient embedded SoC hardware/software codesign using virtual platform","M. A. El-Moursy; A. Sheirah; M. Safar; A. Salem","Mentor Graphics Corporation, Cairo, Egypt","2014 9th International Design and Test Symposium (IDT)","20150212","2014","","","36","38","A complete framework and methodology to design, simulate, and debug large SoC is presented. Full VP creation using efficient tools is described. An efficient tool to allow co-debug of HW/SW on VP is also presented. The tools enable debugging and analyzing an application and a Linux driver that run on the VP. Breakpoints and mon commands can be used to detect and correct errors, access registers and review their values. The tools provide simulation of SW and HW on the same timeline. They also, involve building, uploading and debugging a Linux driver on the VP. The procedure steps for debugging the application code on the VP are provided. How to create an analyzer project with an analyzer session to perform SW and HW analysis, and save the results are also described. Functions and capabilities to investigate the tracing results are presented. Preparing the Environment of Linux Software Development with VP is needed before running the debug flow. How to prepare the system environment is summarized. Complex applications can run on the VP. Debugging both the application and the Linux drivers, and analyzing both the SW and the HW are made easy. Powerful SW tracing is provided. HW architecture analysis is an additional domain to be explored by the methodology. SW and HW profiling is shown to be not only feasible, but also handy. Very graphical waveforms and user friendly environment with easy Graphical User Interface (GUI) show how flexible and powerful the methodology is. A test case demonstrating the flexibility and efficiency of our technique is presented.","2162-0601;21620601","Electronic:978-1-4799-8200-4; POD:978-1-4799-8201-1; USB:978-1-4799-8199-1","10.1109/IDT.2014.7038583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7038583","Co-Design;Co-Simulation;SystemC;TLM;Virtual Platform","Debugging;Hardware;Registers;Software;System-on-chip;Time-domain analysis;Time-varying systems","Linux;graphical user interfaces;hardware-software codesign;integrated circuit design;system-on-chip","GUI;HW-SW co-debugging;Linux Software Development;Linux driver debugging;Linux driver uploading;SW tracing;VP;access registers;debug flow;embedded SoC hardware-software codesign;error correction;error detection;full-VP creation;graphical user interface;graphical waveforms;large-SoC debugging;large-SoC design;large-SoC simulation;user-friendly environment;virtual platform","","0","","9","","","","16-18 Dec. 2014","","IEEE","IEEE Conferences"
"Presentation 10. WaFeR: Model-driven test-framework for testing web UI-based applications","G. N. Iyer; S. Maddala; S. Kishore; P. S. Kolamala","Progress Software Development, India","2014 IT Professional Conference","20150205","2014","","","1","3","With the emergence of Web 2.0 and other high-end technologies such as Cloud Computing, software applications are increasingly developed and used as Software as a Service (SaaS) applications [1]. There are several challenges associated with automated testing of such web-driven applications. Test developers often need to write several lines of automation scripts which is time consuming, needs excellent programming knowledge and results in different coding styles. Moreover, these are error prone. i.e. test developers tend to write workarounds for complex scenarios. Hence these code needs to go through several review phases to stabilize them. Further, code maintenance is difficult and time consuming for frequent changes in product design. Any change in the flow of test case requires code changes and testing of the code. Hence this is difficult to fit in Agile world. In order to overcome the above challenges, we propose a Model-driven test framework called WaFeR. Here, different use cases or application scenarios on the web application will be modeled as the test cases in the framework. Thus, we model the application scenarios (or flows) as the test cases. In addition to solving the above problems, it has many other inherent advantages. For example, the proposed model allows users to incorporate useful features as utility functions. Further, the intelligence is coded into the framework to understand logical and semantic errors in your test flow.","","Electronic:978-1-4799-4141-4; POD:978-1-4799-8858-7","10.1109/ITPRO.2014.7029298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029298","","Cloud computing;Educational institutions;Industries;Semiconductor device modeling;Software as a service;Testing","Internet;product design;program testing;user interfaces","Agile world;SaaS;WaFeR;Web 2.0;Web UI-based application testing;automation scripts;cloud computing;code maintenance;logical errors;model-driven test-framework;product design;semantic errors;software as a service;utility functions","","0","","","","","","22-22 May 2014","","IEEE","IEEE Conferences"
"Rapid Releases and Patch Backouts: A Software Analytics Approach","R. Souza; C. Chavez; R. A. Bittencourt","Federal University of Bahia","IEEE Software","20150310","2015","32","2","89","96","Release engineering deals with decisions that impact the daily lives of developers, testers, and users and thus contribute to a product's success. Although gut feeling is important in such decisions, it's increasingly important to leverage existing data, such as bug reports, source code changes, code reviews, and test results, both to support decisions and to help evaluate current practices. The exploration of software engineering data to obtain insightful information is called software analytics.","0740-7459;07407459","","10.1109/MS.2015.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7006390","Firefox;Mozilla;Web browsers;bug reopening;rapid releases;release engineering;software analytics;software development;software engineering","Browsers;Computer bugs;Continuous production;Linux;Market research;Marketing and sales;Software analytics;Software development;Software engineering","data analysis;software engineering","Mozilla;patch backouts;rapid releases;software analytics approach;software engineering data","","8","","5","","","20150112","Mar.-Apr. 2015","","IEEE","IEEE Journals & Magazines"
"Structural Analysis for Simple Games Source Codes Applied to Programming Learning","E. C. O. d. Santos; G. B. Batista; V. H. V. d. Sousa; E. W. G. Clua","Inst. Fed. de Educ., Cienc. e Tecnol. do Piaui, Teresina, Brazil","2014 Brazilian Symposium on Computer Games and Digital Entertainment","20150108","2014","","","71","79","Teaching programming and algorithms is a big challenge, not only in universities but also in schools and training centers. Many proposals for stimulating this process were made in the last years. Previously to this work we had developed JPlay. The JPlay framework was proposed and developed for teaching programming with the development of simple 2D games. In this paper we propose a heuristic based on the structural analysis of the behaviors of a JPlay program and, based on this heuristic, we developed a tool that makes analyzes of JPlay programs, guiding and teaching a student for a specific game development. The heuristic consists on a comparison approach between the student program and the model program and it has four levels of analysis: the sequential code pattern of the JPlay, standardization model, the comparison of similar classes and construction of behavior trees of similar variables. Thus, the comparison consists on searching behaviors of correspondence between pairs of classes among these programs. In this paper we also present a review of results of BrickBreak game based on the source code of the integrated high school students in the course of Computers.","2159-6654;21596654","Electronic:978-1-4799-8065-9; POD:978-1-4799-8066-6","10.1109/SBGAMES.2014.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000034","JPLAY;behavior;games;heuristic;learning;programming","Algorithm design and analysis;Analytical models;Education;Games;Programming;Proposals;Sprites (computer)","computer aided instruction;computer games;computer science education;educational courses;programming;source code (software);teaching","2D games;BrickBreak game;JPlay framework;JPlay program;JPlay sequential code pattern;algorithm teaching;computer course;game development;integrated high school students;model program;programming learning;programming teaching;similar class comparison;similar variable behavior trees construction;simple games source codes;standardization model;structural analysis;student program","","0","","17","","","","12-14 Nov. 2014","","IEEE","IEEE Conferences"
"Supporting Reviewing of Warnings in Presence of Shared Variables: Need and Effectiveness","T. Muske","Tata Res. Dev. & Design Center, Pune, India","2014 IEEE International Symposium on Software Reliability Engineering Workshops","20141215","2014","","","104","107","Clustering, breaking a large system into multiple clusters is commonly used technique to scale static analysis tools to large systems. Sound static analysis of resulted clusters requires conservative approach for the inter-cluster communication implemented through shared variables. We observe, this approach adds to the large number of warnings generated due to imprecision of static analysis. Further, manual reviewing of warnings resulted due to shared variables takes much more efforts as compared to efforts required in reviewing of the other class of warnings. This paper aims to assist reviewing of such warnings impacted by the shared variables so that reviewing effort is reduced. This is achieved by identification and reporting of suitable information about the impacting shared variables. The reported information includes - 1) marked warnings affected due to shared variables, 2) shared variables present in the code (such as warning point, and code slice) traversed while reviewing a warning, 3) intelligently selected write-points of the involved shared variables, depending on the type of warning, and 4) characteristics of the shared variables extracted from the code, that could potentially help to reduce the efforts further. Our experiments on two embedded systems indicated that the review assisting information presented to the user is quite effective to improve reviewing of warnings affected due to shared variables. The overall reduction in manual efforts varied from 45-70% depending on code size and complexity, type of warnings, reviewer skills, tool support used, etc.","","Electronic:978-1-4799-7377-4; POD:978-1-4799-7378-1","10.1109/ISSREW.2014.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983812","Clustering;False Positives;Manual Review of Warnings;Shared variables;Static Analysis","Arrays;Conferences;Embedded systems;Indexes;Manuals;Software engineering","program diagnostics;software quality","clustering method;code slice;intercluster communication;shared variables;static analysis;warning point","","0","","11","","","","3-6 Nov. 2014","","IEEE","IEEE Conferences"
"A Bottom-Up Quality Model for QVTo","C. M. Gerpheide; R. R. H. Schiffelers; A. Serebrenik","Eindhoven Univ. of Technol., Eindhoven, Netherlands","2014 9th International Conference on the Quality of Information and Communications Technology","20141215","2014","","","85","94","We investigate the notion of quality in QVT Operational Mappings (QVTo), one of the languages defined in the OMG standard on model-to-model transformations. We utilize a bottom-up approach, starting with a broad exploratory study including QVTo expert interviews, a review of existing material, and introspection. We then formalize QVTo transformation quality into a QVTo quality model, consisting of high-level quality goals, quality properties, and evaluation procedures. We validate the quality model by conducting a survey in which a broader group of QVTo developers rate each property on its importance to QVTo code quality. We find that although many quality properties recognized as important for QVTo do have counterparts in traditional languages, a number are specific to QVTo or model transformation languages. Additionally, a selection of QVTo best practices discovered are presented. The primary contribution of this paper is a QVTo quality model relevant to QVTo practitioners, while secondary contributions are a bottom-up approach to building a quality model and a validation approach leveraging developer perceptions to evaluate individual quality properties.","","CD-ROM:978-1-4799-6132-0; Electronic:978-1-4799-6133-7; POD:978-1-4799-6134-4","10.1109/QUATIC.2014.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984096","","Best practices;Data models;Interviews;Measurement;Software;Standards;Unified modeling language","object-oriented programming;software quality;specification languages","OMG standard;QVT operational mappings;QVTo code quality;QVTo quality model;QVTo transformation quality;bottom-up quality model;evaluation procedures;high-level quality goals;model transformation languages;model-to-model transformations;object management group;quality properties","","2","","35","","","","23-26 Sept. 2014","","IEEE","IEEE Conferences"
"Writing Acceptable Patches: An Empirical Study of Open Source Project Patches","Y. Tao; D. Han; S. Kim","","2014 IEEE International Conference on Software Maintenance and Evolution","20141206","2014","","","271","280","Software developers submit patches to handle tens or even hundreds of bugs reported daily. However, not all submitted patches can be directly integrated into the code base, since they might not pass patch review that is adopted in most software projects. As the result of patch review, incoming patches can be rejected or asked for resubmission after improvement. Both scenarios interrupt the workflow of patch writers and reviewers, increase their workload, and potentially delay the general development process. In this paper, we aim to help developers write acceptable patches to avoid patch rejection and resubmission. To this end, we derive a comprehensive list of patch rejection reasons from a manual inspection of 300 rejected Eclipse and Mozilla patches, a large-scale online survey of Eclipse and Mozilla developers, and the literature. We also investigate which patch-rejection reasons are more decisive and which are difficult to judge from the perspective of patch reviewers. Our findings include 1) suboptimal solution and incomplete fix are the most frequent patch-rejection reasons 2) whether a patch introduces new bugs is very important yet very difficult to judge 3) reviewers reject a large patch not solely because of its size, but mainly because of the underlying reasons that induce its large size, such as the involvement of unnecessary changes 4) reviewers consider certain problems to be much more destructive than patch writers expect, such as the inconsistency of documentation in a patch and 5) bad timing of patch submission and a lack of communication with team members can also result in a negative patch review.","1063-6773;10636773","Electronic:978-1-4799-6146-7; POD:978-1-4799-6147-4","10.1109/ICSME.2014.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976093","empirical study;patch","Computer bugs;Documentation;Encoding;Guidelines;Inspection;Manuals","program debugging;program diagnostics;software development management","Eclipse developers;Eclipse patches;Mozilla developers;Mozilla patches;acceptable patch writing;codebase;open source project patches;patch rejection;patch resubmission;patch reviewers;software bugs;software developers;software development process;software projects","","6","","38","","","","Sept. 29 2014-Oct. 3 2014","","IEEE","IEEE Conferences"
"Reviewer Recommender of Pull-Requests in GitHub","Y. Yu; H. Wang; G. Yin; C. X. Ling","Nat. Lab. for Parallel & Distrib. Process., Nat. Univ. of Defense Technol., Changsha, China","2014 IEEE International Conference on Software Maintenance and Evolution","20141206","2014","","","609","612","Pull-Request (PR) is the primary method for code contributions from thousands of developers in GitHub. To maintain the quality of software projects, PR review is an essential part of distributed software development. Assigning new PRs to appropriate reviewers will make the review process more effective which can reduce the time between the submission of a PR and the actual review of it. However, reviewer assignment is now organized manually in GitHub. To reduce this cost, we propose a reviewer recommender to predict highly relevant reviewers of incoming PRs. Combining information retrieval with social network analyzing, our approach takes full advantage of the textual semantic of PRs and the social relations of developers. We implement an online system to show how the reviewer recommender helps project managers to find potential reviewers from crowds. Our approach can reach a precision of 74% for top-1 recommendation, and a recall of 71% for top-10 recommendation.","1063-6773;10636773","Electronic:978-1-4799-6146-7; POD:978-1-4799-6147-4","10.1109/ICSME.2014.107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976151","Distributed Software Development;Pull-request;Reviewer Recommendation;Social Network Analysis","Communities;Conferences;Semantics;Social network services;Software;Software engineering","distributed processing;information retrieval;recommender systems;social networking (online);software quality;text analysis","GitHub;code contributions;distributed software development;information retrieval;pull-requests;reviewer recommender;social network;software projects quality;textual semantic","","17","","9","","","","Sept. 29 2014-Oct. 3 2014","","IEEE","IEEE Conferences"
"Program Dependency Analysis for Consolidating Customized Product Copies","B. Klatt; K. Krogmann; C. Seidl","Software Eng. Dept., Res. Center for Inf. Technol., Karlsruhe, Germany","2014 IEEE International Conference on Software Maintenance and Evolution","20141206","2014","","","496","500","To cope with project constraints, copying and customizing existing software products is a typical practice to flexibly serve customer-specific needs. In the long term, this practice becomes a limitation for growth due to redundant maintenance efforts or wasted synergy and cross selling potentials. To mitigate this limitation, customized copies need to be consolidated into a single, variable code base of a software product line (SPL). However, consolidation is tedious as one must identify and correlate differences between the copies to design future variability. For one, existing consolidation approaches lack support of the implementation level. In addition, approaches in the fields of difference analysis and feature detection are not sufficiently integrated for finding relationships between code modifications. In this paper, we present remedy to this problem by integrating a difference analysis with a program dependency analysis based on Program Dependency Graphs (PDG) to reduce the effort of consolidating developers when identifying dependent differences and deriving clusters to consider in their variability design. We successfully evaluated our approach on variants of the open source ArgoUML modeling tool, reducing the manual review effort about 72% with a precision of 99% and a recall of 80%. We further proved its industrial applicability in a case study on a commercial relationship management application.","1063-6773;10636773","Electronic:978-1-4799-6146-7; POD:978-1-4799-6147-4","10.1109/ICSME.2014.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976125","program dependencies;reengineering;software engineering;software product lines;variability","Algorithm design and analysis;Context;Merging;Object oriented modeling;Software;Software algorithms;Unified modeling language","Unified Modeling Language;customer relationship management;feature extraction;product customisation;program diagnostics;public domain software;software product lines","PDG;code modifications;commercial relationship management application;cross selling potentials;customer-specific needs;customized product copies;difference analysis;feature detection;industrial applicability;open source ArgoUML modeling tool;program dependency analysis;program dependency graphs;project constraints;redundant maintenance efforts;software product line;wasted synergy","","2","","36","","","","Sept. 29 2014-Oct. 3 2014","","IEEE","IEEE Conferences"
"Continuous Integration in a Social-Coding World: Empirical Evidence from GitHub","B. Vasilescu; S. van Schuylenburg; J. Wulms; A. Serebrenik; M. G. J. van den Brand","Eindhoven Univ. of Technol., Eindhoven, Netherlands","2014 IEEE International Conference on Software Maintenance and Evolution","20141206","2014","","","401","405","Continuous integration is a software engineering practice of frequently merging all developer working copies with a shared main branch, e.g., several times a day. With the advent of GitHub, a platform well known for its ""social coding"" features that aid collaboration and sharing, and currently the largest code host in the open source world, collaborative software development has never been more prominent. In GitHub development one can distinguish between two types of developer contributions to a project: direct ones, coming from a typically small group of developers with write access to the main project repository, and indirect ones, coming from developers who fork the main repository, update their copies locally, and submit pull requests for review and merger. In this paper we explore how GitHub developers use continuous integration as well as whether the contribution type (direct versus indirect) and different project characteristics (e.g., main programming language, or project age) are associated with the success of the automatic builds.","1063-6773;10636773","Electronic:978-1-4799-6146-7; POD:978-1-4799-6147-4","10.1109/ICSME.2014.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976106","GitHub;automatic build;collaborative software development;continuous integration","Blogs;Encoding;Java;Open source software;Programming","public domain software;software engineering;source code (software)","GitHub development;code host;collaborative software development;continuous integration;open source world;project characteristics;project repository;social-coding world;software engineering practice","","12","","27","","","","Sept. 29 2014-Oct. 3 2014","","IEEE","IEEE Conferences"
"Research Challenges in Adaptive Case Management: A Literature Review","M. Hauder; S. Pigat; F. Matthes","Dept. of Inf., Tech. Univ. Munchen (TUM), Garching, Germany","2014 IEEE 18th International Enterprise Distributed Object Computing Conference Workshops and Demonstrations","20141206","2014","","","98","107","Non-traditional scenarios for Business Process Management (BPM) are often knowledge-intensive and driven by user decisions making it difficult to specify them into a set of activities with precedence relations at design-time. Adaptive Case Management (ACM) is gaining interest among researchers and practitioners as an emerging paradigm to master situations in which adaptions have to be made at run-time by so called knowledge workers. In contrast to workflow management the ACM paradigm is not dictating knowledge workers a predefined course of action, but provides them with the required information at the right time and authorizes them to make decisions on their own. Understanding current research challenges imposed by ACM is of utmost importance for the future evolution of this discipline as well as for the maturity of the BPM field. In this paper we present 77 codes referring to research challenges in ACM that have been revealed from an extensive literature review with scientific publications and books. We aggregated these codes to 13 concepts and categorized them into five distinct areas for data integration, theoretical foundation, authorization and role management, knowledge worker empowerment as well as knowledge storage and extraction. Main goal of this paper is to provide a thorough basis for the discussion of future research activities in the community which are indispensable for ACM.","2325-6583;23256583","Electronic:978-1-4799-5467-4; POD:978-1-4799-5468-1","10.1109/EDOCW.2014.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975348","Adaptive Case Management;Challenges;Literature Review","Adaptation models;Authorization;Bibliographies;Books;Collaboration;Data models;Organizations","business data processing;workflow management software","ACM;BPM field;adaptive case management;business process management;data integration;knowledge extraction;knowledge storage;knowledge worker empowerment;knowledge workers;role management;workflow management","","10","","49","","","","1-2 Sept. 2014","","IEEE","IEEE Conferences"
"Features in Identification Approaches for MicroRNA Precursors Based on Machine Learning","Z. Hongjun; P. Haiqing; W. Xiuqin; L. Yongqiang","Coll. of Inf. Sci. & Eng., Yanshan Univ., Qinhuangdao, China","2014 Fifth International Conference on Intelligent Systems Design and Engineering Applications","20141206","2014","","","483","488","MicroRNAs (miRNAs) are a group of non-coding small RNA of ~ 22 nucleotides in length. They play important roles in gene regulation in animals and plants. The machine learning approach has become an important way to discover miRNAs, which is complement to experimental approaches. Feature selection is the key step of machine learning approaches to discover miRNA precursors. The performance and generalization ability of classifier is affected by the feature set. Features of miRNA precursors used in machine learning approaches were summarized in this review. According to the properties of features to distinguish the miRNA precursors and the non-miRNA precursors, features were categorized into three classes: sequence features, structure features, structure sequence features.","","CD-ROM:978-1-4799-4262-6; Electronic:978-1-4799-4261-9; POD:978-1-4799-7889-2","10.1109/ISDEA.2014.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977645","features;machine learning approaches;miRNA precursor;microRNA","Bioinformatics;Feature extraction;Genomics;Periodic structures;RNA;Sequential analysis;Support vector machines","RNA;feature selection;generalisation (artificial intelligence);genetics;learning (artificial intelligence)","MicroRNA precursors;animals;feature selection;feature set;gene regulation;generalization ability;identification approach;machine learning approach;noncoding small RNA;nucleotides;plants;structure-sequence features","","0","","38","","","","15-16 June 2014","","IEEE","IEEE Conferences"
"EduPar Keynote","R. H. Katz","EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA","2014 IEEE International Parallel & Distributed Processing Symposium Workshops","20141204","2014","","","1070","1070","We describe our experiences in bringing a traditional sophomore level ‚ÄúMachine Structures‚Äù course into the 21st Century, by revising it to focus instead of instruction sets and machine language on ‚Äúprogramming for performance.‚Äù The course concentrates on just what even a novice programmer needs to know about the underlying hardware to write fast code. And in today's world, that means parallel programming at a variety of levels. The course teaches, through lectures, labs, and programming projects, the essentials of parallel requests at the multi-server level, map-reduce data parallel programming at the multiprocessor level, parallel threads at the multi-core level, and parallel instructions (pipelining) and data (SIMD) at the processor core level. Student teams develop a high performance implementation of a non-trivial parallel application like large-scale matrix multiply or image edge detection, and compete to see who can achieve the highest performance. This kind of course has influenced the recent ACM 2013 Computer Science Curriculum recommendations. We will review the ‚Äúgreat ideas of computer architecture‚Äù embodied by this course, and the implications it has for teaching parallel and distributed systems to undergraduate computer science students.","","Electronic:978-1-4799-4116-2; POD:978-1-4799-4115-5","10.1109/IPDPSW.2014.220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969499","","Awards activities;Educational institutions;Instruction sets;Parallel programming","computer science education;educational courses;further education;multiprocessing systems;parallel programming;teaching","ACM 2013 Computer Science Curriculum;MapReduce data parallel programming;SIMD data;code writing;computer architecture;distributed system teaching;high-performance implementation;image edge detection;large-scale matrix multiply;machine structures course;multicore level;multiprocessor level;multiserver level;nontrivial parallel application;parallel instructions;parallel programming;parallel requests;parallel system teaching;parallel threads;pipelining;processor core level;student teams;undergraduate computer science students","","0","","","","","","19-23 May 2014","","IEEE","IEEE Conferences"
"The Nature and Content of Safety Contracts: Challenges and Suggestions for a Way Forward","P. Graydon; I. Bate","Sch. of Innovation, Malardalen Univ., Vasteras, Sweden","2014 IEEE 20th Pacific Rim International Symposium on Dependable Computing","20141204","2014","","","135","144","Software engineering researchers have extensively explored the reuse of components at source-code level. Contracts explicitly describe component behaviour, reducing development risk by exposing potential incompatibilities early in the development process. But to benefit fully from reuse, developers of safety-critical systems must also reuse safety evidence. Full reuse would require both extending the existing notion of component contracts to cover safety properties and using these contracts in both component selection and system certification. This is not as simple as it first appears. Much of the review, analysis, and test evidence developers provide during certification is system-specific. This makes it difficult to define safety contracts that facilitate both selecting components to reuse and certifying systems. In this paper, we explore the definition and use of safety contracts, identify challenges to component-based software reuse safety-critical systems, present examples to illustrate several key difficulties, and discuss potential solutions to these problems.","","Electronic:978-1-4799-6474-1; POD:978-1-4799-6475-8","10.1109/PRDC.2014.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974780","based software engineering;contracts;modular safety case;safety;safety arguments","Contracts;Fuels;Safety;Sensors;Software;System analysis and design;Vehicles","object-oriented programming;safety-critical software;software reusability","component-based software reuse;safety contracts;safety-critical systems","","3","","46","","","","18-21 Nov. 2014","","IEEE","IEEE Conferences"
"Development of Safety-Critical Software Systems Using Open Source Software -- A Systematic Map","S. M. Sulaman; A. Orucevic-Alagic; M. Borg; K. Wnuk; M. H√∂st; J. L. d. l. Vara","Dept. of Comput. Sci., Lund Univ., Lund, Sweden","2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications","20141120","2014","","","17","24","The popularity of Open Source Software (OSS) has increased the interest in using it in safety critical applications. The aim of this study is to review research carried out on usage of open source code in development of safety-critical software and systems. We conducted a systematic mapping study through searches in library databases and manual identification of articles from open source conferences. We have identified 22 studies about using open source software, mainly in automotive, aerospace, medical and nuclear domains. Moreover, only a few studies present complete safety systems that are released as OSS in full. The most commonly used OSS functionalities are operating systems, imaging, control and data management. Finally most of the integrated OSS have mature code bases and a commit history of more than five years.","1089-6503;10896503","Electronic:978-1-4799-5795-8; POD:978-1-4799-5796-5","10.1109/SEAA.2014.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928784","Open source;mapping study;safety critical","Automotive engineering;Biomedical imaging;Databases;History;Operating systems;Safety","public domain software;safety-critical software","OSS;aerospace domain;medical domain;nuclear domain;open source software;operating systems;safety critical applications;safety-critical software systems;systematic mapping study","","3","","15","","","","27-29 Aug. 2014","","IEEE","IEEE Conferences"
"Bootstrapping Privacy Compliance in Big Data Systems","S. Sen; S. Guha; A. Datta; S. K. Rajamani; J. Tsai; J. M. Wing","Carnegie Mellon Univ., Pittsburgh, PA, USA","2014 IEEE Symposium on Security and Privacy","20141120","2014","","","327","342","With the rapid increase in cloud services collecting and using user data to offer personalized experiences, ensuring that these services comply with their privacy policies has become a business imperative for building user trust. However, most compliance efforts in industry today rely on manual review processes and audits designed to safeguard user data, and therefore are resource intensive and lack coverage. In this paper, we present our experience building and operating a system to automate privacy policy compliance checking in Bing. Central to the design of the system are (a) Legal ease-a language that allows specification of privacy policies that impose restrictions on how user data is handled, and (b) Grok-a data inventory for Map-Reduce-like big data systems that tracks how user data flows among programs. Grok maps code-level schema elements to data types in Legal ease, in essence, annotating existing programs with information flow types with minimal human input. Compliance checking is thus reduced to information flow analysis of Big Data systems. The system, bootstrapped by a small team, checks compliance daily of millions of lines of ever-changing source code written by several thousand developers.","1081-6011;10816011","Electronic:978-1-4799-4686-0; POD:978-1-4799-4685-3","10.1109/SP.2014.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6956573","big data;bing;compliance;information flow;policy;privacy;program analysis","Advertising;Big data;Data privacy;IP networks;Lattices;Privacy;Semantics","Big Data;Web services;cloud computing;computer bootstrapping;conformance testing;data privacy;parallel programming;search engines;source code (software)","Bing;Grok data inventory;Legal ease language;Map-Reduce-like Big Data systems;automatic privacy policy compliance checking;business imperative privacy policies;cloud services;code-level schema element mapping;datatypes;information flow types;minimal human input;personalized user experiences;privacy compliance bootstrapping;privacy policy specification;program annotation;source code;user data handling;user trust","","4","","41","","","","18-21 May 2014","","IEEE","IEEE Conferences"
"The Weird Machines in Proof-Carrying Code","J. Vanegue","Bloomberg L.P., New York, NY, USA","2014 IEEE Security and Privacy Workshops","20141120","2014","","","209","213","We review different attack vectors on Proof-Carrying Code (PCC) related to policy, memory model, machine abstraction, and formal system. We capture the notion of weird machines in PCC to formalize the shadow execution arising in programs when their proofs do not sufficiently capture and disallow the execution of untrusted computations. We suggest a few ideas to improve existing PCC systems so they are more resilient to memory attacks.","","Electronic:978-1-4799-5103-1; POD:978-1-4799-5104-8","10.1109/SPW.2014.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6957306","FPCC;Machines;PCC;Weird","Abstracts;Computational modeling;Program processors;Registers;Safety;Security;Semantics","codes;security of data","PCC systems;formal system;machine abstraction;memory attacks;memory model;proof-carrying code;untrusted computations;weird machines","","3","","9","","","","17-18 May 2014","","IEEE","IEEE Conferences"
"A novel application to secure misuse of information in critical organizations for windows environment","M. B. Ahmad; M. Asif; S. M. M. Gilani; M. H. Islam; Saeed-ur-Rehman","Center for Adv. Studies in Eng., Islamabad, Pakistan","Fifth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","20141120","2014","","","1","7","Critical nature organizations normally deploy strict access rights policies and use of some commercial software to handle the information misuse. But such mechanisms can't be effective to detect or prevent some of the misuse channels e.g. covert channels. An application is developed for secure document exchange in critical nature organizations of our country. The benefit is that source code is available with the higher management of the organization using it and source code reviewing can be done before as well as after the start of using this application inside the organization. Results obtained by deploying it show that it prevented as well as detected most of the channels of information misuse.","","CD-ROM:978-1-4799-2695-4; Electronic:978-1-4799-2696-1; POD:978-1-4799-2697-8","10.1109/ICCCNT.2014.6963037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963037","Critical;information;insider;misuse;organization","Encryption;Organizations;Permission;Portable document format;Servers","document handling;safety-critical software;security of data;source code (software)","critical nature organizations;critical organizations;document exchange security;information misuse security;source code","","0","","10","","","","11-13 July 2014","","IEEE","IEEE Conferences"
"Preliminary Empirical Identification of Barriers Faced by Newcomers to Open Source Software Projects","I. Steinmacher; A. P. Chaves; T. U. Conte; M. A. Gerosa","DACOM, UTFPR, Campo Moura&#x0303;o, Brazil","2014 Brazilian Symposium on Software Engineering","20141103","2014","","","51","60","When newcomers try to join an open source software (OSS) project, they face many barriers that hinder their first contribution, leading in many cases to their dropping out. Many projects leverage the contribution of outsiders, and the sustainability of the project relies on retaining some of these newcomers. This research aims to identify the barriers that hinder newcomers' onboarding to OSS projects. Our method consisted of a qualitative study conducted with data obtained from four different sources: (i) systematic literature review; (ii) feedback from nine graduate and undergraduate students after they tried to join OSS projects; (iii) 24 responses to a questionnaire sent to 9 OSS projects; and (iv) semi-structured interviews with 36 subjects from 14 different projects, including newcomers and experienced members. The method to select the candidate papers in the systematic literature review was querying four digital libraries and backward snowballing. The data obtained from the practitioners from all three sources, and the primary studies obtained in the systematic review were analyzed using used procedures of Grounded Theory's open and axial coding. The analysis resulted in a conceptual model composed of 58 barriers, grouped into six different categories: cultural differences, newcomers' characteristics, reception issues, orientation, technical hurdles, and documentation problems. We could observe recurrent barriers evidenced in different data sources. We could notice that the onboarding process of a newcomer to an OSS can be a tough task. This research brings empirical support relying on data from different sources, organizes and discusses the existing common wisdom about barriers faced by newcomers to OSS projects, which deserve attention from researchers and OSS communities.","","Electronic:978-1-4799-4223-7; POD:978-1-4799-4221-3","10.1109/SBES.2014.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943482","newcomers;onboarding;open source software;qualitative analysis;systematic literature review","Bibliographies;Communities;Documentation;Encoding;Interviews;Software;Systematics","digital libraries;project management;public domain software;query processing","backward snowballing;conceptual model;cultural differences;data sources;digital libraries;documentation problems;empirical barrier identification;experienced members;graduate student feedback;grounded theory axial coding;grounded theory open coding;newcomer OSS project onboarding;newcomer characteristics;open source software projects;orientation;outsider contribution;project sustainability;qualitative study;reception issues;recurrent barriers;semistructured interviews;systematic literature review;technical hurdles;undergraduate student feedback","","8","","44","","","","Sept. 28 2014-Oct. 3 2014","","IEEE","IEEE Conferences"
"How Software Designers Interact with Sketches at the Whiteboard","N. Mangano; T. D. LaToza; M. Petre; A. van der Hoek","Molimur, Mission Viejo, CA","IEEE Transactions on Software Engineering","20150210","2015","41","2","135","156","Whiteboard sketches play a crucial role in software development, helping to support groups of designers in reasoning about a software design problem at hand. However, little is known about these sketches and how they support design `in the moment', particularly in terms of the relationships among sketches, visual syntactic elements within sketches, and reasoning activities. To address this gap, we analyzed 14 hours of design activity by eight pairs of professional software designers, manually coding over 4000 events capturing the introduction of visual syntactic elements into sketches, focus transitions between sketches, and reasoning activities. Our findings indicate that sketches serve as a rich medium for supporting design conversations. Designers often use general-purpose notations. Designers introduce new syntactic elements to record aspects of the design, or re-purpose sketches as the design develops. Designers constantly shift focus between sketches, using groups of sketches together that contain complementary information. Finally, sketches play an important role in supporting several types of reasoning activities (mental simulation, review of progress, consideration of alternatives). But these activities often leave no trace and rarely lead to sketch creation. We discuss the implications of these and other findings for the practice of software design at the whiteboard and for the creation of new electronic software design sketching tools.","0098-5589;00985589","","10.1109/TSE.2014.2362924","10.13039/100000001 - National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6922572","Interaction styles;systems analysis and design;user-centered design","Cognition;Encoding;Software design;Syntactics;Videos;Visualization","software engineering","reasoning activity;software design;software development;visual syntactic elements;whiteboard sketch","","5","","60","","","20141014","Feb. 1 2015","","IEEE","IEEE Journals & Magazines"
"Replicating and Re-Evaluating the Theory of Relative Defect-Proneness","M. D. Syer; M. Nagappan; B. Adams; A. E. Hassan","School of Computing, Queen&#8217;s University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","20150210","2015","41","2","176","197","A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.","0098-5589;00985589","","10.1109/TSE.2014.2361131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914599","Cox Models;Cox models;Defect Modelling;Survival Analysis;Survival analysis;defect modelling","Analytical models;Data models;Hazards;Mathematical model;Measurement;Predictive models;Software","program diagnostics;software quality;software reliability","defect modelling;relative defect-proneness theory;size-defect relationship;software system defects;source code module;survival analysis techniques","","3","","47","","","20141001","Feb. 1 2015","","IEEE","IEEE Journals & Magazines"
"Auditing Buffer Overflow Vulnerabilities Using Hybrid Static-Dynamic Analysis","B. M. Padmanabhuni; H. B. K. Tan","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","2014 IEEE 38th Annual Computer Software and Applications Conference","20140922","2014","","","394","399","Despite being studied for more than two decades buffer overflow vulnerabilities are still frequently reported in programs. In this paper, we propose a hybrid approach that combines static and dynamic program analysis to audit buffer overflows. Using simple rules, test data are generated to automatically confirm some of the vulnerabilities through dynamic analysis and the remaining cases are predicted by mining static code attributes. Confirmed cases can be directly fixed without further verification whereas predicted cases need to be manually reviewed to confirm existence of vulnerabilities. Since our approach combines the strengths of static and dynamic analyses, it results in an overall accuracy improvement. In our evaluation of approach using the standard benchmark suite, our classifiers achieved a recall over 92% and precision greater than 81%. The dynamic analysis component confirmed 51% of known vulnerabilities along with reporting 2 new bugs, thereby reducing by half, otherwise needed manual auditing effort.","","Electronic:978-1-4799-3575-8; POD:978-1-4799-3576-5","10.1109/COMPSAC.2014.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899241","Vulnerability;auditing;buffer overflow;data mining;input validation;static and dynamic analysis;static code attributes","Accuracy;Arrays;Benchmark testing;Buffer overflows;Data mining;Input variables;Predictive models","data mining;program diagnostics","buffer overflow vulnerabilities;hybrid static-dynamic program analysis;static code attribute mining","","2","","8","","","","21-25 July 2014","","IEEE","IEEE Conferences"
"First, Do No Harm","D. Spinellis","Athens University of Economics and Business","IEEE Software","20140915","2014","31","5","12","14","When we maintain existing code, we must be very careful to avoid breaking or degrading the system we're working on. During development, we can minimize problems through reviews, adherence to style rules, defensive programming, maintenance of backward compatibility, and the preservation of architectural properties. Thorough testing at all levels can catch many issues before they reach the deployment stage. Finally, during deployment, a phased rollout, a back-off plan, and careful planning can minimize the occurrence of catastrophic failures. The Web extra at http://youtu.be/pifgzfFXanE is an audio podcast of the Tools of the Trade column in which author Diomidis Spinellis discusses how we must be very careful to avoid breaking or degrading the system while working to maintain existing code.","0740-7459;07407459","","10.1109/MS.2014.112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898723","defensive programming;deployment;maintenance;operations;reviews;software engineering;testing","Codes;Computer applicaitons;Programming;Software engineering;Software testing","software architecture;software maintenance","backward compatibility;code architectural properties;code development;code maintenance;defensive programming","","0","","","","","","Sept.-Oct. 2014","","IEEE","IEEE Journals & Magazines"
"Model Driven Requirements Engineering: Mapping the field and beyond","S. Assar","Institut Mines-T&#x00E9;l&#x00E9;com, Ecole de Management, 9, rue C. Fourier, 91011 Evry, France","2014 IEEE 4th International Model-Driven Requirements Engineering Workshop (MoDRE)","20140908","2014","","","1","6","Model Driven Engineering (MDE) holds the promise of raising the level of abstraction when designing systems by promoting domain specific modeling languages, model transformation techniques and code generation engines. Although requirements engineering (RE) relies on models and on modeling, RE and MDE have evolved separately and in distinct communities. The goal of our ongoing work is to analyze the state of the art regarding the convergence and evolution of MDE and RE. This paper is a preliminary study in which we review all papers published at MoDRE workshops and map them according to three perspectives: research issue, research contribution and evaluation method. Our analysis indicates clear predominance of proposals of new language for requirement representation and the derivation of system specifications. Other facets such as requirements elicitation and requirements validation methods are much less tackled, and traceability is seldom discussed.","","Electronic:978-1-4799-6343-0; POD:978-1-4799-6344-7","10.1109/MoDRE.2014.6890820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890820","Literature review;Mapping study;Model Driven Engineering;Requirement Engineering","Analytical models;Conferences;Embedded systems;Model driven engineering;Software engineering;Software tools;Unified modeling language","formal specification;program compilers;program verification;specification languages","MDE;MoDRE workshops;abstraction level;code generation engines;domain specific modeling languages;model driven requirements engineering;model transformation techniques;requirement representation;requirements validation methods;system specifications;systems design","","2","","14","","","","25-25 Aug. 2014","","IEEE","IEEE Conferences"
"Investigation of the Impact of Compression on the Perceptional Quality of Laparoscopic Videos","B. M√ºnzer; K. Schoeffmann; L. B√∂sz√∂rmenyi; J. F. Smulders; J. J. Jakimowicz","Inst. of Inf. Technol., Klagenfurt Univ., Klagenfurt, Austria","2014 IEEE 27th International Symposium on Computer-Based Medical Systems","20140825","2014","","","153","158","In recent years it has become common practice to archive video recordings of laparoscopic surgeries for documentation purposes and for retrospective review. Typically, the videos are captured in High Definition (HD) format but encoded with legacy coding standards like MPEG-2 requiring an enormous storage capacity. In this paper we present the results of a subjective quality assessment study with 37 medical experts. We identify appropriate encoding configurations for the H.264 AVC coding standard to guarantee a visually loss less quality with a significant bitrate reduction. Further, we show that it is not necessary to capture the highest possible quality for documentation and retrospective analysis. A lower technical quality with a substantially lower bitrate still provides sufficient semantic quality. We finally present basic recommendations for an efficient encoding strategy with an appropriate tradeoff between visual quality and bitrate.","1063-7125;10637125","Electronic:978-1-4799-4435-4; POD:978-1-4799-4434-7","10.1109/CBMS.2014.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881867","","Bit rate;Encoding;High definition video;Laparoscopes;Quantization (signal);Surgery;Videos","data compression;high definition video;image capture;medical image processing;surgery;video coding","H.264/AVC coding standard;HD format;MPEG-2;bitrate reduction;documentation analysis;high-definition format;laparoscopic surgeries;laparoscopic video compression;legacy coding standards;perceptional quality;retrospective analysis;semantic quality;storage capacity;subjective quality assessment;technical quality;video capture;video encoding configurations;video recording archiving;visual quality;visually lossless quality","","5","","8","","","","27-29 May 2014","","IEEE","IEEE Conferences"
"Spatially coupled sparse codes on graphs: theory and practice","D. J. Costello; L. Dolecek; T. E. Fuja; J. Kliewer; D. G. M. Mitchell; R. Smarandache","Univ. of Notre Dame, Notre Dame, IN, USA","IEEE Communications Magazine","20140715","2014","52","7","168","176","Since the discovery of turbo codes 20 years ago and the subsequent rediscovery of low-density parity check codes a few years later, the field of channel coding has experienced a number of major advances. Until that time, code designers were usually happy with performance that came within a few decibels of the Shannon Limit, primarily due to implementation complexity constraints, whereas the new coding techniques now allow performance within a small fraction of a decibel of capacity with modest encoding and decoding complexity. Due to these significant improvements, coding standards in applications as varied as wireless mobile transmission, satellite TV, and deep space communication are being updated to incorporate the new techniques. In this article, we review a particularly exciting new class of low-density parity check codes called spatially coupled codes, which promise excellent performance over a broad range of channel conditions and decoded error rate requirements.","0163-6804;01636804","","10.1109/MCOM.2014.6852099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6852099","","Block codes;Convolutional codes;Decoding;Iterative decoding;Sparse matrices","channel coding;decoding;graph theory;parity check codes;turbo codes","Shannon limit;channel coding;code designers;coding standards;coding techniques;complexity constraints;decoded error rate requirements;decoding complexity;deep space communication;encoding complexity;graphs;low-density parity check codes;satellite TV;spatially coupled sparse codes;turbo codes;wireless mobile transmission","","19","","15","","","","July 2014","","IEEE","IEEE Journals & Magazines"
"On Demand Multicast Routing in Wireless Sensor Networks","C. Sule; P. Shah; K. Doddapaneni; O. Gemikonakli; E. Ever","Sch. of Sci. & Technol., Middlesex Univ., London, UK","2014 28th International Conference on Advanced Information Networking and Applications Workshops","20140626","2014","","","233","238","The wireless networking environment presents imposing challenges to the study of broadcasting and multicasting problems. Developing an algorithm to optimize communication amongst a group of spatially distributed sensor nodes in a WSN (Wireless Sensor Network) has been met with a number challenges due to the characterization of the sensor node device. These challenges include, but are not limited to: energy, memory, and throughput constraints. The traditional approach to overcome these challenges have emphasised the development of low power electronics, efficient modulation, coding, antenna design etc., it has been recognised that networking techniques can also have a strong impact on the energy efficiency of such systems. A variety of networking based approaches to energy efficiency are possible. One of the well-known approaches is to apply clustering techniques to effectively establish an ordered connection of sensor nodes whilst improving the overall network lifetime. This paper proposes an improved clustering based multicast approach that allows any cluster head to be a multicast source with an unlimited number of subscribers, to optimize group communication in WSNs whilst ensuring sensor nodes do not deprecate rapidly in energy levels. We review several clustering approaches and examine multicast versus broadcast communication in WSNs.","","Electronic:978-1-4799-2653-4; POD:978-1-4799-2654-1","10.1109/WAINA.2014.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844643","Wireless Sensor Networks;clustering techniques;energy efficiency;multicast","Base stations;Multicast communication;Protocols;Reliability;Routing;Sensors;Wireless sensor networks","energy conservation;multicast communication;telecommunication network routing;telecommunication power management;wireless sensor networks","cluster head;clustering technique;energy efficiency;multicast routing;multicast source;network lifetime;on-demand routing;wireless sensor networks","","0","","38","","","","13-16 May 2014","","IEEE","IEEE Conferences"
"Volumetric reconstruction applied to perceptual studies of size and weight","J. Balzer; M. Peters; S. Soatto","Comput. Sci. Dept., Univ. of California, Los Angeles, Los Angeles, CA, USA","IEEE Winter Conference on Applications of Computer Vision","20140623","2014","","","45","52","We explore the application of volumetric reconstruction from structured-light sensors in cognitive neuroscience, specifically in the quantification of the size-weight illusion, whereby humans tend to systematically perceive smaller objects as heavier. We investigate the performance of two commercial structured-light scanning systems in comparison to one we developed specifically for this application. Our method has two main distinct features: First, it only samples a sparse series of viewpoints, unlike other systems such as the Kinect Fusion. Second, instead of building a distance field for the purpose of points-to-surface conversion directly, we pursue a first-order approach: the distance function is recovered from its gradient by a screened Poisson reconstruction, which is very resilient to noise and yet preserves high-frequency signal components. Our experiments show that the quality of metric reconstruction from structured light sensors is subject to systematic biases, and highlights the factors that influence it. Our main performance index rates estimates of volume (a proxy of size), for which we review a well-known formula applicable to incomplete meshes. Our code and data will be made publicly available upon completion of the anonymous review process.","1550-5790;15505790","Electronic:978-1-4799-4985-4; POD:978-1-4799-4984-7; USB:978-1-4799-4986-1","10.1109/WACV.2014.6836119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6836119","","Abstracts;Equations;Euclidean distance;Software;Tires","image reconstruction;neurophysiology;stochastic processes","Kinect Fusion;Poisson reconstruction;cognitive neuroscience;size-weight illusion;structured-light sensors;volumetric reconstruction","","1","","20","","","","24-26 March 2014","","IEEE","IEEE Conferences"
"The TrueCrypt On-Disk Format--An Independent View","M. Broz; V. Maty√°s","Masaryk Univ., Brno, Czech Republic","IEEE Security & Privacy","20140617","2014","12","3","74","77","Full disk encryption (FDE) is a common way to prevent unauthorized use of data by encrypting the whole storage device. TrueCrypt is one of the widest-known open source tools providing FDE. It's available for multiple OSs including Windows, Mac OS, and Linux, letting you share encrypted storage among these systems. Unfortunately, there are license problems and questions related to implementation security. A community-funded project aims to review license issues and perform a full security audit of source codes. This article, which includes comments on older and problematic TrueCrypt encryption modes, provides interesting insight into the TrueCrypt data format history.","1540-7993;15407993","","10.1109/MSP.2014.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824535","TrueCrypt;full disk encryption;security","Ciphers;Computer security;Cryptography;Disk drives;Encryption;Standards","cryptography;storage management","FDE;Linux;Mac OS;TrueCrypt on-disk format;Windows;full disk encryption;license issues;open source tools;operating systems;security audit;storage device","","1","","2","","","","May-June 2014","","IEEE","IEEE Journals & Magazines"
"Physical Unclonable Functions and Applications: A Tutorial","C. Herder; M. D. Yu; F. Koushanfar; S. Devadas","Dept. of Electr. Eng. & Comput. Sci. (EECS), Massachusetts Inst. of Technol., Cambridge, MA, USA","Proceedings of the IEEE","20140718","2014","102","8","1126","1141","This paper describes the use of physical unclonable functions (PUFs) in low-cost authentication and key generation applications. First, it motivates the use of PUFs versus conventional secure nonvolatile memories and defines the two primary PUF types: ‚Äústrong PUFs‚Äù and ‚Äúweak PUFs.‚Äù It describes strong PUF implementations and their use for low-cost authentication. After this description, the paper covers both attacks and protocols to address errors. Next, the paper covers weak PUF implementations and their use in key generation applications. It covers error-correction schemes such as pattern matching and index-based coding. Finally, this paper reviews several emerging concepts in PUF technologies such as public model PUFs and new PUF implementation technologies.","0018-9219;00189219","","10.1109/JPROC.2014.2320516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823677","Arbiter;SRAM;index-based coding;pattern matching;physical unclonable function (PUF);public model PUFs;ring oscillator;unclonable","Coding;Cryptography;Hardware;Indexes;Logic gates;Oscillators;Random access memory;Ring oscillators;SRAM chips;Tutorials","Random access memory;Ring oscillators;Tutorials;authorisation;cryptographic protocols","PUF;error-correction scheme;index-based coding;key generation application;low-cost authentication application;pattern matching;physical unclonable functions;public model PUF;secure nonvolatile memories;strong PUF type;weak PUF type","","108","","42","","","20140530","Aug. 2014","","IEEE","IEEE Journals & Magazines"
"Next-Generation 3D Formats with Depth Map Support","Y. Chen; A. Vetro","","IEEE MultiMedia","20140520","2014","21","2","90","94","This article reviews the most recent extensions to the Advanced Video Coding (AVC) and High Efficiency Video Coding (HEVC) coding standards, which integrate depth video to support advanced multiview and 3D video functionalities. All the extensions provide single-view compatibility, while some extensions add depth support on top of conforming stereoscopic bitstreams. To achieve the highest gains in coding efficiency, depth information is utilized in coding the texture views. The compression formats described in this article can be used to support emerging auto-stereoscopic displays and free-viewpoint video functionalities.","1070-986X;1070986X","","10.1109/MMUL.2014.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818898","3D video standards;3D video systems;autostereoscopic multiview displays;depth support;depth-sensing approaches;multimedia;multimedia standards","Decoding;Encoding;Next generation networking;Standards;Syntactics;Three-dimensional displays;Video coding","data compression;image texture;stereo image processing;video coding","3D video functionalities;AVC;HEVC coding standards;advanced multiview functionalities;advanced video coding;auto-stereoscopic displays;coding efficiency;compression formats;depth information;depth map support;depth support;depth video;free-viewpoint video functionalities;high efficiency video coding;next-generation 3D formats;single-view compatibility;stereoscopic bitstreams;texture views coding","","14","1","10","","","","Apr.-June 2014","","IEEE","IEEE Journals & Magazines"
"The Future of Accelerator Programming: Abstraction, Performance or Can We Have Both?","K. Rocki; M. Burtscher; R. Suda","Dept. of Comput. Sci., Univ. of Tokyo, Tokyo, Japan","2013 International Conference on Parallel and Distributed Systems","20140501","2013","","","442","443","In a perfect world, code would only be written once and would run on different devices with high efficiency. A programmer's time would primarily be spent on thinking about the algorithms and data structures, not on implementing them. To a degree, that used to be the case in the era of frequency scaling on a single core. However, due to power limitations, parallel programming has become necessary to obtain performance gains. But parallel architectures differ substantially from each other, often require specialized knowledge, and typically necessitate reimplementation and fine tuning of application code. These slow tasks frequently result in situations where most of the time is spent reimplementing old rather than writing new code. The goal of our research is to find new programming techniques that increase productivity, maintain high performance, and provide abstraction to free the programmer from these unnecessary and time-consuming tasks. However, such techniques usually come at the cost of substantial performance degradation. This paper investigates current approaches to portable accelerator programming, seeking to answer whether they make it possible to combine high efficiency with sufficient algorithm abstraction. It discusses OpenCL as a potential solution and presents three approaches of writing portable code: GPU-centric, CPU-centric and combined. By applying the three approaches to a real-world program, we show that it is at least sometimes possible to run exactly the same code on many different devices with minimal performance degradation using parameterization. The main contributions of this paper are an extensive review of the current state-of-the-art regarding the stated problem and our original approach of addressing this problem with a generalized excessive-parallelism approach.","1521-9097;15219097","Electronic:978-1-4799-2081-5; POD:978-1-4799-2082-2; USB:978-1-4799-2080-8","10.1109/ICPADS.2013.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808213","CPU;GPU;Manycore;Parallel;Programming","Central Processing Unit;Hardware;Instruction sets;Parallel processing;Performance evaluation;Programming;Writing","data structures;graphics processing units;parallel architectures;parallel programming;program diagnostics;software performance evaluation","CPU-centric;GPU-centric;OpenCL;abstraction;accelerator programming;algorithm abstraction;application code;data structures;frequency scaling;generalized excessive-parallelism approach;high performance;minimal performance degradation;parallel architectures;parallel programming;power limitations;productivity;programmer time;time-consuming tasks","","0","","","","","","15-18 Dec. 2013","","IEEE","IEEE Conferences"
"Intermittent Electrical Contact Resistance as a Contributory Factor in the Loss of Automobile Speed Control Functional Integrity","A. F. Anderson","","IEEE Access","20140415","2014","2","","258","289","For three decades, sudden acceleration (SA) incidents have been reported, where automobiles accelerate without warning. These incidents are often diagnosed as no fault found. Investigators, who follow the line of diagnostic reasoning from the 1989 National Highway Traffic Safety Administration (NHTSA) SA report, tend to conclude that SAs are caused by driver pedal error. This paper reviews the diagnostic process in the NHTSA report and finds that: 1) it assumes that an intermittent electronic malfunction should be reproducible either through in-vehicle or laboratory bench tests without saying why and 2) the consequence of this assumption, for which there appears to be no forensic precedent, is to recategorize possible intermittent electronic failures as proven to be nonelectronic. Showing that the supposedly inescapable conclusions of the NHTSA report concerning electronic malfunctions are without foundation opens the way for this paper to discuss electronic intermittency as a potential factor in SA incidents. It then reports a simple practical experiment that shows how mechanically induced electrical contact intermittencies can generate false speed signals that an automobile speed control system may accept as true and that do not trigger any diagnostic fault codes. Since the generation of accurate speed signals is essential for the proper functioning of a number of other automobile safety-critical control systems, the apparent ease with which false speed signals can be generated by vibration of a poor electrical contact is obviously a matter of general concern. Various ways of reducing the likelihood of SAs are discussed, including electrical contact improvements to reduce the likelihood of generating false speed signals, improved battery maintenance, and the incorporation of an independent fail-safe that reduces engine power in an emergency, such as a kill switch.","","","10.1109/ACCESS.2014.2313296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777269","Contact intermittency;No fault found;Sudden acceleration;cruise control;diagnostic reasoning;electronic speed control;electronic throttle control;functional safety;speed sensor;sudden acceleration","Acceleration;Automotive electronics;Electric contact resistance;Engines;Fault diagnosis;Velocity control","automobiles;automotive electronics;electrical contacts;road safety;velocimeters;velocity control","1989 National Highway Traffic Safety Administration;NHTSA;automobile safety-critical control system;automobile speed control functional integrity loss;intermittent electrical contact resistance;intermittent electronic malfunction;speed sensor;sudden acceleration incidents","","3","","53","","","20140321","2014","","IEEE","IEEE Journals & Magazines"
"Towards Model-Driven Engineering for Big Data Analytics -- An Exploratory Analysis of Domain-Specific Languages for Machine Learning","D. Breuker","ERCIS, Univ. of Muenster, Muenster, Germany","2014 47th Hawaii International Conference on System Sciences","20140310","2014","","","758","767","Graphical models and general purpose inference algorithms are powerful tools for moving from imperative towards declarative specification of machine learning problems. Although graphical models define the principle information necessary to adapt inference algorithms to specific probabilistic models, entirely model-driven development is not yet possible. However, generating executable code from graphical models could have several advantages. It could reduce the skills necessary to implement probabilistic models and may speed up development processes. Both advantages address pressing industry needs. They come along with increased supply of data scientist labor, the demand of which cannot be fulfilled at the moment. To explore the opportunities of model-driven big data analytics, I review the main modeling languages used in machine learning as well as inference algorithms and corresponding software implementations. Gaps hampering direct code generation from graphical models are identified and closed by proposing an initial conceptualization of a domain-specific modeling language.","1530-1605;15301605","Electronic:978-1-4799-2504-9; POD:978-1-4799-2505-6","10.1109/HICSS.2014.101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758697","Graphical Models;Machine Learning;Model-driven Engineering","Adaptation models;Computational modeling;Data models;Graphical models;Inference algorithms;Random variables;Unified modeling language","Big Data;computer graphics;data analysis;inference mechanisms;learning (artificial intelligence);program compilers;specification languages","big data analytics;direct code generation;domain-specific languages;domain-specific modeling language;general purpose inference algorithms;graphical models;machine learning problems;model-driven development;model-driven engineering;modeling languages;probabilistic models","","2","","37","","","","6-9 Jan. 2014","","IEEE","IEEE Conferences"
"Patch Reviewer Recommendation in OSS Projects","J. B. Lee; A. Ihara; A. Monden; K. i. Matsumoto","Inf. Syst. & Comput. Sci. Dept., Ateneo de Manila Univ., Quezon City, Philippines","2013 20th Asia-Pacific Software Engineering Conference (APSEC)","20140306","2013","2","","1","6","In an Open Source Software (OSS) project, many developers contribute by submitting source code patches. To maintain the quality of the code, certain experienced developers review each patch before it can be applied or committed. Ideally, within a short amount of time after its submission, a patch is assigned to a reviewer and reviewed. In the real world, however, many large and active OSS projects evolve at a rapid pace and the core developers can get swamped with a large number of patches to review. Furthermore, since these core members may not always be available or may choose to leave the project, it can be challenging, at times, to find a good reviewer for a patch. In this paper, we propose a graph-based method to automatically recommend the most suitable reviewers for a patch. To evaluate our method, we conducted experiments to predict the developers who will apply new changes to the source code in the Eclipse project. Our method achieved an average recall of 0.84 for top-5 predictions and a recall of 0.94 for top-10 predictions.","1530-1362;15301362","CD-ROM:978-1-4799-2143-0; Electronic:978-1-4799-2144-7; POD:978-1-4799-2145-4","10.1109/APSEC.2013.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754342","CVS;mining software repositories;patch reviewer recommendation;random walk","Accuracy;Erbium;History;Large scale integration;Predictive models;Software;Vectors","graph theory;project management;public domain software;recommender systems;source code (software)","Eclipse project;OSS projects;graph-based method;open source software project;patch reviewer recommendation;source code patch","","1","","25","","","","2-5 Dec. 2013","","IEEE","IEEE Conferences"
"Flow-level models for capacity planning and management in interference-coupled wireless data networks","A. Fehske; H. Klessig; J. Voigt; G. Fettweis","","IEEE Communications Magazine","20140212","2014","52","2","164","171","In 4G cellular networks, both the adaptation of data rates to current interference conditions due to adaptive modulation and coding as well as a frequency reuse of one mandate precise techniques to estimate cell capacities and cell loads in order to accurately predict the quality of service delivered to end users. Such estimation happens ideally already during the network planning phase and is further required for self-optimization at runtime. Classic flow-level techniques to estimate cell loads, capacities, and related quality of service metrics assume static and worst case interference, which is analytically simple, but may produce considerable errors and lead to disadvantageous planning and optimization results. Appropriate models where individual cells are coupled through interference are rendered analytically intractable. This article first introduces basic flow-level modeling techniques and then reviews recent results in the field of flow-level network models, which allow the actual loads and capacities in interference- coupled wireless networks to be bound and closely approximated. We discuss trade-offs between accuracy and numerical complexity of different techniques and identify a model based on the notion of average interference as the most practically relevant. Simulation results for a large scenario based on a real network illustrate its applicability to practical network planning.","0163-6804;01636804","","10.1109/MCOM.2014.6736758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6736758","","Complexity theory;Computational modeling;Interference;Load modeling;Numerical models;Signal to noise ratio;Telecommunication network management","adaptive modulation;cellular radio;frequency allocation;quality of service;radiofrequency interference;telecommunication network planning","4G cellular networks;adaptive coding;adaptive modulation;average interference;capacity planning;flow level modeling techniques;flow level models;frequency reuse;interference coupled wireless data networks;interference coupled wireless networks;network planning;quality of service","","6","","10","","","","February 2014","","IEEE","IEEE Journals & Magazines"
"A Taxonomy of SQL Injection Detection and Prevention Techniques","A. Sadeghian; M. Zamani; A. A. Manaf","Adv. Inf. Sch., Univ. Teknol. Malaysia, Kuala Lumpur, Malaysia","2013 International Conference on Informatics and Creative Multimedia","20140109","2013","","","53","56","While using internet for proposing online services is increasing every day, security threats in the web also increased dramatically. One of the most serious and dangerous web application vulnerabilities is SQL injection. SQL injection attack took place by inserting a portion of malicious SQL query through a non-validated input from the user into the legitimate query statement. Consequently database management system will execute these commands and it leads to SQL injection. A successful SQL injection attack interfere Confidentiality, Integrity and availability of information in the database. Based on the statistical researches this type of attack had a high impact on business. Finding the proper solution to stop or mitigate the SQL injection is necessary. To address this problem security researchers introduce different techniques to develop secure codes, prevent SQL injection attacks and detect them. In this paper we present a comprehensive review of different types of SQL injection detection and prevention techniques. We criticize strengths and weaknesses of each technique. Such a structural classification would further help other researchers to choose the right technique for the further studies.","","Electronic:978-0-7695-5133-3; POD:978-1-4799-3702-8","10.1109/ICICM.2013.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702782","Information security;SQL injection;Web application vulnerability","Browsers;Conferences;Context;Databases;Runtime;Security;Testing","Internet;SQL;query processing;security of data","Internet;SQL injection attacks;SQL injection detection technique;SQL injection prevention technique;database management system;legitimate query statement;malicious SQL query;statistical researches;structural classification","","7","","17","","","","4-6 Sept. 2013","","IEEE","IEEE Conferences"
"Model-based testing of NASA's OSAL API ‚Äî An experience report","C. Schulze; D. Ganesan; M. Lindvall; D. Mcf Omas; A. Cudmore","Fraunhofer Center for Exp. Software Eng., College Park, MD, USA","2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)","20140102","2013","","","300","309","We present a case study that evaluates the applicability and effectiveness of model-based testing in detecting bugs in real-world, mission-critical systems. NASA's Operating System ion Layer (OSAL) is the subject system of this paper. The OSAL is a reusable framework that wraps several operating systems (OS) and is used extensively in NASA's flight software missions. We developed a suite of behavioral models, represented as hierarchical finite state machines (FSMs), of the core file system API and generated a large number of test cases automatically. We then automatically executed these test cases against the OSAL. The results show that the OSAL is a high quality product. Naturally, due to the systematic and rigorous nature of MBT, we detected a few previously unknown ‚Äúcorner-case‚Äù bugs and issues, which escaped traditional manual testing and code reviews. We discuss the MBT architecture, the detected bugs, the code coverage of generated tests, as well as threats to validity of the study.","1071-9458;10719458","Electronic:978-1-4799-2366-3; POD:978-1-5090-4462-7","10.1109/ISSRE.2013.6698883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698883","coverage;model-based testing;state machines","Abstracts;Computer architecture;Computer bugs;Documentation;File systems;Testing","application program interfaces;finite state machines;operating systems (computers);program testing","FSM;MBT architecture;NASA;OSAL API;bug detection;flight software missions;hierarchical finite state machines;model-based testing;operating system ion layer;test cases","","8","","22","","","","4-7 Nov. 2013","","IEEE","IEEE Conferences"
"From the Virtual to the Material: The re-Appearance of the Art Object","D. Harrison","Sch. of Art & Design, Univ. of Wolverhampton, Wolverhampton, UK","2013 International Conference on Cyberworlds","20131212","2013","","","226","231","Artists, designers and craft makers are currently exploring new materials and processes such as 'Accumulated Printing' within their practice to bring forth new forms and extend ideas. Often this is through a hybrid dialogic process of a maker's thoughts translated into code in the virtual world for production in the real world. Resulting in unique crafted objects created without the 'touch' of the hand-made, while encapsulating craft-thinking in the machine-made. This real-virtual-real approach is further streamlined, and without the craft signature, when working in shared virtual space. As artists are moving objects across the virtual from the real and back into solid form, so cyberspace gains a foothold in the real through materialisation and a return to the virtual. The second life platform is relatively new and still under development, but there are a number of artists beginning to explore the possibilities of this virtual world outside its commercial premise. The phenomena of the re-materialisation of the art object will be presented through the shift in art-thinking since mid-last century, as evidenced by Lucy Lippard and derived from the impact of earlier art. The paper will review the current situation via contemporary understandings, and where new technologies have allowed for a re-materialisation of the art object, exampling the work of artists such as Intersculpt, Michael Eden and those in the Kritical Works in SL II exhibition. This re-positioning of the art object allows a return to the initial formative conceptual framework, and offers a way through to a cutting-edge form of post conceptual art practice.","","Electronic:978-1-4799-2246-8; POD:978-1-4799-2247-5","10.1109/CW.2013.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680119","Conceptual Art;Readymades;art object;dematerialisation;digital technologies;rematerialisation","Art;Green products;Internet;Painting;Production;Solids","art","accumulated printing;art object;craft-thinking;hybrid dialogic process;initial formative conceptual framework;re-appearance;re-materialisation;real-virtual-real approach;second life platform;unique crafted objects;virtual world","","0","","19","","","","21-23 Oct. 2013","","IEEE","IEEE Conferences"
"Benchmarking Usability and Performance of Multicore Languages","S. Nanz; S. West; K. S. d. Silveira; B. Meyer","ETH Zurich, Zurich, Switzerland","2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","20131212","2013","","","183","192","Developers face a wide choice of programming languages and libraries supporting multicore computing. Ever more diverse paradigms for expressing parallelism and synchronization become available while their influence on usability and performance remains largely unclear. This paper describes an experiment comparing four markedly different approaches to parallel programming: Chapel, Cilk, Go, and Threading Building Blocks (TBB). Each language is used to implement sequential and parallel versions of six benchmark programs. The implementations are then reviewed by notable experts in the language, thereby obtaining reference versions for each language and benchmark. The resulting pool of 96 implementations is used to compare the languages with respect to source code size, coding time, execution time, and speedup. The experiment uncovers strengths and weaknesses in all approaches, facilitating an informed selection of a language under a particular set of requirements. The expert review step furthermore highlights the importance of expert knowledge when using modern parallel programming approaches.","1949-3770;19493770","Electronic:978-0-7695-5056-5; POD:978-1-4799-1144-8","10.1109/ESEM.2013.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681351","","Benchmark testing;Measurement;Multicore processing;Parallel processing;Parallel programming;Usability","benchmark testing;multiprocessing systems;parallel programming;programming languages;software libraries","Chapel;Cilk;Go;TBB;benchmarking usability;coding time;execution time;expert knowledge;libraries;multicore computing;multicore languages;parallel benchmark programs;parallel programming;parallelism;programming languages;sequential benchmark programs;source code size;synchronization;threading building blocks","","12","","24","","","","10-11 Oct. 2013","","IEEE","IEEE Conferences"
"LHDiff: Tracking Source Code Lines to Support Software Maintenance Activities","M. Asaduzzaman; C. K. Roy; K. A. Schneider; M. D. Penta","Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada","2013 IEEE International Conference on Software Maintenance","20131202","2013","","","484","487","Tracking lines across versions of a file is a necessary step for solving a number of problems during software development and maintenance. Examples include, but are not limited to, locating bug-inducing changes, tracking code fragments or vulnerable instructions across versions, co-change analysis, merging file versions, reviewing source code changes, and software evolution analysis. In this tool demonstration, we present a language-independent line-level location tracker, named LHDiff, that can be used to track lines and analyze changes in various kinds of software artifacts, ranging from source code to arbitrary text files. The tool can effectively detect changed or moved lines across versions of a file, has the ability to detect line splits, and can easily be integrated with existing version control systems. It overcomes the limitations of existing language-independent techniques and is even comparable to tools that are language dependent. In addition to describing the tool, we also describe its effectiveness in analyzing source code artifacts.","1063-6773;10636773","Electronic:978-0-7695-4981-1; POD:978-1-4673-5218-5","10.1109/ICSM.2013.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676938","differencing tools;language-independent differencing tool;line tracking","Benchmark testing;Context;Position measurement;Software maintenance;Syntactics;Tracking","file organisation;program diagnostics;software maintenance","LHDiff;arbitrary text files;bug-inducing change location;co-change analysis;code fragment tracking;language-independent line-level location tracker;line split detection;software artifacts;software development;software evolution analysis;software maintenance activity;source code artifact analysis;source code line tracking","","0","","9","","","","22-28 Sept. 2013","","IEEE","IEEE Conferences"
"Empirical evidence of code decay: A systematic mapping study","A. Bandi; B. J. Williams; E. B. Allen","Dept. of Comput. Sci. & Eng., Mississippi State Univ., Starkville, MS, USA","2013 20th Working Conference on Reverse Engineering (WCRE)","20131121","2013","","","341","350","Code decay is a gradual process that negatively impacts the quality of a software system. Developers need trusted measurement techniques to evaluate whether their systems have decayed. The research aims to find what is currently known about code decay detection techniques and metrics used to evaluate decay. We performed a systematic mapping study to determine which techniques and metrics have been empirically evaluated. A review protocol was developed and followed to identify 30 primary studies with empirical evidence of code decay. We categorized detection techniques into two broad groups: human-based and metric-based approaches. We describe the attributes of each approach and distinguish features of several subcategories of both high-level groups. A tabular overview of code decay metrics is also presented. We exclude studies that do not use time (i.e., do not use evaluation of multiple software versions) as a factor when evaluating code decay. This limitation serves to focus the review. We found that coupling metrics are the most widely used at identifying code decay. Researchers use various terms to define code decay, and we recommend additional research to operationalize the terms to provide more consistent analysis.","1095-1350;10951350","Electronic:978-1-4799-2931-3; POD:978-1-4799-2932-0","10.1109/WCRE.2013.6671309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671309","Architecture Violations;Code Decay;Coupling;Design Rules;Metrics;Software Evolution","Computer architecture;Conferences;Data mining;History;Measurement;Software;Systematics","software metrics;software quality","code decay detection;code decay metrics;coupling metrics;human-based approach;metric-based approach;software system quality;systematic mapping;trusted measurement techniques","","2","","48","","","","14-17 Oct. 2013","","IEEE","IEEE Conferences"
"Towards generalizing expert programmers' suggestions for novice programmers","M. Ichinco; A. Zemach; C. Kelleher","Dept. of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO, USA","2013 IEEE Symposium on Visual Languages and Human Centric Computing","20131024","2013","","","143","150","Novice programmers may lack the experience to recognize opportunities to either improve their code or apply unfamiliar programming constructs. Yet, these opportunities are often clear to an experienced programmer. In this paper, we describe an exploratory study investigating 1) the potential value of the suggestions experienced programmers make to novice programmers and 2) the ways experienced programmers envision identifying other programs that would benefit from the same suggestion. The results of our study suggest that experienced programmers make suggestions that can introduce new programming constructs to novice programmers. The participants in our study most commonly made suggestions that improve the code quality of novice programs, rather than changing their output. Furthermore, experienced programmers could often state a simple heuristic rule to use in identifying other novice programs that would benefit from their suggestion. Participants were able to author the rules in pseudocode, mostly using combinations of iteration and comparison to find patterns of problematic code. However, based on a test implementation of a selected set of rules for these suggestions, we conclude that support for improving rules through review and community input will be valuable.","1943-6092;19436092","Electronic:978-1-4799-0369-6; POD:978-1-4799-0370-2; USB:978-1-4799-0368-9","10.1109/VLHCC.2013.6645259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645259","crowdsourcing;independent learning;novice programming;static code analysis","Animation;Educational institutions;Glass;Programming profession;Standards","program diagnostics","crowdsourcing;expert programmers;heuristic rule;novice programmers;program identification;pseudocode;static code analysis","","4","","53","","","","15-19 Sept. 2013","","IEEE","IEEE Conferences"
"The Progress of Supernova Simulation Based on Code WZXW96","M. Liu; J. Zhang","Inst. of Theor. Phys., China West Normal Univ., Nanchong, China","2013 International Conference on Computational and Information Sciences","20131024","2013","","","326","328","Supernova simulation is one of the applications of computer science in astrophysics. Based on the core-collapse supernova Code WZXW96, many works have been done in the recent years. We reviewed the progress including the inner core acceleration mechanism, the convection in the inner core, the screening effect on electron capture and the explosion energy. We also introduce ongoing work for simulation.","","Electronic:978-0-7695-5004-6; POD:978-1-4799-0300-9","10.1109/ICCIS.2013.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643008","Supernova;comechanism;convection;explosion;mechanism;numerical simulation","","astronomy;astronomy computing;computer science;electron capture;supernovae","astrophysics;code WZXW96;computer science;core collapse supernova Code WZXW96;electron capture;explosion energy;inner core acceleration mechanism;screening effect;supernova simulation","","0","","10","","","","21-23 June 2013","","IEEE","IEEE Conferences"
"On the sustainability of web systems evolution","A. Baravalle; C. Boldyreff; A. Capiluppi; R. Marques","ACE - University of East London London, UK","2013 15th IEEE International Symposium on Web Systems Evolution (WSE)","20131024","2013","","","31","34","In the last twenty years, the evolution of web systems has been driven along three dimensions: the processes used to develop, evolve, maintain and re-engineer the systems themselves; the end products (the pages, content and links) of such processes; and finally the people dimension, with the extraordinary shift in how developers and users shape, interact and maintain the code and content that they put online. This paper reviews the questions that each of these dimensions has addressed in the past, and indicates which ones will need to be addressed in the future, in order for web system evolution to be sustainable. We show that the study on websites evolution has shifted from server- to client-side, focusing on better technologies and processes, and that the users becoming creators of content open several open questions, in particular the issue of credibility of the content created and the sustainability of such resources in the long term.","1550-4441;15504441","Electronic:978-1-4799-1610-8; POD:978-1-4799-1609-2","10.1109/WSE.2013.6642413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642413","","Electronic publishing;Encyclopedias;Internet;Maintenance engineering;Software;Web sites","Internet;client-server systems;software maintenance","Web systems evolution;server-to-client-side;sustainability","","0","","20","","","","27-27 Sept. 2013","","IEEE","IEEE Conferences"
"The impact of requirements on software quality across three product generations","J. Terzakis","Intel Corporation, USA","2013 21st IEEE International Requirements Engineering Conference (RE)","20131021","2013","","","284","289","In a previous case study, we presented data demonstrating the impact that a well-written and well-reviewed set of requirements had on software defects and other quality indicators between two generations of an Intel product. The first generation was coded from an unorganized collection of requirements that were reviewed infrequently and informally. In contrast, the second was developed based on a set of requirements stored in a Requirements Management database and formally reviewed at each revision. Quality indicators for the second software product all improved dramatically even with the increased complexity of the newer product. This paper will recap that study and then present data from a subsequent Intel case study revealing that quality enhancements continued on the third generation of the product. The third generation software was designed and coded using the final set of requirements from the second version as a starting point. Key product differentiators included changes to operate with a new Intel processor, the introduction of new hardware platforms and the addition of approximately fifty new features. Software development methodologies were nearly identical, with only the change to a continuous build process for source code check-in added. Despite the enhanced functionality and complexity in the third generation software, requirements defects, software defects, software sightings, feature commit vs. delivery (feature variance), defect closure efficiency rates, and number of days from project commit to customer release all improved from the second to the third generation of the software.","1090-705X;1090705X","Electronic:978-1-4673-5765-4; POD:978-1-4673-5763-0","10.1109/RE.2013.6636731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636731","Requirements specification;multi-generational software products;requirements defects;reviews;software defects;software quality","Complexity theory;Indexes;Mobile communication;Software;Stability analysis;Testing;Writing","formal specification;product development;software quality;software reliability","Intel processor;Intel product;delivery defect closure efficiency rates;feature commit;feature commit rates;feature variance;hardware platforms;product differentiators;product generation;quality enhancements;requirements defects;requirements management database;software defects;software development methodologies;software product;software quality indicators;software sightings;source code;third generation software coding;third generation software design","","2","","10","","","","15-19 July 2013","","IEEE","IEEE Conferences"
"A Systematic Review on the Practice of Evaluating Visualization","T. Isenberg; P. Isenberg; J. Chen; M. Sedlmair; T. M√∂ller","INRIA","IEEE Transactions on Visualization and Computer Graphics","20131016","2013","19","12","2818","2827","We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.","1077-2626;10772626","","10.1109/TVCG.2013.126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634108","Data visualization;Encoding;Evaluation;History;Mathematical model;Systematics;information visualization;scientific visualization;systematic review;validation;visualization","Data visualization;Encoding;History;Mathematical model;Systematics","data visualisation;encoding","IEEE information visualization;IEEE visualization conference;coding scheme;domain-specific work practices;meta-level;requirements analyses;visual tools;visualization evaluation","Algorithms;Computer Graphics;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface","46","","74","","","","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Trace Matrix Analyzer (TMA)","W. Li; J. H. Hayes; F. Yang; K. Imai; J. Yannelli; C. Carnes; M. Doyle","Computer Science, University of Kentucky, Lexington, USA","2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)","20131007","2013","","","44","50","A Trace Matrix (TM) represents the relationship between software engineering artifacts and is foundational for many software assurance techniques such as criticality analysis. In a large project, a TM might represent the relationships between thousands of elements of dozens of artifacts (for example, between design elements and code elements, between requirements and test cases). In mission- and safety-critical systems, a third party agent may be given the job to assess a TM prepared by the developer. Due to the size and complexity of the task, automated techniques are needed. We have developed a technique for analyzing a TM, called Trace Matrix Analyzer (TMA), so that third party agents can perform their work faster and more effectively. To validate, we applied TMA to two TMs with known problems and golden answersets: MoonLander and MODIS. We also asked an experienced software engineer to manually review the TM. We found that TMA properly identified TM issues and was much faster than manual review, but also falsely identified issues for one dataset. This work addresses the Trusted Grand Challenge, research projects 3, 5, and 6.","2157-2186;21572186","Electronic:978-1-4799-0495-2; POD:978-1-4799-0496-9","10.1109/TEFSE.2013.6620153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620153","Formal Specification;Requirement Comprehension;Research Projects 3, 5, and 6;Temporal Requirements;Translation;Trusted Grand Challenge","Educational institutions;MODIS;Manuals;Noise measurement;Radio access networks;Software engineering;Vectors","formal specification;multi-agent systems;program diagnostics;safety-critical software","MODIS;MoonLander;TMA;automated techniques;code elements;mission-critical systems;research projects;safety-critical systems;software assurance techniques;software engineering artifacts;task complexity;third party agent;trace matrix analyzer;trusted grand challenge","","1","","10","","","","19-19 May 2013","","IEEE","IEEE Conferences"
"SOMETHINGit: A prototyping library for live and sound improvisation","T. Oda; K. Nakakoji; Y. Yamamoto","Key Technology Laboratory Software Research Associates, Inc. Tokyo, Japan","2013 1st International Workshop on Live Programming (LIVE)","20131003","2013","","","11","14","Live programming can be considered an interaction with incomplete code. Dynamic languages embrace the similar style of programming, such as pair programming and prototyping in a review session. Static languages require a certain degree of completeness of code, such as type safety and namespace resolution. SOMETHINGit is a Smalltalk library that combines dynamic Smalltalk and static Haskell and VDM-SL. SOMETHINGit enables programmers to write incomplete but yet partially mathematically sound programs by five levels of bridging mechanisms.","","Electronic:978-1-4673-6265-8; POD:978-1-4673-6264-1","10.1109/LIVE.2013.6617341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617341","Live Programming;Prototyping;Sketch;Smalltalk","Arrays;Dynamic programming;Graphical user interfaces;Indexes;Libraries;Programming;Software","software libraries;software prototyping","SOMETHINGit;Smalltalk library;VDM-SL;dynamic Smalltalk;live programming;partially mathematically sound programs;prototyping library;static Haskell","","2","","12","","","","19-19 May 2013","","IEEE","IEEE Conferences"
"Online Object Tracking: A Benchmark","Y. Wu; J. Lim; M. H. Yang","","2013 IEEE Conference on Computer Vision and Pattern Recognition","20131003","2013","","","2411","2418","Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.","1063-6919;10636919","Electronic:978-0-7695-4989-7","10.1109/CVPR.2013.312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619156","","Algorithm design and analysis;Object tracking;Performance evaluation;Robustness;Target tracking;Visualization","computer vision;image sequences;object tracking;performance evaluation","computer vision;image sequences;online object tracking;performance analysis;performance evaluation;robust tracking","","666","1","65","","","","23-28 June 2013","","IEEE","IEEE Conferences"
"A study of architectural decision practices","T. D. LaToza; E. Shabani; A. van der Hoek","Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA","2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE)","20130930","2013","","","77","80","Architectural decisions shape a software architecture and determine its ability to meet its requirements. To better understand architectural decisions in practice, we interviewed developers at two organizations. The results revealed that architectural decisions often become technology decisions, which are in turn influenced by both technical and social factors. Meetings and knowledge repositories help to communicate architectural decisions, but code reviews are ultimately necessary to ensure conformance. Costly changes to architectural decisions are caused by the discovery of an Achilles' heel, an important scenario that cannot be supported by an architectural decision. These findings suggest an important need for social development tools that help developers more easily and successfully share valuable technology knowledge and more effectively make technology choices.","","Electronic:978-1-4673-6290-0; POD:978-1-4673-6289-4","10.1109/CHASE.2013.6614735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614735","architectural decisions;architecture;developer practices;empirical study","Companies;Computer architecture;Databases;Decision making;Software;Software architecture","organisational aspects;software architecture;software development management","Achilles heel;architectural decision practices;knowledge repositories;organizations;social development tools;social factors;software architecture;technical factors;technology choices;technology decisions;valuable technology knowledge","","0","","11","","","","25-25 May 2013","","IEEE","IEEE Conferences"
"LASE: An example-based program transformation tool for locating and applying systematic edits","J. Jacobellis; N. Meng; M. Kim","The University of Texas at Austin, Austin, US","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","1319","1322","Adding features and fixing bugs in software often require systematic edits which are similar, but not identical, changes to many code locations. Finding all edit locations and editing them correctly is tedious and error-prone. In this paper, we demonstrate an Eclipse plug-in called Lase that (1) creates context-aware edit scripts from two or more examples, and uses these scripts to (2) automatically identify edit locations and (3) transform the code. In Lase, users can view syntactic edit operations and corresponding context for each input example. They can also choose a different subset of the examples to adjust the abstraction level of inferred edits. When Lase locates target methods matching the inferred edit context and suggests customized edits, users can review and correct LASE's edit suggestion. These features can reduce developers' burden in repetitively applying similar edits to different methods. The tool's video demonstration is available at https://www.youtube.com/ watch?v=npDqMVP2e9Q.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606707","","Abstracts;Cloning;Concrete;Context;Software;Syntactics;Systematics","program diagnostics;software engineering;software tools;ubiquitous computing","Eclipse plug-in;LASE;code transformation;context-aware edit scripts;customized edits;edit context;edit location identification;edit suggestion;example-based program transformation tool;inferred edit abstraction level;syntactic edit operations;systematic edits","","0","","8","","","","18-26 May 2013","","IEEE","IEEE Conferences"
"Automated software architecture security risk analysis using formalized signatures","M. Almorsy; J. Grundy; A. S. Ibrahim","Centre for Computing and Engineering Software Systems, Swinburne University of Technology, Melbourne, Australia","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","662","671","Reviewing software system architecture to pinpoint potential security flaws before proceeding with system development is a critical milestone in secure software development lifecycles. This includes identifying possible attacks or threat scenarios that target the system and may result in breaching of system security. Additionally we may also assess the strength of the system and its security architecture using well-known security metrics such as system attack surface, Compartmentalization, least-privilege, etc. However, existing efforts are limited to specific, predefined security properties or scenarios that are checked either manually or using limited toolsets. We introduce a new approach to support architecture security analysis using security scenarios and metrics. Our approach is based on formalizing attack scenarios and security metrics signature specification using the Object Constraint Language (OCL). Using formal signatures we analyse a target system to locate signature matches (for attack scenarios), or to take measurements (for security metrics). New scenarios and metrics can be incorporated and calculated provided that a formal signature can be specified. Our approach supports defining security metrics and scenarios at architecture, design, and code levels. We have developed a prototype software system architecture security analysis tool. To the best of our knowledge this is the first extensible architecture security risk analysis tool that supports both metric-based and scenario-based architecture security analysis. We have validated our approach by using it to capture and evaluate signatures from the NIST security principals and attack scenarios defined in the CAPEC database.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606612","Architecture Security Risk analysis;Common attack patterns enumeration and classification (CAPEC);Formal attack patterns specification;Software security","Computer architecture;Measurement;Risk analysis;Security;Software;Software architecture;Unified modeling language","digital signatures;object-oriented languages;software architecture;software metrics","CAPEC database;NIST security principals;OCL;automated software architecture security risk analysis;formal signatures;formalized signatures;metric-based architecture security analysis;object constraint language;scenario-based architecture security analysis;secure software development lifecycles;security architecture;security flaws;security metrics signature specification;signature matches;system development;system security","","12","","25","","","","18-26 May 2013","","IEEE","IEEE Conferences"
"Integrating S<sup>6</sup> code search and Code Bubbles","S. P. Reiss","Department of Computer Science Brown University Providence, RI. 02912 USA","2013 3rd International Workshop on Developing Tools as Plug-Ins (TOPI)","20130916","2013","","","25","30","We wanted to provide a tool for doing code search over open source repositories as part of the Code Bubbles integrated development environment. Integrating code search as a plug-in to Code Bubbles required substantial changes to the S<sup>6</sup> code search engine and the development of appropriate user interfaces in Code Bubbles. After briefly reviewing Code Bubbles and the S<sup>6</sup> search engine, this paper describes the integration strategy, the front end for code search, the modifications to the code search engine to handle context-based search, and the user interface for handling the results of the search.","2327-0748;23270748","Electronic:978-1-4673-6288-7; POD:978-1-4673-6287-0","10.1109/TOPI.2013.6597190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597190","Code search;code search in context;integrated development environments;test-based search","Context;Libraries;Programming environments;Search engines;Servers;Testing;User interfaces","public domain software;software tools;user interfaces","S<sup>6</sup> code search;code bubbles integrated development environment;code search engine;context-based search;open source repositories;user interface;user interfaces","","0","","18","","","","21-21 May 2013","","IEEE","IEEE Conferences"
"Killer-mobiles: The way towards energy efficient high performance computers?","M. Valero","","2013 13th International Conference on Application of Concurrency to System Design","20130916","2013","","","xvi","xvi","Summary form only given. It is widely recognized that Exascale systems will be constrained by power. The Mont-Blanc project aims to build an alternative approach towards Exascale based on aggregating parts from the embedded and mobile market, which offer a better FLOPS/Watt ratio and a lower unit cost, at the expense of lower peak performance per chip. HPC systems built from these parts will require a higher number of processors, or resort to extensive use of compute accelerators. Using a higher number of chips increases the available memory bandwidth, alleviating the bandwidth wall, but increases the pressure on the interconnection network. The use of a high number of processors and accelerators, and the increased pressure on the interconnect require extensive code optimizations to achieve strong scaling, point to point synchronizations, and overlap data transfer with computation. The role of the OmpSs parallel programming model is paramount, as the key enabling technology that hides the complexity from the programmer, and transparently performs all the required optimizations. In this talk, we will review the design philosophies of several vendors, including HPC compute accelerators, and ARM-based mobile application processors in terms of peak performance, memory bandwidth, and energy efficiency; and we will review how the OmpSs programming models exploits the benefits of the Mont-Blanc approach while overcoming the drawbacks.","1550-4808;15504808","Electronic:978-0-7695-5035-0; POD:978-1-4799-0376-4","10.1109/ACSD.2013.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598334","","","mobile computing;parallel programming","ARM-based mobile application processor;Exascale systems;FLOPS-Watt ratio;HPC system;Mont-Blanc project;OmpSs parallel programming model;code optimization;compute accelerator;energy efficiency;high performance computer;interconnection network;overlap data transfer;point to point synchronization","","0","","","","","","8-10 July 2013","","IEEE","IEEE Conferences"
"Electromagnetic Modeling and Simulation: Challenges in Validation, Verification, and Calibration","L. Sevgi","Electron. & Commun. Eng. Dept., Dogus Univ., Istanbul, Turkey","IEEE Transactions on Electromagnetic Compatibility","20140729","2014","56","4","750","758","Modeling and simulation in electromagnetic engineering are discussed. Fundamental terms and concepts, and challenges in model validation and verification and code calibration are reviewed. Time- and frequency-domain models are developed and calibrated. Typical calibration scenarios are presented where tests and comparisons against analytical as well as numerical models are done.","0018-9375;00189375","","10.1109/TEMC.2013.2280135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6600770","Analytic model;aperture leakage;calibration;electromagnetic (EMs);finite-difference time domain (FDTD);method of moments (MoM);numerical model;shielding effectiveness (SE);simulation;transmission line matrix (TLM);validation;verification","Calibration;Computational modeling;Finite difference methods;Mathematical model;Numerical models;Time-domain analysis;Time-varying systems","electromagnetic compatibility;electromagnetic wave scattering","EM compatibility;EM scattering;EMC;code calibration;electromagnetic engineering;electromagnetic modeling;electromagnetic simulation;numerical models;validation;verification calibration","","6","","28","","","20130916","Aug. 2014","","IEEE","IEEE Journals & Magazines"
"A Taxonomy and Mapping of Computer-Based Critiquing Tools","N. M. Ali; J. Hosking; J. Grundy","Fac. of Comput. Sci. & Inf. Technol., Univ. Putra Malaysia, Serdang, Malaysia","IEEE Transactions on Software Engineering","20131028","2013","39","11","1494","1520","Critics have emerged in recent times as a specific tool feature to support users in computer-mediated tasks. These computer-supported critics provide proactive guidelines or suggestions for improvement to designs, code, and other digital artifacts. The concept of a critic has been adopted in various domains, including medical, programming, software engineering, design sketching, and others. Critics have been shown to be an effective mechanism for providing feedback to users. We propose a new critic taxonomy based on extensive review of the critic literature. The groups and elements of our critic taxonomy are presented and explained collectively with examples, including the mapping of 13 existing critic tools, predominantly for software engineering and programming education tasks to the taxonomy. We believe this critic taxonomy will assist others in identifying, categorizing, developing, and deploying computer-supported critics in a range of domains.","0098-5589;00985589","","10.1109/TSE.2013.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570472","Design critics;critic taxonomy;critiquing systems;software tool support;survey","Java;Programming;Recommender systems;Software;Software engineering;Taxonomy;Unified modeling language","computer science education;programming;software engineering","computer-based critiquing tool mapping;computer-based critiquing tool taxonomy;computer-mediated tasks;computer-supported critics;critic taxonomy;digital artifacts;programming education tasks;software engineering","","3","","68","","","20130726","Nov. 2013","","IEEE","IEEE Journals & Magazines"
"The Astronomical Multipurpose Software Environment and the Ecology of Star Clusters","S. Portegies Zwart","Leiden Obs., Leiden Univ., Leiden, Netherlands","2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing","20130624","2013","","","202","202","Star cluster ecology is the field of research where stellar evolution, gravitational dynamics, hydrodynamcs and the background potential dynamics of the parent galaxy interact to a complex non-linear evolution of self gravitating stellar systems. I will review the processes related to the ecology of stellar clusters, discuss the numerical hurdles and the physical principles. In addition, I will introduce the AMUSE framework with which we are performing simulations of the ecology of stellar clusters. AMUSE is a general purpose framework for interconnecting existing scientific software with a homogeneous and unified interface. Since the framework is based on the standard message passing interface, any production ready code that is written in a language that supports its native bindings can be incorporated; in addition, our framework is intrinsically parallel and it conveniently separates all the numerical solvers in memory. The strict separation also enables the possibility to realize unit conversion between the different modules and to recover from fatalities in a unified and structured way. The time spent in the framework is relatively small, and for production simulations we measured an overhead of at most 10%, which in our case is acceptable. Due to the unified structure of the interface, incorporating new modules which address the same physics is relatively straightforward. The time stepping between the codes can be simply consecutive or realized via a mixed variable symplectic method in which the Hamiltonian of the problem is solved in separate steps and combined via a Verlet-leapfrog integration scheme. In our experience with an implementation for multiphysics simulations in astrophysics, we encounter relatively few problems with the strict separation in methods, and the results of our test simulations are consistent with earlier results that use a monolithic framework.","","Electronic:978-0-7695-4996-5; POD:978-1-4673-6465-2","10.1109/CCGrid.2013.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6546093","","Abstracts;Cloud computing;Grid computing","astronomy computing;astrophysical fluid dynamics;gravitation;message passing;parallel processing;star clusters","AMUSE framework;Verlet-leapfrog integration scheme;astronomical multipurpose software environment;astrophysics;background potential dynamics;code time stepping;complex nonlinear evolution;gravitational dynamics;homogeneous interface;hydrodynamcs;message passing interface;mixed variable symplectic method;multiphysics simulation;numerical solver;parallel framework;parent galaxy;problem Hamiltonian;production simulation;scientific software;self gravitating stellar system;star cluster ecology;stellar cluster ecology simulation;stellar evolution;unified interface","","0","","","","","","13-16 May 2013","","IEEE","IEEE Conferences"
"Storage codes ‚Äî Coding rate and repair locality","H. D. L. Hollmann","Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore","2013 International Conference on Computing, Networking and Communications (ICNC)","20130516","2013","","","830","834","The repair locality of a distributed storage code is the maximum number of nodes that ever needs to be contacted during the repair of a failed node. Having small repair locality is desirable, since it is proportional to the number of disk accesses during repair. However, recent publications show that small repair locality comes with a penalty in terms of code distance or storage overhead if exact repair is required. Here, we first review some of the main results on storage codes under various repair regimes and discuss the recent work on possible (information-theoretical) trade-offs between repair locality and other code parameters like storage overhead and code distance, under the exact repair regime. Then we present some new information theoretical lower bounds on the storage overhead as a function of the repair locality, valid for all common coding and repair models. In particular, we show that if each of the n nodes in a distributed storage system has storage capacity a and if, at any time, a failed node can be functionally repaired by contacting some set of r nodes (which may depend on the actual state of the system) and downloading an amount Œ≤ of data from each, then in the extreme cases where Œ± = Œ≤ or a = r Œ≤, the maximal coding rate is at most r/(r + 1) or 1/2, respectively (that is, the excess storage overhead is at least 1/r or 1, respectively).","","Electronic:978-1-4673-5288-8; POD:978-1-4673-5287-1; USB:978-1-4673-5286-4","10.1109/ICCNC.2013.6504196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504196","","Bandwidth;Bismuth;Encoding;Maintenance engineering;Peer-to-peer computing;Reliability;Vectors","codes;maintenance engineering","distributed storage code overhead distance system;failed node repair locality;information-theoretic;maximal coding rate","","11","","30","","","","28-31 Jan. 2013","","IEEE","IEEE Conferences"
"Adaptive multiview video streaming: challenges and opportunities","J. Chakareski","Ecole Polytechnique F&#233;d&#233;rale de Lausanne","IEEE Communications Magazine","20130513","2013","51","5","94","100","Delivering multiview video content over present packet networks poses multiple challenges. First, the best effort nature of the Internet exposes media packets to variable bandwidth, loss, and delay as they traverse the network. Second, the prediction dependencies employed to maximize compression efficiency make the reconstruction process at the client extremely vulnerable to missing data. Third, the heterogeneity of client devices in terms of computing power, display capabilities, and access link capacity necessitates customizing the streaming process per user. My article reviews existing opportunities for addressing these challenges from within each of the three main stages of the content delivery pipeline (i.e., encoding, transmission, and reconstruction). Concretely, I first describe adaptive source coding techniques that construct a compressed representation of the multiview video source that exhibits resilience to network bandwidth variations and client view selection uncertainty. Then I discuss intelligent methods for error protection, caching, and packet scheduling that organize the transmission of multiview data in a bandwidth-effective way. Here, I also review prospective multipath and cloud-assisted techniques for multiview video streaming. Finally, I identify robust client-side content reconstruction schemes and adaptive media playout methods that can minimize the impact of missing data and enhance the user's interactive experience. Then I proceed to describe community-driven streaming techniques for delivering interactive multiview content over a population of social peers. The article concludes with an outline of approaches for synergistic exploitation of the techniques I will present theretofore, jointly across the different layers of the network protocol stack at which they individually operate. Here, I also highlight the main deployment challenges for some of these techniques, and how their design should be addressed accordingly, to overcome them.","0163-6804;01636804","","10.1109/MCOM.2013.6515052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515052","","Bandwidth;Decoding;Encoding;Internet;Packet radio networks;Peer-to-peer computing;Streaming media","adaptive codes;data compression;protocols;source coding;video coding;video streaming","access link capacity;adaptive media playout methods;adaptive multiview video streaming;adaptive source coding techniques;compressed representation;compression efficiency;computing power;content delivery pipeline;display capabilities;encoding;error protection;intelligent methods;interactive multiview content;media packets;multiview video content;multiview video source;network bandwidth;network protocol;packet networks;reconstruction;reconstruction process;robust client-side content reconstruction schemes;transmission","","18","","18","","","","May 2013","","IEEE","IEEE Journals & Magazines"
"Quantum Search Algorithms, Quantum Wireless, and a Low-Complexity Maximum Likelihood Iterative Quantum Multi-User Detector Design","P. Botsinis; S. X. Ng; L. Hanzo","School of Electronics and Computer Science, University of Southampton, Southampton, U.K.","IEEE Access","20130510","2013","1","","94","122","The high complexity of numerous optimal classic communication schemes, such as the maximum likelihood (ML) multiuser detector (MUD), often prevents their practical implementation. In this paper, we present an extensive review and tutorial on quantum search algorithms (QSA) and their potential applications, and we employ a QSA that finds the minimum of a function in order to perform optimal hard MUD with a quadratic reduction in the computational complexity when compared to that of the ML MUD. Furthermore, we follow a quantum approach to achieve the same performance as the optimal soft-input soft-output classic detectors by replacing them with a quantum algorithm, which estimates the weighted sum of a function's evaluations. We propose a soft-input soft-output quantum-assisted MUD (QMUD) scheme, which is the quantum-domain equivalent of the ML MUD. We then demonstrate its application using the design example of a direct-sequence code division multiple access system employing bit-interleaved coded modulation relying on iterative decoding, and compare it with the optimal ML MUD in terms of its performance and complexity. Both our extrinsic information transfer charts and bit error ratio curves show that the performance of the proposed QMUD and that of the optimal classic MUD are equivalent, but the QMUD's computational complexity is significantly lower.","","","10.1109/ACCESS.2013.2259536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515077","BBHT quantum search algorithm;Bit-interleaved coded modulation;D√ºrr‚ÄìH√∏yer algorithm;EXIT chart;Grover's quantum search algorithm;computational complexity;iterative decoding;multi-user detection;quantum amplitude amplification;quantum amplitude estimation;quantum computation;quantum entanglement;quantum mean algorithm","Algorithm design and analysis;Amplitude modulation;Computational complexity;Interleaved codes;Maximum likelihood decoding;Modulation;Parallel processing;Quantum computing;Search methods","code division multiple access;communication complexity;iterative decoding;maximum likelihood decoding;quantum computing","ML MUD;QMUD scheme;QSA;bit-interleaved coded modulation;computational complexity;direct-sequence code division multiple access system;iterative decoding;low complexity maximum likelihood iterative quantum multiuser detector design;optimal soft-input soft-output classic detectors;quantum search algorithms;quantum wireless;soft-input soft-output quantum-assisted MUD scheme","","24","","102","","","","2013","","IEEE","IEEE Journals & Magazines"
"Partial Transmit Sequence PAPR Reduction Method for LTE OFDM Systems","M. Gouda; M. Hussien","Dept. of Commun. Eng., Misr Univ. For Sci. & Technol., Cairo, Egypt","2013 4th International Conference on Intelligent Systems, Modelling and Simulation","20130415","2013","","","507","512","This paper reviews orthogonal frequency division multiple (OFDM) which has been adopted as a standard for various high data rate wireless communication systems. However, implementation of the OFDM system entails several difficulties. One of the major drawbacks is the high peak-to-average power ratio (PAPR) which cause large number of sub-carriers, that make restrictions for practical applications. Block Coding, partial transmit sequence and clipping are some PAPR reduction methods that have been proposed to overcome this problem. In this paper, we mainly investigate the PAPR reduction performance using PTS, this method is sub-entities of phase rotation scheme. A new algorithm using PTS technique which shows better PAPR reduction compared to the existing algorithms is proposed. Results are verified using MATLAB software.","2166-0662;21660662","Electronic:978-0-7695-4963-7; POD:978-1-4673-5653-4","10.1109/ISMS.2013.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498323","Orthogonal frequency division multiplexing (OFDM); partial transmits sequences (PTS); peak-to average power ratio (PAPR)","Complexity theory;MIMO;Modulation;Partial transmit sequences;Peak to average power ratio;Simulation","Long Term Evolution;OFDM modulation;block codes","LTE OFDM systems;Long Term Evolution;Matlab software;PTS;block coding;orthogonal frequency division multiplexing;partial transmit clipping;partial transmit sequence PAPR reduction method;peak-to-average power ratio;phase rotation scheme","","3","","20","","","","29-31 Jan. 2013","","IEEE","IEEE Conferences"
"Zero-Overhead Interfaces for High-Performance Computing Libraries and Kernels","A. Sch√§fer; D. Fey","Dept. of Comput. Sci., Friedrich-Alexander-Univ. Erlangen-Nurnberg (FAU), Nu&#x0308;rnberg, Germany","2012 SC Companion: High Performance Computing, Networking Storage and Analysis","20130411","2012","","","1139","1146","In this paper we propose a domain-specific language-based approach to reduce the overhead associated with accessing external data from computational kernels. Libraries which aid application developers in parallelizing and optimizing their codes need a way to expose their internal data stores to user code. An efficient interface as well as an optimized data layout are imperative for high application performance. We focus on codes which operate on regular grids and require only local interactions. These stencil-based programs form a class of algorithms found at the heart of many computer simulations and PDE solvers. Many stencil codes are memory bound, meaning that their performance depends heavily on an efficient usage of the computers' memory subsystem. This work's contribution is to give an extensive review of the available implementation alternatives and to put them in context with the state of the art. From this we derive our domain-specific language (DSL) which alleviates many of the shortcomings of previous designs, especially related to the utilization of SIMD units and simplifying the address generation. Simultaneously it provides a natural, object-oriented way of expressing data structures and accesses. We validate our DSL with benchmark results obtained from two kernels: one reverse time migration and one Lattice Boltzmann method.","","Electronic:978-0-7695-4956-9; POD:978-1-4673-6218-4","10.1109/SC.Companion.2012.137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495919","domain-specific language;stencil codes;structure of arrays;vectorization","","parallel processing;specification languages","DSL;PDE solver;code optimization;code parallelization;computational kernel;data layout;domain-specific language-based approach;high-performance computing kernel;high-performance computing library;lattice Boltzmann method;partial differential equation;reverse time migration;stencil-based program;zero-overhead interface","","1","","14","","","","10-16 Nov. 2012","","IEEE","IEEE Conferences"
"Work in progress: How engineering students define innovation","N. D. Fila; W. P. Myers; ≈û. Purzer","School of Engineering Education, Purdue University, West Lafayette, IN","2012 Frontiers in Education Conference Proceedings","20130218","2012","","","1","6","Innovation is defined in diverse ways in the literature and often assessed in ways synonymous with creativity. As these arguments continue it is also important to identify student perspectives. In this pilot study, we examine how engineering students define innovation. Fifty-four first-year engineering students were asked to define innovation in an open response survey. Their answers were first reviewed to identify emerging patterns and then a detailed coding method was used to categorize students' responses. The analysis examined students' focus on feasibility, desirability, and viability as well as other important aspects of innovative design. The findings from this open-ended survey will be used to develop an assessment tool that is easy to administer and score.","0190-5848;01905848","Electronic:978-1-4673-1352-0; POD:978-1-4673-1353-7; USB:978-1-4673-1351-3","10.1109/FIE.2012.6462431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462431","Creativity;First-Year Engineering;Innovation","Context;Educational institutions;Encoding;Engineering students;Technological innovation","engineering education;innovation management","coding method;engineering students;innovation definition;innovative design;open response survey","","1","","10","","","","3-6 Oct. 2012","","IEEE","IEEE Conferences"
"The Training Set Selection Methods of microRNA Precursors Prediction Based on Machine Learning Approaches","L. Wenyuan; M. Jing; W. Changwu; W. Baowen; L. Yongqiang","Yanshan Univ., Qinhuangdao, China","2013 Third International Conference on Intelligent System Design and Engineering Applications","20130207","2013","","","1566","1569","Micro RNAs (miRNAs) are single-stranded, endogenous ~22nt small non-coding RNAs (sncRNAs) that can play important regulatory roles in animals and plants by targeting mRNA for cleavage or translational repression. miRNAs which have very low expression levels or are expressed at specific stage are difficult to find by biological experiments. Also biological experiment only can find a small amount of miRNAs. Computational approaches have become another important way of miRNA prediction, especially machine learning approaches. miRNA prediction based on machine learning approaches requires a lot of positive and negative samples. The number of miRNA precursors that are experimentally validated is rare. However, the number of the sequence fragments, which are similar to real miRNA precursors in whole genome, is up to millions and millions. It is important to select reasonable samples for constructing high-performance classifier. In this review, the training set samples used for predicting miRNA precursors based on machine learning approaches are summarized.","","Electronic:978-0-7695-4923-1; POD:978-1-4673-4893-5","10.1109/ISDEA.2012.376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6455233","Machine learning;The Whole Genome;Training Set;microRNA","Intelligent systems","RNA;biology computing;genomics;learning (artificial intelligence);molecular biophysics;molecular configurations;pattern classification","animals;biological experiments;cleavage repression;endogenous 22nt small noncoding RNAs;genome;high-performance classifier;machine learning approaches;microRNA precursors prediction;plants;sequence fragments;single-stranded noncoding RNAs;training set selection methods;translational repression","","0","","37","","","","16-18 Jan. 2013","","IEEE","IEEE Conferences"
"Comparison of Static Analysis Tools for Finding Concurrency Bugs","N. Manzoor; H. Munir; M. Moayyed","Blekinge Inst. of Technol., Karlsrkona, Sweden","2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops","20130110","2012","","","129","133","This paper highlights the issues of detecting Java concurrency bugs using static code analysis tools. Concurrency bugs are often hard to find because of interleaving threads and there is need to use static analysis tools to detect the concurrency bugs. In the literature review, we established that there are number of static analysis tools such as FindBugs, JLint and Chord, used in experiments to determine their ability to detect the Java concurrency bugs. However, there are still tools in the class of open source static analysis that needs experimental evidence for their ability to find concurrency bugs. In this study we selected three tools CheckThread, RacerX and RELAY. The experiment and survey is used to find out the answers for formulated research questions in the introduction section.","","Electronic:978-0-7695-4928-6; POD:978-1-4673-5048-8","10.1109/ISSREW.2012.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405429","concurrency bugs;static analysis tools;static code analysis","Benchmark testing;Computer bugs;Concurrent computing;Java;NASA;Relays;Software","Java;program debugging;program diagnostics;public domain software","CheckThread tool;Chord tool;FindBugs tool;JLint tool;Java concurrency bug detection;RELAY tool;RacerX tool;interleaving threads;open source static analysis tools","","0","","14","","","","27-30 Nov. 2012","","IEEE","IEEE Conferences"
"Assessing Product Quality through PMR Analysis: A Perspective","R. Das","","2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops","20130110","2012","","","106","115","Success of a software product in the marketplace is defined by Quality of the product among other factors Assessing the current quality of the product is essential before it can be improved The assessment needs to be accurate in order to have the right action plan to improve quality Test methods and code reviews used to identify quality gaps in pre-release software are inadequate Product quality can be assessed by analysing PMRs (Problem Management Reports) in post-release software a) A PMR is a well defined document that is used to Report a customer found issue with the product Track the customer issue to closure b) Analysis of PMRs opened in the past are a clear indication of: i) Product components that are Most commonly used and having the most number of bugs, classified by sub-component Least commonly used ii) Sub-components to be focused on, to provide a visible improvement in the overall product quality c) PMR analysis improves the accuracy of the assessment as it is based on actual product usage In turn improves the accuracy / relevance of the action plan formulated to improve quality The paper outlines the quality assessment of a software product through PMR analysis.","","Electronic:978-0-7695-4928-6; POD:978-1-4673-5048-8","10.1109/ISSREW.2012.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405426","IBM;PMR;assessment;component;post-release;pre-release;quality;sub-component","Accuracy;Computer bugs;IEEE Potentials;Product design;Proposals;Quality assessment;Software","customer services;product quality;software quality","PMR analysis;customer found issue;prerelease software;problem management report;product tracking;quality gap identification;software product component;software product quality assessment","","0","","","","","","27-30 Nov. 2012","","IEEE","IEEE Conferences"
"Wavelet based image compression methods ‚Äî State of the art","K. Selvakumarasamy; S. Poornachandra","Anna university of Technology, Chennai, India","Computing Communication & Networking Technologies (ICCCNT), 2012 Third International Conference on","20121231","2012","","","1","5","With the increasing growth of technology and the entrance into the digital world, we have to handle a enormous amount of information every time which often presents difficulties. So, the digital information must be stored and retrieved in a professional and valuable manner, in order for it to be put to practical use. Imaging requires large amount of memory to store the digitized data. Due to the transmission bandwidth constraint, images must be compressed before transmission and storage. The wavelet transform is more and more widely used in image processing algorithms. In this paper, we reviewed the different methods of image coding. We reviewed the different methods in terms of PSNR [Peak Signal to Noise Ratio] and MSE [Mean Square Error] which can be used to better understand the relationship between various methods and their features.","","Electronic:978-1-5090-4669-0; POD:978-1-4673-5693-0","10.1109/ICCCNT.2012.6396077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6396077","ASWDR;EZW;LSK;LZW;NLS;SPECK;SPIHT","Biomedical imaging;Encoding;Image coding;Image resolution;Indexes;PSNR;Partitioning algorithms","data compression;image coding;mean square error methods;wavelet transforms","MSE;PSNR;digital information retrieval;digital information storage;image coding;image compression method;mean square error;peak signal to noise ratio;wavelet transform","","0","","17","","","","26-28 July 2012","","IEEE","IEEE Conferences"
"A reconfigurable DCT/IDCT architecture for video codec: A Review","B. E. Caroline; G. Sheeba; J. Jeyarani; F. S. R. mary","J.J. College of Engineering and Technology, Trichy-09, India","Computing Communication & Networking Technologies (ICCCNT), 2012 Third International Conference on","20121231","2012","","","1","5","Among various transform techniques for image compression, the Discrete Cosine Transform (DCT) is the most popular and effective one in practical applications because it gives an almost optimal performance and can be implemented at an acceptable cost. The transform coding utilizing the Discrete Cosine Transform (DCT) has been commonly adopted in the various standards for image compression. These include the CCITT standard for video telephony, the JPEG (Joint Photographic Expert Group) standard for coloured still-image transmission and the MPEG (Motion Pictures Expert Group) standard for pictures on the storage media. MPEG was originally developed for digital television, its popularity and effectiveness as a storage/transmission format and abundance of content have made it attractive for use in personal video playback devices. DCT based coding schemes are also used for higher quality applications such as HDTV. Therefore, an architecture which rapidly computes DCT has become a key component of image compression VLSIs. This paper reviews the existing efficient algorithms for implementing the reconfigurable Discrete Cosine Transform (DCT)/ Inverse Discrete Cosine Transform (IDCT) architecture for multimedia applications and proposed an algorithm which offers high speed and reduced area. The proposed architecture uses an error compensated adder tree to perform shifting and addition operations. The reconfigurable architecture not only decreases the time of research and development but also saves fabrication cost. The advantages of the proposed architecture are that this architecture does not require multipliers and ROM. It only needs adders and shifters. In digital circuits, the area of the multipliers and ROM are larger than adders and shifters.","","Electronic:978-1-5090-4669-0; POD:978-1-4673-5693-0","10.1109/ICCCNT.2012.6396054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6396054","Discrete Cosine Transform;Inverse Discrete Cosine Transform;VLSI;reconfigurable architecture","Codecs;Computer architecture;Discrete cosine transforms;Encoding;Image coding;Transform coding;Very large scale integration","data compression;digital signal processing chips;discrete cosine transforms;image colour analysis;transform coding;video codecs;video coding","CCITT standard;HDTV;JPEG;Joint Photographic Expert Group standard;MPEG;Motion Pictures Expert Group standard;ROM;VLSI;addition operation;coloured still-image transmission;digital circuit;digital television;error compensated adder tree;fabrication cost;image compression;inverse discrete cosine transform architecture;multimedia application;multiplier;personal video playback device;reconfigurable DCT architecture;reconfigurable IDCT architecture;reconfigurable architecture;reconfigurable discrete cosine transform;shifter;shifting operation;storage format;transform coding;transform technique;transmission format;video codec;video telephony","","0","","13","","","","26-28 July 2012","","IEEE","IEEE Conferences"
"Assuring software quality by code smell detection","E. van Emden; L. Moonen","","2012 19th Working Conference on Reverse Engineering","20121220","2012","","","xix","xix","In this retrospective we will review the paper ""Java Quality Assurance by Detecting Code Smells"" that was published ten years ago at WCRE. The work presents an approach for the automatic detection and visualization of code smells and discusses how this approach could be used in the design of a software inspection tool. The feasibility of the proposed approach was illustrated with the development of jCOSMO, a prototype code smell browser that detects and visualizes code smells in JAVA source code. It was the first tool to automatically detect code smells in source code, and we demonstrated the application of this tool in an industrial quality assessment case study. In addition to reviewing the WCRE 2002 work, we will discuss subsequent developments in this area by looking at a selection of papers that were published in its wake. In particular, we will have a look at recent related work in which we empirically investigated the relation between code smells and software maintainability in a longitudinal study where professional developers were observed while maintaining four different software systems that exhibited known code smells. We conclude with a discussion of the lessons learned and opportunities for further research.","1095-1350;10951350","Electronic:978-0-7695-4891-3; POD:978-1-4673-4536-1","10.1109/WCRE.2012.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385092","Java;code smells;quality assurance;refactoring;software inspection","Electronic mail;Inspection;Java;Software quality;Software systems;Visualization","Java;data visualisation;online front-ends;software maintenance;software quality","JAVA source code;Java software quality assurance;WCRE;automatic code smell detection;automatic code smell visualization;industrial quality assessment;jCOSMO;longitudinal study;prototype code smell browser;software inspection tool design;software maintainability","","0","","","","","","15-18 Oct. 2012","","IEEE","IEEE Conferences"
"Scalable Media Coding Enabling Content-Aware Networking","M. Grafl; C. Timmerer; H. Hellwagner; G. Xilouris; G. Gardikis; D. Renzi; S. Battista; E. Borcoci; D. Negru","Alpen-Adria-Universit&#x00e4;t, Austria","IEEE MultiMedia","20130612","2013","20","2","30","41","Increasingly popular multimedia services are expected to play a dominant role in the future of the Internet. In this context, it is essential that content-aware networking (CAN) architectures explicitly address the efficient delivery and processing of multimedia content. This article proposes the adoption of a content-aware approach into the network infrastructure, thus making it capable of identifying, processing, and manipulating media streams and objects in real time to maximize quality of service (QoS) and experience (QoE). Our proposal is built on the exploitation of scalable media coding technologies within such a content-aware networking environment. This discussion is based on four representative use cases for media delivery (unicast, multicast, peer-to-peer, and adaptive HTTP streaming) and reviews CAN challenges, specifically flow processing, caching/buffering, and QoS/QoE management.","1070-986X;1070986X","","10.1109/MMUL.2012.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362138","Content-based retrieval;Encoding;Internet;Media;Quality of service;Receivers;Static VAr compensators;Streaming media;Unicast;content-aware buffering;content-aware networking;in-network adaptation;multimedia;multimedia applications;multimedia services;scalable media coding","Content-based retrieval;Encoding;Internet;Media;Quality of service;Receivers;Static VAr compensators;Streaming media;Unicast","Internet;cache storage;media streaming;multicast communication;peer-to-peer computing;quality of experience;quality of service","CAN architectures;CAN challenges;Internet;QoE management;QoS management;adaptive HTTP streaming;buffering;caching;content-aware approach;content-aware networking architectures;content-aware networking environment;flow processing;media delivery;media streams;multicast streaming;multimedia content;multimedia services;network infrastructure;peer-to-peer streaming;quality of service;qulity of experience;scalable media coding enabling content-aware networking;scalable media coding technology;unicast streaming","","7","1","17","","","20121127","April-June 2013","","IEEE","IEEE Journals & Magazines"
"A large-scale empirical study of just-in-time quality assurance","Y. Kamei; E. Shihab; B. Adams; A. E. Hassan; A. Mockus; A. Sinha; N. Ubayashi","Kyushu University, Fukuoka","IEEE Transactions on Software Engineering","20130523","2013","39","6","757","773","Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (‚Äúrisky‚Äù) software changes instead of files or packages. We refer to this type of quality assurance activity as ‚ÄúJust-In-Time Quality Assurance,‚Äù because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that ‚ÄúJust-In-Time Quality Assurance‚Äù may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software.","0098-5589;00985589","","10.1109/TSE.2012.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341763","Maintenance;defect prediction;just-in-time prediction;mining software repositories;software metrics","Accuracy;Entropy;Measurement;Object oriented modeling;Predictive models;Quality assurance;Software","program testing;software maintenance;software metrics;software quality","commercial projects;cost reduction;defect prediction models;defect-prone file identification;defect-prone package identification;defect-prone software change identification;just-in-time quality assurance;open source projects;risk model;risky changes;software metrics;software quality assurance activities;software repository mining;software systems;source code inspection;unit testing","","45","1","63","","","20121110","June 2013","","IEEE","IEEE Journals & Magazines"
"The design and development of the server efficiency rating tool (SERT) (abstracts only)","K. D. Lange; M. G. Tricker","","ACM SIGMETRICS Performance Evaluation Review","20121018","2011","39","3","14","14","<p>According to the United States Environmental Protection Agency (US EPA) almost 3% of all electricity consumed within the US in 2010 goes to running datacenters, with the majority of that powering servers and the associated air conditioning systems dedicated to eliminating the heat they produce. The EPA launched the ENERGY STARR Computer Server program in May 2009, intended to deliver information to better enable server purchasing decisions based on projected power consumption.</p> <p>The Server Efficiency Rating Tool (SERT)"" has been developed by the Standard Performance Evaluation Corporation (SPEC) SPECpower committee to address the EPA requirements for Version 2 of the ENERGY STAR server program. Unlike many tools sourced from the SPEC organization the SERT is not intended to be a benchmark, and for Version 2 does not offer a single score model. Instead it produces detailed information regarding the influence of CPU, memory, network and storage I/O configurations on overall server power consumption.</p> <p>This paper describes the design and development of the SERT, including discussion of the collaborative nature of working with the EPA and the various industry stakeholders involved in the design, review and development process. Many of the core ideas behind SERT were derived from the SPECpower ssj2008 and other SPEC-developed benchmarks, and this paper illustrates where ideas and code were shared, as well as where new thinking resulted in entirely new solutions. It also includes thoughts for the future, as the ENERGY STAR server program continues to evolve and the SERT will evolve with it.</p>","0163-5999;01635999","","10.1145/2160803.2160819","","","","","","","","0","","","","","","December 2011","","ACM","ACM Journals & Magazines"
"An Empirical Evaluation of Mutation Testing for Improving the Test Quality of Safety-Critical Software","R. Baker; I. Habli","Aero Engine Controls, Birmingham","IEEE Transactions on Software Engineering","20130523","2013","39","6","787","805","Testing provides a primary means for assuring software in safety-critical systems. To demonstrate, particularly to a certification authority, that sufficient testing has been performed, it is necessary to achieve the test coverage levels recommended or mandated by safety standards and industry guidelines. Mutation testing provides an alternative or complementary method of measuring test sufficiency, but has not been widely adopted in the safety-critical industry. In this study, we provide an empirical evaluation of the application of mutation testing to airborne software systems which have already satisfied the coverage requirements for certification. Specifically, we apply mutation testing to safety-critical software developed using high-integrity subsets of C and Ada, identify the most effective mutant types, and analyze the root causes of failures in test cases. Our findings show how mutation testing could be effective where traditional structural coverage analysis and manual peer review have failed. They also show that several testing issues have origins beyond the test activity, and this suggests improvements to the requirements definition and coding process. Our study also examines the relationship between program characteristics and mutation survival and considers how program size can provide a means for targeting test areas most likely to have dormant faults. Industry feedback is also provided, particularly on how mutation testing can be integrated into a typical verification life cycle of airborne software.","0098-5589;00985589","","10.1109/TSE.2012.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298894","Mutation;certification;safety-critical software;testing;verification","Certification;Guidelines;Industries;Safety;Software systems;Testing","Ada;C language;aerospace computing;certification;integrated software;program testing;program verification;safety-critical software;software quality","Ada;C;airborne software system;certification;coding process;coverage requirement satisfaction;empirical evaluation;industry guideline;mutant type;mutation testing;safety standard;safety-critical software;software failure;software integration;software test quality;structural coverage analysis;test coverage level;test sufficiency measurement;verification life cycle","","9","","41","","","20120911","June 2013","","IEEE","IEEE Journals & Magazines"
"A Survey of Virtualization Technologies Focusing on Untrusted Code Execution","Y. Wen; J. Zhao; G. Zhao; H. Chen; D. Wang","Beijing Inst. of Syst. Eng., Beijing, China","2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing","20120910","2012","","","378","383","In response to a continually advancing threat incurred by untrusted codes from Internet, various virtualization-based technologies have been proposed. Such technologies utilize a software layer, a virtual machine monitor or hyper visor, to achieve the highest privilege in a computer system. Generally, they construct isolated execution environments to run the untrusted code while shielding the other parts of the system from the potential security issues. In this paper, we survey a number of virtualization-based technologies with the goal of finding an appropriate candidate to serve as an untrusted code execution solution on PC platforms. Contenders are reviewed with a number of desirable properties, especially security, transparency portability and performance.","","Electronic:978-0-7695-4684-1; POD:978-1-4673-1328-5","10.1109/IMIS.2012.92","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296882","Untrusted code execution;virtualization","Computers;Hardware;Linux;Security;Software;Virtual machining","Internet;security of data;system monitoring;virtual machines","Internet;PC platform;computer system privilege;continually advancing threat;hyper visor;isolated execution environment;potential security issues;software layer;transparency portability;untrusted code execution;virtual machine monitor;virtualization technology","","3","","48","","","","4-6 July 2012","","IEEE","IEEE Conferences"
"Features of electronic Early Warning systems which impact clinical decision making","A. Zarabzadeh; M. O'Connell; J. O'Donoghue; T. O'Kane; S. Woodworth; J. Gallagher; S. O'Connor; F. Adam","Health Information System Research Centre, University College Cork, Ireland","2012 25th IEEE International Symposium on Computer-Based Medical Systems (CBMS)","20120830","2012","","","1","4","Paper-based Modified Early Warning Scorecards (MEWS) have been developed to help nursing staff detect hospital in-patient deterioration at an early stage. MEWS is based on patient vital signs where these values are transformed into a MEWS score. An electronic Modified Early Warning Scorecard (eMEWS) prototype has been designed and developed to fulfill the role of a computerized Clinical Decision Support System (CDSS) and to assist healthcare professionals in their decision making activities. A review of the existing electronic Early Warning Scorecards (eEWS) revealed they lack certain features that assist in capturing a holistic view of the patient health status for example color codes and vital sign trends. The proposed eMEWS prototype employs these features with the aim of assisting healthcare professionals to obtain a clear understanding of the patient status. A survey was conducted to evaluate the impact of paper-based MEWS and eMEWS as part of the decision making process. The advantages and disadvantages of eMEWS over the paper-based MEWS are presented.","1063-7125;10637125","Electronic:978-1-4673-2051-1; POD:978-1-4673-2049-8","10.1109/CBMS.2012.6266394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6266394","","Color;Context;Decision making;Hospitals;Image color analysis;Prototypes","alarm systems;decision making;decision support systems;hospitals;medical computing","CDSS;MEWS score;clinical decision making;computerized clinical decision support system;eMEWS prototype;electronic early warning system features;electronic modified early warning scorecard prototype;hospital in-patient deterioration;paper-based modified early warning scorecards;patient health status;patient vital signs","","1","","15","","","","20-22 June 2012","","IEEE","IEEE Conferences"
"Icon scanning: Towards next generation QR codes","I. Friedman; L. Zelnik-Manor","Technion, Haifa, Israel","2012 IEEE Conference on Computer Vision and Pattern Recognition","20120726","2012","","","1130","1137","Undoubtedly, a key feature in the popularity of smartmobile devices is the numerous applications one can install. Frequently, we learn about an application we desire by seeing it on a review site, someone else's device, or a magazine. A user-friendly way to obtain this particular application could be by taking a snapshot of its corresponding icon and being directed automatically to its download link. Such a solution exists today for QR codes, which can be thought of as icons with a binary pattern. In this paper we extend this to App-icons and propose a complete system for automatic icon-scanning: it first detects the icon in a snapshot and then recognizes it. Icon scanning is a highly challenging problem due to the large variety of icons (~500K in App-Store) and background wallpapers. In addition, our system should further deal with the challenges introduced by taking pictures of a screen. Nevertheless, the novel solution proposed in this paper provides high detection and recognition rates. We test our complete icon-scanning system on icon snapshots taken by independent users, and search them within the entire set of icons in App-Store. Our success rates are high and improve significantly on other methods.","1063-6919;10636919","Electronic:978-1-4673-1228-8; POD:978-1-4673-1226-4; USB:978-1-4673-1227-1","10.1109/CVPR.2012.6247793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6247793","","Accuracy;Databases;Image color analysis;Image edge detection;Robustness;Shape;Visualization","bar codes;mobile computing;object detection;object recognition;smart phones","App-Store;App-icons;automatic icon-scanning;icon detection;icon recognition;icon snapshot;next generation QR codes;smartmobile devices","","3","","22","","","","16-21 June 2012","","IEEE","IEEE Conferences"
"When open source turns cold on innovation ‚Äî The challenges of navigating licensing complexities in new research domains","C. Forbes; I. Keivanloo; J. Rilling","Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada","2012 34th International Conference on Software Engineering (ICSE)","20120628","2012","","","1447","1448","In this poster, we review the limitations open source licences introduce to the application of Linked Data in Software Engineering. We investigate whether open source licences support special requirements to publish source code as Linked Data on the Internet.","0270-5257;02705257","Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6","10.1109/ICSE.2012.6227071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227071","Linked Data;license;open source;source code","Complexity theory;Databases;Law;Licenses;Software;Software engineering","Internet;public domain software;software engineering","Internet;licensing complexities;linked data in software engineering;open source licences","","0","","10","","","","2-9 June 2012","","IEEE","IEEE Conferences"
"Error mining: Bug detection through comparison with large code databases","A. Breckel","Institute of Software Engineering and Compiler Construction, University of Ulm, Ulm, Germany","2012 9th IEEE Working Conference on Mining Software Repositories (MSR)","20120625","2012","","","175","178","Bugs are hard to find. Static analysis tools are capable of systematically detecting predefined sets of errors, but extending them to find new error types requires a deep understanding of the underlying programming language. Manual reviews on the other hand, while being able to reveal more individual errors, require much more time. We present a new approach to automatically detect bugs through comparison with a large code database. The source file is analyzed for similar but slightly different code fragments in the database. Frequent occurrences of common differences indicate a potential bug that can be fixed by applying the modification back to the original source file. In this paper, we give an overview of the resulting algorithm and some important implementation details. We further evaluate the circumstances under which good detection rates can be achieved. The results demonstrate that consistently high detection rates of up to 50% are possible for certain error types across different programming languages.","2160-1852;21601852","Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3","10.1109/MSR.2012.6224278","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224278","code databases;code similarity;comparison-based bug detection;static analysis","Cloning;Computer bugs;Computer languages;Context;Databases;Software engineering;Syntactics","data mining;database management systems;program debugging;programming languages","bug detection;error mining;error types;individual errors;large code databases;original source file;programming language;static analysis tools","","0","","10","","","","2-3 June 2012","","IEEE","IEEE Conferences"
"App store mining and analysis: MSR for app stores","M. Harman; Y. Jia; Y. Zhang","University College London, Malet Place, London, WC1E 6BT, UK","2012 9th IEEE Working Conference on Mining Software Repositories (MSR)","20120625","2012","","","108","111","This paper introduces app store mining and analysis as a form of software repository mining. Unlike other software repositories traditionally used in MSR work, app stores usually do not provide source code. However, they do provide a wealth of other information in the form of pricing and customer reviews. Therefore, we use data mining to extract feature information, which we then combine with more readily available information to analyse apps' technical, customer and business aspects. We applied our approach to the 32,108 non-zero priced apps available in the Blackberry app store in September 2011. Our results show that there is a strong correlation between customer rating and the rank of app downloads, though perhaps surprisingly, there is no correlation between price and downloads, nor between price and rating. More importantly, we show that these correlation findings carry over to (and are even occasionally enhanced within) the space of data mined app features, providing evidence that our `App store MSR' approach can be valuable to app developers.","2160-1852;21601852","Electronic:978-1-4673-1761-0; POD:978-1-4673-1760-3","10.1109/MSR.2012.6224306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224306","","Business;Clustering algorithms;Correlation;Data mining;Feature extraction;Measurement;Software","data mining;pricing;software development management;software packages","Blackberry app store;app download;app store MSR;app store analysis;app store mining;business aspect;customer aspect;customer rating;customer review;data mining;feature information extraction;pricing;software repository mining;technical aspect","","65","","9","","","","2-3 June 2012","","IEEE","IEEE Conferences"
"A Taxonomy of Change Types and Its Application in Software Evolution","S. Lehnert; Q. u. a. Farooq; M. Riebisch","","2012 IEEE 19th International Conference and Workshops on Engineering of Computer-Based Systems","20120507","2012","","","98","107","Changes play a central role in software evolution, where the types of changes are as multifarious as their consequences. When changing software, impact analysis and regression testing are required to preserve the quality of the system. However, without a consistent classification of types of change operations, a well-founded impact analysis methodology cannot be developed. Existing works which analyze and apply change types are typically limited to a certain aspect of software, e.g. source code or architecture. They also lack a thorough investigation of change types, which lead to duplicated proposals and the absence of a consistent taxonomy. In this paper, we review the usage of change types for impact analysis and regression testing, and illustrate how both activities are affected by different types of changes. Therefore, we outline how existing work deals with different types and granularities of changes. Our main contribution is a generic, graph-based description of changes which distinguishes between atomic and composite change types. We show how existing change types and classifications can be mapped onto our proposed approach and change taxonomy. Finally, we illustrate how our proposed change types can support real developer activities, such as refactorings, impact analysis, and regression testing.","","Electronic:978-0-7695-4664-3; POD:978-1-4673-0912-7","10.1109/ECBS.2012.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195175","change types;graphs;impact analysis;regression testing;software evolution","","","","","3","","41","","","","11-13 April 2012","","IEEE","IEEE Conferences"
"Generalization of an Enhanced ECC Methodology for Low Power PSRAM","P. Y. Chen; C. L. Su; C. H. Chen; C. W. Wu","National Tsing Hua University, Hsinchu","IEEE Transactions on Computers","20130524","2013","62","7","1318","1331","Error control codes (ECCs) have been widely used to maintain the reliability of memories, but ordinary ECC codes are not suitable for memories with long codewords. For portable products, power reduction in memories with DRAM-like cells can be done by reducing the refresh frequency, but the loss of data integrity should be taken care of seriously. To solve these issues, we have proposed a parallel encoding and decoding ECC scheme to reduce refresh power for an industrial pseudo-SRAM (PSRAM) with long codewords. In this paper, we briefly review the scheme and propose a systematic way to generate the parity check matrix for the new ECC scheme. We also modify the parity correction mechanism to reduce the operating power of the scheme. As for the 70 ns access time of the 256-MB PSRAM with 64-bit codewords and 16-bit I/O, experimental results show that the new ECC scheme can be integrated with the READ/WRITE operations with about 0.2 percent circuit area overhead and less than 3.5 ns encoding/decoding time. The new ECC architecture provides a flexible solution for memories with different widths of ECC codewords and I/O ports, without the error masking effect or reduction in reliability.","0018-9340;00189340","","10.1109/TC.2012.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189334","DRAM chips;DRAM-like cell;Decision support systems;Decoding;ECC methodology;Encoding;Error control codes (ECCs);Error correction codes;Handheld computers;Reliability;SRAM chips;Systematics;error control code;fault tolerance;industrial pseudo-SRAM;low power PSRAM;low-power design;low-power electronics;memory size 256 MByte;parallel decoding;parallel encoding;parity check matrix;parity correction mechanism;portable product;power reduction;pseudo-SRAM;reliability;word length 16 bit;word length 64 bit","Decision support systems;Decoding;Encoding;Error correction codes;Handheld computers;Reliability;Systematics","DRAM chips;SRAM chips;low-power electronics;reliability","DRAM-like cell;ECC methodology;error control code;industrial pseudo-SRAM;low power PSRAM;memory size 256 MByte;parallel decoding;parallel encoding;parity check matrix;parity correction mechanism;portable product;power reduction;reliability;word length 16 bit;word length 64 bit","","1","1","20","","","20120424","July 2013","","IEEE","IEEE Journals & Magazines"
"Automated program transformation for migration to new libraries","V. Itsykson; A. Zozulya","Computer Systems and Program Engineering Dept., Saint-Petersburg State Polytechnical University, Russia","2011 7th Central and Eastern European Software Engineering Conference (CEE-SECR)","20120423","2011","","","1","7","In modern software engineering the task of migration from one system environment to another may arise very often (e.g. software migrating from Windows to Linux or from desktop PC to some mobile platform). This task may be accomplished manually, but this will certainly be not cost- and time-effective and will require additional testing and verification. Therefore various automated techniques aimed to solve this problem seem to be very useful. The article takes into consideration one of such approaches, mainly focused on transferring source code between two library environments. A comprehensive review of related works is presented. The classification of typical library entities having strong impact to software characteristics is defined. Special program annotation language (PanLang) is introduced. The main purpose of this language is to specify library structure and behavior. The key feature of PanLang is the high level of abstraction allowing to take out of consideration the library implementation details. Therefore, PanLang annotation applied to a library is a partial specification. Such a specification may be considered as a Moore FSM. The problem of semantic conformance of source and target libraries' specifications is posed. The conformance is guaranteed if set of execution traces corresponding to source library may be represented as a subset of target's one. In our case the solution of this problem may be reduced to FSM homomorphism detection. Our approach considers using Abstract Semantic Graph (ASG) as a model of source code. Therefore after semantic conformance is proved, special transformation rules may be applied ASG. These rules are based on both source and target partial specifications which guarantee correctness of result software. A simple prototype migration tool based on proposed approach is described. This Java-based tool uses CLang frontend to LLVM for C source code parsing and ASG construction. A number of sample migration tasks were used to pro- e the method correctness and efficiency. Possible improvements and future researches are also discussed.","","Electronic:978-1-4673-0844-1; POD:978-1-4673-0843-4; USB:978-1-4673-0842-7","10.1109/CEE-SECR.2011.6188463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188463","function behavior semantic;library's environment;program transformation;reengineering;software migration;specification of library","Automation;Libraries;Linux;Operating systems;Semantics;Sockets","Java;formal specification;grammars;graph theory;program diagnostics;software libraries","ASG construction;C source code parsing;CLang;FSM homomorphism detection;Java-based tool;LLVM;Linux;Moore FSM;PanLang;Windows;abstract semantic graph;automated program transformation;desktop PC;execution traces;library behavior specification;library entity classification;library environments;library structure specification;mobile platform;program annotation language;semantic conformance;software engineering;software migration task;source code model;source code transfer;source library;transformation rules","","0","","13","","","","Oct. 31 2011-Nov. 3 2011","","IEEE","IEEE Conferences"
"Keynote 1: LGTM - Software Sensing and Bug Smelling","H. C. Gall","Univ. of Zurich, Zurich, Switzerland","2012 16th European Conference on Software Maintenance and Reengineering","20120405","2012","","","3","4","Summary form only given. Looks Good To Me (LGTM) is a means to stamp an 'ok' on code reviews and then have the code moved to production. One key part of this process is a detailed quality analysis by checking code, looking for potential known bugs, and evaluating the design. As code bases are changed almost every day by many developers, we need to devise means for effective, regular, and focused analysis. In the recent past we have looked into approaches of software sensing and bug smelling. Sensing software is employed by audio-visual and multi-touch interfaces to a code base and its change history. Bug smelling leverages bug prediction on the levels of classes, methods, or change types. Combining both can lead to a more effective quality analysis for reviewing tasks such that the signing off by LGTM is facilitated. In this talk we will present approaches and tools such as SmellTagger or Coco Viz for software sensing and bug smelling. We will also discuss current limitations and potential new horizons for software evolution research.","1534-5351;15345351","Electronic:978-0-7695-4666-7; POD:978-1-4673-0984-4","10.1109/CSMR.2012.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178870","","Informatics;Sensors;Software;Software engineering","program debugging;software maintenance;user interfaces","Coco Viz;LGTM;SmellTagger;audio-visual interfaces;bug prediction;bug smelling;code checking;design evaluation;looks good to me;multitouch interfaces;quality analysis;software evolution research;software sensing","","0","","","","","","27-30 March 2012","","IEEE","IEEE Conferences"
"NG-PONs 1&2 and beyond: the dawn of the uber-FiWi network","M. Maier; M. Levesque; L. Ivanescu","Optical Zeitgeist Laboratory, INRS","IEEE Network","20120322","2012","26","2","15","21","This article reviews the main thrusts in next-generation passive optical network 1 and 2 technologies that enable short-term evolutionary and long-term revolutionary upgrades of coexistent gigabit-class PONs, respectively. It provides insight into the key requirements and challenges of the major candidate NG-PON 1&2 architectures such as long-reach XG-PON, wavelength-routing WDM PON, OCDMA and OFDMA PON, and reports on recent progress toward enhanced data and control plane functionalities, including real-time dynamic bandwidth allocation, improved privacy and guaranteed QoS, bandwidth flexibility, as well as cost-effective in-service monitoring techniques for NG-PONs. We then elaborate on converged optical fiber-wireless access networks, which may be viewed as the endgame of broadband access, and explain the inherent coverage and QoS issues of conventional radio-over-fiber networks for distributed wireless MAC protocols and how their limitations can be avoided in so-called radio-and-fiber networks. We explore powerful layer-2 optical-wireless, hierarchical frame aggregation, and network coding techniques that significantly improve the throughput-delay performance, resource utilization efficiency, and survivability of NG-PON and FiWi networks. Finally, we inquire into the opportunities of sensor-enhanced FiWi networks and propose our novel UÃàber-FiWi network, whose potential is demonstrated by studying the beneficial impact of inter-home scheduling of emerging plug-in electric vehicles on the resource management of a more sustainable future smart grid.","0890-8044;08908044","","10.1109/MNET.2012.6172270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6172270","","Next generation networking;Optical fiber sensors;Optical fibers;Optical network units;Passive optical networks;Wireless communication","access protocols;broadband networks;next generation networks;optical fibre LAN;passive optical networks;quality of service;radio access networks;radio-over-fibre","NG-PON;OCDMA;OFDMA PON;QoS;broadband access;control plane functionalities;converged optical fiber-wireless access networks;cost-effective in-service monitoring techniques;distributed wireless MAC protocols;enhanced data functionalities;gigabit-class PON;hierarchical frame aggregation techniques;interhome scheduling;layer-2 optical-wireless techniques;long-reach XG-PON;long-term revolutionary;network coding techniques;next-generation passive optical network;plug-in electric vehicles;radio-over-fiber networks;resource management;resource utilization efficiency;sensor-enhanced WiFi networks;short-term evolutionary;smart grid;uber-FiWi network;wavelength-routing WDM PON","","35","","18","","","","March-April 2012","","IEEE","IEEE Journals & Magazines"
"A grounded theory approach to effects of virtual facilitation on team communication and the development of professional skills","U. Mai; D. Swift; T. Wiggins; R. Luechtefeld","University of La Verne","2011 Frontiers in Education Conference (FIE)","20120202","2011","","","T2C-1","T2C-5","This article reports on a grounded theory study into the effects of virtual facilitation on team communication, a critical professional skill for engineering students. Facilitation may assist in that process because it encourages information sharing in order to promote effective teamwork. To aid in online team communications, a virtual facilitator has been developed. This research involves a quasi-experimental study of 40 community college students working on group projects. The students were sorted into treatment and control groups, and asked to work in an online forum designed for team collaboration. In the control group, the virtual facilitator was programmed to remain inactive. For teams in the treatment group, the virtual facilitator intervened with prompts when it was triggered by certain keywords and phrases. Using the grounded theory approach, researchers reviewed and analyzed transcripts to find themes and patterns, which were then coded. In addition, researchers explored communication differences between control and treatment groups.","0190-5848;01905848","Electronic:978-1-61284-469-5; POD:978-1-61284-468-8; USB:978-1-61284-467-1","10.1109/FIE.2011.6142860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142860","Communication;Engineering Professional Skills;Facilitation;Teamwork","Conferences;Educational institutions;Encoding;Face;Teamwork;Testing;Virtual groups","computer mediated communication;engineering education;groupware;professional communication;team working;virtual reality","engineering students;grounded theory approach;information sharing;online team communications;professional skill development;quasiexperimental study;team collaboration;team communication;virtual facilitation","","2","","16","","","","12-15 Oct. 2011","","IEEE","IEEE Conferences"
"Applying Fellegi-Sunter (FS) Model for Traceability Link Recovery between Bug Databases and Version Archives","A. Sureka; S. Lal; L. Agarwal","IIIT-D, New Delhi, India","2011 18th Asia-Pacific Software Engineering Conference","20120116","2011","","","146","153","Defect tracking systems such as Bugzilla and JIRA and source code version control systems such as CVS and SVN are widely used applications to support software development and maintenance activities. Previous studies show that bug databases and version databases are often used as standalone and separate repositories without explicit linkages between issue reports and corresponding commit transactions. This is because developers often do not explicitly mention or tag commit transactions with the relevant bug report IDs. The lack of explicit links between these two databases has been identified as a serious process data quality issue (incomplete and biased data) having implications in predictive model building (such as defect density and error proneness computation) and hypothesis-testing based on the dataset. Researchers have proposed solutions to link the two databases and performed experiments on open source projects such as Fire Fox Mozilla. We review previous approaches and propose a novel technique (based on Fellegi-Sunter (FS) Model for record linkages) to automatically integrate the two databases that overcomes some of the drawbacks of traditional methods. We validate the proposed approach by performing experiments on publicly available bug and version dataset obtained from two open-source projects (Apache HTTP Server and WikiMedia). The results of our experiments demonstrate that the proposed solution is effective in recovering trace ability links (missing links) between bug-fixing commits and corresponding bug reports.","1530-1362;15301362","Electronic:978-0-7695-4609-4; POD:978-1-4577-2199-1","10.1109/APSEC.2011.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130681","Automated Software Engineering;Data Integration;Defect Tracking Systems;Fellegi-Sunter (FS) Model;Mining Software Repositories;Record Linkages;Software Engineering Process Data Analysis;Traceability Link Recovery;Version Archives","Couplings;Databases;Joining processes;Software;Software engineering;Training;Vectors","database management systems;program debugging;system recovery","FS;Fellegi-Sunter application;Fire Fox Mozilla;HTTP Server;WikiMedia;bug databases;error proneness computation;open-source projects;software development;source code version control systems;traceability link recovery;tracking systems;version archives;version databases","","4","","14","","","","5-8 Dec. 2011","","IEEE","IEEE Conferences"
"Notice of Retraction<BR>The performance analysis for the combined STBC and MRC new system based on channel division","Jianwu Zhang; Zebin Zhou; Haiquan Wang; Jianrong Bao","School of Telecommunication, Hangzhou Dianzi University, Zhejiang, 310018, China","2011 Global Mobile Congress","20111215","2011","","","1","5","Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>The article studies the multi-antenna technology, carries on the selective analysis to the array antennas and the MIMO space-time block codes (STBC) technology, in the view of the MIMO STBC is only suitable in the high signal-to-noise ratio and abundant multi-path environment, We proposed to use the maximum ratio combining (MRC) receive technology in the receiving end. Through the channel's analysis, we carry on the division based on the maximum output signal-to-noise ratio to the channel, and then use different multi-antenna technology to achieve optimized the system performance. From the simulation result, the proposed new multi-antenna system based on the channel division is with a better performance than the original sole multi-antenna technology system.","","Electronic:978-1-4673-0348-4; POD:978-1-4673-0346-0","10.1109/GMC.2011.6103927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103927","","Error probability;MIMO;Receiving antennas;Signal to noise ratio;Slot antennas;Transmitting antennas;Wireless communication","MIMO communication;antenna arrays;diversity reception;space-time block codes","MIMO STBC technology;MIMO space-time block code technology;MRC receive technology;array antennas;channel analysis;channel division;combined STBC-MRC system;maximum ratio combining;multiantenna technology;multipath environment;signal-to-noise ratio","","0","","7","","","","17-18 Oct. 2011","","IEEE","IEEE Conferences"
"Visual analytic roadblocks for novice investigators","B. c. Kwon; B. Fisher; J. S. Yi","Purdue University, USA","2011 IEEE Conference on Visual Analytics Science and Technology (VAST)","20111215","2011","","","3","11","We have observed increasing interest in visual analytics tools and their applications in investigative analysis. Despite the growing interest and substantial studies regarding the topic, understanding the major roadblocks of using such tools from novice users' perspectives is still limited. Therefore, we attempted to identify such ‚Äúvisual analytic roadblocks‚Äù for novice users in an investigative analysis scenario. To achieve this goal, we reviewed the existing models, theories, and frameworks that could explain the cognitive processes of human-visualization interaction in investigative analysis. Then, we conducted a qualitative experiment with six novice participants, using a slightly modified version of pair analytics, and analyzed the results through the open-coding method. As a result, we came up with four visual analytic roadblocks and explained these roadblocks using existing cognitive models and theories. We also provided design suggestions to overcome these roadblocks.","","Electronic:978-1-4673-0014-8; POD:978-1-4673-0015-5","10.1109/VAST.2011.6102435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6102435","Visual analytics;cognitive model;framework;investigative analysis;qualitative experiment;roadblock","Analytical models;Calendars;Cognition;Context;Humans;Visual analytics","data analysis;data visualisation","human-visualization interaction cognitive process;investigative analysis scenario;novice investigators;open-coding method;pair analytics;visual analytic roadblocks","","9","","28","","","","23-28 Oct. 2011","","IEEE","IEEE Conferences"
"Common Agile Practices in Software Processes","J. F. Abrantes; G. H. Travassos","COPPE/Syst. Eng. & Comput. Sci. Program, Fed. Univ. of Rio de Janeiro, Rio de Janeiro, Brazil","2011 International Symposium on Empirical Software Engineering and Measurement","20111201","2011","","","355","358","Objective: to investigate studies about software processes looking for practices which can be used to obtain agility in software processes. Method: A systematic review including seven search engines was executed in Feb/2010. To apply the defined criteria to select papers and extract information regarding working practices bringing agility to software processes. Results: from 6696 retrieved papers, 441 were selected to support the identification of 236 occurrences of 51 distinct practices associated with the concept of agility. Their descriptions were deeply analyzed and consolidated. After discarding those which appeared in the technical literature in a small amount of papers, 17 agile practices were identified. Conclusion: although further studies are necessary to evaluate the efficacy of these 17 agile practices, 12 of them have been more commonly approached in the software projects and could be primarily considered: test driven development, continuous integration, pair programming, planning game, onsite customer, collective code ownership, small releases, metaphor, refactoring, sustainable pace, simple design and coding standards.","1949-3770;19493770","Electronic:978-0-7695-4604-9; POD:978-1-4577-2203-5","10.1109/ESEM.2011.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092587","agile methods;agile practices;agile software processes;evidence based software engineering;systematic review","Business;Context;Encoding;Programming profession;Software;Software engineering","search engines;software prototyping","coding standards;collective code ownership;common agile practices;continuous integration;information extraction;onsite customer;pair programming;planning game;search engines;simple design;small releases;software processes;software projects;test driven development","","3","","31","","","","22-23 Sept. 2011","","IEEE","IEEE Conferences"
"System perspective on embedded multimedia","L. G. Chen","National Taiwan University, Taiwan","2011 9th IEEE Symposium on Embedded Systems for Real-Time Multimedia","20111201","2011","","","1","1","Today's user demand for multimedia has moved into an anywhere anytime paradigm. The ubiquitous usage model creates lots of needs for embedded multimedia system design, and traditional module-wise design concept will not suffice. In this talk, the system design view of modern VLSI architectures for multimedia applications, including H.264, scalable video coding (SVC) and stereo/3D video coding, will be reviewed. In addition, several emerging applications, where machine-to-machine and machine-to-human design factors also become important, like distributed video coding (DVC), free-viewpoint TV and intelligent image recognition, will also be introduced. With the growth of these embedded architecture researches, we can expect a fruitful future of multimedia ICs and systems.","2325-1271;23251271","Electronic:978-1-4577-2122-9; POD:978-1-4577-2123-6","10.1109/ESTIMedia.2011.6088533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088533","","","VLSI;embedded systems;multimedia computing","H.264;VLSI architectures;anywhere anytime paradigm;distributed video coding;embedded architecture;embedded multimedia system design;free-viewpoint TV;intelligent image recognition;module-wise design;multimedia application;scalable video coding;stereo/3D video coding;system design view;system perspective;ubiquitous usage model","","0","","","","","","13-14 Oct. 2011","","IEEE","IEEE Conferences"
"Code Hot Spot: A tool for extraction and analysis of code change history","W. Snipes; B. Robinson; E. Murphy-Hill","Industrial Software Systems, ABB Corporate Research, Raleigh, NC USA","2011 27th IEEE International Conference on Software Maintenance (ICSM)","20111117","2011","","","392","401","Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.","1063-6773;10636773","Electronic:978-1-4577-0664-6; POD:978-1-4577-0663-9","10.1109/ICSM.2011.6080806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806","Configuration Management;Decision Support;Metrics;Quality;Refactoring;Verification","Complexity theory;Correlation;Couplings;Data mining;History;Measurement;Software","data flow analysis;software maintenance;software management;software metrics;software tools","ABB;Code Hot Spot;Microsoft TFS configuration management system;change information;code change history analysis tool;code change history extraction tool;commercial software development teams;configuration management system mining;faulty code;metrics;software management;source code","","0","","18","","","","25-30 Sept. 2011","","IEEE","IEEE Conferences"
"Assessing Software Quality by Program Clustering and Defect Prediction","X. Tan; X. Peng; S. Pan; W. Zhao","Sch. of Comput. Sci., Fudan Univ., Shanghai, China","2011 18th Working Conference on Reverse Engineering","20111117","2011","","","244","248","Many empirical studies have shown that defect prediction models built on product metrics can be used to assess the quality of software modules. So far, most methods proposed in this direction predict defects by class or file. In this paper, we propose a novel software defect prediction method based on functional clusters of programs to improve the performance, especially the effort-aware performance, of defect prediction. In the method, we use proper-grained and problem-oriented program clusters as the basic units of defect prediction. To evaluate the effectiveness of the method, we conducted an experimental study on Eclipse 3.0. We found that, comparing with class-based models, cluster-based prediction models can significantly improve the recall (from 31.6% to 99.2%) and precision (from 73.8% to 91.6%) of defect prediction. According to the effort-aware evaluation, the effort needed to review code to find half of the total defects can be reduced by 6% if using cluster-based prediction models.","1095-1350;10951350","Electronic:978-0-7695-4582-0; POD:978-1-4577-1948-6","10.1109/WCRE.2011.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079848","defect prediction;program clustering;software quality","Linear regression;Logistics;Measurement;Object oriented modeling;Predictive models;Semantics;Software","pattern clustering;software metrics;software performance evaluation;software quality","Eclipse 3.0;cluster based prediction models;defect prediction;effort aware evaluation;effort aware performance;problem oriented program clusters;product metrics;program clustering;proper grained program clusters;software module quality;software quality assessment","","3","2","15","","","","17-20 Oct. 2011","","IEEE","IEEE Conferences"
"Application research on network coding in WSN","L. Wang; G. Zhang; C. Ma; X. Fan","Coll. of Comput. Sci. & Technol., Harbin Eng. Univ., Harbin, China","2010 Fifth International Conference on Internet Computing for Science and Engineering","20111114","2010","","","162","166","This paper introduces the characteristics of the wireless network, including self-organizing, dynamic topology, wireless communications and so on, which make the network nodes have limited energy, the same as processing power, storage capacity and communication capabilities, therefore, the efficient use of energy, maximizing its network cycle is the wireless sensor network's core issue. Network coding integrates the concept of routing and coding, which has been proven to be able to approach the theoretical transfer limit of the network capacity. The unreliability of wireless link and the broadcasting characteristics of physical layer are ideal for the use of network coding. This paper reviews the applications of network coding technology in wireless sensor network, including improving network throughput, reducing energy consumption, increasing the reliability and security of the network link, and improving the efficiency of data aggregation.","2330-9857;23309857","Electronic:978-0-7695-4339-0; POD:978-1-4244-9954-0","10.1109/ICICSE.2010.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076562","Network Coding;WSN;data aggregation;energy consumption;security;throughput","Educational institutions;Encoding;Energy consumption;Network coding;Throughput;Wireless communication;Wireless sensor networks","network coding;radio links;telecommunication network reliability;telecommunication network routing;telecommunication security;wireless sensor networks","WSN;data aggregation efficiency;dynamic topology;energy consumption;network capacity;network coding technology;network cycle;network link reliability;network link security;network routing;network throughput;physical layer broadcasting characteristics;storage capacity;wireless communications;wireless link;wireless sensor network","","1","","11","","","","1-2 Nov. 2010","","IEEE","IEEE Conferences"
"Counting Bugs is Harder Than You Think","P. E. Black","Software & Syst. Div., U.S. Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA","2011 IEEE 11th International Working Conference on Source Code Analysis and Manipulation","20111103","2011","","","1","9","Software Assurance Metrics and Tool Evaluation (SAMATE) is a broad, inclusive project at the U.S. National Institute of Standards and Technology (NIST) with the goal of improving software assurance by developing materials, specifications, and methods to test tools and techniques and measure their effectiveness. We review some SAMATE sub-projects: web application security scanners, malware research protocol, electronic voting systems, the SAMATE Reference Dataset, a public repository of thousands of example programs with known weaknesses, and the Static Analysis Tool Exposition (SATE). Along the way we list over two dozen possible research questions, which are also collaboration opportunities. Software metrics are incomplete without metrics of what is variously called bugs, flaws, or faults. We detail numerous critical research problems related to such metrics. For instance, is a warning from a source code scanner a real bug, a false positive, or something else? If a numeric overflow leads to buffer overflow, which leads to command injection, what is the error? How many bugs are there if two sources call two sinks: 1, 2, or 4? Where is a missing feature? We conclude with a list of concepts which may be a useful basis of bug metrics.","","Electronic:978-0-7695-4347-5; POD:978-1-4577-0932-6","10.1109/SCAM.2011.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065191","software debugging;software engineering;software metrics;software tools","Communities;Java;Malware;Measurement;NIST;Software","program debugging;safety-critical software;software metrics","SAMATE project;SAMATE reference dataset;US National Institute of Standards and Technology;Unites States;Web application security scanner;buffer overflow;electronic voting system;malware research protocol;numeric overflow;software assurance metrics and tool evaluation;software metrics;source code scanner","","3","","37","","","","25-26 Sept. 2011","","IEEE","IEEE Conferences"
"Timing and schedulability analysis for distributed automotive control applications","S. Chakraborty; M. Di Natale; H. Falk; M. Lukasiewycz; F. Slomka","TU Munich, Germany","2011 Proceedings of the Ninth ACM International Conference on Embedded Software (EMSOFT)","20111031","2011","","","349","350","High-end cars today consist of more than 100 electronic control units (ECUs) that are connected to a set of sensors and actuators and run multiple distributed control applications. The design flow of such architectures consists of specifying control applications as Simulink/Stateflow models, followed by generating code from them and finally mapping such code onto multiple ECUs. In addition, the scheduling policies and parameters on both the ECUs and the communication buses over which they communicate also need to be specified. These policies and parameters are computed from high-level timing and control performance constraints. The proposed tutorial will cover different aspects of this design flow, with a focus on timing and schedulability problems. After reviewing the basic concepts of worst-case execution time analysis and schedulability analysis, we will discuss the differences between meeting timing constraints (as in classical real-time systems) and meeting control performance constraints (e.g., stability, steady and transient state performance). We will then describe various control performance related schedulability analysis techniques and how they may be tied to model-based software development. Finally, we will discuss various schedule synthesis techniques, both for ECUs as well as for communication protocols like FlexRay, so that control performance constraints specified at the model-level may be satisfied. Throughout the tutorial different commercial as well as academic tools will be discussed and demonstrated.","","Electronic:978-1-4503-0714-7; POD:978-1-4503-0714-7","10.1145/2038642.2038696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064501","Control Applications;Distributed Automotive Systems;Schedulability Analysis;Timing Analysis","Automotive engineering;Complexity theory;Computer architecture;Control systems;Real time systems;Software;Timing","actuators;automobiles;automotive electronics;digital simulation;distributed control;real-time systems;scheduling;sensors;software engineering;timing","FlexRay;Simulink model;Stateflow model;actuator;communication buses;communication protocol;control performance constraints;design flow;distributed automotive control application;electronic control units;high-end cars;high-level timing;meeting timing constraints;model-based software development;real-time system;schedulability analysis;sensor;timing problem;worst-case execution time analysis","","0","","8","","","","9-14 Oct. 2011","","IEEE","IEEE Conferences"
"A Review of DNA Microarray Image Compression","M. Hern¬¥ndez-Cabronero; I. Blanes; J. Serra-Sagrista; M. W. Marcellin","Dept. of Inf. & Commun. Eng., Univ. Autonoma de Barcelona, Barcelona, Spain","2011 First International Conference on Data Compression, Communications and Processing","20111027","2011","","","139","147","We review the state of the art in DNA micro array image compression. First, we describe the most relevant approaches published in the literature and classify them according to the stage of the typical image compression process where each approach makes its contribution. We then summarize the compression results reported for these specific-specific image compression schemes. In a set of experiments conducted for this paper, we obtain results for several popular image coding techniques, including the most recent coding standards. Prediction-based schemes CALIC and JPEG-LS, and JPEG2000 using zero wavelet decomposition levels are the best performing standard compressors, but are all outperformed by the best micro array-specific technique, Battiato's CNN-based scheme.","","Electronic:978-0-7695-4528-8; POD:978-1-4577-1458-0","10.1109/CCP.2011.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061015","JPEG2000;image coding standards;microarray image compression;microarray images","DNA;Data mining;Image coding;Image segmentation;Loss measurement;Transforms","DNA;image coding","CALIC;DNA microarray image compression;JPEG-LS;JPEG2000;image coding standard;prediction-based scheme;standard compressor;zero wavelet decomposition levels","","2","","40","","","","21-24 June 2011","","IEEE","IEEE Conferences"
"Refactoring Object-Oriented Specifications with Inheritance-Based Polymorphism","G. Smith; S. Helke","Sch. of Inf. Technol. & Electr. Eng., Univ. of Queensland, Brisbane, QLD, Australia","2011 Fifth International Conference on Theoretical Aspects of Software Engineering","20111013","2011","","","35","41","Specification notations such as JML and Spec# which are embedded into program code provide a promising approach to formal object-oriented software development. If the program code is refactored, however, the specifications need also to be changed. This can be facilitated by specification refactoring rules which allows such changes to be made systematically along with the changes to the code. A set of minimal and complete set of refactoring rules have been devised for the Object-Z specification language. This paper reviews these rules as a basis for a similar approach for languages like JML and Spec#. Specifically, it modifies the rules for introducing and removing inheritance and polymorphism from specifications. While these concepts are orthogonal in Object-Z, they are closely intertwined in the other notations.","","Electronic:978-0-7695-4506-6; POD:978-1-4577-1487-0","10.1109/TASE.2011.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6042061","object-oriented refactoring;object-z","Annealing;Context;Educational institutions;Electronic mail;Games;Programming;Software engineering","formal languages;formal specification;object-oriented methods;software engineering","JML;Spec#;formal object-oriented software development;inheritance-based polymorphism;object-Z specification language;program code;specification notations;specification refactoring rules","","1","","15","","","","29-31 Aug. 2011","","IEEE","IEEE Conferences"
"Diagnosing new faults using mutants and prior faults (NIER track)","S. S. Murtaza; N. Madhavji; M. Gittens; Z. Li","University of Western Ontario, London, ON, Canada","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","960","963","Literature indicates that 20% of a program's code is responsible for 80% of the faults, and 50-90% of the field failures are rediscoveries of previous faults. Despite this, identification of faulty code can consume 30-40% time of error correction. Previous fault-discovery techniques focusing on field failures either require many pass-fail traces, discover only crashing failures, or identify faulty ""files"" (which are of large granularity) as origin of the source code. In our earlier work (the F007 approach), we identify faulty ""functions"" (which are of small granularity) in a field trace by using earlier resolved traces of the same release, which limits it to the known faulty functions. This paper overcomes this limitation by proposing a new ""strategy"" to identify new and old faulty functions using F007. This strategy uses failed traces of mutants (artificial faults) and failed traces of prior releases to identify faulty functions in the traces of succeeding release. Our results on two UNIX utilities (i.e., Flex and Gzip) show that faulty functions in the traces of the majority (60-85%) of failures of a new software release can be identified by reviewing only 20% of the code. If compared against prior techniques then this is a notable improvement in terms of contextual knowledge required and accuracy in the discovery of finer-grain fault origin.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1985959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032562","decision tree;execution traces;faulty function;mutants","Decision trees;Fault diagnosis;Flexible printed circuits;Measurement;Software;Testing;USA Councils","Unix;program diagnostics","NIER track;UNIX utilities;crashing failures;fault diagnosis;faulty code;program code;source code","","2","","17","","","","21-28 May 2011","","IEEE","IEEE Conferences"
"Understanding broadcast based peer review on open source software projects","P. C. Rigby; M. A. Storey","University of Victoria, Victoria, BC, Canada","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","541","550","Software peer review has proven to be a successful technique in open source software (OSS) development. In contrast to industry, where reviews are typically assigned to specific individuals, changes are broadcast to hundreds of potentially interested stakeholders. Despite concerns that reviews may be ignored, or that discussions will deadlock because too many uninformed stakeholders are involved, we find that this approach works well in practice. In this paper, we describe an empirical study to investigate the mechanisms and behaviours that developers use to find code changes they are competent to review. We also explore how stakeholders interact with one another during the review process. We manually examine hundreds of reviews across five high profile OSS projects. Our findings provide insights into the simple, community-wide techniques that developers use to effectively manage large quantities of reviews. The themes that emerge from our study are enriched and validated by interviewing long-serving core developers.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1985867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032493","case studies;grounded theory;open source software;peer review","Communities;Electronic mail;History;Interviews;Linux;Software","peer-to-peer computing;project management;public domain software;software engineering;software management","OSS project;broadcast based peer review;long-serving core developer;open source software project","","28","1","19","","","","21-28 May 2011","","IEEE","IEEE Conferences"
"How do programmers ask and answer questions on the web?: NIER track","C. Treude; O. Barzilay; M. A. Storey","University of Victoria, Victoria, BC, Canada","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","804","807","Question and Answer (Q&A) websites, such as Stack Overflow, use social media to facilitate knowledge exchange between programmers and fill archives with millions of entries that contribute to the body of knowledge in software development. Understanding the role of Q&A websites in the documentation landscape will enable us to make recommendations on how individuals and companies can leverage this knowledge effectively. In this paper, we analyze data from Stack Overflow to categorize the kinds of questions that are asked, and to explore which questions are answered well and which ones remain unanswered. Our preliminary findings indicate that Q&A websites are particularly effective at code reviews and conceptual questions. We pose research questions and suggest future work to explore the motivations of programmers that contribute to Q&A websites, and to understand the implications of turning Q&A exchanges into technical mini-blogs through the editing of questions and answers.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1985907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032523","q&a;questions;social media;stack overflow","Documentation;Encoding;Knowledge engineering;Media;Programming;Software;USA Councils","Internet;Web sites;software engineering","NIER track;Q&A websites;question and answer;social media;software development;stack overflow","","45","1","12","","","","21-28 May 2011","","IEEE","IEEE Conferences"
"A Systematic Literature Review on Fault Prediction Performance in Software Engineering","T. Hall; S. Beecham; D. Bowes; D. Gray; S. Counsell","Brunel University, Uxbridge","IEEE Transactions on Software Engineering","20121129","2012","38","6","1276","1304","Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.","0098-5589;00985589","","10.1109/TSE.2011.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035727","Systematic literature review;software fault prediction","Analytical models;Context modeling;Data models;Fault diagnosis;Predictive models;Software testing;Systematics","Bayes methods;regression analysis;software fault tolerance;software quality","contextual information;cost reduction;fault prediction models;fault prediction performance;fault prediction study;feature selection;independent variables;logistic regression;methodological information;naive Bayes;predictive performance;reliable methodology;simple modeling techniques;software engineering;software quality;systematic literature review","","171","","","","","20111006","Nov.-Dec. 2012","","IEEE","IEEE Journals & Magazines"
"Dealing with Test Automation Debt at Microsoft","J. Hartmann","Office Shared Services, Microsoft Corp., Redmond, CA, USA","2011 IEEE 35th Annual Computer Software and Applications Conference Workshops","20111003","2011","","","136","136","At Microsoft, substantial time and resources are expended in test case development, execution and verification. Thousands of new tests are added to existing test suites without any kind of review regarding their unique contribution to test suite effectiveness or impact on test suite efficiency. This talk describes how we leverage existing code coverage data, together with reduction and prioritization techniques, to help each test team analyze its test suite and guide them in improving their suite's effectiveness and efficiency. The analysis focuses on identifying and deprecating/prioritizing groups of tests cases, given specific tactical goals for example, increasing current test suite stability and reliability, better structuring of test suite migration efforts, reducing test suite execution time and testing with limited hardware resources.","","Electronic:978-07695-4459-5; POD:978-1-4577-0980-7","10.1109/COMPSACW.2011.99","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032226","","Automation;Conferences;Hardware;Internet;Reliability;Software quality","data reduction;program testing;program verification;software reliability","Microsoft;code coverage data;hardware resources;prioritization techniques;reduction techniques;test automation debt;test case development;test case execution;test case verification;test suite effectiveness;test suite efficiency;test suite execution time;test suite migration efforts;test suite reliability;test suite stability","","0","","","","","","18-22 July 2011","","IEEE","IEEE Conferences"
"Event-Based Mutation Testing vs. State-Based Mutation Testing - An Experimental Comparison","F. Belli; M. Beyazit","Fac. of Comput. Sci., Univ. of Paderborn, Paderborn, Germany","2011 IEEE 35th Annual Computer Software and Applications Conference","20111003","2011","","","650","655","Model-based testing (MBT) focuses on relevant, mostly user-centric features of the system under consideration (SUC) and enables test case generation without requiring source code. Depending on these features and the preferences of the tester, modeling can be event-based or state-based. This paper compares both techniques using mutation testing, which is originally code-based, but has recently been extended to enable also MBT. For the comparison, the paper introduces frameworks that are composed of a set of models, a set of mutation operators, a set of coverage criteria, and a set of test generation algorithms. The introduced concepts and notions are demonstrated over a case study based on a large web-based commercial portal. Analysis of the experimental data yields results on the discussed frameworks reviewing benefits and drawbacks of event-based and state-based testing.","0730-3157;07303157","Electronic:978-0-7695-4439-7; POD:978-1-4577-0544-1","10.1109/COMPSAC.2011.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032412","event sequence graph;fault detection;finite-state machine;graphbased modeling;model-based testing;mutation testing","Analytical models;Automata;Context;Fault detection;Minimization;Redundancy;Testing","portals;program testing;source coding;systems analysis","MBT;Web portal;coverage criteria;event based mutation testing;model-based testing;mutation operators;state-based mutation testing;system under consideration;test case generation;test generation algorithms","","1","","30","","","","18-22 July 2011","","IEEE","IEEE Conferences"
"Techniques for Defending from Buffer Overflow Vulnerability Security Exploits","","","IEEE Internet Computing","","2011","Early Access","Early Access","1","1","Recent reports reveal that majority of security violations are caused by weaknesses in code. Buffer overflow vulnerability is the most severe of security violations. Though wide range of solutions from static analysis techniques to hardware modifications were proposed to tackle the vulnerability they either fail to address the large scope of the problem or have limitations preventing their use and adoption. In this article, we discuss well-known buffer overflow vulnerability exploit mechanisms followed by comprehensive study of proposals for defending from such exploits along with review of tools used for supporting the process.","1089-7801;10897801","","10.1109/MIC.2011.137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025342","B Hardware;C Computer Systems Organization;C.2 Communication/Networking and Information Technology;Computer-supported cooperative work;D Software/Software Engineering;Database Reverse Engineering;H.3.5 Online Information Services;I.4 Image Processing and Computer Vision;Internet applications;K.6.5 Security and Protection;Multimedia Information Systems;Pervasive computing;Program Verification;database technology;mobile applications;software engineering;web applications;web programming","Arrays;Buffer overflow;Databases;Distance measurement;Encoding;Internet;Security","","","","0","","","","","20110923","0","","IEEE","IEEE Early Access Articles"
"Techniques for Defending from Buffer Overflow Vulnerability Security Exploits","B. Padmanabhuni; H. B. K. Tan","Nanyang Technological University, Singapore","IEEE Internet Computing","","2011","Early Access","Early Access","1","1","Recent reports reveal that majority of security violations are caused by weaknesses in code. Buffer overflow vulnerability is the most severe of security violations. Though wide range of solutions from static analysis techniques to hardware modifications were proposed to tackle the vulnerability they either fail to address the large scope of the problem or have limitations preventing their use and adoption. In this article, we discuss well-known buffer overflow vulnerability exploit mechanisms followed by comprehensive study of proposals for defending from such exploits along with review of tools used for supporting the process.","1089-7801;10897801","","10.1109/MIC.2011.109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012544","B Hardware;C Computer Systems Organization;C.2 Communication/Networking and Information Technology;Computer-supported cooperative work;D Software/Software Engineering;Database Reverse Engineering;H.3.5 Online Information Services;I.4 Image Processing and Computer Vision;Internet applications;K.6.5 Security and Protection;Multimedia Information Systems;Pervasive computing;Program Verification;database technology;mobile applications;software engineering;web applications;web programming","Arrays;Buffer overflow;Databases;Distance measurement;Encoding;Hardware;Security","","","","0","","","","","20110908","0","","IEEE","IEEE Early Access Articles"
"Towards a theory of semantic communication","J. Bao; P. Basu; M. Dean; C. Partridge; A. Swami; W. Leland; J. A. Hendler","Tetherless World Constellation, Rensselaer Polytechnic Institute, Troy, NY 03060, USA","2011 IEEE Network Science Workshop","20110905","2011","","","110","117","This paper studies methods of quantitatively measuring semantic information in communication. We review existing work on quantifying semantic information, then investigate a model-theoretical approach for semantic data compression and reliable semantic communication. We relate our approach to the statistical measurement of information by Shannon, and show that Shannon's source and channel coding theorems have semantic counterparts.","","Electronic:978-1-4577-1051-3; POD:978-1-4577-1049-0","10.1109/NSW.2011.6004632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004632","","Channel coding;Entropy;Probability;Receivers;Semantics;Weaving","channel coding;data compression","Shannon theory;channel coding;model-theoretical approach;reliable semantic communication;semantic data compression;semantic information;statistical information measurement","","4","","22","","","","22-24 June 2011","","IEEE","IEEE Conferences"
"IBM PC Retrospective: There Was Enough Right to Make It Work","G. Goth","","Computer","20110818","2011","44","8","26","33","Mark Dean, a participant in the original IBM PC design team, looks back 30 years to review the events that led to the design of a very ""un-IBM"" computer-a small device that used hardware and software from outside vendors and made the logic and BIOS code available to anybody. This Web extra features audio from an interview with Mark Dean, a participant in the original IBM PC design team. This video is also available on YouTube: http://www.youtube.com/watch?v=cYPwVOUcCnc.","0018-9162;00189162","","10.1109/MC.2011.243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984816","History of Computing;IBM PC","Companies;Computer architecture;Design methodology;History;Microcomputers;Product design;Product development","IBM computers;input-output programs","BIOS code;IBM PC design team;IBM PC retrospective;logic code","","0","","","","","","Aug. 2011","","IEEE","IEEE Journals & Magazines"
"Trustrace: Improving Automated Trace Retrieval through Resource Trust Analysis","N. Ali","DGIGL, Ecole Polytech. de Montreal, Montreal, QC, Canada","2011 IEEE 19th International Conference on Program Comprehension","20110801","2011","","","230","233","Traceability is a task to create/recover traceability links among different software artifacts. It uses resources, such as an expert, source and target document, and traceability approach, to create/recover traceability links. However, it does not provide any guidance that how much we can trust on available resources. We propose Trustrace, a trust-based traceability recovery process, to improve expert trust on a recovered link and trust over the traceability inputs. Trustrace has three sub components, in particular, Link trust improver (LTI), traceability factor controller (TFC), and a hybrid traceability approach (HTA). LTI uses various source of information, such as temporal information, design documents, source code structure, and so on, to increase experts' trust over a link. To develop TFC, we will perform a systematic literature review and empirical studies to find out which factors impact the traceability-process inputs and document these factors in a trust pattern. TFC trust pattern will help practitioner and researchers to know which steps they can take to avoid/control these factors to improve their trust on these inputs. In the HTA, we will combine different traceability recovery approaches. All approaches have different positive and negative points, we will combine all the positive points of different approaches to increase experts' trust over the HTA. In Trustrace, HTA will implement the LTI model following TFC instructions to improve the expert trust over recovered link as well as precision and recall.","1092-8138;10928138","Electronic:978-0-7695-4398-7; POD:978-1-61284-308-7","10.1109/ICPC.2011.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970194","Traceability;hybrid approach;quality factors;trust-based model","Conferences;IEEE Computer Society;Q factor;Software;Software engineering;USA Councils;Unified modeling language","data privacy;software engineering","Trustrace process;hybrid traceability approach;link trust improver component;resource trust analysis;software artifacts;trace retrieval;traceability factor controller component;traceability links;traceability recovery process","","0","","24","","","","22-24 June 2011","","IEEE","IEEE Conferences"
"An interactive visualization environment for exploring Java programs: SHriMP views revisited","M. A. D. Storey","Univ. of Victoria, Victoria, BC, Canada","2011 IEEE 19th International Conference on Program Comprehension","20110801","2011","","","xviii","xviii","A decade ago, projects in the program comprehension community investigated how software visualization could enhance the exploration and navigation of large software systems. At IWPC in 2001, we demonstrated the SHriMP (Simple Hierarchical Multiple Perspective) visualization tool, which provided a navigable interface integrating a variety of graph-based layouts with hypertext versions of source code and documentation. At the time, there were a number of tools that shared some of SHriMP's features and researchers shared challenges when implementing these tools, as well as in understanding how they could evaluate these tools for future adoption. In this talk, I will review SHriMP's early features and demonstrate how the tool was eventually integrated with the Eclipse Integrated Development Environment. I will also discuss some of the successful and unsuccessful approaches we used to evaluate this and other visualization tools. I will conclude by discussing the many lessons we learned throughout the SHriMP research project, while highlighting some of the challenges that are still relevant in today's research.","1092-8138;10928138","Electronic:978-0-7695-4398-7; POD:978-1-61284-308-7","10.1109/ICPC.2011.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970199","","","Java;data visualisation;interactive systems","Java programs;SHriMP views;graph-based layouts;interactive visualization environment;simple hierarchical multiple perspective;software visualization","","2","","","","","","22-24 June 2011","","IEEE","IEEE Conferences"
"Detecting resource leaks through dynamical mining of resource usage patterns","H. Zhang; G. Wu; K. Chow; Z. Yu; X. Xing","School of Software, Shanghai Jiao Tong University, Shanghai, 200240, China","2011 IEEE/IFIP 41st International Conference on Dependable Systems and Networks Workshops (DSN-W)","20110721","2011","","","265","270","Resource management is crucial to software productions. Resources must be carefully acquired and released, or a resource leak might occur. For open source projects, resource leaks can be easily introduced during code check-in, and it is laborious to review, identify, report, and fix such leaks. Recently, there has been a growing interest in data mining API usage patterns to discover potential bugs such as resource leaks. However, the usage patterns mined are specific to a certain library, which cannot be applied to detect bugs in other libraries. In this paper, we present an idea called MODE, ‚ÄúMine Once, Detect Everywhere‚Äù, to address the universality of such patterns, and use them to detect potential resource leaks automatically before code check-in. We propose an efficient algorithm to record the most valuable API calls that are related to resource usage during program execution, and mine resource usage patterns from the traces with a sequence miner. To verify the effectiveness of the patterns, experiments are given to use them to detect real resource leaks in large open source projects.","2325-6648;23256648","Electronic:978-1-4577-0375-1; POD:978-1-4577-0374-4","10.1109/DSNW.2011.5958824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958824","Data Mining;Dynamical Analysis;Resource Leak Detection","Association rules;Computer bugs;Integrated circuits;Java;Libraries;Software","application program interfaces;data mining;pattern classification;program verification;resource allocation","MODE;code check-in;data mining API usage pattern;dynamical resource usage pattern mining;large open source project;open source project;potential bugs;potential resource leak detection;program execution;real resource leak detection;resource management;sequence miner;software production","","3","1","18","","","","27-30 June 2011","","IEEE","IEEE Conferences"
"Refactoring as Testability Transformation","M. Harman","Dept. of Comput. Sci., Univ. Coll. London, London, UK","2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops","20110714","2011","","","414","421","This paper briefly reviews the theory of Testability Transformation and outlines its implications for and relationship to refactoring for testing. The paper introduces testability refactorings, a subclass of Testability Transformations and discusses possible examples of testability refactorings. Several approaches to testability refactoring are also introduced. These include the novel concept of test-carrying code and the use of pareto optimization for balancing the competing needs of machine and human in search based testability refactoring.","","Electronic:978-0-7695-4345-1; POD:978-1-4577-0019-4","10.1109/ICSTW.2011.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954441","refactoring;testability;testability transformation;testing;transformation","Documentation;Humans;Measurement;Programming;Search problems;Semantics;Testing","Pareto optimisation;program testing;search problems;software maintenance","Pareto optimization;search based testability refactoring;test-carrying code;testability transformation","","8","","83","","","","21-25 March 2011","","IEEE","IEEE Conferences"
"Artificial Intelligence Design in a Multiplayer Online Role Playing Game","C. A. Ballinger; D. A. Turner; A. I. Concepcion","Sch. of Comput. Sci. & Eng., California State Univ. San Bernardino, San Bernardino, CA, USA","2011 Eighth International Conference on Information Technology: New Generations","20110711","2011","","","816","821","In this paper, we describe our experiences and lessons learned utilizing a rule-based system for implementing the AI in a multi-player online role playing game called Mythic. We explain how we organize AI rules, how those rules are assembled from a database, how sets of rules are assigned to game entities, the different sets of inference rules, the different phases of inference rules, and how we manage facts used by the inference engine. We also review some of the history behind the Mythic project, where it is headed, what a rule-based system is and why we chose to use one for our project. The result of our project is a design that allows us to have diverse AIbehavior and flexibility to reuse code to create new behaviors, but may prove to be inefficient if implemented on systems with a large number of players or many AI controlled game entities.","","Electronic:978-0-7695-4367-3; POD:978-1-61284-427-5","10.1109/ITNG.2011.142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945341","artificial intelligence;games;knowledge engineering;rule-based systems","Artificial intelligence;Databases;Engines;Games;Graphics;Organizations;Production","Internet;artificial intelligence;computer games;inference mechanisms;knowledge based systems","AI;Mythic project;artificial intelligence design;game entities;inference engine;inference rules;multiplayer online role playing game;rule based system","","0","","10","","","","11-13 April 2011","","IEEE","IEEE Conferences"
"Categorization of real-time software components for code size estimation","K. Lind; R. Heldal","Saab Automobile AB, Trollh&#228;ttan, Sweden","Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","20110708","2010","","","1","10","<p><b>Background:</b> To estimate Software Code Size early in the development process is important both for Cost/Effort estimation and electronic hardware design reasons. The COSMIC FSM (Functional Size Measurement) method treats the intended software to be measured as a black box, and measures CFP (COSMIC Function Points) based only on data movement in and out of the software. Therefore, CFP can be measured on requirements defined early, and be used to estimate Code Size if there exists a strong correlation between CFP and Code Size. We have conducted four experiments in the automotive industry showing strong correlation between CFP and implemented Code Size in Bytes. All four experiments, of which two have not been published before, show equally strong correlation but the linear relationship is different between the experiments.</p> <p><b>Goal:</b> This paper aims to identify the factors affecting the linear relationship. With these factors, we can categorize new requirements to be measured and select the proper linear relationship to convert CFP into Bytes, i.e. estimate Code Size.</p> <p><b>Method:</b> We replicate our earlier experiments with software components of new types, and review the results from all our experiments. Potential factors affecting implemented Code Size are identified by performing open-ended interviews with domain experts.</p> <p><b>Results:</b> We have in the automotive industry identified a set of factors that can be used to categorize the software components we want to measure; functionality type, quality constraints, and development methods and tools.</p> <p><b>Conclusions:</b> COSMIC can produce accurate Code Size Estimates provided that sub-sets of cohesive and uniform requirements can be identified. Moreover, similar requirements must have been measured before to establish the linear relationship between CFP and Bytes. Finally, the sub-sets of requirements need to be able to categorize based on factors that affect the linear relation- - ship. With this approach, even complex calculations can be measured, provided that they are proportional to the number of data movements.</p>","","","10.1145/1852786.1852821","","","COSMIC function points;UML components;categorization;functional size measurement;software code size;system architecture","","","","","4","2","","","","","16-17 Sept. 2010","","ACM","ACM Conferences"
"Building empirical support for automated code smell detection","J. Schumacher; N. Zazworka; F. Shull; C. Seaman; M. Shaw","University of Applied Sciences, Mannheim, Germany","Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","20110708","2010","","","1","10","<p>Identifying refactoring opportunities in software systems is an important activity in today's agile development environments. The concept of code smells has been proposed to characterize different types of design shortcomings in code. Additionally, metric-based detection algorithms claim to identify the ""smelly"" components automatically. This paper presents results for an empirical study performed in a commercial environment. The study investigates the way professional software developers detect god class code smells, then compares these results to automatic classification. The results show that, even though the subjects perceive detecting god classes as an easy task, the agreement for the classification is low. Misplaced methods are a strong driver for letting subjects identify god classes as such. Earlier proposed metric-based detection approaches performed well compared to the human classification. These results lead to the conclusion that an automated metric-based pre-selection decreases the effort spent on manual code inspections. Automatic detection accompanied by a manual review increases the overall confidence in the results of metric-based classifiers.</p>","","","10.1145/1852786.1852797","","","code inspection;code smells;empirical study;god class;maintainability","","","","","12","","","","","","16-17 Sept. 2010","","ACM","ACM Conferences"
"Functional language compiler experiences at Intel","L. Petersen; N. Glew","Intel","ACM SIGPLAN Commercial Users of Functional Programming","20110708","2010","","","1","1","<p>For five years Intel's Programming Systems Lab (PSL) has been collaborating with an external partner on a new functional programming language designed for productivity on many-core processors. While the language is not yet public, this talk outlines motivations behind the language and describes our experiences in implementing it using a variety of functional languages. The reference interpreter is written in Haskell and compiled with GHC while PSL's performance implementation is written in SML and compiled with Mlton. We have also generated Scheme code compiled with PLT Scheme as part of a prototyping effort.</p> <p>At several points, the project has had several contributors that did not have a background in functional languages working on the compiler and on writing benchmarks. We describe their experiences working in SML and with functional languages in general. Specifically:</p> <p>‚Ä¢what they liked and disliked about using functional languages</p> <p>‚Ä¢what was easy and hard about learning and using functional languages</p> <p>‚Ä¢what worked/didn't work for helping them learn to program in functional languages</p> <p>‚Ä¢which functional features they used and didn't use</p> <p>‚Ä¢general observations about the code that they wrote</p> <p>‚Ä¢what (if anything) they took away from the experience</p> <p>A design principle of the implementation effort was to by default avoid the use of imperative features. Previous experience and review of the literature suggested that many parts of a compiler could be written as well or better using primarily functional code, but that restricting ourselves entirely to the functional fragment of SML was probably not reasonable. We describe our experiences with this, and the tradeoffs that we encountered. Specifically:</p> <p>‚Ä¢During the project we experimented with both immutable and mutable intermediate representations (IRs). We describe and motivate some of the scenarios where we used - - one or the other, explain our experiences with this, and describe cases where we feel that we made an inappropriate initial choice.</p> <p>‚Ä¢We chose to avoid all global mutable state in the compiler. Most notably, symbol tables and global configuration information are always passed explicitly as parameters to parts of the compiler that require them. This choice had benefits and costs, and we discuss our experience with this.</p> <p>We also describe some of the features of SML that we found useful, and discuss some of the lack of features and quirks of SML that annoyed us. Specifically:</p> <p>‚Ä¢We discuss places where we encountered limitations in the module system. These include the lack of uniformity between the signature and structure language, and the lack of recursive modules.</p> <p>‚Ä¢We describe some of the ways in which we believe that better syntactic support could have lessened the programming burden, especially with respect to attempting to be purely functional. Examples of this include a type class mechanism and/or monadic syntax to lessen the burden of passing around explicit state, better syntactic support for using the imperative features of the language, and better support for custom control operators.</p>","","","10.1145/1900160.1900175","","","","","","","","0","","","","","","1-2 Oct. 2010","","ACM","ACM Conferences"
"Feature oriented-programming: back to the future","C. Prehofer","Fraunhofer ESK, Munich, Germany","Proceedings of the 2nd International Workshop on Feature-Oriented Software Development","20110708","2010","","","1","1","<p>Feature-oriented programming has its origin in the old quest of computer science: modular composition of software ""artifacts"" in software development. While there exist many notions of modularity and composition, feature-oriented programming tackles the case of highly-entangled software features where typical module or component concepts fail. The reasons for this are feature interactions and dependencies which make it very hard to write modular software which can be composed in a flexible way. In contrast to other efforts like aspect-oriented programming, feature-oriented programming has from the beginning focused on the semantics or behavior of software, not just on code modularity.</p> <p>In this presentation, we review the origins of feature composition and feature interactions, which first occurred as an explicit research problem in telecommunication software. We argue that feature interactions are pervasive in many areas of software development and are a common source of modularity and quality problems. Then, we present more precise formalization of modularity and compositionality of features. We discuss when adding features is modular or ""harmless"" from a semantic or behavioral point of view. In particular, we focus on modularity and interaction for multiple feature compositions. A future challenge for feature-oriented software is the graphical modeling of features and feature interactions. As we aim to have modular composition for graphical models and to generate code from these graphical artifacts, it is important to have precise semantics and clear refinement concepts. For this, we discuss how features and interactions can be represented by state-based specifications in a graphical way. Refinement concepts are presented which allow the flexible composition of features and interactions in statechart diagrams.</p>","","","10.1145/1868688.1868689","","","composition;feature interactions;feature-oriented programming;keynote;modularity","","","","","0","","","","","","10-10 Oct. 2010","","ACM","ACM Conferences"
"Eating our own dog food: DSLs for generative and transformational engineering","J. R. Cordy","Queen's University, Kingston, ON, Canada","ACM SIGPLAN Notices","20110708","2010","45","2","3","4","<p>Languages and systems to support generative and transformational solutions have been around a long time. Systems such as XVCL, DMS, ASF+SDF, Stratego and TXL have proven mature, efficient and effective in a wide range of applications. Even so, adoption remains a serious issue - almost all successful production applications of these systems in practice either involve help from the original authors or years of experience to get rolling. While work on accessibility is active, with efforts such as ETXL, Stratego XT, Rascal and Colm, the fundamental big step remains - it's not obvious how to apply a general purpose transformational system to any given generation or transformation problem, and the real power is in the paradigms of use, not the languages themselves.</p> <p>In this talk I will propose an agenda for addressing this problem by taking our own advice - designing and implementing domain specific languages (DSLs) for specific generative, transformational and analysis problem domains. We widely advise end users of the need for DSLs for their kinds of problems - why not for our kinds? And we use our tools for implementing their DSLs - why not our own? I will outline a general method for using transformational techniques to implement transformational and generative DSLs, and review applications of the method to implementing example text-based DSLs for model-based code generation and static code analysis. Finally, I will outline some first steps in implementing model transformation DSLs using the same idea - retaining the maturity and efficiency of our existing tools while bringing them to the masses by ""eating our own dogfood"".</p>","0362-1340;03621340","","10.1145/1837852.1621609","","","domain-specific languages;generative programming;model driven engineering;source transformation systems","","","","","0","","","","","","February 2010","","ACM","ACM Journals & Magazines"
"Safe composition of non-monotonic features","M. Kuhlemann; D. Batory; C. K√§stner","University of Magdeburg, Magdeburg, Germany","ACM SIGPLAN Notices","20110708","2010","45","2","177","186","<p>Programs can be composed from features. We want to verify automatically that all legal combinations of features can be composed safely without errors. Prior work on this problem assumed that features add code monotonically. We generalize prior work to enable features to add <i>and remove</i> code, describe our analyses and implementation, and review case studies. We observe that more expressive features increase the complexity of developed programs rapidly -- up to the point where tools and automated concepts as presented in this paper are indispensable for verification.</p>","0362-1340;03621340","","10.1145/1837852.1621634","","","AHEAD;feature-oriented programming;refactoring;safe composition","","","","","0","","","","","","February 2010","","ACM","ACM Journals & Magazines"
"Fine-grained privilege separation for web applications","A. Krishnamurthy; A. Mettler; D. Wagner","University of California, Berkeley, Berkeley, CA, USA","Proceedings of the 19th international conference on World wide web","20110708","2010","","","551","560","<p>We present a programming model for building web applications with security properties that can be confidently verified during a security review. In our model, applications are divided into isolated, privilege-separated components, enabling rich security policies to be enforced in a way that can be checked by reviewers. In our model, the web framework enforces privilege separation and isolation of web applications by requiring the use of an object-capability language and providing interfaces that expose limited, explicitly-specified privileges to application components. This approach restricts what each component of the application can do and quarantines buggy or compromised code. It also provides a way to more safely integrate third-party, less-trusted code into a web application. We have implemented a prototype of this model based upon the Java Servlet framework and used it to build a webmail application. Our experience with this example suggests that the approach is viable and helpful at establishing reviewable application-specific security properties.</p>","","","10.1145/1772690.1772747","","","object-capabilities;privilege separation;web applications","","","","","1","","","","","","26-30 April 2010","","ACM","ACM Conferences"
"Peer review in CS2: conceptual learning","S. Turner; M. A. P√©rez-Qui√±ones; S. Edwards; J. Chase","ECPI College of Technology, Hampton, VA, USA","Proceedings of the 41st ACM technical symposium on Computer science education","20110708","2010","","","331","335","<p>In computer science, students could benefit from exposure to critical programming concepts from multiple perspectives. Peer review is one method to allow students to experience authentic uses of the concepts in a non-programming manner. In this work, we examine the use of the peer review process in early, object-oriented, computer science courses as a way to develop the reviewers' knowledge of object-oriented programming concepts, specifically Abstraction, Decomposition, and Encapsulation.</p> <p>To study these ideas, we used peer review exercises in two CS2 classes at local universities over the course of a semester. Using three groups (one reviewing their peers, one reviewing the instructor, and one completing small design or coding exercises), we measured the students' conceptual understanding throughout the semester with concept maps and the reviews they completed. We found that reviewing helped students learn Decomposition, especially those reviewing the instructor's programs. Overall, peer reviews are a valuable method for teaching Decomposition to CS2 students and can be used as an alternative way to learn object-oriented programming concepts.</p>","","","10.1145/1734263.1734379","","","cs education;learning;object-oriented concepts;peer review","","","","","3","","","","","","10-13 March 2010","","ACM","ACM Conferences"
"Effect of object oriented refactorings on testability, error proneness and other maintainability attributes","G. Heged≈±s; G. Hrabovszki; D. Heged≈±s; I. Siket","University of Szeged, Szeged, Hungary","Proceedings of the 1st Workshop on Testing Object-Oriented Systems","20110708","2010","","","1","7","<p>Refactoring (object-oriented) code aims to improve some of the quality attributes of the software system under maintenance. However, as any other changes to a system, refactoring actions may have side-effects too, which need to be taken carefully into account during their implementation. Consequences of refactoring are only moderately discussed in literature. In this work, we emphasize the importance of documenting such consequences, by reviewing relevant literature and giving our views on the topic. We do this in three steps: first, we investigate how high level quality attributes (including testability and other maintainability aspects based on the ISO 9126 standard and fault proneness) can be estimated based on measurable metrics in the code, like complexity, size, and coupling. In the second step, we examine what effect of individual refactoring techniques can have on such metrics. Finally, we combine these findings to get a view on how refactoring can influence high level quality characteristics. With this work, we want to foster discussion in this important topic, rather than giving a solution to the problem, as it requires a vast amount of further research by the testing and software quality communities.</p>","","","10.1145/1890692.1890700","","","ISO 9126 quality model;code metrics;error proneness;refactoring;static software measurement;testability","","","","","0","","","","","","21-21 June 2010","","ACM","ACM Conferences"
"The stochastic root-finding problem: Overview, solutions, and open questions","R. Pasupathy; S. Kim","Virginia Tech","ACM Transactions on Modeling and Computer Simulation (TOMACS)","20110708","2011","21","3","1","23","<p>The stochastic root-finding problem (SRFP) is that of finding the zero(s) of a vector function, that is, solving a nonlinear system of equations when the function is expressed implicitly through a stochastic simulation. SRFPs are equivalently expressed as stochastic fixed-point problems, where the underlying function is expressed implicitly via a noisy simulation. After motivating SRFPs using a few examples, we review available methods to solve such problems on constrained Euclidean spaces. We present the current literature as three broad categories, and detail the basic theoretical results that are currently known in each of the categories. With a view towards helping the practitioner, we discuss specific variations in their implementable form, and provide references to computer code when easily available. Finally, we list a few questions that are worthwhile research pursuits from the standpoint of advancing our knowledge of the theoretical underpinnings and the implementation aspects of solutions to SRFPs.</p>","1049-3301;10493301","","10.1145/1921598.1921603","","","Stochastic root finding;sample-average approximation;stochastic approximation","","","","","7","","","","","","March 2011","","ACM","ACM Journals & Magazines"
"SE-155 DBSA: a device-based software architecture for data mining","J. K√§tev√§; P. Laurinen; T. Rautio; J. Suutala; L. Tuovinen; J. R√∂ning","University of Oulu, Finland","Proceedings of the 2010 ACM Symposium on Applied Computing","20110708","2010","","","2273","2280","<p>In this paper a new architecture for a variety of data mining tasks is introduced. The Device-Based Software Architecture (DBSA) is a highly portable and generic data mining software framework where processing tasks are modeled as components linked together to form a data mining application. The name of the architecture comes from the analogy that each processing task in the framework can be thought of as a device. The framework handles all the devices in the same manner, regardless of whether they have a counterpart in the real world or whether they are just logical devices inside the framework. The DBSA offers many reusable devices, ready to be included in applications, and the application programmer can easily code new devices for the architecture. The framework is bundled with connections to several widely used external tools and languages, making prototyping new applications easy and fast. In the paper we compare DBSA to existing data mining frameworks, review its design and present a case study application implemented with the framework. The paper shows that the DBSA can act as a base for diverse data mining applications.</p>","","","10.1145/1774088.1774562","","","data mining;software frameworks","","","","","1","","","","","","22-26 March 2010","","ACM","ACM Conferences"
"Towards security testing with taint analysis and genetic algorithms","A. Avancini; M. Ceccato","Fondazione Bruno Kessler--IRST, Trento, Italy","Proceedings of the 2010 ICSE Workshop on Software Engineering for Secure Systems","20110708","2010","","","65","71","<p>Cross site scripting is considered the major threat to the security of web applications. Removing vulnerabilities from existing web applications is a manual expensive task that would benefit from some level of automatic assistance. Static analysis represents a valuable support for security review, by suggesting candidate vulnerable points to be checked manually. However, potential benefits are quite limited when too many false positives, safe portions of code classified as vulnerable, are reported.</p> <p>In this paper, we present a preliminary investigation on the integration of static analysis with genetic algorithms. We show that this approach can suggest candidate false positives reported by static analysis and provide input vectors that expose actual vulnerabilities, to be used as test cases in security testing.</p>","","","10.1145/1809100.1809110","","","cross site scripting;genetic algorithms;security testing;taint analysis","","","","","5","","","","","","2-2 May 2010","","ACM","ACM Conferences"
"Jype - a program visualization and programming exercise tool for Python","J. Helminen; L. Malmi","Aalto University School of Science and Technology, Espoo, Finland","Proceedings of the 5th international symposium on Software visualization","20110708","2010","","","153","162","<p>Based on research into learning programming and a review of program visualization research, we designed an educational software tool that aims to target students' apparent fragile knowledge of elementary programming which manifests as difficulties in tracing and writing even simple programs. Most existing tools build on a single supporting technology and focus on one aspect of learning. For example, visualization tools support the development of a conceptual-level understanding of how programs work, and automatic assessment tools give feedback on submitted tasks. We implemented a combined tool that closely integrates programming tasks with visualizations of program execution and thus lets students practice writing code and more easily transition to visually tracing it in order to locate programming errors. In this paper we present Jype, a web-based tool that provides an environment for visualizing the line-by-line execution of Python programs and for solving programming exercises with support for immediate automatic feedback and an integrated visual debugger. Moreover, the debugger allows stepping back in the visualization of the execution as if executing in reverse. Jype is built for Python, when most research in programming education support tools revolves around Java.</p>","","","10.1145/1879211.1879234","","","Jype;Python;automatic assessment;computer science education;program visualization;visual debugging","","","","","1","","","","","","25-26 Oct. 2010","","ACM","ACM Conferences"
"SequenceL: transparency and multi-core parallelisms","B. Nemanich; D. Cooke; J. N. Rushtom","Texas Tech University, Lubbock, TX, USA","Proceedings of the 5th ACM SIGPLAN workshop on Declarative aspects of multicore programming","20110708","2010","","","45","52","<p>In this paper, we review the computational laws upon which SequenceL is based, give evidence of the transparency of SequenceL code, and compare performance results of Parallel Haskell and SequenceL codes running on multi-core processors. For the comparisons with Haskell we employ a recently developed SequenceL compiler that generates multi-threaded C++ and has a runtime system that manages threads for multi-core processors.</p>","","","10.1145/1708046.1708057","","","computational laws;multi-core;sequencel;transparency","","","","","0","","","","","","19-19 Jan. 2010","","ACM","ACM Conferences"
"Towards revealing JavaScript program intents using abstract interpretation","G. Blanc; Y. Kadobayashi","Nara Institute of Science and Technology, Ikoma, Nara, Japan","Proceedings of the Sixth Asian Internet Engineering Conference","20110708","2010","","","87","94","<p>Everyday, millions of Internet users access AJAX-powered web applications. However, such richness is prone to security issues. In particular, Web 2.0 attacks are difficult to detect and block since it is similar to legitimate traffic. As a ground for our research, we review past related works and explain what might be missing to tackle Web 2.0 security issues. Especially, we show that tackling AJAX-based attacks often lacks a context that can only be conveyed during real-time analysis.</p> <p>In our research, we advocate the usage of abstract interpretation of JavaScript code to provide maximum coverage and to ensure completeness. Besides, we introduce a proxy-based proposal to provide analysis of JavaScript malware.</p>","","","10.1145/1930286.1930298","","","JavaScript malware;Web 2.0;abstract interpretation;client-side","","","","","1","","","","","","15-17 Nov. 2010","","ACM","ACM Conferences"
"x86-TSO: a rigorous and usable programmer's model for x86 multiprocessors","P. Sewell; S. Sarkar; S. Owens; F. Z. Nardelli; M. O. Myreen","University of Cambridge","Communications of the ACM","20110708","2010","53","7","89","97","<p>Exploiting the multiprocessors that have recently become ubiquitous requires high-performance and reliable concurrent systems code, for concurrent data structures, operating system kernels, synchronization libraries, compilers, and so on. However, concurrent programming, which is always challenging, is made much more so by two problems. First, real multiprocessors typically do not provide the sequentially consistent memory that is assumed by most work on semantics and verification. Instead, they have relaxed memory models, varying in subtle ways between processor families, in which different hardware threads may have only loosely consistent views of a shared memory. Second, the public vendor architectures, supposedly specifying what programmers can rely on, are often in ambiguous informal prose (a particularly poor medium for loose specifications), leading to widespread confusion.</p> <p>In this paper we focus on x86 processors. We review several recent Intel and AMD specifications, showing that all contain serious ambiguities, some are arguably too weak to program above, and some are simply unsound with respect to actual hardware. We present a new <i>x86-TSO</i> programmer's model that, to the best of our knowledge, suffers from none of these problems. It is mathematically precise (rigorously defined in HOL4) but can be presented as an intuitive abstract machine which should be widely accessible to working programmers. We illustrate how this can be used to reason about the correctness of a Linux spinlock implementation and describe a general theory of data-race freedom for x86-TSO. This should put x86 multiprocessor system building on a more solid foundation; it should also provide a basis for future work on verification of such systems.</p>","0001-0782;00010782","","10.1145/1785414.1785443","","","","","","","","28","","","","","","July 2010","","ACM","ACM Journals & Magazines"
"Open source legality patterns: architectural design decisions motivated by legal concerns","I. Hammouda; T. Mikkonen; V. Oksanen; A. Jaaksi","Tampere University of Technology, Tampere, Finland","Proceedings of the 14th International Academic MindTrek Conference","20110708","2010","","","207","214","<p>Complications emerge when various open source software components, governed by different licenses, are used in the same software system. For various reasons, these licenses introduce different privileges and requirements on the use and distribution of composed code, and are therefore often fundamentally incompatible with each other when combined arbitrarily. Consequently the way the different components can be integrated requires attention at the level of software architecture. In this paper, we introduce open source legality patterns -- architectural design decisions motivated by legal concerns associated with open source licensing issues and licenses themselves. Towards the end of the paper, we also review some related work and discuss why it is important to create common guidelines for designs that mix and match different open source systems and proprietary software, and provide directions for future work.</p>","","","10.1145/1930488.1930533","","","design patterns;legal concerns;open source licensing","","","","","2","","","","","","6-8 Oct. 2010","","ACM","ACM Conferences"
"An analysis of object oriented variability implementation mechanisms","Fazal-e-Amin; A. K. Mahmood; A. Oxley","Universiti Teknologi PETRONAS, Perak, Malaysia","ACM SIGSOFT Software Engineering Notes","20110708","2011","36","1","1","4","<p>Software variability is the capacity of software to satisfy variant requirements. Component based software engineering and reuseintense software development, such as software product line engineering, demand software components with high variability. Increased variability influences a component's utility as it can be reused in multiple applications. In this paper a review and analysis of variability implementation mechanisms is presented. It builds on earlier work on software variability by providing an analysis and a synthesis. The purpose of this work is to classify the available variability mechanisms in terms of type, scope, and the artefact to be targeted. Examples to illustrate the points under discussion are given in the form of Java code.</p>","0163-5948;01635948","","10.1145/1921532.1921538","","","reuse;software variability;variability implantation","","","","","0","","","","","","2011","","ACM","ACM Journals & Magazines"
"The Google FindBugs fixit","N. Ayewah; W. Pugh","University of Maryland, College Park, MD, USA","Proceedings of the 19th international symposium on Software testing and analysis","20110708","2010","","","241","252","<p>In May 2009, Google conducted a company wide FindBugs ""fixit"". Hundreds of engineers reviewed thousands of FindBugs warnings, and fixed or filed reports against many of them. In this paper, we discuss the lessons learned from this exercise, and analyze the resulting dataset, which contains data about how warnings in each bug pattern were classified. Significantly, we observed that even though most issues were flagged for fixing, few appeared to be causing any serious problems in production. This suggests that most interesting software quality problems were eventually found and fixed without FindBugs, but FindBugs could have found these problems early, when they are cheap to remediate. We compared this observation to bug trends observed in code snapshots from student projects.</p> <p>The full dataset from the Google fixit, with confidential details encrypted, will be published along with this paper.</p>","","","10.1145/1831708.1831738","","","bug patterns;bugs;false positives;findbugs;java;software defects;software quality;static analysis","","","","","21","","","","","","12-16 July 2010","","ACM","ACM Conferences"
"From scratch to system: a hands-on introductory embedded systems course","S. M. Loo; J. Kiepert; M. Pook; J. Hall; D. Klein; V. Patel; C. Lee; A. Planting","Boise State University, Boise, Idaho","Proceedings of the 2010 Workshop on Embedded Systems Education","20110708","2010","","","1","5","<p>This paper describes a hands-on introductory embedded systems course, which continues from the first microprocessor course. Instead of using an off-the-shelf microcontroller development board, it shows how students can build one from scratch and add components when required for assignments or as the need arises. The course begins with wiring a microcontroller system from scratch, continues through interfacing to various sensors, and culminates in a final project. The course also focuses on embedded systems code layering concepts and enforces their usage. Lectures on practical analog interfacing circuits, such as op-amp circuitry, were presented. There were two written tests and seven hands-on laboratory assignments. The course reviews indicated students like this approach tremendously.</p>","","","10.1145/1930277.1930279","","","curriculum;microcontroller","","","","","0","","","","","","28-28 Oct. 2010","","ACM","ACM Conferences"
"Reconfigurable asynchronous logic automata: (RALA)","N. Gershenfeld; D. Dalrymple; K. Chen; A. Knaian; F. Green; E. D. Demaine; S. Greenwald; P. Schmidt-Nielsen","MIT Center for Bits and Atoms, Cambridge, MA, USA","ACM SIGPLAN Notices","20110708","2010","45","1","1","6","<p>Computer science has served to insulate programs and programmers from knowledge of the underlying mechanisms used to manipulate information, however this fiction is increasingly hard to maintain as computing devices decrease in size and systems increase in complexity. Manifestations of these limits appearing in computers include scaling issues in interconnect, dissipation, and coding. Reconfigurable Asynchronous Logic Automata (RALA) is an alternative formulation of computation that seeks to align logical and physical descriptions by exposing rather than hiding this underlying reality. Instead of physical units being represented in computer programs only as abstract symbols, RALA is based on a lattice of cells that asynchronously pass state tokens corresponding to physical resources. We introduce the design of RALA, review its relationships to its many progenitors, and discuss its benefits, implementation, programming, and extensions</p>","0362-1340;03621340","","10.1145/1707801.1706301","","","asynchronous;automata;logic;reconfigurable","","","","","1","","","","","","January 2010","","ACM","ACM Journals & Magazines"
"Reconfigurable asynchronous logic automata: (RALA)","N. Gershenfeld; D. Dalrymple; K. Chen; A. Knaian; F. Green; E. D. Demaine; S. Greenwald; P. Schmidt-Nielsen","MIT Center for Bits and Atoms, Cambridge, MA, USA","Proceedings of the 37th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages","20110708","2010","","","1","6","<p>Computer science has served to insulate programs and programmers from knowledge of the underlying mechanisms used to manipulate information, however this fiction is increasingly hard to maintain as computing devices decrease in size and systems increase in complexity. Manifestations of these limits appearing in computers include scaling issues in interconnect, dissipation, and coding. Reconfigurable Asynchronous Logic Automata (RALA) is an alternative formulation of computation that seeks to align logical and physical descriptions by exposing rather than hiding this underlying reality. Instead of physical units being represented in computer programs only as abstract symbols, RALA is based on a lattice of cells that asynchronously pass state tokens corresponding to physical resources. We introduce the design of RALA, review its relationships to its many progenitors, and discuss its benefits, implementation, programming, and extensions</p>","","","10.1145/1706299.1706301","","","asynchronous;automata;logic;reconfigurable","","","","","2","","","","","","17-23 Jan. 2010","","ACM","ACM Conferences"
"Class properties for security review in an object-capability subset of Java: (short paper)","A. Mettler; D. Wagner","University of California, Berkeley","Proceedings of the 5th ACM SIGPLAN Workshop on Programming Languages and Analysis for Security","20110708","2010","","","1","7","<p>Joe-E is a subset of the Java language, with additional restrictions enforced by a static source-code verifier. We explore several semantic properties of classes relating to immutability and object identity that can be declared by the programmer and are checked by the Joe-E verifier. We present the simple, modular analyses we use to verify these properties and describe how they are useful in performing security reviews of applications.</p>","","","10.1145/1814217.1814224","","","","","","","","1","","","","","","10-10 June 2010","","ACM","ACM Conferences"
"A subspace coding approach to MIMO compound broadcast channel","R. Yu; J. Chen; L. Song; Y. Zhang; J. Cai","South China University of Technology, China","Proceedings of the 6th International Wireless Communications and Mobile Computing Conference","20110708","2010","","","306","310","<p>In this paper, we review some results on the multiplexing gain of of the sum rate of the Gaussian multi-antenna compound broadcast channel in the high SNR regime. The transmitter transmits to each user one private message. The channel realization for each user is arbitrarily chosen from a finite set known to the transmitter. To achieve the multiplexing gain region of the channel, we discuss the methods of interference alignment and subspace coding. We interpret the interference alignment scheme for compound MIMO broadcast channel as a special case of subspace coding. We also point out that the subspace coding method could be applied in wireless multihop network to improve the efficiency of intra-cluster broadcasting.</p>","","","10.1145/1815396.1815467","","","MIMO broadcast channel;interference alignment;multiplexing;subspace coding","","","","","0","","","","","","June 28 2010-July 2 2010","","ACM","ACM Conferences"
"Software fault prediction tool","T. J. Ostrand; E. J. Weyuker","AT&#38;T Labs - Research, Florham Park, NJ, USA","Proceedings of the 19th international symposium on Software testing and analysis","20110708","2010","","","275","278","<p>We have developed an interactive tool that predicts fault likelihood for the individual files of successive releases of large, long-lived, multi-developer software systems. Predictions are the result of a two-stage process: first, the extraction of current and historical properties of the system, and second, application of a negative binomial regression model to the extracted data. The prediction model is presented to the user as a GUI-based tool that requires minimal input from the user, and delivers its output as an ordered list of the system's files together with an expected percent of faults each file will have in the release about to undergo system test. The predictions can be used to prioritize testing efforts, to plan code or design reviews, to allocate human and computer resources, and to decide if files should be rewritten.</p>","","","10.1145/1831708.1831743","","","defect;fault;gui tool;negative binomial;prediction","","","","","2","","","","","","12-16 July 2010","","ACM","ACM Conferences"
"3DTV and 3d video communications","A. M. Tekalp","Ko&#231; University, Stambul, Turkey","Proceedings of the 13th ACM international conference on Modeling, analysis, and simulation of wireless and mobile systems","20110708","2010","","","3","4","<p>With wider availability of low cost multi-view cameras, 3D displays, and broadband communication options, 3D media is destined to move from the movie theater to home and mobile platforms. In the near term, popular 3D media will most likely be in the form of stereoscopic video with associated spatial audio. Recent trials indicate that consumers are willing to watch stereoscopic 3D media on their TVs, laptops, and mobile phones. While it is possible to broadcast 3D stereoscopic media (two-views) over digital TV platforms today, streaming over IP will provide a more flexible approach for distribution of 3D media to users with different connection bandwidths and different 3D displays. In the intermediate term, free-view 3D video and 3DTV with multi-view capture are next steps in the evolution of 3D media technology. Recent free-view 3D auto-stereoscopic displays can display multi-view video, ranging from 5 to 200 views. Transmission of multi-view 3D media, via broadcast or on-demand, to end users with varying 3D display terminals and bandwidths is one of the biggest challenges to realize the vision of bringing 3D media experience to the home and mobile devices. This requires flexible rate-scalable, resolution-scalable, view-scalable, view-selective, and packet-loss resilient transport methods. In this talk, first I will briefly review the state of the art in 3D video formats, coding methods, IP streaming protocols and streaming architectures. We will then take a look at 3D video transport options. There are two main platforms for 3D broadcasting: standard digital television (DTV) platforms and the IP platform. I will summarize the approach of European project DIOMEDES which is developing novel methods for adaptive streaming of multi-view video over a combination of DVB and IP platforms. I will also summarize additional challenges associated with real-time interactive 3D video communications for applications such as 3D telepresence. Finally, open research challenges f- - or the long term vision of haptic video and holographic 3D video will be presented.</p>","","","10.1145/1868521.1868523","","","3dtv;media streaming;video communication","","","","","0","","","","","","17-21 Oct. 2010","","ACM","ACM Conferences"
"Evaluation of work product defects during corrective & enhancive software evolution: a field study comparison","D. P. Hale; J. E. Hale; R. K. Smith","University of Alabama, Tuscaloosa, AL, USA","ACM SIGMIS Database","20110708","2011","42","1","59","73","<p>Information systems portfolio management assumes that software will evolve to maintain alignment with operational needs, a goal that must be met through effective ongoing maintenance. Thus, a primary goal of software maintainers is to ensure that production code is updated without the introduction of defects. However, there is a dearth of research that examines the work product defects that occur as these applications evolve.</p> <p>The goal of this study is to characterize software evolution lifecycle work product defects and factors that may increase or reduce their occurrence. The study takes place within a global consulting organization conducting ongoing software maintenance for a Fortune 100 telecommunications firm by a project team assessed at Capability Maturity Model Integration (CMMI) Level 3. This study reports on 991 work product reviews conducted across the evolution activities of the ISO/IEC 12207 Software Development Life Cycle Processes.</p> <p>After controlling for team and expertise differences, the study's major finding is that corrective evolution projects inject a greater number of work product defects than enhancive evolution projects. This result does not arise from the schedule compression often associated with corrective evolution. Rather, it is concluded that the increase in work product defects is associated with the increased complexity of analysis-stage problem diagnosis found in corrective evolution projects. The analysis is augmented by additional covariates including the number of work product reviewers, preparation time of reviewers, and size of the project.</p>","0095-0033;00950033","","10.1145/1952712.1952716","","","development;documentation;inspections;maintenance;management;measurement;problem diagnosis;reliability;reviews;software engineering;verification","","","","","1","","","","","","February 2011","","ACM","ACM Journals & Magazines"
"Observations from semi-automatic testing of program codes in the high school student maturity exam","B. Skupas; V. Dagiene","Institute of Mathematics and Informatics, Vilnius LT, Lithuania","Proceedings of the 10th Koli Calling International Conference on Computing Education Research","20110708","2010","","","31","36","<p>The optional graduation (maturity) exam in programming, considered as an outcome of the high school curriculum in Lithuania, could play a significant role in involving students for further studies in computer science or related disciplines. An important part of the exam is the evaluation of the students' program codes. An automatic testing environment for evaluation of exam solutions has been developed. The environment was adjusted to be applied also for manual evaluation of program constructs and programming style. In this paper we present a study of programs designed by the students during the maturity exam in the spring of 2010. We reviewed the assessments of students' programs obtained during both automatic testing and manual evaluation, and for detailed analysis selected the programs which failed automatic testing but got high scores during manual evaluation. We performed a qualitative analysis of 92 Pascal programs written by 77 students and estimated that the automated testing is not enough to evaluate the skills and knowledge of the students.</p>","","","10.1145/1930464.1930468","","","automatic testing;criteria for evaluation codes;high school;introductory programming;maturity exam;semi-automatic evaluation","","","","","0","","","","","","28-31 Oct. 2010","","ACM","ACM Conferences"
"DNA sequence representation methods","G. Santhosh Kumar; S. H. Shiji","Cochin University of Science & Technology, Cochin, Kerala, India","Proceedings of the International Symposium on Biocomputing","20110708","2010","","","1","4","<p>DNA sequence representation methods are used to denote a gene structure effectively and help in similarities/dissimilarities analysis of coding sequences. Many different kinds of representations have been proposed in the literature. They can be broadly classified into Numerical, Graphical, Geometrical and Hybrid representation methods. DNA structure and function analysis are made easy with graphical and geometrical representation methods since it gives visual representation of a DNA structure. In numerical method, numerical values are assigned to a sequence and digital signal processing methods are used to analyze the sequence. Hybrid approaches are also reported in the literature to analyze DNA sequences. This paper reviews the latest developments in DNA Sequence representation methods. We also present a taxonomy of various methods. A comparison of these methods where ever possible is also done.</p>","","","10.1145/1722024.1722073","","","DNA sequence analysis;DNA sequence representation;fractal method;geometric representation;graphical representation;numeric representation","","","","","0","","","","","","15-17 Feb. 2010","","ACM","ACM Conferences"
"Clone detection: Why, what and how?","M. Akhin; V. Itsykson","Saint-Petersburg State, Polytechnical University, Russia","2010 6th Central and Eastern European Software Engineering Conference (CEE-SECR)","20110531","2010","","","36","42","Excessive code duplication is a bane of modern software development. Several experimental studies show that on average 15 percent of a software system can contain source code clones - repeatedly reused fragments of similar code. While code duplication may increase the speed of initial software development, it undoubtedly leads to problems during software maintenance and support. That is why many developers agree that software clones should be detected and dealt with at every stage of software development life cycle. This paper is a brief survey of current state-of-the-art in clone detection. First, we highlight main sources of code cloning such as copy-and-paste programming, mental code patterns and performance optimizations. We discuss reasons behind the use of these techniques from the developer's point of view and possible alternatives to them. Second, we outline major negative effects that clones have on software development. The most serious drawback duplicated code have on software maintenance is increasing the cost of modifications - any modification that changes cloned code must be propagated to every clone instance in the program. Software clones may also create new software bugs when a programmer makes some mistakes during code copying and modification. Increase of source code size due to duplication leads to additional difficulty of code comprehension. Third, we review existing clone detection techniques. Classification based on used source code representation model is given in this work. We also describe and analyze some concrete examples of clone detection techniques highlighting main distinctive features and problems that are present in practical clone detection. Finally, we point out some open problems in the area of clone detection. Currently questions like ""What is a code clone?"", ""Can we predict the impact clones have on software quality"" and ""How can we increase both clone detection precision and recall at the same time? "" stay open to further re- - search. We list the most important questions in modern clone detection and explain why they continue to remain unanswered despite all the progress in clone detection research.","","Electronic:978-1-4577-0606-6; POD:978-1-4577-0605-9","10.1109/CEE-SECR.2010.5783148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783148","Clone detection;overview;program analysis;quality assurance;software maintenance","Cloning;Data mining;Electronic mail;Linux;Programming;Software maintenance","program debugging;software maintenance","clone detection techniques;code comprehension;code duplication;copy-and-paste programming;mental code patterns;performance optimizations;software bugs;software development life cycle;software maintenance;software support;source code size","","4","","21","","","","13-15 Oct. 2010","","IEEE","IEEE Conferences"
"Factors Limiting Industrial Adoption of Test Driven Development: A Systematic Review","A. Causevic; D. Sundmark; S. Punnekkat","Sch. of Innovation, Design & Eng., Malardalen Univ., Vasteras, Sweden","2011 Fourth IEEE International Conference on Software Testing, Verification and Validation","20110519","2011","","","337","346","Test driven development (TDD) is one of the basic practices of agile software development and both academia and practitioners claim that TDD, to a certain extent, improves the quality of the code produced by developers. However, recent results suggest that this practice is not followed to the extent preferred by industry. In order to pinpoint specific obstacles limiting its industrial adoption we have conducted a systematic literature review on empirical studies explicitly focusing on TDD as well as indirectly addressing TDD. Our review has identified seven limiting factors viz., increased development time, insufficient TDD experience/knowledge, lack of upfront design, domain and tool specific issues, lack of developer skill in writing test cases, insufficient adherence to TDD protocol, and legacy code. The results of this study is of special importance to the testing community, since it outlines the direction for further detailed scientific investigations as well as highlights the requirement of guidelines to overcome these limiting factors for successful industrial adoption of TDD.","2159-4848;21594848","Electronic:978-0-7695-4342-0; POD:978-1-61284-174-8","10.1109/ICST.2011.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770623","Test driven developmen;agile software development;empirical studies;systematic review;unit testing","Data mining;Databases;Limiting;Programming;Protocols;Systematics;Testing","program testing;software maintenance;software prototyping","agile software development;industrial adoption;legacy code;test driven development","","13","","58","","","","21-25 March 2011","","IEEE","IEEE Conferences"
"A Power Adapting CODE Algorithm Applying in the Wireless Sensor Networks","M. Li; W. Li","Sch. of Commun. Eng., Xi'an Univ. of Sci. & Technol., Xi'an, China","2010 International Conference on Intelligent System Design and Engineering Application","20110407","2010","1","","137","141","In order to sustain a longer lifetime, an important research in the Wireless Sensor Network (WSN) is to improve power efficiency. Review of many technologies in the involvement, the essential idea is to remove power redundancy. The GAF algorithm is able to get a WSN with longer lifetime by reducing the node redundancy in transmitting data, On the other hand, the CODE, based on the GAF algorithm, is able to deliver data according to the data direction that is, as a result, reduce the probability of blandness of data delivery and reduce the power consumption. Algorithms if rooted on the GAF algorithm will have an important characteristic that the denser the nodes are distributed, the longer the lifetime is, However, when node distribution is getting denser, the distance between nodes will be closer, and the power if transmitted still in the same power level for data delivery will no doubt bring about power redundancy and even commit data confliction. In this paper, we come forward an algorithm on the basis of the CODE algorithm, namely the adapting CODE (ACODE) which is able to adaptively change the data transmitting power on each node, reducing power consumption, and accordingly prolong the lifecycle for a specific WSN. According to ACODE algorithm, when a route is establishing, a specific package for measuring power is sent out to measure the optimal transmitting power, and this optimal power will be used throughout the whole procedure of data transmission. The NS simulation in this paper verifies that, comparing with the CODE algorithm, the ACODE can improve lifecycle for a WSN as much as 20 percentages.","","Electronic:978-0-7695-4212-6; POD:978-1-4244-8333-4","10.1109/ISDEA.2010.244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5743147","Adaptive Power;CODE;NS2;Wireless Sensor Networks","Delay;Jitter;Power demand;Power measurement;Protocols;Throughput;Wireless sensor networks","data communication;power consumption;redundancy;wireless sensor networks","adapting CODE algorithm;data delivery;data transmission;data transmitting power;node distribution;node redundancy;power consumption;power redundancy;wireless sensor networks","","0","","8","","","","13-14 Oct. 2010","","IEEE","IEEE Conferences"
"Revealing Mistakes in Concern Mapping Tasks: An Experimental Evaluation","C. Nunes; A. Garcia; E. Figueiredo; C. Lucena","Inf. Dept., PUC-Rio, Rio de Janeiro, Brazil","2011 15th European Conference on Software Maintenance and Reengineering","20110405","2011","","","101","110","Concern mapping is the activity of assigning a stakeholder's concern to its corresponding elements in the source code. This activity is primordial to guide software maintainers in several tasks, such as understanding and restructuring the implementation of existing concerns. Even though different techniques are emerging to facilitate the concern mapping process, they are still manual and error-prone according to recent studies. Existing work does not provide any guidance to developers to review and correct concern mappings. In this context, this paper presents the characterization and classification of eight concern mapping mistakes commonly made by developers. These mistakes were found to be associated with various properties of concerns and modules in the source code. The mistake categories were derived from actual mappings of 10 concerns in 12 versions of industry systems. In order to further evaluate to what extent these mistakes also occur in wider contexts, we ran two experiments where 26 subjects mapped 10 concerns in two systems. Our experimental results confirmed the mapping mistakes that often occur when developers need to interact with the source code.","1534-5351;15345351","Electronic:978-0-7695-4343-7; POD:978-1-61284-259-2","10.1109/CSMR.2011.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741266","Concern Mapping;Experimental Evaluation;Mapping Mistakes","Accuracy;Cloning;Context;Industries;Logistics;Manuals;Software","software maintenance","concern mapping task;software maintenance;source code","","3","","30","","","","1-4 March 2011","","IEEE","IEEE Conferences"
"Sense of Touch in Robots With Self-Organizing Maps","M. Johnsson; C. Balkenius","Lund University Cognitive Science, Sweden","IEEE Transactions on Robotics","20110606","2011","27","3","498","507","We review a number of self-organizing-robot systems that are able to extract features from haptic sensory information. They are all based on self-organizing maps (SOMs). First, we describe a number of systems based on the three-fingered-robot hand, i.e., the Lund University Cognitive Science (LUCS) Haptic-Hand II, that successfully extracts the shapes of objects. These systems explore each object with a sequence of grasps while superimposing the information from individual grasps after cross-coding proprioceptive information for different parts of the hand and the registrations of tactile sensors. The cross-coding is done by employing either the tensor-product operation or a novel self-organizing neural network called the tensor multiple peak SOM (T-MPSOM). Second, we present a system based on proprioception that uses an anthropomorphic robot hand, i.e., the LUCS haptic-hand III. This system is able to distinguish objects both according to shape and size. Third, we present systems that are able to extract and combine the texture and hardness properties from explored materials.","1552-3098;15523098","","10.1109/TRO.2011.2130090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5739536","Cognitive robotics;manipulators;self-organizing feature maps;tactile sensors;unsupervised learning","Haptic interfaces;Joints;Neurons;Robot sensing systems;Shape;Tensile stress","cognitive systems;control engineering computing;haptic interfaces;manipulators;self-organising feature maps;tactile sensors","Lund University cognitive science haptic hand II;anthropomorphic robot hand;cross coding proprioceptive information;grasps sequence;haptic sensory information;self-organizing maps;tactile sensor registrations;tensor multiple peak SOM;three flngered robot hand","","15","","29","","","20110328","June 2011","","IEEE","IEEE Journals & Magazines"
"Funk2: A Distributed Processing Language for Reflective Tracing of a Large Critic-Selector Cognitive Architecture","B. Morgan","MIT Media Lab., Cambridge, MA, USA","2010 Fourth IEEE International Conference on Self-Adaptive and Self-Organizing Systems Workshop","20110310","2010","","","269","274","We see the field of metareasoning to be the answer to many large organizational problems encountered when putting together an understandable cognitive architecture, capable of commonsense reasoning. In this paper we review the EM1 implementation of the Emotion Machine critic-selector architecture, as well as explain the current progress we have made in redesigning this first version implementation. For this purpose of redesign and large-scale implementation, we have written a novel programming language, Funk2, that focuses on efficient metareasoning and procedural reflection, the keystones of the critic-selector architecture. We present an argument for why the Funk2 programming language lends itself to easing the burden on programmers that prefer to not be restricted to strictly declarative programming paradigms by allowing the learning of critic and selector activation strengths by credit assignment through arbitrary procedural code.","","Electronic:978-0-7695-4229-4; POD:978-1-4244-8684-7","10.1109/SASOW.2010.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5729634","EM1;Funk2;cognitive architecture;common sense reasoning;credit assignment;critic-selector architecture;metareasoning;programming language;reflective causal tracing","Architecture;Cognition;Computer architecture;Computer languages;Debugging;Instruction sets;Programming","cognitive systems;common-sense reasoning;distributed processing;programming languages","EM1 implementation;Funk2 programming language;credit assignment;distributed processing language;emotion machine critic-selector cognitive architecture;metareasoning;procedural code;procedural reflection;programming language;selector activation","","1","","10","","","","27-28 Sept. 2010","","IEEE","IEEE Conferences"
"Effort-Aware Defect Prediction Models","T. Mende; R. Koschke","Fachbereich Math. und Inf., Univ. of Bremen, Bremen, Germany","2010 14th European Conference on Software Maintenance and Reengineering","20110217","2010","","","107","116","Defect Prediction Models aim at identifying error-prone modules of a software system to guide quality assurance activities such as tests or code reviews. Such models have been actively researched for more than a decade, with more than 100 published research papers. However, most of the models proposed so far have assumed that the cost of applying quality assurance activities is the same for each module. In a recent paper, we have shown that this fact can be exploited by a trivial classifier ordering files just by their size: such a classifier performs surprisingly good, at least when effort is ignored during the evaluation. When effort is considered, many classifiers perform not significantly better than a random selection of modules. In this paper, we compare two different strategies to include treatment effort into the prediction process, and evaluate the predictive power of such models. Both models perform significantly better when the evaluation measure takes the effort into account.","1534-5351;15345351","Electronic:978-0-7695-4321-5; POD:978-1-61284-369-8","10.1109/CSMR.2010.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714425","Cost-Benefits;Defect Prediction Models;Evaluation","Complexity theory;Data models;Object oriented modeling;Prediction algorithms;Predictive models;Quality assurance;Testing","program testing;quality assurance;software performance evaluation;software quality","effort-aware defect prediction models;error-prone modules;prediction process;quality assurance activity;random module selection;software system;trivial classifier ordering files","","36","1","42","","","","15-18 March 2010","","IEEE","IEEE Conferences"
"Passive optical network monitoring: challenges and requirements","M. M. Rad; K. Fouli; H. A. Fathallah; L. A. Rusch; M. Maier","University of Waterloo","IEEE Communications Magazine","20110204","2011","49","2","s45","S52","As PONs carry increasing amounts of data, issues relating to their protection and maintenance are becoming crucial. In-service monitoring of the PON's fiber infrastructure is a powerful enabling tool to those ends, and a number of techniques have been proposed, some of them based on optical time-domain reflectometry. In this work we address the required features of PON monitoring techniques and review the major candidate technologies. We highlight some of the limitations of standard and adapted OTDR techniques as well as non-OTDR schemes. Among the proposed optical-layer monitoring schemes, we describe our novel optical-coding-based reflection monitoring proposal and report on recent progress. We end with a discussion of promising solution paths.","0163-6804;01636804","","10.1109/MCOM.2011.5706313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5706313","","Fiber optics;Monitoring;Optical fiber devices;Optical network units;Passive optical networks","encoding;light reflection;optical time-domain reflectometry;passive optical networks","OTDR technique;PON fiber infrastructure;nonOTDR scheme;optical coding based reflection monitoring;optical time-domain reflectometry;passive optical network monitoring","","37","5","15","","","","February 2011","","IEEE","IEEE Journals & Magazines"
"A Conceptual Framework for Object-oriented Design Assessment","C. Serban","Dept. of Comput. Sci., Babes-Bolyai Univ., Cluj-Napoca, Romania","2010 Fourth UKSim European Symposium on Computer Modeling and Simulation","20110128","2010","","","90","95","Nowadays software systems have a continuous evolution, being enhanced, modified and adopted to new requirements. The code becomes more and more complex and drifts away from its original design. One of the first steps in maintaining an object-oriented system is to assess its design, repeatedly throughout the system development lifecycle. Early identification of those parts of the system that need to be reviewed, ensures a more stable design of the successive versions, hindering errors propagation. The main objective of this research is to define a conceptual framework concerning object-oriented design (OOD) assessment. The proposed evaluation methodology is based on metrics and uses fuzzy analysis for the interpretation of the obtained measurement results. This work could pave the way for providing a complex evaluation methodology through the four layers of abstraction: a meta-model for OOD assessment, defined in a standard terminology and formalism using algebraic knowledge, a library of OOD metrics definitions, setting the assessment objectives and a method for the assessment results interpretation. Furthermore, the proposed model is general and scalable and allows other properties and interactions to be added.","","Electronic:978-0-7695-4308-6; POD:978-1-4244-9313-5","10.1109/EMS.2010.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703663","fuzzy analysis;metrics;object-oriented design","Clustering algorithms;Couplings;Libraries;Measurement;Object oriented modeling;Partitioning algorithms;Software systems","fuzzy set theory;object-oriented programming;software maintenance;software metrics;software performance evaluation","algebraic knowledge;assessment objective;error propagation;fuzzy analysis;metamodel;object-oriented design;object-oriented system maintenance;software metrics;software system;system development lifecycle","","0","","20","","","","17-19 Nov. 2010","","IEEE","IEEE Conferences"
"Developing a Geo-spatial Information Framework to Facilitate National Identification System (NIS) in Ghana","W. Owusu-Banahene; I. K. Nti; P. J. Sallis","Comput. Eng. Dept., Univ. of Ghana, Accra, Ghana","2010 Fourth UKSim European Symposium on Computer Modeling and Simulation","20110128","2010","","","68","74","The National Identification System (NIS), an initiative of the government to create a computerized database of Ghanaians and foreign nationals resident in Ghana using a biometric system of registration to captures the biodata of an individual and stores it in a database. The NIS is expected to provide some benefits such as development planning based on sufficient accurate population data, delivery of social services such as health, retirement benefits and social administration; delivery of credit facilities; and identification of individuals for voting, insurance, licensing and general national security purposes. The NIS lacks a geospatial database and this could be a setback to the realization of the benefits of the NIS to the citizenry in Ghana. Literature reviewed shows that high quality geographic database is essential to having an effective citizenry identification system, to take care of the spatio-temporal dynamics which is key to the sustainability of such a system. Also this will aid policy makers, social scientist and government to geovisualise available citizenry data for research and decision making. This paper attempts to propose answers to the following questions: Can the NIS keep track of the spatial information pertaining to an individual or a group of people due to relocation? How can spatial information be validated during NIS registration? The paper first investigates novel geo-coding system that would be appropriate for providing spatial information of persons in Ghana. Secondly, the paper looks at the architectural framework for integrating a distributed geo-database into the NIS of Ghana. The paper then proposes a prototype system in the form of a Geo-demographic Information System for delivery of geographical data, demographic data, mapping capabilities and geo-information services. A system for validating location information within the NIS is discussed.","","Electronic:978-0-7695-4308-6; POD:978-1-4244-9313-5","10.1109/EMS.2010.112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703660","","Decision making;Geospatial analysis;Government;Information systems;Planning;Spatial databases","biometrics (access control);decision making;demography;distributed databases;encoding;geographic information systems;government data processing;terrain mapping;visual databases","Ghana;biodata;biometric system;citizenry identification system;computerized database;decision making;demographic data;distributed geo-database;foreign nationals;geo-coding system;geo-demographic information system;geo-spatial information framework;geographic database;geographical data;government initiative;mapping capabilities;national identification system;policy makers;prototype system;social scientist;spatio-temporal dynamics;system sustainability","","0","","16","","","","17-19 Nov. 2010","","IEEE","IEEE Conferences"
"Work in progress ‚Äî Military troops to engineers","D. T. Hayhurst; D. M. Johnson; A. L. Lighthart","College of Engineering, San Diego State University","2010 IEEE Frontiers in Education Conference (FIE)","20101223","2010","","","F3E-1","F3E-2","As part of a seven-university, NSF-sponsored initiative aimed at engaging U.S. military veterans in engineering, San Diego State University is conducting a comprehensive review of the U.S. military technical training in engineering and related fields. The specific objective is to determine ways to shorten the time to degree for military veterans entering engineering majors. To that end, a systematic examination of the student learning outcomes in military training courses as compared to courses in the engineering undergraduate degree plan is currently underway. This comparative review has required in-depth analysis of both the military education and the potential obstacles that veterans bring to an engineering degree program. Because they require a known series of courses in a specific field, the framework of military professions - as Military Occupational Specialties (Marines, Army), Naval Rates, and Air Force Specialty Codes - provides the best guidance in determining potential articulation pathways.","0190-5848;01905848","Electronic:978-1-4244-6262-9; POD:978-1-4244-6261-2","10.1109/FIE.2010.5673440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5673440","curriculum review;military training and experience;student learning outcomes;veteran students","Book reviews;Civil engineering;Educational institutions;Electric potential;Force;Training","engineering education;human factors","U.S. military technical training;engineering fields;engineering majors;military education;military occupational specialties;military training courses;military troops;military veterans;student learning","","0","","1","","","","27-30 Oct. 2010","","IEEE","IEEE Conferences"
"Test-Driven Development - Still a Promising Approach?","S. Kollanus","Fac. of Inf. Technol., Univ. of Jyvaskyla, Jyvaskyla, Finland","2010 Seventh International Conference on the Quality of Information and Communications Technology","20101203","2010","","","403","408","Test-Driven Development (TDD) has been regarded as a useful practice during the last decade as well in industry as in academia. It has been suggested to have several benefits in software development process. This paper is focused on the reported empirical research on TDD. A systematic literature review was conducted in order to analyze the current empirical evidence. Based on the review data, TDD may improve external code quality, but it also leads to increase in development time. However, there are a lot of contradictory results and it raises a question about the actual factors behind them. More systematic research, specifically controlled experiments and well reported case studies, is needed in order to better understand TDD.","","Electronic:978-0-7695-4241-6; POD:978-1-4244-8539-0","10.1109/QUATIC.2010.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655657","","Book reviews;Measurement;Productivity;Programming;Software engineering;Systematics;Testing","program testing;software engineering","external code quality;software development process;test driven development","","6","","47","","","","Sept. 29 2010-Oct. 2 2010","","IEEE","IEEE Conferences"
"Do Testers' Preferences Have an Impact on Effectiveness?","M. Lazaro; N. Juristo; E. Marcos","","2010 Seventh International Conference on the Quality of Information and Communications Technology","20101203","2010","","","304","309","Both verification and validation aim to improve the quality of software products during the development process. They use techniques like formal methods, symbolic execution, formal reviews, testing techniques, etc. Technique effectiveness depends not only on project size and complexity but also on the experience of the subject responsible for testing. We have looked at whether the opinions and preferences of subjects match the number of detected defects. Opinions and preferences can influence the decisions that testers have to make. In this paper, we present a piece of research that has explored this aspect by comparing the opinions of subjects (qualitative aspects) with the quantitative results. To do this, we use qualitative methods applied to a quantitative study of code evaluation technique effectiveness.","","Electronic:978-0-7695-4241-6; POD:978-1-4244-8539-0","10.1109/QUATIC.2010.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655565","Empirical research;Human factors;Opinions and preferences;Quantitative and qualitative research;Software engineering research;Validation;Verification","Complexity theory;Fault diagnosis;Human factors;Humans;Radio access networks;Software;Testing","formal verification;program testing;software quality","code evaluation technique;defect detection;software development process;software products quality;testing;validation;verification","","0","","11","","","","Sept. 29 2010-Oct. 2 2010","","IEEE","IEEE Conferences"
"Architectural Analysis of Systems Based on the Publisher-Subscriber Style","D. Ganesan; M. Lindvall; L. Ruley; R. Wiegand; V. Ly; T. Tsui","Fraunhofer CESE, College Park, MD, USA","2010 17th Working Conference on Reverse Engineering","20101129","2010","","","173","182","Architectural styles impose constraints on both the topology and the interaction behavior of involved parties. In this paper, we propose an approach for analyzing implemented systems based on the publisher subscriber architectural style. From the style definition, we derive a set of reusable questions and show that some of them can be answered statically whereas others are best answered using dynamic analysis. The paper explains how the results of static analysis can be used to orchestrate dynamic analysis. The proposed method was successfully applied on the NASA's Goddard Mission Services Evolution Center (GMSEC) software product line. The results show that the GMSEC has a) a novel reusable vendor-independent middleware abstraction layer that allows the NASA's missions to configure the middleware of interest without changing the publishers' or subscribers' source code, and b) a high-priority bug due to behavioral discrepancies, which were eluded during testing and code reviews, among different implementations of the same APIs for different vendors.","1095-1350;10951350","Electronic:978-0-7695-4123-5; POD:978-1-4244-8911-4","10.1109/WCRE.2010.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645557","Architectural Styles;Colored Petri Nets;Component-Connector Views;Middleware;Static and Dynamic Analysis;Vendors","Computer architecture;Computer languages;Middleware;Monitoring;Probes;Software;Software architecture","message passing;middleware;program debugging;program diagnostics","API;NASA goddard mission services evolution center;architectural analysis;behavioral discrepancy;dynamic analysis;implemented system;interaction behavior;publisher subscriber architectural style;reusable questions;reusable vendor independent middleware abstraction layer;software product line;static analysis","","3","","23","","","","13-16 Oct. 2010","","IEEE","IEEE Conferences"
"Adding Optimization to the Decompilable Code Editor","S. Ribic; A. Salihbegovic","Fac. of Electr. Eng., Univ. of Sarajevo, Sarajevo, Bosnia-Herzegovina","2010 Fifth International Conference on Software Engineering Advances","20101101","2010","","","88","93","The authors of this paper recently researched the possibility of developing programming language implementation, that is neither compiler, nor interpreter. The concept is based on keeping the complete program in native machine code, but the specialized editor can 'on the fly' decompile the machine code and display it as high level language. The displayed code can be edited and saved again as pure machine code. This paper reviews the possibility of optimizing generated code, while still retaining the possibility of decompilation. We found many important code sequences which can be replaced with shorter ones while keeping the code in decompilable executable format.","","Electronic:978-0-7695-4144-0; POD:978-1-4244-7788-3","10.1109/ICSEA.2010.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5614983","compilation;decompilation;programming languages","Arrays;Artificial neural networks;High level languages;Optimization;Program processors;Testing","high level languages;optimising compilers","code generation;code sequences;decompilable code editor;high level language;native machine code;optimization;programming language","","0","","13","","","","22-27 Aug. 2010","","IEEE","IEEE Conferences"
"CoDocent: Support API Usage with Code Example and API Documentation","Y. C. Wu; L. W. Mar; H. C. Jiau","Inst. of Comput. & Commun. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","2010 Fifth International Conference on Software Engineering Advances","20101101","2010","","","135","140","API documentation and code example are two major resources to support API usage. To find the best way to use APIs within specific programming tasks, an effective strategy to link related APIs becomes critical. Currently, many code search engines have been proposed to solve this issue. Through those search results, programmers must manually traverse across all API documents to learn the referred API calls. To ensure the productivity in the style of programming with APIs, this work provides CoDocent to help programmers review code examples found by search engines. For each found code example, CoDocent can automatically link related API documents to provide diagrams as abstractions to reflect the semantics of API calls. Two evaluations are conducted to show the effectiveness of CoDocent in investigating and adapting API calls from code examples.","","Electronic:978-0-7695-4144-0; POD:978-1-4244-7788-3","10.1109/ICSEA.2010.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615107","API documentation;code example;programming with APIs","Book reviews;Clouds;Context;Layout;Programming;Search engines;Semantics","application program interfaces;search engines;system documentation","API documentation;CoDocent;application programming interface;code search engine;programming task;semantic","","5","","18","","","","22-27 Aug. 2010","","IEEE","IEEE Conferences"
"A Web Services Status Monitoring Technology for Distributed System Management in the Cloud","C. F. Lin; R. S. Wu; S. M. Yuan; C. T. Tsai","Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan","2010 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery","20101101","2010","","","502","505","Web services define standard interfaces those provide good solution for enterprise integration. Recently, many researches focus on creating management standards to manage Web services. Web Services Distributed Management (WSDM) is one of the industry standards. However, to implement the WSDM interfaces needs to understand server Web service standards. It increases the complexity and difficulty to build the management system. In this research, we had simplified the Web service management effort between services using hook technology. Our management systems provide message flow oriented management atomically without modifying service code. Enterprise can control all flows and review them at any time. Finally, we evaluate the overhead for our proposed technology. Enterprise can build a quickly, efficient and extensible management system in the Web services environment.","","Electronic:978-0-7695-4235-5; POD:978-1-4244-8434-8","10.1109/CyberC.2010.97","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5616981","Distubted system;Hook;Web Service","Computer architecture;Monitoring;Protocols;Service oriented architecture;Simple object access protocol;Standards","Web services;standards","Web services distributed management;Web services status monitoring technology;cloud computing;distributed system management;enterprise integration;industry standard;management standard;message flow oriented management;standard interface","","1","","16","","","","10-12 Oct. 2010","","IEEE","IEEE Conferences"
"An approach to improving software inspections performance","A. L. Ferreira; R. J. Machado; J. G. Silva; R. F. Batista; L. Costa; M. C. Paulk","Dept. de Sistemas de Informa&#x00E7;&#x00E3;o, Universidade do Minho, Portugal","2010 IEEE International Conference on Software Maintenance","20101025","2010","","","1","8","Software inspections allow finding and removing defects close to their point of injection and are considered a cheap and effective way to detect and remove defects. A lot of research work has focused on understanding the sources of variability and improving software inspections performance. In this paper we studied the impact of inspection review rate in process performance. The study was carried out in an industrial context effort of bridging the gap from CMMI level 3 to level 5. We supported a decision for process change and improvement based on statistical significant information. Study results led us to conclude that review rate is an important factor affecting code inspections performance and that the applicability of statistical methods was useful in modeling and predicting process performance.","1063-6773;10636773","Electronic:978-1-4244-8629-8; POD:978-1-4244-8630-4; USB:978-1-4244-8628-1","10.1109/ICSM.2010.5609700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609700","Software Process Improvement;Software inspections","Inspection;Software","Capability Maturity Model;program testing;software performance evaluation;software quality","CMMI;capability maturity model;software inspections performance improvement;software process improvement;statistical method;statistical significant information","","3","","30","","","","12-18 Sept. 2010","","IEEE","IEEE Conferences"
"Revisiting common bug prediction findings using effort-aware models","Y. Kamei; S. Matsumoto; A. Monden; K. i. Matsumoto; B. Adams; A. E. Hassan","Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Canada","2010 IEEE International Conference on Software Maintenance","20101025","2010","","","1","10","Bug prediction models are often used to help allocate software quality assurance efforts (e.g. testing and code reviews). Mende and Koschke have recently proposed bug prediction models that are effort-aware. These models factor in the effort needed to review or test code when evaluating the effectiveness of prediction models, leading to more realistic performance evaluations. In this paper, we revisit two common findings in the bug prediction literature: 1) Process metrics (e.g., change history) outperform product metrics (e.g., LOC), 2) Package-level predictions outperform file-level predictions. Through a case study on three projects from the Eclipse Foundation, we find that the first finding holds when effort is considered, while the second finding does not hold. These findings validate the practical significance of prior findings in the bug prediction literature and encourage their adoption in practice.","1063-6773;10636773","Electronic:978-1-4244-8629-8; POD:978-1-4244-8630-4; USB:978-1-4244-8628-1","10.1109/ICSM.2010.5609530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609530","","Computational modeling;Computer bugs;Mathematical model;Measurement;Predictive models;Radio frequency;Regression tree analysis","program debugging;software metrics;software packages;software quality","bug prediction literature;common bug prediction finding;effort aware model;package level prediction;process metrics;software quality assurance","","51","","34","","","","12-18 Sept. 2010","","IEEE","IEEE Conferences"
"Migrating from COBOL to Java","H. M. Sneed","ANECON GMBH, Testing Department, Vienna, Austria","2010 IEEE International Conference on Software Maintenance","20101025","2010","","","1","7","This paper is an industrial report on a project for migrating an airport management system from a Bull mainframe using COBOL as a programming language and IDS as a database system to a distributed UNIX platform using Java and Oracle. The focus here is on the automated language transformation, performed in three phases - reengineering, conversion and refinement. The tools used are COBRedo for reengineering the COBOL code, COB2Java for converting COBOL to Java and JavRedoc for documenting the converted Java code. The paper describes the migration process and the tools used in it and the reviews the current state of the project.","1063-6773;10636773","Electronic:978-1-4244-8629-8; POD:978-1-4244-8630-4; USB:978-1-4244-8628-1","10.1109/ICSM.2010.5609583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5609583","COBOL;Java;code transformation;legacy systems;migration","DH-HEMTs;Java","COBOL;Java;Unix;airports;software maintenance;systems re-engineering","Bull mainframe;COB2Java;COBOL;COBRedo;IDS;JavRedoc;Oracle;airport management system;automated language transformation;distributed UNIX platform;reengineering","","9","2","21","","","","12-18 Sept. 2010","","IEEE","IEEE Conferences"
"The Impact of Model Driven Development on the Software Architecture Process","W. Heijstek; M. R. V. Chaudron","Leiden Inst. of Adv. Comput. Sci., Leiden Univ., Leiden, Netherlands","2010 36th EUROMICRO Conference on Software Engineering and Advanced Applications","20101011","2010","","","333","341","While Model-Driven Development (MDD) is an increasingly popular software development approach, its impact on the development process in large-scale, industrial practice is not yet clear. For this study the application of MDD in a large-scale industrial software development project is analyzed over a period of two years. Applying a grounded theory approach we identified 14 factors which impact the architectural process. We found that scope creep is more likely to occur, late changes can imply more extensive rework and that business engineers need to be more aware of the technical impact of their decisions. In addition, the introduced Domain-Specific Language (DSL) provides a new common idiom that can be used by more team members and will ease communication among team members and with clients. Also, modelers need to be much more explicit and complete in their descriptions. Parallel development of a code generator and defining a proper meta-model require additional time investments. Lastly, the more central role of software architecture design documentation requires more structured, detailed and complete architectural information and consequently, more frequent reviews.","1089-6503;10896503","Electronic:978-0-7695-4170-9; POD:978-1-4244-7901-6","10.1109/SEAA.2010.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598116","Industrial Case Study;Model Driven Development (MDD);Software Architecture","Business;DSL;Interviews;Lead;Programming;Software;Unified modeling language","software architecture;specification languages","code generator;domain-specific language;large-scale industrial software development project;model driven development;software architecture process","","5","","13","","","","1-3 Sept. 2010","","IEEE","IEEE Conferences"
"Software Architecture and Characteristic Functions in Learning Management System ""NOBASU""","N. Funabiki; T. Nakanishi; N. Amano; H. Kawano; Y. Fukuyama; M. Isogai","Dept. of Commun. Network Eng., Okayama Univ., Okayama, Japan","2010 10th IEEE/IPSJ International Symposium on Applications and the Internet","20101007","2010","","","109","112","Recently, a learning management system (LMS) using the Web technology has been used at many educational institutions. To support educational activities in our department, we have independently developed an LMS called NOBASU (NetwOrk-Based Assistant System for University education) to improve the quality while reducing costs. Besides conventional functions in LMS, NOBASU has incorporated several original ones such as the keyword submission function for preparing and reviewing class contents. In this paper, we present two new characteristic functions for the Java programming education in NOBASU. The Java code test function verifies the specifications in source codes from students using the test-driven development (TDD) method. The Java test code submission function requests students to submit test codes so that they can generate detailed program specifications by themselves.","","Electronic:978-1-4244-7527-8; POD:978-1-4244-7526-1","10.1109/SAINT.2010.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598167","Java code test;Java programming education;NOBASU;Web;learning management system","Education;Java;Least squares approximation;Programming;Software;Software architecture;User interfaces","Java;computer aided instruction;computer science education;program testing;software architecture","Java code test function;Java programming education;Java test code submission function;NOBASU;Web technology;characteristic functions;class contents;educational institutions;keyword submission function;learning management system;network-based assistant system;program specifications;software architecture;source codes;test-driven development;university education","","2","","9","","","","19-23 July 2010","","IEEE","IEEE Conferences"
"Lightweight protection against brute force login attacks on Web applications","C. Adams; G. V. Jourdan; J. P. Levac; F. Prevost","School of Information Technology and Engineering, University of Ottawa, Canada","2010 Eighth International Conference on Privacy, Security and Trust","20100930","2010","","","181","188","Password-based systems and, more generally, authentication systems based on something you know, are commonplace on the Internet. Web applications using these systems can be the target of brute force login attacks, in which an attacker tries to compromise a given account or any user account on the system. These applications rarely implement effective protection mechanisms against these attacks. In this paper, we review the situation and propose a practical, simple, security mechanism. Our system is non-intrusive and can be incorporated into most web applications with very little modification to the application code.","","Electronic:978-1-4244-7550-6; POD:978-1-4244-7551-3","10.1109/PST.2010.5593241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5593241","brute force attacks;denial of service;trawling attacks;web applications","Authentication;Computer crime;Databases;Electronic mail;Force;IP networks","Internet;message authentication","Internet;Web applications;authentication systems;brute force login attacks;lightweight protection;password-based systems","","4","","5","","","","17-19 Aug. 2010","","IEEE","IEEE Conferences"
"Notice of Retraction<BR>Traceability and Quality of Agricultural Produce Fast Traceability System Design","J. Haishui; L. Junhua","Bussiness Sch., Beijing WUZI Univ., Beijing, China","2010 International Conference on E-Business and E-Government","20100930","2010","","","4981","4988","Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>At present, the agricultural products quality and safety problems have become increasingly prominent, and quality of agricultural products quality system of agricultural products back system is an important element of the building. In this paper, fast traceability system for agricultural product quality to define concepts based on the proposed enterprise should be a higher level of information silos connections, from a higher level of abstraction, general flow of agricultural products unit, through the unique coding, data intersection of the collection and realization of agricultural product quality and traceability of our agro-fast traceability system model is built general idea; and quality of agricultural produce rapid traceable patterns were studied.","","Electronic:978-1-4244-6647-4; POD:978-1-4244-6646-7","10.1109/ICEE.2010.1251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5592838","Quality of agricultural products;traceability;traceability model","Agricultural products;Buildings;Electronic government;Global Positioning System;Radiofrequency identification;Safety;System analysis and design","agricultural products;quality management;safety;supply chain management","agricultural product quality;agricultural product safety;agricultural product traceability;fast traceability system design;supply chain management","","0","","18","","","","7-9 May 2010","","IEEE","IEEE Conferences"
"Improve the Throughput of AODV Based on Network Coding Ideas","Y. Tao; Y. Li; J. Li; H. c. Huang","Center of Software Technol., ChongQing Univ. of Posts & Telecommun., Chongqing, China","2010 WASE International Conference on Information Engineering","20100916","2010","1","","280","283","Network coding (NC) has a broad application prospects in wireless Ad hoc networks. Firstly, this paper introduces the development process about network coding in wireless network environment as well as the advantages and disadvantages. Secondly, reviewing the AODV routing protocol, using network coding ideas to improve the throughput of AODV routing protocol, and proposes a new routing protocol NCAODV, also describes the packet processing of NCAODV routing protocol. Lastly, use the internationally well-known simulation tool NS2 to simulate the NCAODV routing protocol, the simulation results have shown that NCAODV routing protocol which based on AODV routing protocol improved the throughput.","","Electronic:978-1-4244-7507-0; POD:978-1-4244-7506-3","10.1109/ICIE.2010.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5571097","AODV;network coding;throughput;wireless","Ad hoc networks;Encoding;Network coding;Routing;Routing protocols;Throughput;Wireless networks","ad hoc networks;network coding;routing protocols","AODV routing protocol;network coding;on demand distance vector;packet processing;wireless ad hoc networks","","1","","10","","","","14-15 Aug. 2010","","IEEE","IEEE Conferences"
"CheckDep: A Tool for Tracking Software Dependencies","D. Beyer; A. Fararooy","Simon Fraser Univ., Burnaby, BC, Canada","2010 IEEE 18th International Conference on Program Comprehension","20100726","2010","","","42","43","Many software developers use a syntactical `diff' in order to performa quick review before committing changes to the repository. Others are notified of the change by e-mail (containing diffs or change logs), and they review the received information to determine if their work is affected. We lift this simple process from the code level to the more abstract level of dependencies: a software developer can use CheckDep to inspect introduced and removed dependencies before committing new versions, and other developers receive summaries of the changed dependencies via e-mail. We find the tool useful in our software-development activities and now make the tool publicly available.","1092-8138;10928138","Electronic:978-1-4244-7603-9; POD:978-1-4244-7604-6; USB:978-0-7695-4113-6","10.1109/ICPC.2010.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521774","Dependency Analysis;Program Understanding;Refactoring;Software Structure","Cement industry;Computer industry;Concrete;Feedback;Industrial control;Licenses;Software performance;Software tools;Uniform resource locators;Visualization","software engineering;software tools","CheckDep tool;code level;e-mail;software dependency tracking;software development","","1","","2","","","","June 30 2010-July 2 2010","","IEEE","IEEE Conferences"
"Spectral Techniques: The First Decade of the XXI Century (Invited Paper)","C. Moraga","Eur. Centre for Soft Comput., Mieres, Spain","2010 40th IEEE International Symposium on Multiple-Valued Logic","20100621","2010","","","3","8","Following an ‚Äúunwritten tradition‚Äù this paper offers a review of another decade of achievements in the area of Spectral Techniques. Topics like transforms of matrix-valued functions, transforms based on non-Abelian groups, latest advancements related to Linear Independent transforms, a space-efficient algorithm to calculate spectral transforms, will be discussed. A special mention will be given to other areas, like signal processing, and coding theory, where Spectral Techniques have been (and continue to be) used, even though with possibly different names.","0195-623X;0195623X","Electronic:978-1-4244-6753-2; POD:978-1-4244-6752-5","10.1109/ISMVL.2010.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489200","Spectral Techniques;non-Abelian groups","Books;Codes;Equations;Logic;Signal processing algorithms;Space technology;Stacking;Transforms","","","","1","","34","","","","26-28 May 2010","","IEEE","IEEE Conferences"
"Adaptive MIMO transmission techniques for broadband wireless communication systems [Topics in Wireless Communications]","C. B. Chae; A. Forenza; R. W. Heath; M. R. McKay; I. B. Collings","Bell Laboratories, Alcatel-Lucent","IEEE Communications Magazine","20100506","2010","48","5","112","118","Link adaptation is a way to increase data rates in wireless systems by adapting transmission parameters such as the modulation and coding rate. While link adaptation in single antenna systems is now mature, its application to multiple-input multiple-output communication links, presented in several emerging wireless standards, has been challenging. The main reason is that the space-time transmission strategy can also be adjusted in MIMO communication links, introducing a new dimension for adaptation. This means that practical MIMO link adaptation algorithms must also provide a dynamic adaptation between diversity and multiplexing modes of operation. This article reviews a recently proposed framework for adaptive MIMO architectures and shows how to use this framework to reduce adaptive control overhead. We also discuss practical implementation issues. Simulations in an IEEE 802.16e (mobile WiMAX) system illustrate the frame-work's potential improvements in data rates.","0163-6804;01636804","","10.1109/MCOM.2010.5458371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5458371","","Adaptive control;Algorithm design and analysis;Broadband communication;Concrete;Diversity methods;Error analysis;MIMO;Modulation coding;Switches;Wireless communication","IEEE standards;MIMO communication;WiMax;adaptive control;telecommunication standards","IEEE 802.16e;MIMO communication links;MIMO link adaptation algorithms;adaptive MIMO architectures;adaptive MIMO transmission;adaptive control overhead;broadband wireless communication systems;coding rate;data rates;dynamic adaptation;mobile WiMAX system;modulation rate;multiple-input multiple-output communication links;single antenna systems;space-time transmission;wireless systems","","38","4","15","","","","May 2010","","IEEE","IEEE Journals & Magazines"
"Crypto: Not Just for the Defensive Team","C. P. Pfleeger","","IEEE Security & Privacy","20100329","2010","8","2","63","66","Cryptography has long been a useful, important tool for defensive computer security. Increasingly, however, attackers are using cryptographic techniques for the same reason as the defenders: to protect code's confidentiality and integrity. But in this case, the code is malicious. This paper reviews uses of encryption by writers of malicious code, through some recent examples. Malicious-code writers are using state-of-the-art cryptography, just as commercial security engineers are.","1540-7993;15407993","","10.1109/MSP.2010.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439531","Conficker;Morris worm;Virtob;Virut;cryptanalysis;cryptography;encryption;malicious code;packers;polymorphic viruses;security & privacy","Computer security;Cryptography;Protection","cryptography;invasive software","cryptography;defensive computer security;encryption use;malicious code","","2","","","","","","March-April 2010","","IEEE","IEEE Journals & Magazines"
"Identifying Security Relevant Warnings from Static Code Analysis Tools through Code Tainting","D. Baca","Blekinge Inst. of Technol., Ericsson AB, Karlskrona, Sweden","2010 International Conference on Availability, Reliability and Security","20100325","2010","","","386","390","Static code analysis tools are often used by developers as early vulnerability detectors. Due to their automation they are less time-consuming and error-prone then manual reviews. However, they produce large quantities of warnings that developers have to manually examine and understand.In this paper, we look at a solution that makes static code analysis tools more useful as an early vulnerability detector. We use flow-sensitive, interprocedural and context-sensitive data flow analysis to determine the point of user input and its migration through the source code to the actual exploit. By determining a vulnerabilities point of entry we lower the number of warnings a tool produces and we provide the developer with more information why this warning could be a real security threat. We use our approach in three different ways depending on what tool we examined. First, With the commercial static code analysis tool, Coverity, we reanalyze its results and create a set of warnings that are specifically relevant from a security perspective. Secondly, we altered the open source analysis tool Findbugs to only analyze code that has been tainted by user input. Third, we created an own analysis tool that focuses on XSS vulnerabilities in Java code.","","Electronic:978-1-4244-5880-6; POD:978-1-4244-5879-0","10.1109/ARES.2010.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5438066","Static code analysis;coverity;data flow;dua;findbugs;security;taint analysis","Automation;Availability;Data analysis;Data security;Detectors;Information security;Java;Manufacturing;Paper technology;Telecommunications","Java;data analysis;public domain software;security of data;software tools","Findbugs analysis tool;Java code;XSS vulnerabilities;code tainting;data flow analysis;early vulnerability detectors;security relevant warning identification;static code analysis tools","","1","1","16","","","","15-18 Feb. 2010","","IEEE","IEEE Conferences"
"Tactile Sensing‚ÄîFrom Humans to Humanoids","R. S. Dahiya; G. Metta; M. Valle; G. Sandini","Robotics, Brain, and Cognitive Sciences Department, Italian Institute of Technology, Genoa, Italy","IEEE Transactions on Robotics","20100205","2010","26","1","1","20","Starting from human ??sense of touch,?? this paper reviews the state of tactile sensing in robotics. The physiology, coding, and transferring tactile data and perceptual importance of the ??sense of touch?? in humans are discussed. Following this, a number of design hints derived for robotic tactile sensing are presented. Various technologies and transduction methods used to improve the touch sense capability of robots are presented. Tactile sensing, focused to fingertips and hands until past decade or so, has now been extended to whole body, even though many issues remain open. Trend and methods to develop tactile sensing arrays for various body sites are presented. Finally, various system issues that keep tactile sensing away from widespread utility are discussed.","1552-3098;15523098","","10.1109/TRO.2009.2033627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5339133","Cutaneous sensing;extrinsic sensing;humanoid robots;robotic skin;tactile sensing;touch sensing system","","humanoid robots;tactile sensors","body sites;humanoid robotics;sense of touch;tactile sensing arrays","","484","11","232","","","20091120","Feb. 2010","","IEEE","IEEE Journals & Magazines"
"State of the Journal","M. Parashar","","IEEE Transactions on Parallel and Distributed Systems","20171211","2018","29","1","1","1","Welcome to 2018's first issue of the IEEE Transactions on Parallel and Distributed Systems (TPDS). I'm excited with my new role as incoming Editor-in-Chief (EIC) of TPDS and look forward to serving the community over the next few years. My goal as EIC is to continue to work on increasing the visibility and relevance and impact of TPDS, as well as the quality and timeliness of the review process, to ensure that TPDS is the premier Transactions in the field. The IEEE is a hallmark of quality for technical publication. The value TPDS brings to the international community is in its collection of the highest quality research that is relevant to academia, industry, and laboratories. I will investigate new opportunities for TPDS to capture the best research while maintaining its emphasis on highest quality papers. TPDS also needs to respond to a dynamic and rapidly evolving research and publication landscape. As EIC, I will work with the IEEE community to ensure that TPDS does respond appropriately, and will carefully work with the editorial board to revisit the scope and recruit new editorial board members as needed. An important and rapid growing conversation is related to the repeatability of published research and the submission of supplementary material such as code and data. I am part of this conversation, and will work with the EB and the community to explore how to bring these practices in meaningful and measured ways to TPDS.","1045-9219;10459219","","10.1109/TPDS.2017.2768538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8173510","","","","","","","","","","","","Jan. 1 2018","","IEEE","IEEE Journals & Magazines"
"Detection and location of passive and semi-passive RFID tags in in-door environments","I. White","University of Cambridge, U.K.","2015 International Conference on Pervasive and Embedded Computing and Communication Systems (PECCS)","20160602","2015","","","IS-11","IS-12","This presentation will review the development of Radio frequency identification (RFID) technology, which has received increasing research interest in recent years due to its huge potential in asset tracking and localization. The most promising feature of this technique is its ability to identify an RFID tagged object from a distance, which, unlike traditional bar-code techniques, does not require line of sight between the reader and tag. Clear applications have been identified for active, semi-passive and passive RFID tags, with battery-free tags being recognised to be particularly appropriate for very low cost situations. A challenge with such tags however is in ensuring highly reliable detection particularly in environments where in-door features lead to RF nulls. This, in turn affects location accuracy. In recent years, the problem of nulls has been overcome by using phased array antennas and distributed antenna system (DAS) approaches. For example, researchers have achieved wide area coverage by maximizing the link budget through space time array techniques, smart antennas and digital beam forming. In respect of DAS, excellent performance has been achieved by distributed transmitter and receiver signal control and signal processing, so that >99% reliability has been achieved for tag numbers in excess of 100 over areas of up to 20 m √ó 20 m. Research has additionally identified that by careful design, larger areas can be monitored using multiple cells where cell to cell interference can be used to enhance read reliability. Wireless antenna nodes can also be used to enhance read range. This paper will therefore review the current status of this rapidly developing field, and describe recent advances in location.","","Electronic:978-989-758-137-3; POD:978-1-4673-8406-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483721","","","","","","","","","","","","11-13 Feb. 2015","","IEEE","IEEE Conferences"
"Editorial: Reflections on 2015 and entering the active content age","B. Lehman; H. S. H. Chung","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA","IEEE Transactions on Power Electronics","20150921","2016","31","1","3","4","This January 2016 issue represents the IEEE Transactions on Power Electronics (TPEL) entry into the active content/digital age. Through monthly e-mails and last year's editorial, we have been asking authors to consider adding active content to their paper submissions when it enhances the paper's presentation. In this issue, we are publishing four papers, each with different types of active content. Our TPEL active content guidelines will continue to evolve as more papers submit data, movies, source codes, or other active content material. However, as with these four papers, any future papers must upload supplemental digital content at the time of initial manuscript submission, allowing the content to be fully reviewed by both the paper reviewers and editors. For example, each of the four papers was accepted based on its technical research contributions, independent from the active content. This is a requirement for the acceptance of any TPEL manuscript. A footnote explaining the active content will appear on the paper, and there is a special icon on the IEEE Xplore platform for the paper that allows readers to download the multimedia. TPEL has now entered the digital content era.","0885-8993;08858993","","10.1109/TPEL.2015.2478282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272810","","","","","","","","","","","","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"EMC society board of directors meeting","","","IEEE Electromagnetic Compatibility Magazine","20150817","2015","4","2","136","140","President Bob Scully called the meeting to order at 9:00 am. A round of introductions was made. Board members present included B. Archambeault, H. Benitez, C. Brench, C. Bunting, R. Davis, A. Duffy, H. Garbe, E. Hare, K. Hatashita, D. Heirman, D. Hoolihan, I. Kasperovich, R. Koga, J. LaSalle, D. Lewis, W. Lumpkins, M. Montrose, J. Norgard, M. Oliver, J. O'Neil, G. Pettit, V. Rajamani, V. Rodriguez, F. Sabath, C. Sartori, C. Schuster, R. Scully, and D. Sweeney. Absent was F. Canavero. Guests included J. Cerone, F. Heather, H.R. Hofmann, K. Williams, J. Ramie, and C. Chan. He reviewed Board meeting protocol, the IEEE code of ethics, and upcoming activities.","2162-2264;21622264","","10.1109/MEMC.2015.7204067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7204067","","","","","","0","","","","","","2nd Quarter 2015","","IEEE","IEEE Journals & Magazines"
"Conceptual modeling in Agile information systems development","A. Oliv√©","Universitat Polit&#x00E8;cnica de Catalunya, Spain","2014 9th International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)","20150402","2014","","","1","1","The nature and the role of conceptual modeling in information systems development has neither in theory nor in practice been established satisfactorily. There are diverse views on what conceptual modeling is and on how to perform it. In one extreme, there is the view that conceptual modeling is an (optional) activity whose main purpose is to improve communication between the parties involved in the development process. In the other extreme, there is the view (shared by us) that conceptual modeling is an activity that is necessarily performed in all cases, and whose purpose is to define the conceptual schema, that is, the general knowledge a system needs to know to perform its functions. The latter has been captured in what we call the principle of necessity of conceptual Schemas, which states that ""To develop an information system it is necessary to define its conceptual schema"". Agile development processes have added even more confusion to conceptual modeling. The value of ""Working software over comprehensive documentation"", stated in the manifesto for agile software development, seems to undermine conceptual Schemas in favor of working code. However, as we explain in the talk, it does not need to be so. In the talk, we present a framework that describes the contents of conceptual Schemas, the form they may take and the roles they play in information systems development. Based on that framework, we review the principle of necessity of conceptual Schemas. We then apply the framework to the particular case of agile development, and discuss the validity of the principle of necessity in that case. The framework is intended to be useful for inspiring future research, and for improving the practice and teaching of conceptual modeling.","","Electronic:978-989-758-030-7; POD:978-1-4799-7919-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7077102","","","","","","0","","","","","","28-30 April 2014","","IEEE","IEEE Conferences"
"EMC society Board of Directors meeting","","","IEEE Electromagnetic Compatibility Magazine","20140708","2014","3","2","88","93","President Scully called the meeting to order at 9:00 am. A round of introductions was made. Board members present included H. Benitez, B. Archambeault, C. Brench, C. Bunting, F. Canavero, B. Davis, A. Duffy, E. Hare, F. Heather, E. Joffe, J. LaSalle, R. Koga, M. Montrose, J. Norgard, M. Oliver, J. O'Neil, G. Pettit, A. Pinchuk, C. Sartori, D. Staggs, D. Sweeney, and R. Scully. Absent Board members included D. Hoolihan, D. Heirman, and F. Sabath. Guests included J. Amodeo, K. Galuchie, and C. Johnson - all with IEEE. President Scully reviewed Board meeting protocol, the IEEE code of ethics, and upcoming activities. He reminded Board members of the material on the Board FTP site and asked that everyone remember to review this prior to the Board meetings.","2162-2264;21622264","","10.1109/MEMC.2014.6849551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849551","","","","","","0","","","","","","2nd Quarter 2014","","IEEE","IEEE Journals & Magazines"
"Message from the ComManTel 2014 chairs","Le Cong Co; K. BenLetaief; Hsiao-Hwa Chen; T. Q. Duong; M. D. Nguyen; S. Shen; N. Nasser; Phan Anh; G. K. Karagiannidis; M. Dohler; S. Guizani; V. N. Quoc Bao","Duy Tan University, Vietnam","2014 International Conference on Computing, Management and Telecommunications (ComManTel)","20140605","2014","","","1","1","On behalf of the Technical Program Committee, we welcome all of you to the IEEE Computing, Management and Telecommunications Conference (IEEE ComManTel 2014) in the beautiful campus of Duy Tan University, Da Nang City, Vietnam! We are indeed delighted that the conference aims to providing a premier forum for presentation of research results and experience reporting on the cutting edge research in the general areas of computing, telecommunications, management and their applications. This year, we have received many high quality submissions from more than 22 countries. Each paper received at least three peer technical reviews, comprised of more than 160 TPC members from academia, government laboratories, and industries. After carefully examining all the received review reports, the IEEE ComManTel 2014 TPC finally selected only very high quality papers for presentation at the conference and publication in the IEEE ComManTel 2014 proceedings. The first day will start with a free Tutorial presentation chosen from renowned world-class leaders in the area. On Sunday, Prof. Hossam Hassanein, from Queen's University, Canada will talk about the ‚ÄúThe Evolution of Information Networks around Data.‚Äù On Monday, we start the with a Keynote Speech delivered by IEEE Fellow and Distinguished Professor Hsiao-Hwa Chen of the National Cheng Kung University, Taiwan. He will talk to us about the ‚ÄúDesign of MIMO systems based on chip-level space-time coded CDMA system.‚Äù Right after the Keynote, Prof. Markus Fiedler from the Blekinge Institute of Technology, Sweden, will also deliver a free Tutorial on ‚ÄúQuantitative Relationships Between QoE and Energy Consumption.‚Äù This year, the technical sessions reflect the growing interest in a wide range of spectrum, including wireless communications and networks, computing aspects, management and applications of these technologies in ou- daily lives. Outstanding papers will be selected and invited for Special Issues in well-known international journals and top quality papers will receive the Best Paper Award. Our objective in the future is to increase the quality and therefore reduce the acceptance rate further. In addition, we would like to increase the number of Tracks to cover other areas and introduce Workshops that of interest to meet the conference theme.","","CD-ROM:978-1-4799-2904-7; Electronic:978-1-4799-2903-0; POD:978-1-4799-5385-1","10.1109/ComManTel.2014.6825560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825560","","","","","","0","","","","","","27-29 April 2014","","IEEE","IEEE Conferences"
"EMC Society Board of Directors meeting","","","IEEE Electromagnetic Compatibility Magazine","20140415","2014","3","1","88","92","President Pettit called the meeting to order at 9:00 am. A round of introductions was made. Board members present included H. Benitez, C. Bunting, F. Canavero, A. Duffy, E. Hare, F. Heather, D. Heirman, T. Hubing, R. Koga, F. Maradei, M. Montrose, J. Norgard, J. O'Neil, G. Pettit, A. Pinchuk, F. Sabath, and R. Scully. Members absent included B. Archambeault, C. Brench, B. Davis, D. Hoolihan, E. Joffe, J. LaSalle, D. Staggs, and D. Sweeney. It was noted Bob Davis attempted to call in to the meeting or connect via WebEx, but the connections were not successful. Mr. Pettit reviewed Board meeting protocol, the IEEE code of ethics, and upcoming activities.","2162-2264;21622264","","10.1109/MEMC.2014.6798804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798804","","","","","","0","","","","","","1st Quarter 2014","","IEEE","IEEE Journals & Magazines"
"EMC Society board of directors meeting","","","IEEE Electromagnetic Compatibility Magazine","20140116","2013","2","4","84","90","President Pettit called the meeting to order at 9:00 am. A round of introductions was made. Board members present included B. Archambeault, H. Benitez, C. Brench, C. Bunting, F. Canavero, B. Davis (by telecom), A. Duffy, E. Hare, F. Heather, D. Heirman, D. Hoolihan, T. Hubing, E. Joffe, R. Koga, J. LaSalle, F. Maradei, M. Montrose, J. O'Neil, G. Pettit, A. Pinchuk, F. Sabath, R. Scully, D. Staggs and D. Sweeney. Guests present included C. Chan, J. Ramie, H. Garbe, H. Krauth??user, D. Ray, D. Odum, S. Pignari, and S. Heather. He reviewed Board meeting protocol, the IEEE code of ethics, and upcoming activities. He reminded Board members of the material on the Board FTP site and asked that everyone remember to review this prior to the Board meetings (ftp://ewh.ieee.org). He acknowledged new Board member Ed Hare who was appointed to fill the position vacated by the late Kermit Phipps.","2162-2264;21622264","","10.1109/MEMC.2013.6714705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714705","","","","","","0","","","","","","4th Quarter 2013","","IEEE","IEEE Journals & Magazines"
"The Cycles of Continuous Improvement","D. A. Grier","Center for International Science and Technology Policy","Computer","20131219","2013","46","12","6","8","A review of 2013 activities from the president's perspective reminds us--through the life cycle of change--of the important progress we made in the areas of communities, global membership, and leveraging our role within IEEE. The Web extra at http://youtu.be/qMHPZeDufnQ is a video segment in which IEEE Computer Society President David Alan Grier discusses the Computer Science Education Week Hour of Code, a one-hour introduction to computer science, designed to demystify ""code"" and show that anyone can learn the basics to be a maker, a creator, and an innovator.","0018-9162;00189162","","10.1109/MC.2013.437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689277","Christopher Johnson;Code.org;EPEAT;IEEE Computer Society;Jack Dongarra;Ken Kennedy Award;Sydney Fernbach Award","","","","","0","","","","","","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"EMC society board of directors meeting","","","IEEE Electromagnetic Compatibility Magazine","20130702","2013","2","2","109","115","President Pettit called the meeting to order at 9:00 am. A round of introductions was made. Members present included B. Archambeault, H. Benitez, C. Brench, C. Bunting, F. Canavero, R. Davis, A. Duffy, F. Heather, D. Heirman, D. Hoolihan, T. Hubing, E. Joffe, R. Koga, J. LaSalle, F. Maradei, M. Montrose (by telecom), M. Oliver, J. O'Neil, G. Pettit, F. Sabath, R. Scully, D. Staggs (by telecom), and D. Sweeney. Members absent included A. Pinchuk and K. Phipps. Guests present included M. Demydenko, F. Mingione, and S. Welch (all with IEEE) and K. Williams (by telecom). President Pettit reviewed Board meeting protocol, the IEEE code of ethics, and upcoming activities. Newly elected Board members for 2013 were recognized, including Henry Benitez, Flavio Canavero, Alistair Duffy, Elya Joffe, Dan Hoolihan, and Bob Scully.","2162-2264;21622264","","10.1109/MEMC.2013.6550943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6550943","","","","","","0","","","","","","Second Quarter 2013","","IEEE","IEEE Journals & Magazines"
"EMC Society Board of Directors meeting","","","IEEE Electromagnetic Compatibility Magazine","20130502","2013","2","1","96","104","President Pettit called the meeting to order at 9:00 am. A round of introductions was made. Members present included B. Archambeault, C. Brench, C. Bunting, L. Cohen, R. Davis, A. Duffy, F. Heather, D. Heirman, D. Hoolihan, T. Hubing, R. Koga, J. LaSalle, F. Mara-dei, M. Montrose, M. Oliver, J. O'Neil, G. Pettit, A. Pinchuk, K. Phipps, R. Scully, D. Sweeney, P. Wilson, and T. Yoshino. Absent members included A. Marvin, V. Roje, F. Sabath, and D. Staggs. Guests included F. Canavero, P. Clout, S. Connor, R. Ford, J. Nor-gard, and K. Williams (by telecom). President Pettit reviewed Board meeting protocol, the IEEE code of ethics, and upcoming activities. Newly elected Board members for 2013 were recognized, including Henry Benitez, Flavio Canavero, Alistair Duffy, Elya Joffe, Dan Hoolihan, and Bob Scully.","2162-2264;21622264","","10.1109/MEMC.2013.6512230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6512230","","","","","","0","","","","","","1St Quarter 2013","","IEEE","IEEE Journals & Magazines"
"EMC Society Board of Directors Meeting","","","IEEE Electromagnetic Compatibility Magazine","20130104","2012","1","4","104","111","President Pettit called the meeting to order at 9:00 am. A round of introductions was made. Members present included B. Archambeault, C. Brench, C. Bunting, L. Cohen, R. Davis, A. Duffy, F. Heather, D. Heirman, D. Hoolihan, T. Hubing, R. Koga, J. LaSalle, F. Maradei, A. Marvin, M. Montrose, M. Oliver, J. O‚ÄôNeil, G. Pettit, A. Pinchuk, K. Phipps, F. Sabath, R. Scully, D. Staggs, D. Sweeney, P. Wilson, and T. Yoshino. The absent Board member was V. Roje. Guest present included R. Adams, J. Anderson, D. Beetner, H. Benitez, C. Chan, H. Denny, A. Drozd, H. Garbe, S. Hernandez, H.R. Hofmann, E. Joffe, B. Lawrence, K. Lawrence, A. Maxwell, J. Norgard, S. Pignari, P. Roselle, R. Vick, M. Violette, K. Williams, Q. Ye, and A. Zimbalatti. Mr. Pettit reviewed Board meeting protocol, the IEEE code of ethics, and upcoming activities. He reminded Board members of the material on the Board FTP site and asked that everyone remember to review this prior to the Board meetings. A moment of silence was observed in memory of the late Jim Klouda, Clayton Paul and Gene Cory.","2162-2264;21622264","","10.1109/MEMC.2012.6397078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6397078","","","","","","0","","","","","","Fourth Quarter 2012","","IEEE","IEEE Journals & Magazines"
"Message from TPC co-chairs","","","2012 24th International Teletraffic Congress (ITC 24)","20121018","2012","","","V","V","On behalf of the Technical Program Committee (TPC), we are proud to present to you an excellent technical program covering a wide range of topics focused on the design, analysis and control of communication networks and computer systems. This year the program includes sessions on new methods and tools for traffic monitoring, bandwidth sharing and caching, content delivery and peer-to-peer, wireless and coding, cloud computing, and queuing models. The technical program is highlighted by three keynote talks to be given by Prof. Hisashi Kobayashi (Princeton University & NICT), Dr. Jean-Pierre Hamaide (Bell Labs, Alcatel-Lucent) and Prof. George Pavlou (University College London, UK) who will present their visions of the future of communication networks and systems. This year 73 papers were submitted by authors from 27 countries, with about 60% of the submissions from Europe, 20% from USA and Canada, 15% from Asia/Pacific and 5% from Africa/Middle East/Latin America. We accepted 24 papers in total, resulting in the acceptance rate of 33%. The review process involved 90 TPC members from both academia and industry. Most papers received 4 reviews. An oversight committee consisting of 15 TPC members led the online discussion phase and made preliminary recommendation through meta-reviews. Based on the reviews and meta-reviews, the TPC co-chairs recommended to accept 14 papers and to reject 26 papers, leaving 33 papers for further discussion.","","Electronic:978-0-9836283-4-7; POD:978-1-4673-1292-9; USB:978-0-9836283-3-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6331806","","","","","","0","","","","","","4-7 Sept. 2012","","IEEE","IEEE Conferences"
"Foreword","","","2012 Third International Workshop on Managing Technical Debt (MTD)","20120628","2012","","","iii","iv","The Third International Workshop on Managing Technical Debt, MTD 2012, was this year co-located with the 34rd International Conference on Software Engineering at Zurich, Switzerland. This is the second year that we are holding this workshop co-located with ICSE. The technical debt metaphor has gained significant traction in the software development community as a way to understand and communicate issues of intrinsic quality, value, and cost in the past few years. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a ""debt:"" on which ""interest"" has to be paid and which should be repaid at some point for the long-term health of the project. Little is known about technical debt, beyond feelings and opinions. The software engineering research community has an opportunity to study this phenomenon and improve the way it is handled. We can offer software engineers a foundation for managing such tradeoffs based on models of their economic impacts. The software engineering community is in the process of building the research agenda around managing technical debt. The purpose of these initial workshops is to bring forward work in progress and ideas from the entire community to collectively vet their validity for the future. In order to support this goal, submissions were open to the members of the program committee as well as the organizing committee. Following a conflict of interest policy, the papers were selected after a peer review by at least three members of the program committee. For this third workshop we accepted 7 full research and 4 short position papers. The accepted submissions cover a range of topics such as: estimating the size and cost of debt, eliciting and visualizing debt, the technical debt landscape ranging from technical debt in software ecosystems to requirements, design and build, and the relationshi- between code defects and debt. Managing technical debt is a broad concern of software engineering that blends research and practice. This can be seen from the program and those involved in the workshop program selection process. To encourage interactive discussion, foster brainstorming and community building the workshop will consist of only short presentations from the accepted papers. These short presentations will provide a basis for the participants to investigate further open research questions and challenges in practice. It is for that purpose the program includes sessions dedicated to open discussion.","","Electronic:978-1-4673-1749-8; POD:978-1-4673-1748-1","10.1109/MTD.2012.6225992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225992","","","","","","0","","","","","","5-5 June 2012","","IEEE","IEEE Conferences"
"Testing Ourselves","L. Sevgi","Ege Vocational Sch., Dept. of Control &amp; Autom. Technol., Ege Univ., Izmir, Turkey","IEEE Antennas and Propagation Magazine","20110303","2010","52","6","222","222","This article discusses preparing tutorials for a review of both the systematic method of microstrip filter design from LC circuits to transmission lines and microstrip lines, and practical design guidelines. This also includes useful computer code and another tutorial on path finding and planning for swarm robotics.","1045-9243;10459243","","10.1109/MAP.2010.5723275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5723275","","","microstrip filters;microstrip lines;state feedback","LC circuits;chaotification;dynamic state reedback;microstrip filter design;microstrip lines;path finding;planning;real systems;swarm robotics;transmission lines","","0","","","","","","Dec. 2010","","IEEE","IEEE Journals & Magazines"
"elytS edoC","D. Spinellis","Athens University of Economics and Business","IEEE Software","20110228","2011","28","2","104","104","This paper presents a elytS edoC - a software used in writing source codes. The code's style encompasses formatting, things like indentation and spacing, commenting, program element order and identifier names. Although most style choices won't affect the compiled code or the program's run time behavior, style is a key aspect of the code's maintainability. And be cause we write code once, but over its life, we read it many times, it pays to keep the code in style that's easy to analyze, compehend, review, test and change.","0740-7459;07407459","","10.1109/MS.2011.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720718","formatting guidelines;programming style","Codes;Economics;Encoding;Programming","software maintenance;source coding","code maintainability;elytS edoC;program run time behavior;source codes","","0","","","","","","March-April 2011","","IEEE","IEEE Journals & Magazines"
"Table of contents","","","2017 24th Asia-Pacific Software Engineering Conference (APSEC)","20180305","2017","","","v","xiii","The following topics are dealt with: program analysis; software engineering; systematic reviews; program traceability; code refactory; code clone; social network; software testing; code patterns; embedded systems; robots; software security; software vulnerability; software management; decision making; software ecosystems; bug localization; repository mining; requirements analysis; model checking; software architecture; software detection; software prediction; software development; software evolution; software maintenance; formal methods; code generation; code quality; and software process.","","Electronic:978-1-5386-3681-7; POD:978-1-5386-3682-4","10.1109/APSEC.2017.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305756","","","data mining;decision making;embedded systems;formal specification;formal verification;program compilers;program diagnostics;program testing;robots;security of data;social networking (online);software architecture;software maintenance;software management;software reliability;systems analysis","bug localization;code clone;code generation;code patterns;code quality;code refactory;decision making;embedded systems;formal methods;model checking;program analysis;program traceability;repository mining;requirements analysis;robots;social network;software architecture;software detection;software development;software ecosystems;software engineering;software evolution;software maintenance;software management;software prediction;software process;software security;software testing;software vulnerability;systematic reviews","","","","","","","","4-8 Dec. 2017","","IEEE","IEEE Conferences"
"Book Reviews [7 Books Reviewed]","J. J. Shea","","IEEE Electrical Insulation Magazine","20170424","2017","33","3","42","47","Books reviewed are: Photovoltaic Laboratory-Safety, Code-Compliance,and Commercial Off-the-Shelf Equipment, by P. T. Parrish, CRC Press Taylor & Francis Group; Dielectrics in Electric Fields,2nd Edition by, G. G. Raju CRC Press Taylor & Francis Group; Advanced Piezoelectric Materials-Science and Technology by K. Uchino, editor Woodhead Publishing; Introduction to Modern Power Electronics, 3rd Edition, by A. M. Trzynadlowski John Wiley & Sons Inc.; Engineering Electrodynamics, by J. Turowski and M. Turowski CRC Press Taylor & Francis Group; Discharge in Long Air Gaps-Modeling and Applications, by A. Beroual and I. Fofana IOP Publishing; Impedance Source Power Electronic Converters by Y. Liu, H. Abu-Rub, F. Blaabjerg, O.Ellabban, and P. C. Loh, John Wiley & Sons Inc.","0883-7554;08837554","","10.1109/MEI.2017.7906166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906166","","Book reviews;Dielectrics;Electric fields;Materials engineering;Photovoltaics;Power electronics","","","","","","","","","","May-June 2017","","IEEE","IEEE Journals & Magazines"
"The Enigma Andred Hodges [Book Reviews]","S. Zabell","","IEEE Technology and Society Magazine","20150916","2015","34","3","7","16","Reports on the life and work of Alan Turing that was presented in this book. Discusses the time in which Alan Turing conducted his work in attempting to crack the enigma code. Perceptions and options of Turing work changed after the publication of this book. Since 1983 a tsunami of information has become available about almost every aspect of the operations of Bletchley Park and its parent organization GCCS (the predecessor of GCHQ, the present-day U.K. signals intelligence and information security organization). This includes the release of the 500+ page ‚ÄúGeneral Report on Tunny‚Äù in 2000 (‚ÄúTunny‚Äù was the code name for an important online German teleprinter encryption device), as well as wartime manuals and technical papers of Turing, Alexander, and others, other archival materials, and memoirs of many people who worked in the different units at Bletchley. The book under review is a reissue of Hodges‚Äôs original 1983 biography.","0278-0097;02780097","","10.1109/MTS.2015.2473980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7270429","","Book reviews;Cryptography;Encoding;Turing, Alan","","","","","","","","","","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Optimal and Robust Scheduling for Networked Control Systems [Bookshelf]","K. G. Vamvoudakis","","IEEE Control Systems","20150316","2015","35","2","101","103","The book under review is unique in that it blends tools known from optimal and robust control theory to design performance-oriented, robust, and reliable NCSs with time-deterministic behavior. The problem of computation complexity of offline optimization is solved through fast stochastic algorithms, while the appropriate network configuration is given through measures of robust performance. The authors also provide practical examples driven mainly from the automotive field to show the effectiveness of their approaches. The book is organized in ten chapters, and most chapters represent results published by the authors in scientific journals. For readers with a basic grasp of optimal control, robust control, and communication theory, this books presents a foundation for designing reliable, optimal, and robust controllers across networks. The remarkable points of this book include the development of an engineering tool for NCSs with guaranteed performance and robustness and that the design techniques are applied to hardware-in-the-loop automotive control systems. The material is successfully presented in a form that is pleasant to read. Control practitioners and engineers will be motivated by this blend of theory and practice to enhance the results while also dealing with new challenges that will arise, such as developing plug-and-play model-free controllers that self-heal and solve the optimization problems online, while also attenuating any disturbances entering the network. Two flaws of the book are that the authors expect a certain degree of mathematical sophistication from the reader, but, at the same time, some mathematical derivations seem unclear. The authors should have provided more introductory chapters or appendices. Moreover, they should make available the Matlab code that they used to generate the simulations.","1066-033X;1066033X","","10.1109/MCS.2014.2385294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7061648","","Book reviews;Control systems;Optimal scheduling;Robust control;Scheduling;Stability analysis","","","","1","","9","","","","April 2015","","IEEE","IEEE Journals & Magazines"
"Advanced Video Communications over Wireless Networks (Zhu, C.; 2013) [Book Reviews]","S. Misra","","IEEE Wireless Communications","20150309","2015","22","1","5","5","This book explores the use of advanced video communications in wireless networks. Wireless devices are fast becoming the device of choice for the majority of computing users. Smartphones, tablets, and notebooks, are changing the Internet-consumer paradigm. Compared to the recent past where most Internet traffic requests emanated from fixed wired devices (desktops/servers), today a majority of the requests are from fixed/mobile wireless devices. A point of note is that the majority of this traffic would consist of video; global IP video traffic will be 79 percent of all consumer Internet traffic in 2018. In addition, the nature of video traffic will change from single-source video on demand-like traffic to multi-source, interactive video communication; each mobile device can become a video source in the future Internet. This has made advancements in wireless/mobile video applications important. Video communication over wireless has to grapple with several stringent constraints: distributed sourcing, resource constrained devices, heterogeneous communication technologies, limited channel capacity, and high error rates in data delivery. This requires a holistic study of video coding and delivery to meet the quality of service (QoS) requirements. In this book, the editors have presented a compilation of 15 peer-reviewed chapters from several well known researchers in the area of wireless video technologies to provide an overview of the state of the art, challenges, and emerging trends.","1536-1284;15361284","","10.1109/MWC.2015.7054710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054710","","Book reviews;Mobile communication;Streaming media;Video coding;Wireless communication;Wireless sensor networks","","","","0","","","","","","February 2015","","IEEE","IEEE Journals & Magazines"
"A word from the general and technical program co-chair","Y. Hong; J. Evans; U. Mitra; E. Viterbo","Monash University, Australia","2014 IEEE Information Theory Workshop (ITW 2014)","20141204","2014","","","v","v","It is our great pleasure to welcome you to Hobart for the 2014 IEEE Information Theory Workshop (ITW). We thank the Technical Program Committee Co-Chairs Urbashi Mitra and Emanuele Viterbo who prepared, with the great help of the Technical Program Committee members, an excellent technical program. We thank all the TPC members and reviewers for their high quality peer reviews of all regular papers. The program covers a broad range of topics in Coding and Information theory with a variety of new applications and includes three parallel sessions.","1662-9019;16629019","Electronic:978-1-4799-5999-0; POD:978-1-4799-6000-2; USB:978-1-4799-5998-3","10.1109/ITW.2014.6970778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970778","","","","","","0","","","","","","2-5 Nov. 2014","","IEEE","IEEE Conferences"
"Book reviews [8 books reviewed]","J. J. Shea","","IEEE Electrical Insulation Magazine","20141104","2014","30","6","52","56","The following eight books are reviewed: Design of Rotating Electrical Machines, 2nd Edition (J. Pyrhonen, T. Jokinen, and V. Hrabovcova; 2014); The Leader's Guide to Speaking with Presence (J. Baldoni; 2014); Laser Spectroscopy for Sensing (M. Baudelet; 2014); Low Temperature Plasma Technology (P. K. Chu and X. Lu; 2014); National Electrical Code 2014 Handbook, 28th Edition (F. P. Hartwell, J. F. McPartland, and B. J. McPartland; 2014); Electric Safety-Practice and Standards (M. El-Sharkawi; 2014); Analyzing Baseball Data with R (M. Marchi and J. Albert; 2014); High-Speed Penetration Dynamics, Engineering Models and Methods (G. Ben-Dor, A. Dubinsky, and T. Elperin; 2013);","0883-7554;08837554","","10.1109/MEI.2014.6943436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943436","","Book reviews;Electric machines;Electrical safety;Electricity supply industry;Laser spectroscopy;Plasmas;Professional communication;Projectiles;Sensors","","","","0","","","","","","November-December 2014","","IEEE","IEEE Journals & Magazines"
"Keynote speaker: Design multi-user Multiple Input Multiple Output (MIMO) systems based on chip-level space-time coded CDMA system","Hsiao-Hwa Chen","Nat. Cheng Kung Univ., Tainan, Taiwan","2014 International Conference on Computing, Management and Telecommunications (ComManTel)","20140605","2014","","","1","1","Summary form only given. This talk is a review on our ongoing research effort to design multi-user Multiple Input Multiple Output (MIMO) systems based on chip-level space-time coded CDMA system. It is characterized by its innovative spreading and space-time codes, namely three dimensional complementary codes (3DCCs), which offer a unique paradigm to design space-time coded signature codes for multi-user MIMO systems. The proposed system achieves interference-resistant performance owing to the unique correlation properties of 3DCCs. It has an ability to offer diversity and multiplexing advantages at the same time with a flexible tradeoff between diversity and multiplexing gains to suit for varying channel conditions and application requirements. Additionally, its multi-dimensional sequence design provides an extra frequency diversity gain for its further performance improvement.","","CD-ROM:978-1-4799-2904-7; Electronic:978-1-4799-2903-0; POD:978-1-4799-5385-1","10.1109/ComManTel.2014.6825563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825563","","","MIMO communication;code division multiple access;radiofrequency interference;space-time codes;wireless channels","3DCC;chip-level space-time coded CDMA system;diversity advantages;frequency diversity gain;interference-resistant performance;multidimensional sequence design;multiplexing;multiuser MIMO systems;multiuser multiple input multiple output systems;space-time coded signature codes;three dimensional complementary codes;varying channel conditions","","0","","","","","","27-29 April 2014","","IEEE","IEEE Conferences"
"Keynote: Formal specification level: Towards verification-driven design based on natural language processing","R. Drechsler; D. Borrione","University of Bremen, Germany","Proceeding of the 2012 Forum on Specification and Design Languages","20121025","2012","","","52","52","The steadily increasing complexity of the design of embedded systems led to the development of both an elaborated design flow that includes various abstraction levels and corresponding methods for synthesis and verification. However, until today the initial system specification is provided in natural language which is manually translated into a formal implementation e.g. at the Electronic System Level (ESL) by means of SystemC in a time-consuming and error-prone process. We envision a design flow which incorporates a Formal Specification Level (FSL) thereby bridging the gap between the informal textbook specification and the formal ESL implementation. Modeling languages such as UML or SysML are envisaged for this purpose. Recent accomplishments towards this envisioned design flow, namely the automatic derivation of formal models from natural language descriptions, verification of formal models in the absence of an implementation, and code generation techniques, are briefly reviewed.","1636-9874;16369874","Electronic:978-2-9530504-5-5; POD:978-1-4673-1240-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6336983","","","","","","0","","","","","","18-20 Sept. 2012","","IEEE","IEEE Conferences"
"Fortranning It with Style [review of ""Modern Fortran: Style and Usage"" (Clerman, N.S. and Spector, W.; 2012)]","J. Moldenhauer","Francis Marion University","Computing in Science & Engineering","20121003","2012","14","5","5","6","The author reviews the book &#x201C;Modern Fortran: Style and Usage&#x201D;, explaining why it's a good reference guide not only for novice Fortran users, but for anyone who wants to improve their code for clarity and performance.","1521-9615;15219615","","10.1109/MCSE.2012.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320571","Fortran;MPI;OOP;OpenMP;object-oriented programming;scientific computing","Book reviews;Computer languages;Fortran","","","","0","","","","","","Sept.-Oct. 2012","","IEEE","IEEE Journals & Magazines"
"Book reviews [7 books reviewed]","J. J. Shea","","IEEE Electrical Insulation Magazine","20120706","2012","28","4","50","54","The following seven books are reviewed: ""National Electrical Safety Code (NESC) 2012 Handbook"" (Marne, D.J.; 2012); ""Handbook of Magnetic Measurements"" (Tumanski, S.; 2011); ""Industrial Tribology"" (Mang, T. and Bartels, T.; 2011); ""Analog Circuit Design"" (Dobkin, B. and Williams, J.; 2011); ""Polymers??A Property Database, 2nd Edition"" (Ellis, B. and Smith, R.; Eds.; 2009); ""Power System Analysis, 2nd Edition"" (Das, J.C.; 2011); and ""Annual Review of Condensed Matter Physics, Volume 3"" (Langer, J.S., Ed.; 2012)","0883-7554;08837554","","10.1109/MEI.2012.6232014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6232014","","Analog integrated circuits;Book reviews;Circuit synthesis;Electrical safety;Magnetic variables measurement;Physics;Polymers;Power system planning","","","","0","","","","","","July-August 2012","","IEEE","IEEE Journals & Magazines"
"Proceedings of the 2012 IEEE International Power Engineering and Optimization Conference (PEOCO2012) [Copyright notice]","","","2012 IEEE International Power Engineering and Optimization Conference Melaka, Malaysia","20120705","2012","","","2","2","Copyright and Reprint Permission: Abstracting is permitted with credit to the source. Libraries are permitted to photocopy beyond the limit of US copyright law for private use of patrons those articles in this volume that carry a code at the bottom of the first page, provided the per-copy fee indicated in the code is paid through Copyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923. For other copying, reprint or republication permission, write to IEEE Copyrights Manager, IEEE Operations Center, 445 Hoes Lane, Piscataway, NJ 08854. All rights reserved. Copyright (c) 2012 by IEEE. All papers of the present volume were peer reviewed by no less that two independent reviewers.","","Electronic:978-1-4673-0662-1; POD:978-1-4673-0660-7","10.1109/PEOCO.2012.6230929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6230929","","","","","","0","","","","","","6-7 June 2012","","IEEE","IEEE Conferences"
"Book reviews [Three books reviewed]","P. Cholda; V. Casares-Giner; F. Martinez-Zaldivar; M. Kantor","","IEEE Communications Magazine","20120606","2012","50","6","12","14","The following books are reviewed: ""Behavior Dynamics in Media-Sharing Social Networks"" (Zhao, H.V. et al; 2011); ""Modulation and Coding Techniques and Wireless Communications"" (Krouk, E. and Semenov, S.; 2011); and ""Enterprise Network Testing: Testing Throughput the Network Lifecycle to Maximize Availability and Performance"" (Sholomon, A. and Kunath, T.; 2011).","0163-6804;01636804","","10.1109/MCOM.2012.6211478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211478","","Book reviews;Encoding;Multiaccess communication;OFDM;Radio frequency;Social network services;Telecommunication network management;Testing;Wireless communication","","","","0","","","","","","June 2012","","IEEE","IEEE Journals & Magazines"
"Network testing series [Series Editorial]","Y. D. Lin; E. Johnson; E. Joo","Computer science, National Chiao Tung University (NCTU), Taiwan","IEEE Communications Magazine","20120305","2012","50","3","138","139","Slightly different from the previous two issues, this issue consists of one contribution from open calls, on reusing simulated code onto the experimented wireless platforms, and two invited contributions from National Chiao Tung University Network Benchmarking Laboratory (NCTU-NBL), on false positive/negative analysis of intrusion detection, and the University of New Hampshire InterOperability Laboratory (UNH-IOL), on interoperability testing of the emerging Ethernet-centric data center bridging. These two laboratories are directed by two of the editors of this series. The invited submissions also went through the review process to check and improve their quality. The major reason we invited our colleagues to submit to this series was based on an observation made in the last issue: Soliciting highly industry-oriented submissions from the test laboratories and test equipment providers might be more promising than soliciting from the manufacturers. Although we do not rule out the possibility of submissions from manufacturers, our experiences have indicated us they are either too busy to write at the quality we require or reluctant to leak test results on their own products. The open calls for future issues are still valid, with two due dates on June 1 and December 1 every year. But we shall continue to invite contributions from the leading test laboratories and test equipment providers. The inputs from NBL and IOL in this issue serve as the first move and also an example in soliciting these types of input.","0163-6804;01636804","","10.1109/MCOM.2012.6163593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6163593","","Database management;Special issues and sections;Statistical analysis;Telecommunication network management;Testing;Wireless communication","","","","0","","","","","","March 2012","","IEEE","IEEE Journals & Magazines"
"Book Reviews [7 books reviewed]","J. J. Shea","","IEEE Electrical Insulation Magazine","20111110","2011","27","6","48","52","The following books are reviewed: High Voltage and Electrical Insulation Engineering (Arora, R. and Mosch, W.; 2011); Submarine Power Cables (Worzyk, T.; 2009); Graphene (Pati, S. K. et al.; 2011); Magnetics, Dielectrics, and Wave Propagation with MATLAB Codes (Vittoria, C.; 2011); Handbook of Surface and Nanometrology, 2nd Edition (Whitehouse, D. J.; 2011); Adaptive and Functional Polymers, Textiles and Their Applications (Hu, J.; 2011); Electricity Generation Using Wind Power (Shepherd, W. and Zhang, L.; 2011).","0883-7554;08837554","","10.1109/MEI.2011.6059987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059987","","Book reviews;Graphene;High-voltage techniques;MATLAB;Magnetics;Metrology;Nanotechnology;Polymers;Power generation;Underwater vehicles","","","","0","","","","","","November-December 2011","","IEEE","IEEE Journals & Magazines"
"So You Think You Understand Magnetism...[Book/Software Reviews]","A. Riddle","","IEEE Microwave Magazine","20110510","2011","12","4","148","150","The reviewed book is Magnetics, Dielectrics, and Wave Propagation with MATLAB Codes (Vittoria, C.; 2011).","1527-3342;15273342","","10.1109/MMM.2011.940604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5764967","","Book reviews;Dielectrics;MATLAB;Magnetics;Propagation","","","","0","","","","","","June 2011","","IEEE","IEEE Journals & Magazines"
"Book Review [7 Book reviews]","J. J. Shea","","IEEE Electrical Insulation Magazine","20101203","2010","26","6","66","69","The seven books reviewed in this issue are: Power System Transients (Martinez-Velasco, J.A., Ed.; 2010); Inorganic Nanowires (Meyyappan, M. and Sunkara, M.K.; 2010; Dielectric and Related Molecular Processes, Volume 2 (Davies, M., Ed.; 1975); Robert Lacoste's The Darker Side (Lacoste, R.; 2010); Nanotechnology - Volume 6: Nanoprobes (Fuchs, H.; 2009); Electrical Codes, Standards, Recommended Practices and Regulations (Alonzo, R.J.; 2010); and Annual Review of Condensed Matter Physics, Volume 1 (Langer, J.S.; 2010).","0883-7554;08837554","","10.1109/MEI.2010.5599984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5599984","","Book reviews;Electrical engineering;Inorganic materials;Molecular electronics;Nanotechnology;Nanowires;Physics;Power system transients;Standards","","","","0","","","","","","November-December 2010","","IEEE","IEEE Journals & Magazines"
"WCRE 2000 Most Influential Paper","M. Balazinska; E. Merlo; M. Dagenais; B. Lague; K. Kontogiannis","Dept. of Comput. Sci. & Eng., Univ. of Washington, Seattle, WA, USA","2010 17th Working Conference on Reverse Engineering","20101129","2010","","","xvi","xvi","Clone detection and re-factoring have grown in importance over the past 10 years. In this talk, we will brief y review the WCRE 2000 work and discuss the advances in the field. The WCRE 2000 paper presented a computer assisted clone re-factoring approach. The process was based on metric-based clone analysis that produced clone clusters. Clones in the same cluster were then compared using token-based dynamic programming (DP) matching. Token-based clone differences, which included insertions, deletions, and substitutions, were then projected on to the ASTs corresponding to clones. Re-factoring opportunities were evaluated using: (1) classification of differences involving superficial differences, signature changes, and type changes, (2) number of differences, and (3) size of candidate clones. Selected clones were automatically re-factored using ‚Äústrategy‚Äù and ‚Äútemplate‚Äù design patterns. Experimental evaluation was performed on JDK1.1.5 from Sun Microsystems. Significant work followed over the next years addressing problems that include matching algorithms, scalability, and integration of clone detection in software engineering activities such as maintenance, evolution, and re-factoring. Several interesting surveys can be found in the literature together with a list of problems, many of which remain open today. In particular, recent work on software clones included new approaches to clone detection based on prefix and suffix trees, approaches to detection involving source code analysis based on latent semantic analysis, and clone identification techniques using analysis of program dependence graphs. In other works, a canonical representation of clones was developed and used for matching and comparison; interesting discussions about harmfulness of clones have also been reported; and empirical studies and evaluations of clone detection approaches can be found in several research papers. Evolution aspects have been taken into consideration- in terms of evolution of clones and their lifetime over several versions of a system and in terms of software evolution by computing various similarity measures between versions. Clone research has also touched upon several interesting applications: intellectual property issues such as license infringement and plagiarism of source code have been addressed using software similarity concepts; incremental approaches to clone detection have been investigated; clones and similarity between structured software artifacts such as trees and graphs has been introduced; detection of bugs caused by inconsistent modifications between clones in a systems and between fragments in several software releases has been investigated; domain specific clones have been studied; and approaches for clone visualization have been proposed. Finally, new specialized workshops and conferences on clones and on mining software repositories have been organized. There are many open problems that remain and possible areas for future work in CLAN (CLone Analysis) toolset including the definition of clones; addressing type III (similar) and simple type IV (semantic) clones; performance and scalability aspects; taxonomies of clones; clone classification and statistics including frequent patterns of similarity in large systems; inconsistent modifications of clones in one version of a system and inconsistent source code changes over several versions of a system leading to a taxonomy of identifiable bugs; clone matching by parallelizing and implementing it on a Graphical Processing Unit (GPU); intellectual property and plagiarism detection using spectral clone analysis; increase recall while maintaining precision; clone maximality issues under thresholds; and more.","1095-1350;10951350","Electronic:978-0-7695-4123-5; POD:978-1-4244-8911-4","10.1109/WCRE.2010.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645476","","","dynamic programming;object-oriented methods;software metrics","advanced clone analysis;clone cluster;clone detection;clone identification;clone maximality;computer assisted clone refactoring;intellectual property;latent semantic analysis;matching algorithm;metric-based clone analysis;plagiarism detection;program dependence graphs;similarity measure;software clone;software engineering;software evolution;software similarity concept;source code analysis;spectral clone analysis;structured software artifact;support object-oriented system refactoring;token-based clone difference;token-based dynamic programming matching","","0","","","","","","13-16 Oct. 2010","","IEEE","IEEE Conferences"
"Distributed Video Coding: Status, challenges and outlook","F. Dufaux","Multimedia Signal Processing Group, Ecole Polytechnique F&#x00E9;d&#x00E9;rale de Lausanne (EPFL), CH-1015, Switzerland","2010 2nd International Conference on Image Processing Theory, Tools and Applications","20100927","2010","","","6","7","This presentation surveys the current status of research activities on Distributed Video Coding (DVC). In particular, we review some recent contributions, identify challenges ahead, and outline perspectives.","2154-5111;21545111","Electronic:978-1-4244-7249-9; POD:978-1-4244-7247-5","10.1109/IPTA.2010.5586828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586828","Distributed Video Coding;coding efficiency;complexity;error resilience;multi-view;scalability","","distributed processing;video coding","coding complexity;coding efficiency;distributed video coding","","0","","8","","","","7-10 July 2010","","IEEE","IEEE Conferences"
"On the effectiveness of unit tests in test-driven development","A. Tosun; M. Ahmed; B. Turhan; N. Juristo","Istanbul Technical University, Istanbul, Turkey","Proceedings of the 2018 International Conference on Software and System Process","20180614","2018","","","113","122","<p>Background: Writing unit tests is one of the primary activities in test-driven development. Yet, the existing reviews report few evidence supporting or refuting the effect of this development approach on test case quality. Lack of ability and skills of developers to produce sufficiently good test cases are also reported as limitations of applying test-driven development in industrial practice. <b>Objective:</b> We investigate the impact of test-driven development on the effectiveness of unit test cases compared to an incremental test last development in an industrial context. <b>Method:</b> We conducted an experiment in an industrial setting with 24 professionals. Professionals followed the two development approaches to implement the tasks. We measure unit test effectiveness in terms of mutation score. We also measure branch and method coverage of test suites to compare our results with the literature. <b>Results:</b> In terms of mutation score, we have found that the test cases written for a test-driven development task have a higher defect detection ability than test cases written for an incremental test-last development task. Subjects wrote test cases that cover more branches on a test-driven development task compared to the other task. However, test cases written for an incremental test-last development task cover more methods than those written for the second task. <b>Conclusion:</b> Our findings are different from previous studies conducted at academic settings. Professionals were able to perform more effective unit testing with test-driven development. Furthermore, we observe that the coverage measure preferred in academic studies reveal different aspects of a development approach. Our results need to be validated in larger industrial contexts.</p>","","","10.1145/3202710.3203153","","","code coverage;empirical study;mutation score;test-driven development;unit testing","","","","","","","","","","","26-27 May 2018","","ACM","ACM Conferences"
"K-12 CS Teacher Certification: What Should New CS Teachers Know and Be Able to Do? (Abstract Only)","J. Owen; C. Fletcher; P. Yongpradit; D. Benedetto","University of Texas at Austin, Montgomery, TX, USA","Proceedings of the 49th ACM Technical Symposium on Computer Science Education","20180611","2018","","","1070","1070","<p>Currently, 27 states and the District of Columbia have some form of teacher certification in computer science. This includes a variety of pathways such as endorsement, certification, licensure or other authorization that explicitly names CS. Many of these states, as well as those that have no CS teacher certification, are in the process of reviewing certification standards and pathways with the goal of increasing the number and quality of K-12 CS instructors and thus, access to high quality CS coursework in K-12. The Praxis exam is one of the few nationally recognized measures of CS teacher content knowledge but the current exam is out of date. A group of state leaders have been working with ETS to update the competencies measured by the Praxis. The objective of this BOF is to bring together stakeholders interested in contributing to the conversation around what a beginning CS teacher should know and be able to do to in CS, provide an overview of what has happened thus far to address these questions, share the recently revised Praxis draft framework, and give individuals an opportunity to provide input on the development of a national consensus related to new K-12 CS teacher competencies.</p>","","","10.1145/3159450.3162171","","","certification;endorsement;licensure;praxis;standards","","","","","","","","","","","21-24 Feb. 2018","","ACM","ACM Conferences"
"Solving Social Media Text Classification Problems Using Code Fragment-Based XCSR","M. H. Arif; J. Li; M. Iqbal","Inst. of Adv. Comput. Technol., Beihang Univ., Beijing, China","2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)","20180607","2017","","","485","492","Sentiment analysis and spam detection of social media text messages are two challenging data analysis tasks due to sparse and high-dimensional feature vectors. Learning classifier systems (LCS) are rule-based evolutionary computing systems and have limited capabilities to handle real valued sparse high-dimensional big data sets. LCS techniques use interval based representations to handle real valued feature vectors. In the work presented here, interval based representation is replaced by genetic programming based tree like structures to classify high-dimensional real valued text feature vectors. Multiple experiments are conducted on different social media text data sets, i.e. tweets, movie reviews, amazon and yelp reviews, SMS and Email spam message to evaluate the proposed scheme. Real valued feature vectors are generated from these data sets using term frequency inverse document frequency and/or sentiment lexicons-based features. Results depicts the supremacy of the new encoding scheme over interval based representations in both small and large social media text data sets.","","Electronic:978-1-5386-3876-7; POD:978-1-5386-3877-4","10.1109/ICTAI.2017.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8371983","Learning Classifier Systems;Sentiment Analysis;Spam Detection;Text Classification","Feature extraction;Sentiment analysis;Social network services;Sociology;Statistics;Task analysis;Tools","data mining;genetic algorithms;learning (artificial intelligence);pattern classification;sentiment analysis;social networking (online)","LCS techniques;code fragment-based XCSR;data analysis tasks;evolutionary computing systems;genetic programming based tree like structures;high-dimensional Big Data sets;high-dimensional feature vectors;high-dimensional real valued text feature vectors;interval based representation;learning classifier systems;sentiment analysis;sentiment lexicons;social media text classification problems;social media text data sets;social media text messages;spam detection;sparse feature vectors;term frequency inverse document frequency;valued feature vectors","","","","","","","","6-8 Nov. 2017","","IEEE","IEEE Conferences"
"Recent Progress in Software Security","E. Amoroso","TAG Cyber","IEEE Software","20180312","2018","35","2","11","13","To reduce cybersecurity risk in software, the security community has widely adopted an approach involving a collage of techniques, tools, and methods, each addressing some aspect of the threat implications of bad code. This article briefly surveys recent progress in each element of this combined approach, including the pros and cons for reducing cybersecurity risk.","0740-7459;07407459","","10.1109/MS.2018.1661316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8314164","DevOps;Invited Content;RASP;malware;malware detection;runtime application self-protection;software development;software engineering;software process maturity;software review;software scanning;software security","Computer bugs;Computer security;Malware;Runtime;Software reliability","security of data;software engineering","bad code;cybersecurity risk;security community;software security;threat implications","","","","","","","","March/April 2018","","IEEE","IEEE Journals & Magazines"
"An efficient visual fiducial localisation system","P. Lightbody; T. Krajn√≠k; M. Hanheide","University of Lincoln, United Kingdom","ACM SIGAPP Applied Computing Review","20180222","2017","17","3","28","37","<p>With use cases that range from external localisation of single robots or robotic swarms to self-localisation in marker-augmented environments and simplifying perception by tagging objects in a robot's surrounding, fiducial markers have a wide field of application in the robotic world. We propose a new family of circular markers which allow for both computationally efficient detection, tracking and identification and full 6D position estimation. At the core of the proposed approach lies the separation of the detection and identification steps, with the former using computationally efficient circular marker detection and the latter utilising an open-ended 'necklace encoding', allowing scalability to a large number of individual markers. While the proposed algorithm achieves similar accuracy to other state-of-the-art methods, its experimental evaluation in realistic conditions demonstrates that it can detect markers from larger distances while being up to two orders of magnitude faster than other state-of-the art fiducial marker detection methods. In addition, the entire system is available as an open-source package at https://github.com/LCAS/whycon.</p>","1559-6915;15596915","","10.1145/3161534.3161537","","","fiducial markers;necklace code;swarm robotics;visual tracking","","","","","","","","","","","September 2017","","ACM","ACM Journals & Magazines"
"Recent Advancements in User Effect Mitigation for Mobile Terminal Antennas: A Review","R. Khan; A. A. Al-Hadi; P. J. Soh","School of Computer and Communication Engineering, Universiti Malaysia Perlis, 02600 Arau, Malaysia&#x00A0;(e-mail: jadoon.rizwankhan57@gmail.com).","IEEE Transactions on Electromagnetic Compatibility","","2018","Early Access","Early Access","1","9","A dramatic evolution can be observed in cellular wireless communication. It started from analog 2G (global system for mobile communication) toward high-data-rate systems such as 3G (wideband code-division multiplexing access), 3.5G (high-speed packet access), 4G (long-term evolution (LTE) and LTE advanced), and currently converging in a more optimized and compact 5G (orthogonal frequency-division multiple access) form. The main challenges for designing of mobile terminal antennas are the compact size requirement of the built-in structure, multiband capabilities, and the number of integrated antennas that form a multiple-input multiple-output (MIMO) terminal system. Moreover, they are required to fulfill all performance and safety standards. For mobile antennas, their radiation efficiency may be affected by the interaction of user head/hand, which consequently affects the correlation of MIMO antenna systems. This paper provides an overview of user effect on mobile terminal antennas in the last seven years. An overview of the Cellular Telecommunication and Internet Association specifications for mobile terminal antennas will first be explained. This is followed by a discussion on the user‚Äôs effects on MIMO parameters, channel capacity on mobile terminal antennas, and millimeter-wave applications prior to the presentation of recent technologies that are aimed at improving the antenna performance. Finally, a perspective and potential future investigations on mobile terminal antennas will be discussed in the conclusion.","0018-9375;00189375","","10.1109/TEMC.2018.2791418","Malaysian Ministry of Science Technology and Innovation under Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268569","Antenna efficiency;SAR;electromagnetic radiation effects;mobile antennas;multielement antenna;multiple-input multiple-output (MIMO) system;mutual coupling","MIMO communication;Mobile antennas;Mobile communication;Mobile handsets;Phantoms;Wireless communication","","","","","","","","","20180124","","","IEEE","IEEE Early Access Articles"
"Empirical evaluations of active learning strategies in legal document review","R. Chhatwal; N. Huber-Fliflet; R. Keeling; J. Zhang; H. Zhao","Legal, AT&T Services, Inc., Washington, D.C. USA","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","1428","1437","One type of machine learning, text classification, is now regularly applied in the legal matters involving voluminous document populations because it can reduce the time and expense associated with the review of those documents. One form of machine learning - Active Learning - has drawn attention from the legal community because it offers the potential to make the machine learning process even more effective. Active Learning, applied to legal documents, is considered a new technology in the legal domain and is continuously applied to all documents in a legal matter until an insignificant number of relevant documents are left for review. This implementation is slightly different than traditional implementations of Active Learning where the process stops once achieving acceptable model performance. The purpose of this paper is twofold: (i) to question whether Active Learning actually is a superior learning methodology and (ii) to highlight the ways that Active Learning can be most effectively applied to real legal industry data. Unlike other studies, our experiments were performed against large data sets taken from recent, real-world legal matters covering a variety of areas. We conclude that, although these experiments show the Active Learning strategy popularly used in legal document review can quickly identify informative training documents, it becomes less effective over time. In particular, our findings suggest this most popular form of Active Learning in the legal arena, where the highest-scoring documents are selected as training examples, is in fact not the most efficient approach in most instances. Ultimately, a different Active Learning strategy may be best suited to initiate the predictive modeling process but not to continue through the entire document review.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258076","CAL;Continuous Active Learning;Machine Learning;TAR;ediscovery;electronic discovery;predictive coding;technology assisted review;text classification","Electronic mail;Law;Predictive coding;Predictive models;Text categorization;Training","data mining;law;learning (artificial intelligence);text analysis","Active Learning strategy;active learning strategies;document review;legal document review;legal documents;legal matter;machine learning process;real-world legal matters;superior learning methodology","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"A feasibility experiment on the application of predictive coding to instant messaging corpora","T. Schoinas; G. Qadir","Legal Technology Solutions, Navigant Consulting Europe Ltd., London, United Kingdom","2017 IEEE International Conference on Big Data (Big Data)","20180115","2017","","","3835","3840","Predictive coding, the term used in the legal industry for document classification using machine learning, presents additional challenges when the dataset comprises instant messages, due to their informal nature and smaller sizes. In this paper, we exploit a data management workflow to group messages into day chats, followed by feature selection and a logistic regression classifier to provide an economically feasible predictive coding solution. We also improve the solution's baseline model performance by dimensionality reduction, with focus on quantitative features. We test our methodology on an Instant Bloomberg dataset, rich in quantitative information. In parallel, we provide an example of the cost savings of our approach.","","Electronic:978-1-5386-2715-0; POD:978-1-5386-2716-7; USB:978-1-5386-2714-3","10.1109/BigData.2017.8258386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258386","dimensionality reduction;electronic discovery;instant bloomberg;predictive coding;technology assisted review","Classification algorithms;Law;Logistics;Numerical models;Predictive coding;Training","electronic messaging;learning (artificial intelligence);pattern classification;regression analysis","Instant Bloomberg dataset;data management workflow;day chats;document classification;economically feasible predictive coding solution;feasibility experiment;feature selection;informal nature;instant messages;instant messaging;legal industry;logistic regression classifier;machine learning;quantitative features;quantitative information","","","","","","","","11-14 Dec. 2017","","IEEE","IEEE Conferences"
"A review paper on digital image forgery detection techniques","N. K. Gill; R. Garg; E. A. Doegar","CSE, NITTTR, Chandigarh, India","2017 8th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","20171214","2017","","","1","7","These days digital image forgery has turned out to be unsophisticated because of capable PCs, propelled image editing softwares and high resolution capturing gadgets. Checking the respectability of pictures and identifying hints of altering without requiring additional pre-embedded information of the picture or pre-installed watermarks are essential examine field. An endeavor is prepared to review the current improvements in the research area of advanced picture fraud detection and comprehensive reference index has been exhibited on passive methods for forgery identification. Passive techniques donot require pre-embedded information in the image. Several image forgery detection techniques are arranged first and after that their summed up organization is produced. Author will review the various image forgery detection techniques along with their results and also compare the various different techniques based on their accuracy.","","Electronic:978-1-5090-3038-5; POD:978-1-5090-3039-2","10.1109/ICCCNT.2017.8203904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8203904","Copy-move forgery detection;Image Forensics;Image splicing;JPEG artifacts","Discrete cosine transforms;Feature extraction;Forgery;Image coding;Splicing;Support vector machines;Transform coding","fraud;image coding;image resolution;image watermarking;security of data","advanced picture fraud detection;comprehensive reference index;digital image forgery detection;forgery identification;high resolution capturing gadgets;image editing softwares;pre-installed watermarks","","","","","","","","3-5 July 2017","","IEEE","IEEE Conferences"
"Simplex Queues for Hot-Data Download","M. F. Aktas; E. Najm; E. Soljanin","Rutgers University, New Brunswick, NJ, USA","ACM SIGMETRICS Performance Evaluation Review","20171207","2017","44","1","35","36","<p>In distributed systems, reliable data storage is accomplished through redundancy, which has traditionally been achieved by simple replication of data across multiple nodes [6]. A special class of erasure codes, known as locally repairable codes (LRCs) [7], has started to replace replication in practice [8], as a more storage-efficient way to provide a desired reliability. It has recently been recognized, that storage redundancy can also provide fast access of stored data (see e.g. [5,9,10] and references therein). Most of these papers consider download scenarios of all jointly encoded pieces of data, and very few [11,12,14] are concerned with download of only some, possibly hot, pieces of data that are jointly encoded with those of less interest. So far, only low traffic regime has been partially addressed.</p> <p>In this paper, we are concerned with hot data download from systems implementing a special class of locally repairable codes, known as LRCs with availability [13,15]. We consider simplex codes, a particular subclass of LRCs with availability, because 1) they are in a certain sense optimal [2] and 2) they are minimally different from replication.</p>","0163-5999;01635999","","10.1145/3143314.3078553","","","availability;locally repairable codes;queues with redundancy","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"A Survey of Naturalistic Programming Technologies","O. Pulido-Prieto; U. Ju√°rez-Mart√≠nez","Instituto Tecnol&#x000F3;gico de Orizaba, Veracruz, M&#233;xico","ACM Computing Surveys (CSUR)","20171207","2017","50","5","1","35","<p>Mainly focused on solving abstraction problems, programming paradigms limit language expressiveness, thus leaving unexplored natural language descriptions that are implicitly expressive. Several authors have developed tools for programming with a natural language subset limited to specific domains to deal with the ambiguity occurring with artificial intelligence technique use. This article presents a review of tools and languages with naturalistic features and highlights the problems that authors have resolved and those they have not addressed, going on to discuss the fact that a ‚Äúnaturalistic‚Äù language based on a well-defined model is not reported.</p>","0360-0300;03600300","","10.1145/3109481","","","Naturalistic programming;automatic source code generation;controlled natural english;expressiveness","","","","","","","","","","","September 2017","","ACM","ACM Journals & Magazines"
"Design-Induced Latency Variation in Modern DRAM Chips: Characterization, Analysis, and Latency Reduction Mechanisms","D. Lee; S. Khan; L. Subramanian; S. Ghose; R. Ausavarungnirun; G. Pekhimenko; V. Seshadri; O. Mutlu","NVIDIA &#38; Carnegie Mellon University, Austin, TX, USA","ACM SIGMETRICS Performance Evaluation Review","20171207","2017","44","1","54","54","<p>Variation has been shown to exist across the cells within a modern DRAM chip. Prior work has studied and exploited several forms of variation, such as manufacturing-process- or temperature-induced variation. We empirically demonstrate a new form of variation that exists within a real DRAM chip, induced by the design and placement of different components in the DRAM chip: different regions in DRAM, based on their relative distances from the peripheral structures, require different minimum access latencies for reliable operation. In particular, we show that in most real DRAM chips, cells closer to the peripheral structures can be accessed much faster than cells that are farther. We call this phenomenon design-induced variation in DRAM. Our goals are to i) understand design-induced variation that exists in real, state-of-the-art DRAM chips, ii) exploit it to develop low-cost mechanisms that can dynamically find and use the lowest latency at which to operate a DRAM chip reliably, and, thus, iii) improve overall system performance while ensuring reliable system operation. To this end, we first experimentally demonstrate and analyze designed-induced variation in modern DRAM devices by testing and characterizing 96 DIMMs (768 DRAM chips). Our experimental study shows that i) modern DRAM chips exhibit design-induced latency variation in both row and column directions, ii) access latency gradually increases in the row direction within a DRAM cell array (mat) and this pattern repeats in every mat, and iii) some columns require higher latency than others due to the internal hierarchical organization of the DRAM chip.</p> <p>Our characterization identifies DRAM regions that are vulnerable to errors, if operated at lower latency, and finds consistency in their locations across a given DRAM chip generation, due to design-induced variation. Variations in the vertical and horizontal dimensions, together, divide the cell array into heterogeneous-latency regions, where cells in s- me regions require longer access latencies for reliable operation. Reducing the latency uniformly across all regions in DRAM would improve performance, but can introduce failures in the inherently slower regions that require longer access latencies for correct operation. We refer to these inherently slower regions of DRAM as design-induced vulnerable regions.</p> <p>Based on our extensive experimental analysis, we develop two mechanisms that reliably reduce DRAM latency. First, DIVI Profiling uses runtime profiling to dynamically identify the lowest DRAM latency that does not introduce failures. DIVA Profiling exploits design-induced variation and periodically profiles only the vulnerable regions to determine the lowest DRAM latency at low cost. It is the first mechanism to dynamically determine the lowest latency that can be used to operate DRAM reliably. DIVA Profiling reduces the latency of read/write requests by 35.1%/57.8%, respectively, at 55C. Our second mechanism, DIVA Shuffling, shuffles data such that values stored in vulnerable regions are mapped to multiple error-correcting code (ECC) codewords. As a result, DIVA Shuffling can correct 26% more multi-bit errors than conventional ECC. Combined together, our two mechanisms reduce read/write latency by 40.0%/60.5%, which translates to an overall system performance improvement of 14.7%/13.7%/13.8% (in 2-/4-/8-core systems) over a variety of workloads, while ensuring reliable operation.</p>","0163-5999;01635999","","10.1145/3143314.3078533","","","DRAM;fault tolerance;latency variation;memory systems","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"Hadoop on Named Data Networking: Experience and Results","M. Gibbens; C. Gniady; L. Ye; B. Zhang","University of Arizona, Tucson, AZ, USA","ACM SIGMETRICS Performance Evaluation Review","20171207","2017","44","1","55","55","<p>In today's data centers, clusters of servers are arranged to perform various tasks in a massively distributed manner: handling web requests, processing scientific data, and running simulations of real-world problems. These clusters are very complex, and require a significant amount of planning and administration to ensure that they perform to their maximum potential. Planning and configuration can be a long and complicated process; once completed it is hard to completely re-architect an existing cluster. In addition to planning the physical hardware, the software must also be properly configured to run on a cluster. Information such as which server is in which rack and the total network bandwidth between rows of racks constrain the placement of jobs scheduled to run on a cluster. Some software may be able to use hints provided by a user about where to schedule jobs, while others may simply place them randomly and hope for the best. Every cluster has at least one bottleneck that constrains the overall performance to less than the optimal that may be achieved on paper. One common bottleneck is the speed of the network: communication between servers in a rack may be unable to saturate their network connections, but traffic flowing between racks or rows in a data center can easily overwhelm the interconnect switches. Various network topologies have been proposed to help mitigate this problem by providing multiple paths between points in the network, but they all suffer from the same fundamental problem: it is cost-prohibitive to build a network that can provide concurrent full network bandwidth between all servers. Researchers have been working on developing new network protocols that can make more efficient use of existing network hardware through a blurring of the line between network layer and applications. One of the most well-known examples of this is Named Data Networking (NDN), a data-centric network architecture that has been in development for - everal years.</p> <p>While NDN has received significant attention for wide-area Internet, a detailed understanding of NDN benefits and challenges in the data center environment has been lacking. The Named Data Networking architecture retrieves content by names rather than connecting to specific hosts. It provides benefits such as highly efficient and resilient content distribution, which fit well to data-intensive distributed computing. This paper presents and discusses our experience in modifying Apache Hadoop, a popular MapReduce framework, to operate on an NDN network. Through this first-of-its-kind implementation process, we demonstrate the feasibility of running an existing, large, and complex piece of distributed software commonly seen in data centers over NDN. We show advantages such as simplified network code and reduced network traffic, which are beneficial in a data center environment. There are also challenges faced by NDN that are being addressed by the community, which can be magnified under data center traffic. Through detailed evaluation, we show a reduction of 16% for overall data transmission between Hadoop nodes while writing data with default replication settings. Preliminary results also show promise for in-network caching of repeated reads in distributed applications. We show that while overall performance is currently slower under NDN, there are challenges and opportunities for further NDN improvements.</p>","0163-5999;01635999","","10.1145/3143314.3078508","","","HADOOP;data centers;emerging technologies;large-scale systems;named data networking","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"An Overview of Time-Based Computing with Stochastic Constructs","M. H. Najafi; S. Jamali-Zavareh; D. J. Lilja; M. D. Riedel; K. Bazargan; R. Harjani","University of Minnesota, Minneapolis","IEEE Micro","20171123","2017","37","6","62","71","Computing on time-based data is a recent evolution of research in stochastic computing. As with stochastic computing, complex functions can be computed with remarkably low area cost. Unlike stochastic computing, the latency and energy efficiency are very favorable compared to computations on conventional binary radix. In this article, the authors review and evaluate the design and implementation of arithmetic operations on time-encoded signals, with a particular focus on low power. The advantages, challenges, and potential applications are discussed.","0272-1732;02721732","","10.1109/MM.2017.4241345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119711","energy-efficient computing;mixed-signal design;pulse width modulation;stochastic computing (SC);time-based computing;time-encoded values;ultra-low-power processing","Clocks;Generators;Hardware;Image coding;Logic gates;Pulse width modulation;Sensors","encoding;signal processing;stochastic processes","arithmetic operations;binary radix;complex functions;energy efficiency;stochastic computing;stochastic constructs;time-based computing;time-based data;time-encoded signals","","","","","","","","November/December 2017","","IEEE","IEEE Journals & Magazines"
"An Evolution of Software Metrics: A Review","R. S. Chhillar; S. Gahlot","Deptt. of Computer Science, Maharshi Dayanand University, Rohtak (India)","Proceedings of the International Conference on Advances in Image Processing","20171107","2017","","","139","143","<p>The main goal of software engineering is to produce good quality reusable software within the given time-period, with minimum cost and that satisfies the user's needs. To enhance the quality of the software, number of techniques should be adopted. In the present scenario lots of quantifiable methods/tools are used to measure the various aspects of the software but with the rapid pace of the technology/paradigms. It requires much more effort and research work with the new paradigms. Software metrics measure different attributes of software like size, complexity, reliability, thereby providing useful information about the external quality aspects of software like reusability, maintainability and testability. This paper discusses the evolution of software metrics from traditional function-oriented to object-oriented to component-based to aspect-oriented paradigm along with advantages and limitations of software metrics. As traditional function-oriented metrics lack in quality parameters like reusability, maintainability and so, object-oriented metrics have become one of the popular concepts in today's software development environment. Object-oriented metrics have been widely accepted because of many attributes like reusability, better abstraction, polymorphism, simplicity, time-saving, cost-effectiveness and easy maintenance in the software code. Similarly, component-based and aspect-oriented metrics take this journey forward to achieve these quality aspects in the software more effectively. This paper also describes how these new paradigms help to improve the software quality, system performance and productivity in software development.</p>","","Electronic:978-1-4503-5295-6","10.1145/3133264.3133297","","","Software Metrics;abstraction;aspect-oriented metrics;component-based metrics;function-oriented metrics;object-oriented metrics;polymorphism;reliability;reusability","","","","","","","","","","","25-27 Aug. 2017","","ACM","ACM Conferences"
"A Review on Mobile Technologies: 3G, 4G and 5G","E. Ezhilarasan; M. Dinakaran","Sch. of Inf. Technol. & Eng., VIT Univ., Vellore, India","2017 Second International Conference on Recent Trends and Challenges in Computational Models (ICRTCCM)","20171005","2017","","","369","373","In the recent decades, wireless communication system development has been changing enormously. The Wireless application services are increasing quickly and the service provider is very hard to manage the user requested services. As per the Ericson mobility report on 2016 utters in 2021, the worldwide mobile subscriptions will accomplish 9,000 million, then W-CDMA and HSPA subscriptions will reach 3,100 million and LTE will attain 4,300 million subscriptions, so in upcoming years 3G and 4G technologies will difficult to handle the mobile traffics. 5G subscriptions are expecting to be commercial technology from 2020 onwards and it can assure 5G is capable of managing the mobile traffic. The purpose of the research is a detailed study on 3G, 4G, and 5G network technologies.","","Electronic:978-1-5090-4799-4; POD:978-1-5090-4800-7","10.1109/ICRTCCM.2017.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8057566","C-RAN;CRN;LTE;MIMO;OFDMA;WI-MAX","5G mobile communication;Computer architecture;Long Term Evolution;Mobile computing;Multiaccess communication;Wireless communication","3G mobile communication;4G mobile communication;5G mobile communication;Long Term Evolution;code division multiple access","3G mobile technology;4G mobile technology;5G mobile technology;5G network technology;Ericson mobility report;HSPA subscriptions;LTE;W-CDMA subscriptions;Wireless application services;mobile traffic;service provider;user requested services;wireless communication system development;worldwide mobile subscriptions","","","","","","","","3-4 Feb. 2017","","IEEE","IEEE Conferences"
"A Kind of Partially Measurable Useful Signal's Optimal Control","X. Xiaonan","Xiamen Univ. Tan Kah Kee Coll., Zhangzhou, China","2016 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)","20170921","2016","","","564","568","The article makes use of nonlinear filter theory and reviews the best control of being quadratic functional linear systems under the condition of incomplete number and continuous time and obtains two best control mathematic models which contain quadratic boss functions under tow cases. It also provides another effective probability statistic method to the best statistic decision.","","CD:978-1-5090-6060-3; Electronic:978-1-5090-6061-0; POD:978-1-5090-6062-7","10.1109/ICITBS.2016.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8047224","majorized algorithm;nonstationary stochastic process;optimal coding;optimal control;optimal decoding;transmission system","Big Data;Smart cities;Transportation","linear systems;nonlinear filters;optimal control;probability;statistical analysis","continuous time condition;control mathematic models;incomplete number condition;nonlinear filter theory;optimal control;quadratic boss functions;quadratic functional linear systems","","","","","","","","17-18 Dec. 2016","","IEEE","IEEE Conferences"
"Security evaluation for block scrambling-based ETC systems against extended jigsaw puzzle solver attacks","T. Chuman; K. Kurihara; H. Kiya","Tokyo Metropolitan University, Tokyo, 191-0065, Japan","2017 IEEE International Conference on Multimedia and Expo (ICME)","20170831","2017","","","229","234","The aim of this paper is to adapt automatic jigsaw puzzle solvers, which are methods of assembling jigsaw puzzles, to the field of information security. Encryption-then-Compression (ETC) systems have been considered for the user-controllable privacy protection of digital images in social network services. Block scrambling-based encryption schemes, which have been proposed to construct ETC systems, have enough key spaces for protecting brute-force attacks. However, each block in encrypted images has almost the same correlation as that of original images. Therefore, it is required to consider the security from different viewpoints from number theory-based encryption methods with provable security such as RSA and DES. In this paper, existing jigsaw puzzle solvers are first reviewed in terms of attacking strategies on encrypted images. Then, an extended jigsaw puzzle solver is considered to improve some limitations of the conventional ones. In the experiments, the jigsaw puzzle solvers are applied to encrypted images to consider the security conditions of the encryption schemes.","","Electronic:978-1-5090-6067-2; POD:978-1-5090-6068-9; USB:978-1-5090-6066-5","10.1109/ICME.2017.8019487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019487","ETC system;JPEG;encryption;jigsaw puzzle","Correlation;Encryption;Image color analysis;Safety;Transforms","cryptography;data privacy;image coding","block scrambling-based ETC systems;brute-force attacks;encryption-then-compression systems;extended jigsaw puzzle solver attacks;provable security;security evaluation;user-controllable privacy protection","","1","","","","","","10-14 July 2017","","IEEE","IEEE Conferences"
"Review of iOS Malware Analysis","Z. Yixiang; Z. Kang","Nat. Eng. Lab. for Mobile Internet Syst. & Applic. Security, China Telecom Corp. Ltd., Shanghai, China","2017 IEEE Second International Conference on Data Science in Cyberspace (DSC)","20170810","2017","","","511","515","The security architecture and distribution of iOS was introduced. The threats to user's data security and privacy from iOS malicious applications were analyzed. The reasons that the number of iOS malicious applications is less than Android malicious applications were discussed. The attacking methods of emerging iOS malicious applications were summed up. And prospect for future mitigation and confrontation strategies was given.","","Electronic:978-1-5386-1600-0; POD:978-1-5386-1601-7; USB:978-1-5386-1599-7","10.1109/DSC.2017.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005524","iOS security;malicious application;malicious code","Kernel;Malware;Mobile communication;Security;Smart phones","Android (operating system);data privacy;invasive software","Android malicious applications;attacking methods;confrontation strategies;data privacy;iOS malware analysis;mitigation strategies;security architecture;user data security","","","","","","","","26-29 June 2017","","IEEE","IEEE Conferences"
"A Review on Behavior-Based Detection for Network Threats","X. Sun; Z. Wang; B. Lv; J. Ou","Electr. Power Res. Inst. of State Grid, Zhejiang Electr. Power Co., Hangzhou, China","2017 ieee 3rd international conference on big data security on cloud (bigdatasecurity), ieee international conference on high performance and smart computing (hpsc), and ieee international conference on intelligent data and security (ids)","20170717","2017","","","127","132","With the appearance and development of the technology of malicious codes and other unknown threats, information security has drawn people's attention. In this paper, we investigate on behavior-based detection which is different from traditional static detection technology. Firstly, we discuss the procedure in detail, especially feature extraction and classification. Several machine learning methods such as SVM, bayesian decision are adopted to solve these problems. What's more, we outline the architecture to specify operating mechanism. The features and conclusion are discussed finally.","","Electronic:978-1-5090-6296-6; POD:978-1-5090-6297-3","10.1109/BigDataSecurity.2017.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7980329","behavior-based detection;classification;feature extraction;machine learning;malicious codes","Bayes methods;Entropy;Feature extraction;Malware;Security;Support vector machines;Training","learning (artificial intelligence);pattern classification;security of data;support vector machines","Bayesian decision;SVM;behavior-based detection;feature classification;feature extraction;information security;machine learning methods;network threat;static detection technology;support vector machines","","","","","","","","26-28 May 2017","","IEEE","IEEE Conferences"
"Four decades of space-borne radio sounding","R. Benson","Geospace Physics Laboratory (Code 673), NASA Goddard Space Flight Center, Greenbelt, Maryland 20771, USA","URSI Radio Science Bulletin","20170707","2010","2010","332","24","44","A review is given of the 38 rocket, satellite, and planetary payloads dedicated to ionospheric/magnetospheric radio sounding since 1961. Between1961 and 1995, eleven sounding-rocket payloads from four countries evolved from proof-of-concept flights to sophisticated instruments. Some involved dual payloads, with the sounder transmitter on one and the sounder receiver on the other. The rocket sounders addressed specific space-plasma-wave questions, and provided improved measurements of ionospheric electron-density (N<inf>e</inf>) field-aligned irregularities (FAI). Four countries launched 12 ionospheric topside-sounder satellites between 1962 and 1994, and an ionospheric sounder was placed on the Mir Space Station in 1998. Eleven magnetospheric radio sounders, most of the relaxation type, were launched from 1977 to 2000. The relaxation sounders used low-power transmitters, designed to stimulate plasma resonances for accurate local N<inf>e</inf> determinations. The latest magnetospheric sounder designed for remote sensing incorporated long antennas and digital signal processing techniques to overcome the challenges posed by low N<inf>e</inf> values and large propagation distances. Three radio sounders from three countries were included on payloads to extraterrestrial destinations from 1990 to 2003. The scientific accomplishments of space-borne radio sounders included (1) a wealth of global N<inf>e</inf> information on the topside ionosphere and magnetosphere, based on vertical and magnetic-field-aligned N<inf>e</inf> profiles; (2) accurate in-situ N<inf>e</inf> values, even under low-density conditions; and (3) fundamental advances in our understanding of the excitation and propagation of plasma waves, which have even led to the prediction of a new plasma-wave mode.","1024-4530;10244530","","10.23919/URSIRSB.2010.7909284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7909284","","Magnetic resonance imaging;Magnetosphere;Plasmas;Rockets;Satellite broadcasting;Satellites;Space vehicles","","","","","","","","","","June 2010","","URSI","URSI Journals & Magazines"
"Optimizing CNNs on Multicores for Scalability, Performance and Goodput","S. Rajbhandari; Y. He; O. Ruwase; M. Carbin; T. Chilimbi","The Ohio State University, Columbus, OH, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","267","280","<p>Convolutional Neural Networks (CNN) are a class of Ar- tificial Neural Networks (ANN) that are highly efficient at the pattern recognition tasks that underlie difficult AI prob- lems in a variety of domains, such as speech recognition, object recognition, and natural language processing. CNNs are, however, computationally intensive to train. This paper presents the first characterization of the per- formance optimization opportunities for training CNNs on CPUs. Our characterization includes insights based on the structure of the network itself (i.e., intrinsic arithmetic inten- sity of the convolution and its scalability under parallelism) as well as dynamic properties of its execution (i.e., sparsity of the computation).</p> <p>Given this characterization, we present an automatic framework called spg-CNN for optimizing CNN training on CPUs. It comprises of a computation scheduler for efficient parallel execution, and two code generators: one that opti- mizes for sparsity, and the other that optimizes for spatial reuse in convolutions.</p> <p>We evaluate spg-CNN using convolutions from a variety of real world benchmarks, and show that spg-CNN can train CNNs faster than state-of-the-art approaches by an order of magnitude.</p>","0163-5980;01635980","","10.1145/3093315.3037745","","","avx;convolution kernel generation;convolution neural network;convolution with sparsity;cpu;cpu optimization;deep learning;optimizing convolution;parallel gemm;simd;sparsity","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"Browsix: Bridging the Gap Between Unix and the Browser","B. Powers; J. Vilk; E. D. Berger","University of Massachusetts Amherst, Amherst, MA, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","253","266","<p>Applications written to run on conventional operating systems typically depend on OS abstractions like processes, pipes, signals, sockets, and a shared file system. Porting these applications to the web currently requires extensive rewriting or hosting significant portions of code server-side because browsers present a nontraditional runtime environment that lacks OS functionality.</p> <p>This paper presents Browsix, a framework that bridges the considerable gap between conventional operating systems and the browser, enabling unmodified programs expecting a Unix-like environment to run directly in the browser. Browsix comprises two core parts: (1) a JavaScript-only system that makes core Unix features (including pipes, concurrent processes, signals, sockets, and a shared file system) available to web applications; and (2) extended JavaScript runtimes for C, C++, Go, and Node.js that support running programs written in these languages as processes in the browser. Browsix supports running a POSIX shell, making it straightforward to connect applications together via pipes.</p> <p>We illustrate Browsix's capabilities via case studies that demonstrate how it eases porting legacy applications to the browser and enables new functionality. We demonstrate a Browsix-enabled LaTeX editor that operates by executing unmodified versions of pdfLaTeX and BibTeX. This browser-only LaTeX editor can render documents in seconds, making it fast enough to be practical. We further demonstrate how Browsix lets us port a client-server application to run entirely in the browser for disconnected operation. Creating these applications required less than 50 lines of glue code and no code modifications, demonstrating how easily Browsix can be used to build sophisticated web applications from existing parts without modification.</p>","0163-5980;01635980","","10.1145/3093315.3037727","","","browser;browsix;javascript;operating system","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"An Architecture Supporting Formal and Compositional Binary Analysis","J. McMahan; M. Christensen; L. Nichols; J. Roesch; S. Y. Guo; B. Hardekopf; T. Sherwood","University of California, Santa Barbara, Santa Barbara, CA, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","177","191","<p>Building a trustworthy life-critical embedded system requires deep reasoning about the potential effects that sequences of machine instructions can have on full system operation. Rather than trying to analyze complete binaries and the countless ways their instructions can interact with one another --- memory, side effects, control registers, implicit state, etc. --- we explore a new approach. We propose an architecture controlled by a thin computational layer designed to tightly correspond with the lambda calculus, drawing on principles of functional programming to bring the assembly much closer to myriad reasoning frameworks, such as the Coq proof assistant. This approach allows assembly-level verified versions of critical code to operate safely in tandem with arbitrary code, including imperative and unverified system components, without the need for large supporting trusted computing bases. We demonstrate that this computational layer can be built in such a way as to simultaneously provide full programmability and compact, precise, and complete semantics, while still using hardware resources comparable to normal embedded systems. To demonstrate the practicality of this approach, our FPGA-implemented prototype runs an embedded medical application which monitors and treats life-threatening arrhythmias. Though the system integrates untrusted and imperative components, our architecture allows for the formal verification of multiple properties of the end-to-end system, including a proof of correctness of the assembly-level implementation of the core algorithm, the integrity of trusted data via a non-interference proof, and a guarantee that our prototype meets critical timing requirements.</p>","0163-5980;01635980","","10.1145/3093315.3037733","","","assembly analysis;binary verification;formal methods;functional programming;heterogeneous architecture;isa semantics;non-interference;static analysis","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"Sound Loop Superoptimization for Google Native Client","B. Churchill; R. Sharma; J. Bastien; A. Aiken","Stanford University, Stanford, CA, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","313","326","<p>Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google.</p>","0163-5980;01635980","","10.1145/3093315.3037754","","","assembly;bounded verification;data-driven verification;equivalence checking;native client;superoptimization;verification;x86-64","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"CHERI JNI: Sinking the Java Security Model into the C","D. Chisnall; B. Davis; K. Gudka; D. Brazdil; A. Joannou; J. Woodruff; A. T. Markettos; J. E. Maste; R. Norton; S. Son; M. Roe; S. W. Moore; P. G. Neumann; B. Laurie; R. N. M. Watson","University of Cambridge, Cambridge, United Kingdom","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","569","583","<p>Java provides security and robustness by building a high-level security model atop the foundation of memory protection. Unfortunately, any native code linked into a Java program -- including the million lines used to implement the standard library -- is able to bypass both the memory protection and the higher-level policies. We present a hardware-assisted implementation of the Java native code interface, which extends the guarantees required for Java's security model to native code.</p> <p>Our design supports safe direct access to buffers owned by the JVM, including hardware-enforced read-only access where appropriate. We also present Java language syntax to declaratively describe isolated compartments for native code.</p> <p>We show that it is possible to preserve the memory safety and isolation requirements of the Java security model in C code, allowing native code to run in the same process as Java code with the same impact on security as running equivalent Java code. Our approach has a negligible impact on performance, compared with the existing unsafe native code interface. We demonstrate a prototype implementation running on the CHERI microprocessor synthesized in FPGA.</p>","0163-5980;01635980","","10.1145/3093315.3037725","","","architecture;capability systems;cheri;compartmentalization;compilers;hardware security;java;jni;language security;memory protection;sandboxing","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"3DGates: An Instruction-Level Energy Analysis and Optimization of 3D Printers","J. Ajay; C. Song; A. S. Rathore; C. Zhou; W. Xu","University at Buffalo, Buffalo, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","419","433","<p>As the next-generation manufacturing driven force, 3D printing technology is having a transformative effect on various industrial domains and has been widely applied in a broad spectrum of applications. It also progresses towards other versatile fields with portable battery-powered 3D printers working on a limited energy budget. While reducing manufacturing energy is an essential challenge in industrial sustainability and national economics, this growing trend motivates us to explore the energy consumption of the 3D printer for the purpose of energy efficiency. To this end, we perform an in-depth analysis of energy consumption in commercial, off-the-shelf 3D printers from an instruction-level perspective. We build an instruction-level energy model and an energy profiler to analyze the energy cost during the fabrication process. From the insights obtained by the energy profiler, we propose and implement a cross-layer energy optimization solution, called 3DGates, which spans the instruction-set, the compiler and the firmware. We evaluate 3DGates over 338 benchmarks on a 3D printer and achieve an overall energy reduction of 25%.</p>","0163-5980;01635980","","10.1145/3093315.3037752","","","3d printers;energy characterization and optimization;g-code instruction profiling","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"Mallacc: Accelerating Memory Allocation","S. Kanev; S. L. Xi; G. Y. Wei; D. Brooks","Harvard Unirevsity, Cambridge, MA, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","33","45","<p>Recent work shows that dynamic memory allocation consumes nearly 7% of all cycles in Google datacenters. With the trend towards increased specialization of hardware, we propose Mallacc, an in-core hardware accelerator designed for broad use across a number of high-performance, modern memory allocators. The design of Mallacc is quite different from traditional throughput-oriented hardware accelerators. Because memory allocation requests tend to be very frequent, fast, and interspersed inside other application code, accelerators must be optimized for latency rather than throughput and area overheads must be kept to a bare minimum. Mallacc accelerates the three primary operations of a typical memory allocation request: size class computation, retrieval of a free memory block, and sampling of memory usage. Our results show that malloc latency can be reduced by up to 50% with a hardware cost of less than 1500 um2 of silicon area, less than 0.006% of a typical high-performance processor core.</p>","0163-5980;01635980","","10.1145/3093315.3037736","","","accelerators;datacenter tax;memory allocation","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"GRIFFIN: Guarding Control Flows Using Intel Processor Trace","X. Ge; W. Cui; T. Jaeger","Microsoft Research, Redmond, WA, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","585","598","<p>Researchers are actively exploring techniques to enforce control-flow integrity (CFI), which restricts program execution to a predefined set of targets for each indirect control transfer to prevent code-reuse attacks. While hardware-assisted CFI enforcement may have the potential for advantages in performance and flexibility over software instrumentation, current hardware-assisted defenses are either incomplete (i.e., do not enforce all control transfers) or less efficient in comparison. We find that the recent introduction of hardware features to log complete control-flow traces, such as Intel Processor Trace (PT), provides an opportunity to explore how efficient and flexible a hardware-assisted CFI enforcement system may become. While Intel PT was designed to aid in offline debugging and failure diagnosis, we explore its effectiveness for online CFI enforcement over unmodified binaries by designing a parallelized method for enforcing various types of CFI policies. We have implemented a prototype called GRIFFIN in the Linux 4.2 kernel that enables complete CFI enforcement over a variety of software, including the Firefox browser and its jitted code. Our experiments show that GRIFFIN can enforce fine-grained CFI policies with shadow stack as recommended by researchers at a performance that is comparable to software-only instrumentation techniques. In addition, we find that alternative logging approaches yield significant performance improvements for trace processing, identifying opportunities for further hardware assistance.</p>","0163-5980;01635980","","10.1145/3093315.3037716","","","control-flow integrity;intel processor trace","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"Towards Practical Default-On Multi-Core Record/Replay","A. J. Mashtizadeh; T. Garfinkel; D. Terei; D. Mazieres; M. Rosenblum","Stanford University, Stanford, CA, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","693","708","<p>We present Castor, a record/replay system for multi-core applications that provides consistently low and predictable overheads. With Castor, developers can leave record and replay on by default, making it practical to record and reproduce production bugs, or employ fault tolerance to recover from hardware failures.</p> <p>Castor is inspired by several observations: First, an efficient mechanism for logging non-deterministic events is critical for recording demanding workloads with low overhead. Through careful use of hardware we were able to increase log throughput by 10x or more, e.g., we could record a server handling 10x more requests per second for the same record overhead. Second, most applications can be recorded without modifying source code by using the compiler to instrument language level sources of non-determinism, in conjunction with more familiar techniques like shared library interposition. Third, while Castor cannot deterministically replay all data races, this limitation is generally unimportant in practice, contrary to what prior work has assumed.</p> <p>Castor currently supports applications written in C, C++, and Go on FreeBSD. We have evaluated Castor on parallel and server workloads, including a commercial implementation of memcached in Go, which runs Castor in production.</p>","0163-5980;01635980","","10.1145/3093315.3037751","","","fault-tolerance;multi-core replay;replay debugging","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"REDSPY: Exploring Value Locality in Software","S. Wen; M. Chabbi; X. Liu","College of William &#38; Mary, Williamsburg, VA, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","47","61","<p>Complex code bases with several layers of abstractions have abundant inefficiencies that affect the execution time. Value redundancy is a kind of inefficiency where the same values are repeatedly computed, stored, or retrieved over the course of execution. Not all redundancies can be easily detected or eliminated with compiler optimization passes due to the inherent limitations of the static analysis.</p> <p>Microscopic observation of whole executions at instruction- and operand-level granularity breaks down abstractions and helps recognize redundancies that masquerade in complex programs. We have developed REDSPY---a fine-grained profiler to pinpoint and quantify redundant operations in program executions. Value redundancy may happen over time at same locations or in adjacent locations, and thus it has temporal and spatial locality. REDSPY identifies both temporal and spatial value locality. Furthermore, REDSPY is capable of identifying values that are approximately the same, enabling optimization opportunities in HPC codes that often use floating point computations. REDSPY provides intuitive optimization guidance by apportioning redundancies to their provenance---source lines and execution calling contexts. REDSPY pinpointed dramatically high volume of redundancies in programs that were optimization targets for decades, such as SPEC CPU2006 suite, Rodinia benchmark, and NWChem---a production computational chemistry code. Guided by REDSPY, we were able to eliminate redundancies that resulted in significant speedups.</p>","0163-5980;01635980","","10.1145/3093315.3037729","","","approximate computing;cctlib;performance tools;pin;redundancy detection and elimination;value profiling","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"Graspan: A Single-machine Disk-based Graph System for Interprocedural Static Analyses of Large-scale Systems Code","K. Wang; A. Hussain; Z. Zuo; G. Xu; A. Amiri Sani","University of California, Irvine, Irvine, CA, USA","ACM SIGOPS Operating Systems Review","20170615","2017","51","2","389","404","<p>There is more than a decade-long history of using static analysis to find bugs in systems such as Linux. Most of the existing static analyses developed for these systems are simple checkers that find bugs based on pattern matching. Despite the presence of many sophisticated interprocedural analyses, few of them have been employed to improve checkers for systems code due to their complex implementations and poor scalability. In this paper, we revisit the scalability problem of interprocedural static analysis from a ""Big Data"" perspective. That is, we turn sophisticated code analysis into Big Data analytics and leverage novel data processing techniques to solve this traditional programming language problem. We develop Graspan, a disk-based parallel graph system that uses an edge-pair centric computation model to compute dynamic transitive closures on very large program graphs.</p> <p>We implement context-sensitive pointer/alias and dataflow analyses on Graspan. An evaluation of these analyses on large codebases such as Linux shows that their Graspan implementations scale to millions of lines of code and are much simpler than their original implementations. Moreover, we show that these analyses can be used to augment the existing checkers; these augmented checkers uncovered 132 new NULL pointer bugs and 1308 unnecessary NULL tests in Linux 4.4.0-rc5, PostgreSQL 8.3.9, and Apache httpd 2.2.18.</p>","0163-5980;01635980","","10.1145/3093315.3037744","","","disk-based systems;graph processing;static analysis","","","","","","","","","","","June 2017","","ACM","ACM Journals & Magazines"
"Towards clustering-based device-to-device communications for supporting applications","A. Hematian; W. Yu; C. Lu; D. Griffith; N. Golmie","Towson Univ.","ACM SIGAPP Applied Computing Review","20170608","2017","17","1","35","48","<p>In this paper, we address the issue of how to leverage Wi-Fi Direct (as an outband solution) to enable the Device-to-Device (D2D) communication, which can not only offload massive data traffic from the LTE (Long Term Evolution)-based cellular network, but also support the communications of Internet-of-Things (IoT) applications. Particularly, we develop a clustering-based scheme that automatically finds the best candidates to remain connected to the LTE network while the rest of the devices can be disconnected directly from the LTE-based cellular network. By doing so, we can reduce the signal interference, increase the average throughput and spectral efficiency of the network, and also reduce unnecessary data traffic that can be transmitted locally by D2D communications instead of going through the LTE-based cellular network. Devices in established clusters can indirectly communicate with the LTE network via the cluster head, which can be dynamically selected and remains connected to the LTE network directly. Using the real-world cellular data collected from a public database related to the deployment of LTE networks, and Google geo-coding APIs (Application Program Interfaces) to locate the real-world smart meters, we show the effectiveness of our proposed scheme based on an extensive study of two scenarios: the first is in traffic offloading in the cellular network, and the second is in IoT applications, specifically, smart grid communications.</p>","1559-6915;15596915","","10.1145/3090058.3090063","","","applications;clustering;device-to-device communication;traffic offloading","","","","","","","","","","","March 2017","","ACM","ACM Journals & Magazines"
"The Emotional Work of Doing eHealth Research","M. K. Wolters; Z. Mkulo; P. M. Boynton","University of Edinburgh, Edinburgh, United Kingdom","Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems","20170608","2017","","","816","826","<p>Within Human-Computer Interaction (HCI), researchers have become more aware of the interplay between the work they are doing and their own health and wellbeing. These issues have been discussed mostly in the context of HCI research around sensitive issues (Sensitive HCI). We argue that researcher wellbeing needs to be considered in all eHealth and mHealth research. Here, we focus on the emotional labour required by the political and organisational structures of eHealth research, and illustrate it with autoethnographic observations from two research studies, a trial of an eMentalHealth intervention, and an evaluation of a health insurance scheme in Tanzania. Based on a review of relevant literature from health and social care research, we suggest a process for supporting emotional work that can be adapted to other application contexts.</p>","","","10.1145/3027063.3052764","","","codes of good practice;ehealth;emotional labour;research process;researcher wellbeing;sensitive hci","","","","","","","","","","","6-11 May 2017","","ACM","ACM Conferences"
"SIFT Meets CNN: A Decade Survey of Instance Retrieval","L. Zheng; Y. Yang; Q. Tian","Centre for AI, University of Technology at Sydney, Ultimo, NSW, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","20180402","2018","40","5","1224","1244","In the early days, content-based image retrieval (CBIR) was studied with global features. Since 2003, image retrieval based on local descriptors (de facto SIFT) has been extensively studied for over a decade due to the advantage of SIFT in dealing with image transformations. Recently, image representations based on the convolutional neural network (CNN) have attracted increasing interest in the community and demonstrated impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of instance retrieval over the last decade. Two broad categories, SIFT-based and CNN-based methods, are presented. For the former, according to the codebook size, we organize the literature into using large/medium-sized/small codebooks. For the latter, we discuss three lines of methods, i.e., using pre-trained or fine-tuned CNN models, and hybrid methods. The first two perform a single-pass of an image to the network, while the last category employs a patch-based feature extraction scheme. This survey presents milestones in modern instance retrieval, reviews a broad selection of previous works in different categories, and provides insights on the connection between SIFT and CNN-based methods. After analyzing and comparing retrieval performance of different categories on several datasets, we discuss promising directions towards generic and specialized instance retrieval.","0162-8828;01628828","","10.1109/TPAMI.2017.2709749","ARO; Google Faculty Award and the Data to Decisions Cooperative Research Centre; NEC Laboratories of America and Blippar; National Science Foundation of China (NSFC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935507","Instance retrieval;SIFT;convolutional neural network;literature survey","Computational modeling;Detectors;Encoding;Feature extraction;Image retrieval;Indexes;Visualization","content-based retrieval;feature extraction;feedforward neural nets;image classification;image coding;image representation;image retrieval;learning (artificial intelligence);transforms","SIFT;codebook size;content-based image retrieval;convolutional neural network;feature extraction scheme;fine-tuned CNN models;generic instance retrieval;global features;hybrid methods;image representations;image transformations;local descriptors;modern instance retrieval;retrieval performance;specialized instance retrieval","","3","","","","","20170530","May 1 2018","","IEEE","IEEE Journals & Magazines"
"The problem of multimodality: what data-driven research can tell us about online writing practices","A. M. Licastro","Stevenson University","Communication Design Quarterly Review","20170511","2016","4","4","55","73","<p>This article investigates the writing mode, multimodal aspects, and folksonomic elements of digital composition gathered from a WordPress-based ePortfolio platform.* Focusing on the student perspective, data was gathered through both surveys of first year students and text analysis of digital compositions in order to produce quantitative results that can be replicated and aggregated. This research demonstrates the impact of assignment design and platform affordances on student composition practices. Results show that incoming students do not fit the ""digital native"" myth, nor are they prepared to engage in digital scholarship at the college level without significant guidance and specific requirements that scaffold digital work.</p>","2166-1200;21661200","","10.1145/3071088.3071094","","","coding;composition;data;ePortfolios;multimodality;pedagogy","","","","","","","","","","","December 2016","","ACM","ACM Journals & Magazines"
"Distributed Power-Generation Systems and Protection","F. Blaabjerg; Y. Yang; D. Yang; X. Wang","Department of Energy Technology, Aalborg University, Aalborg, Denmark","Proceedings of the IEEE","20170616","2017","105","7","1311","1331","Continuously expanding deployments of distributed power-generation systems (DPGSs) are transforming the conventional centralized power grid into a mixed distributed electrical network. The modern power grid requires flexible energy utilization but presents challenges in the case of a high penetration degree of renewable energy, among which wind and solar photovoltaics are typical sources. The integration level of the DPGS into the grid plays a critical role in developing sustainable and resilient power systems, especially with highly intermittent renewable energy resources. To address the challenging issues and, more importantly, to leverage the energy generation, stringent demands from both utility operators and consumers have been imposed on the DPGS. Furthermore, as the core of energy conversion, numerous power electronic converters employing advanced control techniques have been developed for the DPGS to consolidate the integration. In light of the above, this paper reviews the power-conversion and control technologies used for DPGSs. The impacts of the DPGS on the distributed grid are also examined, and more importantly, strategies for enhancing the connection and protection of the DPGS are discussed.","0018-9219;00189219","","10.1109/JPROC.2017.2696878","10.13039/100008398 - VILLUM FONDEN; 10.13039/501100000781 - European Research Council (ERC); 10.13039/501100000781 - European Union&#8217;s Seventh Framework Program (FP/20072013)/ERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926394","Control;distributed power-generation systems (DPGSs);grid codes;grid resilience;photovoltaic (PV) power systems;power conversion;power electronics;power grid protection;wind power generation","Generators;Power electronics;Power generation;Power grids;Renewable energy sources;Resilience;Wind power generation;Wind turbines","distributed power generation;power control;power distribution control;power distribution protection","DPGS;advanced control techniques;distributed grid;distributed power-generation protection;distributed power-generation systems;power control technologies;power electronic converters;power-conversion","","4","","","","","20170511","July 2017","","IEEE","IEEE Journals & Magazines"
"Iris: An optimized I/O stack for low-latency storage devices","A. Papagiannis; G. Saloustros; M. Marazakis; A. Bilas","Institute of Computer Science, FORTH (ICS), Heraklion, Greece","ACM SIGOPS Operating Systems Review","20170504","2016","50","3","3","11","<p>System software overheads in the I/O path, including VFS and file system code, become more pronounced with emerging low-latency storage devices. Currently, these overheads constitute the main bottleneck in the I/O path and they limit efficiency of modern storage systems. In this paper we present a taxonomy of the current state-of-the-art systems on accelerating accesses to fast storage devices. Furthermore, we present Iris, a new I/O path for applications, that minimizes overheads from system software in the common I/O path. The main idea is the separation of the control and data planes. The control plane consists of an unmodified Linux kernel and is responsible for handling data plane initialization and the normal processing path through the kernel for non-file related operations. The data plane is a lightweight mechanism to provide direct access to storage devices with minimum overheads and without sacrificing strong protection semantics. Iris requires neither hardware support from the storage devices nor changes in user applications. We evaluate our early prototype and we find that it achieves on a single core up to 1:7x and 2:2x better read and write random IOPS, respectively, compared to the XFS and EXT4 file systems. It also scales with the number of cores; using 4 cores Iris achieves 1:84x and 1:96x better read and write random IOPS, respectively. In sequential reads we provide similar performance and in sequential writes we are about 20% better compared to other file systems.</p>","0163-5980;01635980","","10.1145/3041710.3041713","","","I/O;NVM;low latency;protection;storage systems","","","","","","","","","","","December 2016","","ACM","ACM Journals & Magazines"
"Taxonomy of malware detection techniques: A systematic literature review","H. M. Deylami; R. C. Muniyandi; I. T. Ardekani; A. Sarrafzadeh","School of Computer Science, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, 43600 UKM Bangi, Malaysia","2016 14th Annual Conference on Privacy, Security and Trust (PST)","20170424","2016","","","629","636","Malware is an international software disease. Research shows that the effect of malware is becoming chronic. To protect against malware detectors are fundamental to the industry. The effectiveness of such detectors depends on the technology used. Therefore, it is paramount that the advantages and disadvantages of each type of technology are scrutinized analytically. This study's aim is to scrutinize existing publications on this subject and to follow the trend that has taken place in the advancement and development with reference to the amount of information and sources of such literature. Many of the malware programs are huge and complicated and it is not easy to comprehend the details. Dissemination of malware information among users of the Internet and also training them to correctly use anti-malware products are crucial to protecting users from the malware onslaught. This paper will provide an exhaustive bibliography of methods to assist in combating malware.","","Electronic:978-1-5090-4379-8; POD:978-1-5090-4380-4","10.1109/PST.2016.7906998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906998","Anomaly-based;Malicious code;Malware;Signature-based;System requirements;Taxonomy","Computers;Databases;Detectors;Internet;Malware;Unsolicited electronic mail","computer network security;invasive software","international software disease;malware detection techniques;malware information;malware programs","","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conferences"
"Eliph: Effective Visualization of Code History for Peer Assessment in Programming Education","J. Park; Y. H. Park; S. Kim; A. Oh","Korea Advanced Institute of Science and Technology, Daejoen, Republic of Korea","Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing","20170417","2017","","","458","467","<p>In this paper, we investigate the effectiveness of visualization of code history on peer assessment in computer science education. Peer assessment is found to be an effective learning tool for programming education. While many systems are proposed to support peer assessment in programming education, little effort has been devoted to finding ways to improve the peer assessment by assisting the students to understand the programs they are assessing. We introduce Eliph, a web-based peer assessment system for programming education with code history visualization. Eliph incorporates the visualization of character-level code history, selection-based history tracking and the integration of execution events to assist students in understanding programs written by peers, thereby leading to more effective peer assessment. We evaluate Eliph with an experiment in an undergraduate CS course. We show that visualization of code history has positive effects on promoting higher quality of peer feedback by understanding the intention and thought process.</p>","","","10.1145/2998181.2998285","","","code history;data visualization;peer assessment;peer review;time series visualization","","","","","","","","","","","Feb. 25 2017-March 1 2017","","ACM","ACM Conferences"
"Learning analytics in higher education --- challenges and policies: a review of eight learning analytics policies","Y. S. Tsai; D. Gasevic","The University of Edinburgh, Edinburgh, United Kingdom","Proceedings of the Seventh International Learning Analytics & Knowledge Conference","20170417","2017","","","233","242","<p>This paper presents the results of a review of eight policies for learning analytics of relevance for higher education, and discusses how these policies have tried to address prominent challenges in the adoption of learning analytics, as identified in the literature. The results show that more considerations need to be given to establishing communication channels among stakeholders and adopting pedagogy-based approaches to learning analytics. It also reveals the shortage of guidance for developing data literacy among end-users and evaluating the progress and impact of learning analytics. Moreover, the review highlights the need to establish formalised guidelines to monitor the soundness, effectiveness, and legitimacy of learning analytics. As interest in learning analytics among higher education institutions continues to grow, this review will provide insights into policy and strategic planning for the adoption of learning analytics.</p>","","","10.1145/3027385.3027400","","","challenge;code of practice;higher education;learning analytics;policy;strategy","","","","","","","","","","","13-17 March 2017","","ACM","ACM Conferences"
"Systematic Mapping Study of Metrics based Clone Detection Techniques","D. Rattan; J. Kaur","Department of Computer Engineering, Punjabi University, Patiala","Proceedings of the International Conference on Advances in Information Communication Technology & Computing","20170413","2016","","","1","7","<p>A code clone is a code fragment that is similar or identical to other code fragments in the source code. Code clones generally occurs in large systems and affects the system maintenance and quality. Removing clones is one way to avoid problems that occur due to the presence of code clones. Clone detection techniques using software metrics provides less complexity in finding the clones. This systematic mapping study focuses on metric based clone detection techniques and various tools used in previous studies. The existing work is classified into different categories and presented in systematic maps. This paper also indicates some problems related to clone detection research.</p>","","Electronic:978-1-4503-4213-1","10.1145/2979779.2979855","","","Code clone;software metrics;systematic mapping;systematic review","","","","","","","","","","","12-13 Aug. 2016","","ACM","ACM Conferences"
"Scalability of Continuous Active Learning for Reliable High-Recall Text Classification","G. V. Cormack; M. R. Grossman","University of Waterloo, Waterloo, ON, Canada","Proceedings of the 25th ACM International on Conference on Information and Knowledge Management","20170413","2016","","","1039","1048","<p>For finite document collections, continuous active learning ('CAL') has been observed to achieve high recall with high probability, at a labeling cost asymptotically proportional to the number of relevant documents. As the size of the collection increases, the number of relevant documents typically increases as well, thereby limiting the applicability of CAL to low-prevalence high-stakes classes, such as evidence in legal proceedings, or security threats, where human effort proportional to the number of relevant documents is justified. We present a scalable version of CAL ('S-CAL') that requires <i>O</i>(log <i>N</i>) labeling effort and <i>O</i>(<i>N</i> log <i>N</i>) computational effort---where <i>N</i> is the number of unlabeled training examples---to construct a classifier whose effectiveness for a given labeling cost compares favorably with previously reported methods. At the same time, S-CAL offers calibrated estimates of class prevalence, recall, and precision, facilitating both threshold setting and determination of the adequacy of the classifier.</p>","","","10.1145/2983323.2983776","","","cal;continuous active learning;ediscovery;electronic discovery;predictive coding;relevance feedback;tar;technology-assisted review;test collections;text categorization;volume estimation","","","","","","","","","","","24-28 Oct. 2016","","ACM","ACM Conferences"
"Embedded software education: an RTOS-based approach","J. Archibald; D. Wilde","Brigham Young University","ACM SIGBED Review","20170412","2017","14","1","71","80","<p>Embedded computer systems are proliferating, but the complexities of embedded software make it increasingly difficult to produce systems that are robust and reliable. These challenges increase as embedded systems are connected to networks and relied on to control or monitor physical processes in critical infrastructure. This paper describes a senior-level course that exposes students to foundational characteristics of embedded software, such as concurrency, synchronization and communication. The core of the class is a sequence of laboratory assignments in which students design and implement a real-time operating system. Each student-developed RTOS has the same API, so all can run the same application code, but internal implementations vary widely. The principal challenges that arise in the design and debugging of a multi-tasking RTOS tend to be instances of the general problems that arise in embedded software. In our experience, the activity of creating a working RTOS is effective in helping students acquire the knowledge and skills required to be successful embedded software developers.</p>","","","10.1145/3036686.3036695","","","concurrency;embedded systems;embedded systems education;real-time computing;real-time operating systems;software bugs","","","","","","","","","","","October 2017","","ACM","ACM Journals & Magazines"
"A model based design approach to system building using the e-Yantra educational robot","K. Arya; B. Coelho; S. Pandya","IIT Bombay, Powai, Mumbai, India","ACM SIGBED Review","20170412","2017","14","1","37","43","<p>The e-Yantra robot is the basis for a highly scalable embedded systems teaching program setting up 500 embedded systems labs in Indian engineering colleges. A key strategy to encourage rapid prototyping of applications has been to encourage reuse of code using a commodity robot with a standard API along with excellent documentation and training material. An important challenge has been to teach the reasoning process from a design through to an implementation deployed on an actual machine. Model based design is key to articulating such reasoning. A further challenge is to do this in an affordable manner where most available model- based IDEs are expensive proprietary systems using languages such as Esterel and SCADE. We illustrate with a ""Valet Parking"" application how our robotic eco-system facilitates the learning of important model-based design principles taking a high-level specification of a problem down to working code and even deriving test cases in the process. A novel feature of our approach is that we carry out design-time scheduling of various (concurrent) activities by analyzing dependencies between modules and obtain purely sequential C-code implemented on a microcontroller without the need for an RTOS. This case study is an exemplar of a model-based design approach for a large class of such robotic projects.</p>","","","10.1145/3036686.3036691","","","UML;design time scheduling;educational robot;model based design;model based testing;project based learning;state machines","","","","","","","","","","","October 2017","","ACM","ACM Journals & Magazines"
"Current Status and Performance Analysis of Optical Camera Communication Technologies for 5G Networks","T. Nguyen; A. Islam; T. Hossan; Y. M. Jang","Kookmin University, Seoul, South Korea","IEEE Access","20170520","2017","5","","4574","4594","This paper investigates optical camera communication (OCC) technologies, targeting new spectrum, multiple-input-multiple-output diversity, transmission access, and novel architectures with augmented reality user experience for the extended 5G wireless network. It provides the current OCC research status and trend pertaining to these technologies, especially an inside view on the revision of IEEE 802.15.7-2011 known as the IEEE 802.15.7m (TG7m) Optical Wireless Communication Task Group. Such standardization activities have a major impact on the development of OCC technologies. In addition, it provides a detailed review of the related literature. Herein, OCC technologies are classified into five categories to elucidate their operations and technical characteristics. Furthermore, a concise performance analysis, numerical simulations, and some comparison of the results obtained for associated systems are presented, and the future directions of research and development are discussed.","","","10.1109/ACCESS.2017.2681110","10.13039/100010002 - Ministry of Education; 10.13039/501100003725 - Basic Science Research Program through the National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890427","5G network;IEEE 802.15.7m;OCC;TG7m;coding;modulation;optical camera communication;optical spectrum;performance analysis","5G mobile communication;Cameras;IEEE 802.15 Standard;Optical fiber networks;Radio frequency;Wireless communication","5G mobile communication;MIMO communication;augmented reality;cameras;free-space optical communication;numerical analysis;optical sensors;telecommunication standards","IEEE 802.15.7-2011 standard;IEEE 802.15.7m standard;OCC technology;Optical Wireless Communication Task Group;TG7m;augmented reality user experience;extended 5G wireless network;multiple-input-multiple-output diversity;numerical simulation;optical camera communication technology;transmission access","","4","","","","","20170330","2017","","IEEE","IEEE Journals & Magazines"
"Notice of Violation of IEEE Publication Principles<BR>Group-Aware Delay-Constrained Video Transmission Over Multihomed Device-to-Device Networks","J. Huang; Z. Lin","Network and Information Center, Institute of Network Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Access","20170520","2017","5","","2651","2664","Notice of Violation of IEEE Publication Principles<BR><BR> ""Group-Aware Delay-Constrained Video Transmission Over Multihomed Device-to-Device Networks""<BR> by Junfei Huang and Zhaowen Lin,<BR> in IEEE Access, Volume 5, 2017, pp 2651-2664<BR><BR> After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE‚Äôs Publication Principles.<BR><BR> This paper is a duplication of the original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR> ""Group-Aware Delay-Constrained Streaming in Wireless Device-to-Device Networks""<BR> by Xiaoyi Zhang, Jiawei Liang, Jiyan Wu, and Lin Zhang,<BR> in the ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM). Submitted December 26, 2016<BR><BR>The technological advancements in wireless communication systems enable mobile users to leverage different radio interfaces (e.g., cellular and WiFi) for concurrent data transmission. However, the existing transmission schemes do not seriously consider the problem of real-time video multicast to a cluster of co-located multihomed mobile devices. Conventionally, each client fetches the video streaming to the best of its capability, and this results in competing resources that degrade user-perceived video quality. Several literatures investigated the problem of using cellular to obtain video contents from the remote server and sharing them through WiFi. However, the stringent delay constraint of real-time video is not addressed in these solutions. In this paper, a cooperative transmission scheme is proposed to tackle the problem. First, a mathematical framework dubbed (Group-Aware Delay-COnstraint) is developed to formulate the delay-constrained goodput maximization problem of real-time video transmission to a group of - ultihomed mobiles. Second, a dataflow distribution mechanism is presented to conserve the cellular bandwidth and maximize user experience. Then, a prototype is implemented on the Android platform involving real-time video encoded with H.264 codec. Experimental results show that the proposed scheme achieves appreciable improvements over the reference schemes in video peak signal-to-noise ratio, end-to-end delay, and goodput.","","","10.1109/ACCESS.2017.2671900","Fundamental Research Funds for the Central Universities; National High Technology Research and Development Program of China (863 Program); 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859328","Video transmission;delay constraint;device-to-device communications;multihoming","Delays;IEEE 802.11 Standard;Mobile communication;Notice of Violation;Real-time systems;Servers;Smart phones;Streaming media","cellular radio;cooperative communication;data communication;multicast communication;optimisation;pattern clustering;smart phones;video coding;video communication;video streaming;wireless LAN","Android platform;H.264 codec;Wi-Fi;cellular bandwidth conservation;concurrent data transmission;cooperative transmission scheme;dataflow distribution mechanism;end-to-end delay;goodput;group-aware delay-constrained video transmission;mathematical framework;multihomed device-to-device network;multihomed mobile device cluster;radio interface;real-time video encoding;real-time video multicast;real-time video stringent delay constraint;real-time video transmission delay-constrained goodput maximization problem;remote server;user experience maximization;user-perceived video quality;video content;video peak signal-to-noise ratio;video streaming;wireless communication system","","","","","","","20170220","2017","","IEEE","IEEE Journals & Magazines"
"Opening up dark digital archives through the use of analytics to identify sensitive content","B. B. Borden; J. R. Baron","Drinker Biddle & Reath LLP, Washington, D.C.","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3224","3229","The Nation's history is going dark: without technological solutions, presidential and federal e-mail and other electronic records accessioned into the US National Archives will remain effectively inaccessible to the public due to sensitive content considerations, including most notably PII, for many decades. Analytics offers the means for achieving earlier public access to digital collections of public records while protecting the privacy of records creators and third-parties.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840978","PII;analytics;archives;predictive coding;sensitive data;technology assisted review","Electronic mail;Law;Libraries;NIST;Privacy","data privacy;electronic mail;government data processing;information retrieval systems","US National Archives;dark digital archives;e-mail;electronic records;privacy protection;public records digital collection;sensitive content identification","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conferences"
"EdgeCentric: Anomaly Detection in Edge-Attributed Networks","N. Shah; A. Beutel; B. Hooi; L. Akoglu; S. Gunnemann; D. Makhija; M. Kumar; C. Faloutsos","","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","327","334","Given a network with attributed edges, how can we identify anomalous behavior? Networks with edge attributes are ubiquitous, and capture rich information about interactions between nodes. In this paper, we aim to utilize exactly this information to discern suspicious from typical behavior in an unsupervised fashion, lending well to the traditional scarcity of ground-truth labels in practical anomaly detection scenarios. Our work has a number of notable contributions, including (a) formulation: while most other graph-based anomaly detection works use structural graph connectivity or node information, we focus on the new problem of leveraging edge information, (b) methodology: we introduce EdgeCentric, an intuitive and scalable compression-based approach for detecting edge-attributed graph anomalies, and (c) practicality: we show that EdgeCentric successfully spots numerous such anomalies in several large, edge-attributed real-world graphs, including the Flipkart e-commerce graph with over 3 million product reviews between 1.1 million users and 545 thousand products, where it achieved 0.87 precision over the top 100 results.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836684","anomaly detection;edge attributes;network;ranking;unsupervised","Channel coding;Conferences;Data mining;Data models;Hidden Markov models;Image edge detection","graph theory;unsupervised learning","EdgeCentric;Flipkart e-commerce graph;anomaly detection;compression-based approach;edge-attributed graph anomalies;ground-truth labels;structural graph connectivity;unsupervised fashion","","1","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conferences"
"Findings from GitHub: Methods, Datasets and Limitations","V. Cosentino; J. L. C. Izquierdo; J. Cabot","Mines de Nantes, LINA, Atlanmod, Inria, Nantes, France","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","20170126","2016","","","137","141","GitHub, one of the most popular social coding platforms, is the platform of reference when mining Open Source repositories to learn from past experiences. In the last years, a number of research papers have been published reporting findings based on data mined from GitHub. As the community continues to deepen in its understanding of software engineering thanks to the analysis performed on this platform, we believe it is worthwhile to reflect how research papers have addressed the task of mining GitHub repositories over the last years. In this regard, we present a meta-analysis of 93 research papers which addresses three main dimensions of those papers: i) the empirical methods employed, ii) the datasets they used and iii) the limitations reported. Results of our meta-analysis show some concerns regarding the dataset collection process and size, the low level of replicability, poor sampling techniques, lack of longitudinal studies and scarce variety of methodologies.","","Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7","10.1109/MSR.2016.023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832894","GitHub;Meta-analysis;Systematic review","Data collection;Data mining;Encoding;Interviews;Libraries;Metadata;Software","data mining;software engineering","GitHub;data mining;dataset collection;open source repositories mining;social coding platform;software engineering","","","","","","","","14-15 May 2016","","IEEE","IEEE Conferences"
"Understanding the Factors That Impact the Popularity of GitHub Repositories","H. Borges; A. Hora; M. T. Valente","Dept. of Comput. Sci., Fed. Univ. of Minas Gerais, Belo Horizonte, Brazil","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20170116","2016","","","334","344","Software popularity is a valuable information to modern open source developers, who constantly want to know if their systems are attracting new users, if new releases are gaining acceptance, or if they are meeting user's expectations. In this paper, we describe a study on the popularity of software systems hosted at GitHub, which is the world's largest collection of open source software. GitHub provides an explicit way for users to manifest their satisfaction with a hosted repository: the stargazers button. In our study, we reveal the main factors that impact the number of stars of GitHub projects, including programming language and application domain. We also study the impact of new features on project popularity. Finally, we identify four main patterns of popularity growth, which are derived after clustering the time series representing the number of stars of 2,279 popular GitHub repositories. We hope our results provide valuable insights to developers and maintainers, which could help them on building and evolving systems in a competitive software market.","","Electronic:978-1-5090-3806-0; POD:978-1-5090-3807-7","10.1109/ICSME.2016.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816479","GitHub;Open Source software;Social coding;Software Popularity","Documentation;HTML;Java;Libraries;Organizations;Software","public domain software;software reviews;source code (software);time series","GitHub Repositories;GitHub projects;open source developers;open source software;programming language;project popularity;software acceptance;software market;software system popularity;stargazers button;time series","","3","","","","","","2-7 Oct. 2016","","IEEE","IEEE Conferences"
"A Novel Approach for Embedding Information in Image Cover Media with Security","A. Fatnassi; H. Gharsellaoui; S. Bouamama","Nat. Eng. Sch. of Manouba, Manouba Univ., Manouba, Tunisia","2016 World Symposium on Computer Applications & Research (WSCAR)","20161222","2016","","","104","109","Steganalysis is the mechanism of detecting the presence of hidden information in the stego media and it can lead to the prevention of disastrous security incidents. This paper deals with the study of interest in the fields of Steganography and Steganalysis. Steganography involves hiding information in a cover media to obtain the stego media, in such a way that the cover media is perceived not to have any embedded message for its unintended recipients. In this paper work, we provide a critical review of the steganalysis algorithms available to analyze the characteristics of an image stego media against the corresponding cover media and understand the process of embedding the information and its detection. We anticipate that this paper can also give a clear picture of the current trends in steganography so that we can develop and improvise appropriate steganalysis algorithms.","","Electronic:978-1-5090-4114-5; POD:978-1-5090-4115-2","10.1109/WSCAR.2016.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7791989","Embedded Systems;Heuristics and Metaheuristics Algorithms;Low-power Consumption;Optimization;Steganalysis Heuristic Approach","Discrete cosine transforms;Gray-scale;Image color analysis;Internet;Media;Security;Transform coding","image watermarking;steganography","disastrous security incident prevention;hidden information detection;image cover media;image stego media;information embedding;steganalysis algorithms;steganography","","","","","","","","12-14 March 2016","","IEEE","IEEE Conferences"
"A Review of Lightweight Thread Approaches for High Performance Computing","A. Castell√≥; A. J. Pe√±a; S. Seo; R. Mayo; P. Balaji; E. S. Quintana-Ort√≠","","2016 IEEE International Conference on Cluster Computing (CLUSTER)","20161208","2016","","","471","480","High-level, directive-based solutions are becoming the programming models (PMs) of the multi/many-core architectures. Several solutions relying on operating system (OS) threads perfectly work with a moderate number of cores. However, exascale systems will spawn hundreds of thousands of threads in order to exploit their massive parallel architectures and thus conventional OS threads are too heavy for that purpose. Several lightweight thread (LWT) libraries have recently appeared offering lighter mechanisms to tackle massive concurrency. In order to examine the suitability of LWTs in high-level runtimes, we develop a set of microbenchmarks consisting of commonly-found patterns in current parallel codes. Moreover, we study the semantics offered by some LWT libraries in order to expose the similarities between different LWT application programming interfaces. This study reveals that a reduced set of LWT functions can be sufficient to cover the common parallel code patterns andthat those LWT libraries perform better than OS threads-based solutions in cases where task and nested parallelism are becoming more popular with new architectures.","","Electronic:978-1-5090-3653-0; POD:978-1-5090-3654-7","10.1109/CLUSTER.2016.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776544","Lightweight Threads;OpenMP;Programming Models","Concurrent computing;Hardware;Instruction sets;Libraries;Message systems;Semantics;Synchronization","application program interfaces;multi-threading;multiprocessing systems;operating systems (computers);parallel architectures;software libraries","LWT application programming interfaces;LWT functions;LWT libraries;high performance computing;high-level directive-based solutions;high-level runtimes;lightweight thread approaches;lightweight thread libraries;many-core architectures;massive concurrency;multicore architectures;operating system threads;parallel code patterns;parallel codes","","2","","","","","","12-16 Sept. 2016","","IEEE","IEEE Conferences"
"Interference Management via Sliding-Window Coded Modulation for 5G Cellular Networks","K. T. Kim; S. K. Ahn; Y. S. Kim; J. Park; C. Y. Chen; Y. H. Kim","","IEEE Communications Magazine","20161115","2016","54","11","82","89","SWCM aims to mitigate the adverse effects of interference at the physical layer by tracking the optimal maximum likelihood decoding performance with low-complexity decoding and minimal coordination overhead. This article reviews the basic structure of the SWCM scheme built on the principles of network information theory, and discusses how it can be extended and implemented in practical wireless communication systems. Using a representative implementation based on LTE OFDM MIMO systems, extensive link-level and system-level performance simulations are carried out, which demonstrate that SWCM offers significant gains for all users over conventional interference-aware communication schemes. Network operating prerequisites for SWCM are also discussed to facilitate the standardization effort for its adoption in the fifth generation cellular network.","0163-6804;01636804","","10.1109/MCOM.2016.1600356CM","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744815","","Binary phase shift keying;Interference;Iterative decoding;MIMO;Maximum likelihood decoding;Receivers","5G mobile communication;Long Term Evolution;OFDM modulation;cellular radio;interference suppression;maximum likelihood decoding;radiofrequency interference","5G cellular network;LTE OFDM MIMO system;SWCM scheme;extensive link level;fifth generation cellular network;interference adverse effect mitigation;interference management;interference-aware communication scheme;low-complexity decoding;minimal coordination overhead;network information theory principle;optimal maximum likelihood decoding performance;physical layer;representative implementation;sliding-window coded modulation;standardization effort;system-level performance simulation","","1","","","","","","November 2016","","IEEE","IEEE Journals & Magazines"
"JPEG Pleno: Toward an Efficient Representation of Visual Reality","T. Ebrahimi; S. Foessel; F. Pereira; P. Schelkens","Ecole Polytechnique F&#x00E9;d&#x00E9;rale de Lausanne","IEEE MultiMedia","20161114","2016","23","4","14","20","In discussing the rationale behind the vision for JPEG Pleno and how the new standardization initiative aims to reinvent the future of imaging, the authors review plenoptic representation and its underlying practical implications and challenges in implementing real-world applications with an enhanced quality of experience.","1070-986X;1070986X","","10.1109/MMUL.2016.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7742781","JPEG Pleno;augmented reality;graphics;holography;light field;mixed reality;multimedia;plenoptic representation;point cloud;virtual reality;virtualization;visualization","Cameras;Encoding;Metadata;Rendering (computer graphics);Three-dimensional displays;Transform coding;Visualization","image coding;image representation","JPEG Pleno;plenoptic representation;standardization initiative;visual reality representation","","3","","","","","","Oct.-Dec. 2016","","IEEE","IEEE Journals & Magazines"
"An Energy-interference-free Hardware-Software Debugger for Intermittent Energy-harvesting Systems","A. Colin; G. Harvey; B. Lucia; A. P. Sample","Carnegie Mellon University, Pittsburgh, PA, USA","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","577","589","<p>Energy-autonomous computing devices have the potential to extend the reach of computing to a scale beyond either wired or battery-powered systems. However, these devices pose a unique set of challenges to application developers who lack both hardware and software support tools. Energy harvesting devices experience power intermittence which causes the system to reset and power-cycle unpredictably, tens to hundreds of times per second. This can result in code execution errors that are not possible in continuously-powered systems and cannot be diagnosed with conventional debugging tools such as JTAG and/or oscilloscopes. We propose the Energy-interference-free Debugger, a hardware and software platform for monitoring and debugging intermittent systems without adversely effecting their energy state. The Energy-interference-free Debugger re-creates a familiar debugging environment for intermittent software and augments it with debugging primitives for effective diagnosis of intermittence bugs. Our evaluation of the Energy-interference-free Debugger quantifies its energy-interference-freedom and shows its value in a set of debugging tasks in complex test programs and several real applications, including RFID code and a machine-learning-based activity recognition system.</p>","0163-5980;01635980","","10.1145/2954680.2872409","","","energy-harvesting","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"A design space for engineering graphical adaptive menus","S. Bouzit; G. Calvary; D. Ch√™ne; J. Vanderdonckt","Orange Labs, Meylan, France and Univ. Grenoble Alpes, Grenoble, France","Proceedings of the 8th ACM SIGCHI Symposium on Engineering Interactive Computing Systems","20161111","2016","","","239","244","<p>This paper presents a design space for exploring design options of adaptive graphical menus based on Bertin's eight visual variables: position, size, shape, value, color, orientation, texture, and motion. In order to transform a traditional (static) menu into an adaptive one, at least one visual variable should be exploited to convey which menu items have been promoted or demoted depending on five characteristics: select, associative, quantitative, order, and length. The paper reviews selected adaptive menu interaction techniques belonging to each of these variables, classifies them according to the variables and characteristics and suggests not yet explored adaptive menu designs. It then defines four stability properties depending on which variables remain constant after adaptivity. A supporting software for prototyping the rendering of graphical adaptive menus is then introduced.</p>","","","10.1145/2933242.2935874","","","adaptation;adaptive menu;coding scheme;prediction window;stability property;visual variable","","","","","","","","","","","21-24 June 2016","","ACM","ACM Conferences"
"A wait-free multi-producer multi-consumer ring buffer","S. Feldman; D. Dechev","University of Central Florida","ACM SIGAPP Applied Computing Review","20161111","2015","15","3","59","71","<p>The ring buffer is a staple data structure used in many algorithms and applications. It is highly desirable in high-demand use cases such as multimedia, network routing, and trading systems. This work presents a new ring buffer design that is, to the best of our knowledge, the only array-based first-in-first-out ring buffer to provide wait-freedom. Wait-freedom guarantees that each thread completes its operation within a finite number of steps. This property is desirable for real-time and mission critical systems.</p> <p>This work is an extension and refinement of our earlier work. We have improved and expanded algorithm descriptions and pseudo-code, and we have performed all new performance evaluations.</p> <p>In contrast to other concurrent ring buffer designs, our implementation includes new methodology to prevent thread starvation and livelock from occurring.</p>","1559-6915;15596915","","10.1145/2835260.2835264","","","concurrent;non-blocking;parallel;ring buffer;wait-free","","","","","","","","","","","September 2015","","ACM","ACM Journals & Magazines"
"Perspectives on task ownership in mobile operating system development (invited talk)","S. Datta","Singapore University of Technology and Design, Singapore","Proceedings of the 2nd International Workshop on Software Development Lifecycle for Mobile","20161111","2014","","","11","12","<p> There can be little contention about Stroustrup's epigrammatic remark: our civilization runs on software. However a caveat is increasingly due, much of the software that runs our civilization, runs on mobile devices today. Mobile operating systems have come to play a preeminent role in the ubiquity and utility of such devices. The development ecosystem of Android - one of the most popular mobile operating systems - presents an interesting context for studying whether and how collaboration dynamics in mobile development differ from conventional software development. In this paper, we examine factors that influence task ownership in Android development. Our results can inform project governance decisions at the individual and organizational levels. </p>","","","10.1145/2661694.2661702","","","Android;code changes;merging;reviews","","","","","","","","","","","17-17 Nov. 2014","","ACM","ACM Conferences"
"CSR: Core Surprise Removal in Commodity Operating Systems","N. Shalev; E. Harpaz; H. Porat; I. Keidar; Y. Weinsberg","Technion, Haifa, Israel","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","773","787","<p>One of the adverse effects of shrinking transistor sizes is that processors have become increasingly prone to hardware faults. At the same time, the number of cores per die rises. Consequently, core failures can no longer be ruled out, and future operating systems for many-core machines will have to incorporate fault tolerance mechanisms.</p> <p>We present CSR, a strategy for recovery from unexpected permanent processor faults in commodity operating systems. Our approach overcomes surprise removal of faulty cores, and also tolerates cascading core failures. When a core fails in user mode, CSR terminates the process executing on that core and migrates the remaining processes in its run-queue to other cores. We further show how hardware transactional memory may be used to overcome failures in critical kernel code. Our solution is scalable, incurs low overhead, and is designed to integrate into modern operating systems. We have implemented it in the Linux kernel, using Haswell's Transactional Synchronization Extension, and tested it on a real system.</p>","0163-5980;01635980","","10.1145/2954680.2872369","","","CPU-hotplug;CSR;OS reliability;core surprise removal;fault-tolerant operating system;hardware transactional memory in kernel;hotplug;htm in kernel;kernel transactions","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"HIPStR: Heterogeneous-ISA Program State Relocation","A. Venkat; S. Shamasunder; H. Shacham; D. M. Tullsen","University of California, San Diego, La Jolla, CA, USA","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","727","741","<p>Heterogeneous Chip Multiprocessors have been shown to provide significant performance and energy efficiency gains over homogeneous designs. Recent research has expanded the dimensions of heterogeneity to include diverse Instruction Set Architectures, called Heterogeneous-ISA Chip Multiprocessors. This work leverages such an architecture to realize substantial new security benefits, and in particular, to thwart Return-Oriented Programming. This paper proposes a novel security defense called HIPStR -- Heterogeneous-ISA Program State Relocation -- that performs dynamic randomization of run-time program state, both within and across ISAs. This technique outperforms the state-of-the-art just-in-time code reuse (JIT-ROP) defense by an average of 15.6%, while simultaneously providing greater security guarantees against classic return-into-libc, ROP, JOP, brute force, JIT-ROP, and several evasive variants.</p>","0163-5980;01635980","","10.1145/2954680.2872408","","","heterogeneous-ISA CMP;return-oriented programming","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"Ziria: language for rapid prototyping of wireless PHY","G. Stewart; M. Gowda; G. Mainland; B. Radunovic; D. Vytiniotis; D. Patterson","Princeton University, Princeton, USA","ACM SIGCOMM Computer Communication Review","20161111","2014","44","4","357","358","<p>Software-defined radios (SDR) have the potential to bring major innovation in wireless networking design. However, their impact so far has been limited due to complex programming tools. Most of the existing tools are either too slow to achieve the full line speeds of contemporary wireless PHYs or are too complex to master. In this demo we present our novel SDR programming environment called Ziria. Ziria consists of a novel programming language and an optimizing compiler. The compiler is able to synthesize very efficient SDR code from high-level PHY descriptions written in Ziria language. To illustrate its potential, we present the design of an LTE-like PHY layer in Ziria. We run it on the Sora SDR platform and demonstrate on a test-bed that it is able to operate in real-time.</p>","0146-4833;01464833","","10.1145/2740070.2631427","","","DSL;SDR;domain specific language;programming;software-defined radio;wireless","","","","","","","","","","","October 2014","","ACM","ACM Journals & Magazines"
"Data Center Energy Efficiency Standards in India","S. Raje; H. Maan; S. Ganguly; T. Singh; N. Jayaram; G. Ghatikar; S. Greenberg; S. Kumar; D. Sartor","Confederation of Indian Industry, New Delhi, India","Proceedings of the 2015 ACM Sixth International Conference on Future Energy Systems","20161111","2015","","","233","240","<p>Global data center energy consumption is growing rapidly. In India, information technology industry growth, fossil-fuel generation, and rising energy prices add significant operational costs and carbon emissions from energy-intensive data centers. Adoption of energy-efficient practices can improve the global competitiveness and sustainability of data centers in India. Previous studies have concluded that advancement of energy efficiency standards through policy and regulatory mechanisms is the fastest path to accelerate the adoption of energy-efficient practices in the Indian data centers. In this study, we reviewed data center energy efficiency practices in the United States, Europe, and Asia. Using evaluation metrics, we identified an initial set of energy efficiency standards applicable to the Indian context using the existing policy mechanisms. These preliminary findings support next steps to recommend energy efficiency standards and inform policy makers on strategies to adopt energy-efficient technologies and practices in Indian data centers.</p>","","","10.1145/2768510.2768524","","","best practices;carbon emissions;data centers;energy consumption;energy efficiency;evaluation metrics;standards and codes;technologies","","","","","","","","","","","14-17 July 2015","","ACM","ACM Conferences"
"Noisy Bloom Filters for Multi-Set Membership Testing","H. Dai; Y. Zhong; A. X. Liu; W. Wang; M. Li","StaNanjing University., Nanjing, China","ACM SIGMETRICS Performance Evaluation Review","20161111","2016","44","1","139","151","<p>This paper is on designing a compact data structure for multi-set membership testing allowing fast set querying. Multi-set membership testing is a fundamental operation for computing systems and networking applications. Most existing schemes for multi-set membership testing are built upon Bloom filter, and fall short in either storage space cost or query speed. To address this issue, in this paper we propose Noisy Bloom Filter (NBF) and Error Corrected Noisy Bloom Filter (NBF-E) for multi-set membership testing. For theoretical analysis, we optimize their classification failure rate and false positive rate, and present criteria for selection between NBF and NBF-E. The key novelty of NBF and NBF-E is to store set ID information in a compact but noisy way that allows fast recording and querying, and use denoising method for querying. Especially, NBF-E incorporates asymmetric error-correcting coding technique into NBF to enhance the resilience of query results to noise by revealing and leveraging the asymmetric error nature of query results. To evaluate NBF and NBF-E in comparison with prior art, we conducted experiments using real-world network traces. The results show that NBF and NBF-E significantly advance the state-of-the-art on multi-set membership testing.</p>","0163-5999;01635999","","10.1145/2964791.2901451","","","asymmetric error-correcting code;bloom filter;constant weight code.;multi-set membership testing;noise","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"Turbocharging ambient backscatter communication","A. N. Parks; A. Liu; S. Gollakota; J. R. Smith","University of Washington, Seattle, WA, USA","ACM SIGCOMM Computer Communication Review","20161111","2014","44","4","619","630","<p>Communication primitives such as coding and multiple antenna processing have provided significant benefits for traditional wireless systems. Existing designs, however, consume significant power and computational resources, and hence cannot be run on low complexity, power constrained backscatter devices. This paper makes two main contributions: (1) we introduce the first multi-antenna cancellation design that operates on backscatter devices while retaining a small form factor and power footprint, (2) we introduce a novel coding mechanism that enables long range communication as well as concurrent transmissions and can be decoded on backscatter devices. We build hardware prototypes of the above designs that can be powered solely using harvested energy from TV and solar sources. The results show that our designs provide benefits for both RFID and ambient backscatter systems: they enable RFID tags to communicate directly with each other at distances of tens of meters and through multiple walls. They also increase the communication rate and range achieved by ambient backscatter systems by 100X and 40X respectively. We believe that this paper represents a substantial leap in the capabilities of backscatter communication.</p>","0146-4833;01464833","","10.1145/2740070.2626312","","","backscatter;energy harvesting;internet of things;wireless","","","","","13","1","","","","","October 2014","","ACM","ACM Journals & Magazines"
"Multi-Faceted Recall of Continuous Active Learning for Technology-Assisted Review","G. V. Cormack; M. R. Grossman","University of Waterloo, Waterloo, ON, Canada","Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval","20161111","2015","","","763","766","<p>Continuous active learning achieves high recall for technology-assisted review, not only for an overall information need, but also for various facets of that information need, whether explicit or implicit. Through simulations using Cormack and Grossman's TAR Evaluation Toolkit (SIGIR 2014), we show that continuous active learning, applied to a multi-faceted topic, efficiently achieves high recall for each facet of the topic. Our results assuage the concern that continuous active learning may achieve high overall recall at the expense of excluding identifiable categories of relevant information.</p>","","","10.1145/2766462.2767771","","","cal;continuous active learning;e-discovery;electronic discovery;predictive coding;relevance feedback;tar;technology-assisted review;test collections","","","","","","","","","","","9-13 Aug. 2015","","ACM","ACM Conferences"
"Lightweight Capability Domains: Towards Decomposing the Linux Kernel","C. Jacobsen; M. Khole; S. Spall; S. Bauer; A. Burtsev","University of Utah, Salt Lake City, UT","ACM SIGOPS Operating Systems Review","20161111","2015","49","2","44","50","<p>Despite a number of radical changes in how computer systems are used, the design principles behind the very core of the systems stack--an operating system kernel--has remained unchanged for decades.We run monolithic kernels developed with a combination ofan unsafe programming language, global sharing of data structures, opaque interfaces, and no explicit knowledge of kernel protocols. Today, the monolithic architecture of a kernel is the main factor undermining its security, and even worse, limiting its evolution towards a safer, more secure environment. Lack of isolation across kernel subsystems allows attackers to take control over the entire machine with a single kernel vulnerability. Furthermore, complex, semantically rich monolithic code with globally shared data structures and no explicit interfaces is not amenable to formal analysis and verification tools. Even after decades of work to make monolithic kernels more secure, over a hundred serious kernel vulnerabilities are still reported every year.</p> <p>Modern kernels need decomposition as a practical means of confining the effects of individual attacks. Historically, decomposed kernels were prohibitively slow. Today, the complexity of a modern kernel prevents a trivial decomposition effort. We argue, however, that despite all odds modern kernels can be decomposed. Careful choice of communication abstractions and execution model, a general approach to decomposition, a path for incremental adoption, and automation through proper language tools can address complexity of decomposition and performance overheads of decomposed kernels. Our work on lightweight capability domains (LCDs) develops principles, mechanisms, and tools that enable incremental, practical decomposition of a modern operating system kerne.</p>","0163-5980;01635980","","10.1145/2883591.2883601","","","Linux;decomposition;microkernels","","","","","","","","","","","December 2015","","ACM","ACM Journals & Magazines"
"Mahimahi: a lightweight toolkit for reproducible web measurement","R. Netravali; A. Sivaraman; K. Winstein; S. Das; A. Goyal; H. Balakrishnan","MIT, Cambridge, MA, USA","ACM SIGCOMM Computer Communication Review","20161111","2014","44","4","129","130","<p>This demo presents a measurement toolkit, Mahimahi, that records websites and replays them under emulated network conditions. Mahimahi is structured as a set of arbitrarily composable UNIX shells. It includes two shells to record and replay Web pages, RecordShell and ReplayShell, as well as two shells for network emulation, DelayShell and LinkShell. In addition, Mahimahi includes a corpus of recorded websites along with benchmark results and link traces (https://github.com/ravinet/sites).</p> <p>Mahimahi improves on prior record-and-replay frameworks in three ways. First, it preserves the multi-origin nature of Web pages, present in approximately 98% of the Alexa U.S. Top 500, when replaying. Second, Mahimahi isolates its own network traffic, allowing multiple instances to run concurrently with no impact on the host machine and collected measurements. Finally, Mahimahi is not inherently tied to browsers and can be used to evaluate many different applications.</p> <p>A demo of Mahimahi recording and replaying a Web page over an emulated link can be found at http://youtu.be/vytwDKBA-8s. The source code and instructions to use Mahimahi are available at http://mahimahi.mit.edu/.</p>","0146-4833;01464833","","10.1145/2740070.2631455","","","page load time;record-and-replay;web measurements","","","","","1","","","","","","October 2014","","ACM","ACM Journals & Magazines"
"Merging network coding with feedback management in multicast streaming","A. Moreira; L. Almeida; D. E. Lucani","University of Porto, Porto, Portugal","ACM SIGBED Review","20161111","2015","12","3","49","52","<p>Reliable multicast over wireless poses interesting challenges arising from the unreliable nature of the wireless medium. Recovering lost packets is particularly challenging in multicast scenarios since different receivers lose different packets. For this reason, simply retransmitting packets does not scale well with the number of receivers and particularly with the packet loss rate. A more efficient alternative is to use erasure codes to generate packets that can help many receivers at the same time. In this paper, we propose using online network coding to send coded packets that repair losses according to feedback reports sent by the clients. In particular, we propose using a recently developed scheduler for controlling feedback reports, which also allows differentiating the QoS provided to clients, and combine it with an online coding approach to provide novel stochastic guarantees of worst-case delay as required for QoS sensitive applications. We show preliminary simulation results that confirm the bounded decoding delay of our approach in a streaming application.</p>","","","10.1145/2815482.2815492","","","","","","","","2","","","","","","June 2015","","ACM","ACM Journals & Magazines"
"Technology and adherence in web-based interventions for weight control: a systematic review","S. M. Kelders; R. N. Kok; J. E. W. C. Van Gemert-Pijnen","University of Twente, Enschede, The Netherlands","Proceedings of the 6th International Conference on Persuasive Technology","20161111","2011","","","1","8","<p>While technology based health interventions can be effective, high attrition rates are commonly observed in research and practice and are a major issue in eHealth. Research on adherence has recently gained some scientific attention, but little has been done as to how technology itself engages users. It seems plausible that technology plays a role in persuading people to use an eHealth intervention and keep on using it. The present study seeks to apply the PSD-model with regard to this major issue in eHealth. A comprehensive literature search was conducted using the databases Web of Knowledge, EBSCOHOST, Picarta, Scopus and ScienceDirect. The following characteristics were coded: Health care domain, Study level, Sample size, Intended usage, Usage web-based intervention, Adherence, Predictors of adherence, Intervention, Interaction, Persuasive technology in the intervention. The search yielded 4939 unique titles, of which 460 articles were deemed relevant. After title, abstract and full text screening by two researchers, 109 articles were included. Concluding, we can say that it seems that when designing web-based interventions, most attention is given to support the primary task. Dialogue support and social support appear neglected. Taking into account that more extensive use of persuasive technology seems to be positively related to adherence, more attention should be paid to all forms of support through technology.</p>","","","10.1145/2467803.2467806","","","adherence;eHealth;persuasive technology;systematic review","","","","","1","","","","","","2-5 June 2011","","ACM","ACM Conferences"
"Combining Phase Identification and Statistic Modeling for Automated Parallel Benchmark Generation","Y. Jin; X. Ma; M. Liu; Q. Liu; J. Logan; N. Podhorszki; J. Y. Choi; S. Klasky","North Carolina State University, Raleigh, NC, USA","ACM SIGMETRICS Performance Evaluation Review","20161111","2015","43","1","309","320","<p>Parallel application benchmarks are indispensable for evaluating/optimizing HPC software and hardware. However, it is very challenging and costly to obtain high-fidelity benchmarks reflecting the scale and complexity of state-of-the-art parallel applications. Hand-extracted synthetic benchmarks are time- and labor-intensive to create. Real applications themselves, while offering most accurate performance evaluation, are expensive to compile, port, reconfigure, and often plainly inaccessible due to security or ownership concerns. This work contributes APPrime, a novel tool for trace-based automatic parallel benchmark generation. Taking as input standard communication-I/O traces of an application's execution, it couples accurate automatic phase identification with statistical regeneration of event parameters to create compact, portable, and to some degree reconfigurable parallel application benchmarks. Experiments with four NAS Parallel Benchmarks (NPB) and three real scientific simulation codes confirm the fidelity of APPrime benchmarks. They retain the original applications' performance characteristics, in particular their relative performance across platforms. Also, the result benchmarks, already released online, are much more compact and easy-to-port compared to the original applications.</p>","0163-5999;01635999","","10.1145/2796314.2745876","","","asynchronous i/o;benchmark generation;hpc applications;markov chain model;phase identification;traces","","","","","1","","","","","","June 2015","","ACM","ACM Journals & Magazines"
"Findings from GitHub: methods, datasets and limitations","V. Cosentino; J. Luis; J. Cabot","Atlanmod, Inria, Mines, Nantes, LINA, Nantes, France","Proceedings of the 13th International Conference on Mining Software Repositories","20161111","2016","","","137","141","<p>GitHub, one of the most popular social coding platforms, is the platform of reference when mining Open Source repositories to learn from past experiences. In the last years, a number of research papers have been published reporting findings based on data mined from GitHub. As the community continues to deepen in its understanding of software engineering thanks to the analysis performed on this platform, we believe it is worthwhile to reflect how research papers have addressed the task of mining GitHub repositories over the last years. In this regard, we present a meta-analysis of 93 research papers which addresses three main dimensions of those papers: i) the empirical methods employed, ii) the datasets they used and iii) the limitations reported. Results of our meta-analysis show some concerns regarding the dataset collection process and size, the low level of replicability, poor sampling techniques, lack of longitudinal studies and scarce variety of methodologies.</p>","","","10.1145/2901739.2901776","","","GitHub;meta-analysis;systematic review","","","","","1","","","","","","14-22 May 2016","","ACM","ACM Conferences"
"Architectural Run-time Models for Performance and Privacy Analysis in Dynamic Cloud Applications","R. Heinrich","Karlsruhe Institute of Technology, Karlsruhe, Germany","ACM SIGMETRICS Performance Evaluation Review","20161111","2016","43","4","13","22","<p>Building software systems by composing third-party cloud services promises many benefits such as flexibility and scalability. Yet at the same time, it leads to major challenges like limited control of third party infrastructures and runtime changes which mostly cannot be foreseen during development. While previous research focused on automated adaptation, increased complexity and heterogeneity of cloud services as well as their limited observability, makes evident that we need to allow operators (humans) to engage in the adaptation process. Models are useful for involving humans and conducting analysis, e.g. for performance and privacy. During operation the systems often drifts away from its design-time models. Run-time models are kept insync with the underlying system. However, typical run-time models are close to an implementation level of abstraction which impedes understandability for humans. In this vision paper, we present the iObserve approach to target aforementioned challenges while considering operationlevel adaptation and development-level evolution as two mutual interwoven processes. Central to this perception is an architectural run-time model that is usable for automatized adaptation and is simultaneously comprehensible for humans during evolution. The run-time model builds upon a technology-independent monitoring approach. A correspondence model maintains the semantic relationships between monitoring outcomes and architecture models. As an umbrella a megamodel integrates design-time models, code generation, monitoring, and run-time model update. Currently, iObserve covers the monitoring and analysis phases of the MAPE control loop. We come up with a roadmap to include planning and execution activities in iObserve.</p>","0163-5999;01635999","","10.1145/2897356.2897359","","","Architectural Run-time Model;Palladio Component Model;Performance Model;Privacy;Usage Profile","","","","","4","","","","","","March 2016","","ACM","ACM Journals & Magazines"
"Latent Aspect Mining via Exploring Sparsity and Intrinsic Information","Y. Xu; T. Lin; W. Lam; Z. Zhou; H. Cheng; A. M. C. So","The Chinese University of Hong Kong, Hong Kong, Hong Kong","Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management","20161111","2014","","","879","888","<p>We investigate latent aspect mining problem that aims at automatically discovering aspect information from a collection of review texts in a domain in an unsupervised manner. One goal is to discover a set of aspects which are previously unknown for the domain, and predict the user's ratings on each aspect for each review. Another goal is to detect key terms for each aspect. Existing works on predicting aspect ratings fail to handle the aspect sparsity problem in the review texts leading to unreliable prediction. We propose a new generative model to tackle the latent aspect mining problem in an unsupervised manner. By considering the user and item side information of review texts, we introduce two latent variables, namely, user intrinsic aspect interest and item intrinsic aspect quality facilitating better modeling of aspect generation leading to improvement on the accuracy and reliability of predicted aspect ratings. Furthermore, we provide an analytical investigation on the Maximum A Posterior (MAP) optimization problem used in our proposed model and develop a new block coordinate gradient descent algorithm to efficiently solve the optimization with closed-form updating formulas. We also study its convergence analysis. Experimental results on the two real-world product review corpora demonstrate that our proposed model outperforms existing state-of-the-art models.</p>","","","10.1145/2661829.2662062","","","aspect mining;sparse coding;topic model","","","","","1","","","","","","3-7 Nov. 2014","","ACM","ACM Conferences"
"Architecture-Adaptive Code Variant Tuning","S. Muralidharan; A. Roy; M. Hall; M. Garland; P. Rai","University of Utah, Salt Lake City, UT, USA","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","325","338","<p>Code variants represent alternative implementations of a computation, and are common in high-performance libraries and applications to facilitate selecting the most appropriate implementation for a specific execution context (target architecture and input dataset). Automating code variant selection typically relies on machine learning to construct a model during an offline learning phase that can be quickly queried at runtime once the execution context is known. In this paper, we define a new approach called architecture-adaptive code variant tuning, where the variant selection model is learned on a set of source architectures, and then used to predict variants on a new target architecture without having to repeat the training process. We pose this as a multi-task learning problem, where each source architecture corresponds to a task; we use device features in the construction of the variant selection model. This work explores the effectiveness of multi-task learning and the impact of different strategies for device feature selection. We evaluate our approach on a set of benchmarks and a collection of six NVIDIA GPU architectures from three distinct generations. We achieve performance results that are mostly comparable to the previous approach of tuning for a single GPU architecture without having to repeat the learning phase.</p>","0163-5980;01635980","","10.1145/2954680.2872411","","","autotuning;cross-architectural tuning;device feature selection;input-adaptive;multi-task learning","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"Joint latency and cost optimization for erasurecoded data center storage","Y. Xiang; T. Lan; V. Aggarwal; Y. F. R. Chen","George Washington University, DC, USA","ACM SIGMETRICS Performance Evaluation Review","20161111","2014","42","2","3","14","<p>Modern distributed storage systems offer large capacity to satisfy the exponentially increasing need of storage space. They often use erasure codes to protect against disk and node failures to increase reliability, while trying to meet the latency requirements of the applications and clients. This paper provides an insightful upper bound on the average service delay of such erasure-coded storage with arbitrary service time distribution and consisting of multiple heterogeneous files. Not only does the result supersede known delay bounds that only work for homogeneous files, it also enables a novel problem of joint latency and storage cost minimization over three dimensions: selecting the erasure code, placement of encoded chunks, and optimizing scheduling policy. The problem is efficiently solved via the computation of a sequence of convex approximations with provable convergence. We further prototype our solution in an open-source, cloud storage deployment over three geographically distributed data centers. Experimental results validate our theoretical delay analysis and show significant latency reduction, providing valuable insights into the proposed latency-cost tradeoff in erasure-coded storage.</p>","0163-5999;01635999","","10.1145/2667522.2667524","","","","","","","","5","","","","","","September 2014","","ACM","ACM Journals & Magazines"
"SampleLite: A Hybrid Approach to 802.11n Link Adaptation","L. Kriara; M. K. Marina","The University of Edinburgh, Edinburgh, United Kingdom","ACM SIGCOMM Computer Communication Review","20161111","2015","45","2","4","13","<p>We consider the link adaptation problem in 802.11n wireless LANs that involves adapting MIMO mode, channel bonding, modulation and coding scheme, and frame aggregation level with varying channel conditions. Through measurement-based analysis, we find that adapting all available 802.11n features results in higher goodput than adapting only a subset of features, thereby showing that holistic link adaptation is crucial to achieve best performance. We then design a novel hybrid link adaptation scheme termed SampleLite that adapts all 802.11n features while being efficient compared to sampling-based open-loop schemes and practical relative to closed loop schemes. SampleLite uses sender-side RSSI measurements to significantly lower the sampling overhead, by exploiting the monotonic relationship between best settings for each feature and the RSSI. Through analysis and experimentation in a testbed environment, we show that our proposed approach can reduce the sampling overhead by over 70% on average compared to the widely used Minstrel HT scheme. We also experimentally evaluate the goodput performance of SampleLite in a wide range of controlled and real-world interference scenarios. Our results show that SampleLite, while performing close to the ideal, delivers goodput that is 35-100% better than with existing schemes.</p>","0146-4833;01464833","","10.1145/2766330.2766332","","","802.11n;algorithms;design;experimentation;link adaptation;performance","","","","","6","","","","","","April 2015","","ACM","ACM Journals & Magazines"
"A Comparative Look into Public IXP Datasets","R. Kl√∂ti; B. Ager; V. Kotronis; G. Nomikos; X. Dimitropoulos","ETH Zurich, Zurich, Switzerland","ACM SIGCOMM Computer Communication Review","20161111","2016","46","1","21","29","<p>Internet eXchange Points (IXPs) are core components of the Internet infrastructure where Internet Service Providers (ISPs) meet and exchange traffic. During the last few years, the number and size of IXPs have increased rapidly, driving the flattening and shortening of Internet paths. However, understanding the present status of the IXP ecosystem and its potential role in shaping the future Internet requires rigorous data about IXPs, their presence, status, participants, etc. In this work, we do the first cross-comparison of three well-known publicly available IXP databases, namely of PeeringDB, Euro-IX, and PCH. A key challenge we address is linking IXP identifiers across databases maintained by different organizations. We find different AS-centric versus IXP-centric views provided by the databases as a result of their data collection approaches. In addition, we highlight differences and similarities w.r.t. IXP participants, geographical coverage, and co-location facilities. As a side-product of our linkage heuristics, we make publicly available the union of the three databases, which includes 40.2% more IXPs and 66.3% more IXP participants than the commonly-used PeeringDB. We also publish our analysis code to foster reproducibility of our experiments and shed preliminary insights into the accuracy of the union dataset.</p>","0146-4833;01464833","","10.1145/2875951.2875955","","","internet exchange points;internet topology;peering databases","","","","","2","","","","","","January 2016","","ACM","ACM Journals & Magazines"
"Efficient SQL querying on embedded devices using pre-compilation","G. Douglas; R. Lawrence","University of British Columbia","ACM SIGAPP Applied Computing Review","20161111","2016","16","2","42","47","<p>Microprocessors and embedded devices are used for data collection and analysis applications in infrastructure and en- vironmental monitoring, medical technology, wearable com- puting, and sensor network and mobile systems. Such appli- cations demand low energy solutions without using too much of a device's extremely limited RAM (1KB-100KB) and code space. Previously available database software for embedded devices and sensor networks relied heavily on data trans- mission across networks for centralized data processing. Re- cently, relational database systems for resource-constrained devices have been developed to execute queries on a per- device basis, which saves network transmission overhead. This work extends the applicability of such systems by lower- ing the code space and execution time requirements further through serializing queries at application build time and re- moving the query translation component from the device. By eliminating the need for complex query translation sys- tems on device, our technique can reduce ROM usage by as much as 50% while improving memory utilization. Our ex- periments demonstrate that pre-compiling can reduce query initialization times by 90% compared to typical parsing tech- niques. This translates to a further savings of up to 50% in on-device total execution times. The technique developed is applicable to a wide variety of embedded systems and</p>","1559-6915;15596915","","10.1145/2993231.2993235","","","Arduino;SQL;database;embedded;query","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"Design and experiment of testbed using network coding for power management","K. s. Koo; M. Govindarasu","Iowa State University, Ames, IA","ACM SIGBED Review","20161111","2015","12","3","40","44","<p>Energy optimization is an important problem in modern wireless embedded systems. In recent years, several techniques have been proposed to optimize power consumption at the computing and communication layers of these systems. Network coding is one of the promising techniques at the communication layer in which multiple messages are opportunistically encoded into a single message to reduce the number of transmissions. While network coding is a theoretically promising technique, its benefit in energy efficiency depends on the practical overheads associated with various computations such as encoding, decoding, and the coding opportunity algorithm itself. Analytical and simulation-based studies are often inadequate to sufficiently evaluate the effectiveness of network coding. In this paper, we present the design and implementation of a wireless testbed for the purpose of energy-aware resource management in networked embedded systems. We also evaluate the energy performance of network coding technique under a wide range of channel and traffic conditions. Our results show that the network coding is indeed beneficial under poor channel and heavy network traffic conditions even when all the encoding and decoding overheads are accounted.</p>","","","10.1145/2815482.2815490","","","opportunistic network coding;power management;wireless testbed","","","","","1","","","","","","June 2015","","ACM","ACM Journals & Magazines"
"Hardening an L4 Microkernel Against Soft Errors by Aspect-Oriented Programming and Whole-Program Analysis","C. Borchert; O. Spinczyk","Technische Universit&#228;t Dortmund","ACM SIGOPS Operating Systems Review","20161111","2015","49","2","37","43","<p>Transient hardware faults in computer systems have become widespread as shrinking structures and low supply voltages reduce the amount of energy needed to trigger a fault. This paper describes the latest improvements of a software-based fault-tolerance mechanism called Generic Object Protection (GOP). It is based on Aspect-Orientied Programming in AspectC++ and has been used in a case study to harden the L4/Fiasco.OC microkernel. As a result, the improved GOP avoids 60% of kernel failures at an acceptable overhead of 19% code size and less than 1% runtime. The GOP improvements use static whole-program analysis and have been implemented in a prototypical manner. As an outlook, the paper presents envisioned language extensions providing whole-program control-flow and data-flow analyses in future AspectC++ versions.</p>","0163-5980;01635980","","10.1145/2883591.2883600","","","","","","","","","","","","","","December 2015","","ACM","ACM Journals & Magazines"
"Hidden Aspect Rating Discovery from Text Reviews of E-Commerce Web Sites","Y. Xu; W. Lam; R. Fan","Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong.","Proceedings of the 2014 International Conference on Big Data Science and Computing","20161111","2014","","","1","4","<p>We investigate hidden aspect mining problem that aims at automatically discovering aspect information from a collection of review texts in an unsupervised manner. The goal is to predict the user's ratings on each aspect. It does not require users to specify predefined seed terms for each aspect. We propose a generative model to tackle the hidden aspect mining problem. Our proposed model can detect the aspects of a particular domain. When predicting reviewer's ratings on each aspect, our model employs l1-regularizer to control the sparsity on the aspect and obtains more reliable predicted aspect ratings. Existing works on predicting aspect ratings fail to explore the aspect sparsity problem in the review texts leading to unreliable prediction. Experimental results on the two real-world product review corpora demonstrate that our model outperforms existing state-of-the-art models in the aspect rating prediction task.</p>","","","10.1145/2640087.2644158","","","Aspect Mining;Sentiment Analysis;Sparse Coding;Topic Model","","","","","","","","","","","4-7 Aug. 2014","","ACM","ACM Conferences"
"Designing a time predictable memory hierarchy for single-path code","B. Cilku; P. Puschner","Vienna University of Technology, Wien, Austria","ACM SIGBED Review","20161111","2015","12","2","16","21","<p>Trustable Worst-Case Execution-Time (WCET) bounds are a necessary component for the construction and verification of hard real-time computer systems. Deriving such bounds for contemporary hardware/software systems is a complex task. The single-path conversion overcomes this difficulty by transforming all unpredictable branch alternatives in the code to a sequential code structure with a single execution trace. However, the simpler code structure and analysis of single-path code comes at the cost of a longer execution time. In this paper we address the problem of the execu- tion performance of single-path code. We propose a new instuction-prefetch scheme and cache organization that uti- lize the knowledge of the future"" properties of single-path code to reduce the main memory access latency and the number of cache misses, thus speeding up the execution of single-path programs.</p>","","","10.1145/2782753.2782755","","","cache memories;hard real-time systems;memory hierarchy;prefetching;time predictability","","","","","","","","","","","April 2015","","ACM","ACM Journals & Magazines"
"ANVIL: Software-Based Protection Against Next-Generation Rowhammer Attacks","Z. B. Aweke; S. F. Yitbarek; R. Qiao; R. Das; M. Hicks; Y. Oren; T. Austin","University of Michigan, Ann Arbor, MI, USA","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","743","755","<p>Ensuring the integrity and security of the memory system is critical. Recent studies have shown serious security concerns due to ""rowhammer"" attacks, where repeated accesses to a row of memory cause bit flips in adjacent rows. Recent work by Google's Project Zero has shown how to leverage rowhammer-induced bit-flips as the basis for security exploits that include malicious code injection and memory privilege escalation. Being an important security concern, industry has attempted to defend against rowhammer attacks. Deployed defenses employ two strategies: (1) doubling the system DRAM refresh rate and (2) restricting access to the <i>CLFLUSH</i> instruction that attackers use to bypass the cache to increase memory access frequency (i.e., the rate of rowhammering). We demonstrate that such defenses are inadequte: we implement rowhammer attacks that both avoid using the <i>CLFLUSH</i> instruction and cause bit flips with a doubled refresh rate. Our next-generation <i>CLFLUSH-free</i> rowhammer attack bypasses the cache by manipulating cache replacement state to allow frequent misses out of the last-level cache to DRAM rows of our choosing.</p> <p>To protect existing systems from more advanced rowhammer attacks, we develop a <i>software-based</i> defense, ANVIL, which thwarts all known rowhammer attacks on existing systems. ANVIL detects rowhammer attacks by tracking the locality of DRAM accesses using existing hardware performance counters. Our detector identifies the rows being frequently accessed (i.e., the aggressors), then selectively refreshes the nearby victim rows to prevent hammering. Experiments running on real hardware with the SPEC2006 benchmarks show that ANVIL has less than a 1% false positive rate and an average slowdown of 1%. ANVIL is low-cost and robust, and our experiments indicate that it is an effective approach for protecting existing and future systems from even advanced rowhammer attacks.</p>","0163-5980;01635980","","10.1145/2954680.2872390","","","CLFLUSH;DRAM;bit-plru;disturbance errors;intel pebs;kernel module;performance counters;rowhammer","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"DRoP: DNS-based router positioning","B. Huffaker; M. Fomenkov; k. claffy","University of California, San Diego, La Jolla, CA, USA","ACM SIGCOMM Computer Communication Review","20161111","2014","44","3","5","13","<p>In this paper we focus on geolocating Internet routers, using a methodology for extracting and decoding geography-related strings from fully qualified domain names (hostnames). We first compiled an extensive dictionary associating geographic strings (e.g., airport codes) with geophysical locations. We then searched a large set of router hostnames for these strings, assuming each autonomous naming domain uses geographic hints consistently within that domain. We used topology and performance data continually collected by our global measurement infrastructure to discern whether a given hint appears to co-locate different hostnames in which it is found. Finally, we generalized geolocation hints into domain-specific rule sets. We generated a total of 1,711 rules covering 1,398 different domains and validated them using domain-specific ground truth we gathered for six domains. Unlike previous efforts which relied on labor-intensive domain-specific manual analysis, we automate our process for inferring the domain specific heuristics, substantially advancing the state-of-the-art of methods for geolocating Internet resources.</p>","0146-4833;01464833","","10.1145/2656877.2656879","","","active measurement;internet toology;router geolocation, dns","","","","","1","","","","","","July 2014","","ACM","ACM Journals & Magazines"
"Tapir: A Language for Verified OS Kernel Probes","I. Yanok; N. Nystrom","University of Lugano","ACM SIGOPS Operating Systems Review","20161111","2015","49","2","51","56","<p>Kernel probes allow code to be inserted into a running operating system kernel to gather information for debugging or profiling. Inserting code into the kernel raises a number of safety issues. Current solutions follow one of the two paths: a VM-based approach, where safety properties are checked dynamically by an interpreter, or a static-analysis approach, where probe code is guaranteed to be safe statically. While more attractive, existing static solutions depend on ad-hoc and error-prone analysis. We propose to explore enforcing safety properties using a type system, thus building our analysis on top of the well-studied ground of type theory.</p>","0163-5980;01635980","","10.1145/2883591.2883602","","","dependent types;kernel probes;type systems","","","","","","","","","","","December 2015","","ACM","ACM Journals & Magazines"
"Aligning single path loops to reduce the number of capacity cache misses","B. Cilku; R. Kammerer; P. Puschner","Vienna University of Technology, Wien, Austria","ACM SIGBED Review","20161111","2015","12","1","13","18","<p>In this paper we address the problem of improving the instruction cache performance for single-path code. The properties of single-path code allow us to align single-path loops within the cache in order to reduce the number of cache misses during the loop execution. We propose an algorithm that categorizes loops in a simple way so that the loops can be aligned and NOP instructions can be inserted to support this loop alignment. Our experimental results show the predictability for cache misses in single-path loops and demonstrate the benefit of the single-path loop alignment.</p>","","","10.1145/2752801.2752803","","","cache memories;hard real-time systems;memory hierarchy;time predictability","","","","","","","","","","","February 2015","","ACM","ACM Journals & Magazines"
"Performance overhead of KVM on Linux 3.9 on ARM cortex-a15","L. Rasmusson; D. Corcoran","SICS Swedish Institute of Computer Science, Kista, Sweden","ACM SIGBED Review","20161111","2014","11","2","32","38","<p>A number of simple performance measurements on network, CPU and disk speed were done on a dual ARM Cortex-A15 machine running Linux inside a KVM virtual machine that uses virtio disk and networking. Unexpected behaviour was observed in the CPU and memory intensive benchmarks, and in the networking benchmarks. The average overhead of running inside KVM is between zero and 30 percent when the host is lightly loaded (running only the system software and the necessary qemu-system-arm virtualization code), but the relative overhead increases when both host and VM is busy. We conjecture that this is related to the scheduling inside the host Linux.</p>","","","10.1145/2668138.2668143","","","ARM cortex-a15;HVM;Linux KVM;performance;virtualization","","","","","3","","","","","","June 2014","","ACM","ACM Journals & Magazines"
"Towards the 5G Revolution: A Software Defined Network Architecture Exploiting Network Coding as a Service","D. Szab√≥; F. N√©meth; B. Sonkoly; A. Guly√°s; F. H. P. Fitzek","Budapest University of Technology and Economics, Budapest, Hungary","ACM SIGCOMM Computer Communication Review","20161111","2015","45","4","105","106","<p>Many networking visioners agree that 5G will be much more than the incremental improvement, in terms of data rate, of 4G. Besides the mobile networks, 5G will fundamentally influence the core infrastructure as well. In our vision the realization of the challenging promises of 5G (e.g. extremely fast, low-overhead, low-delay access of mostly cloudified services and content) will require the massive use of multipathing equipped with low overhead transport solutions tailored to fast, reliable and secure data retrieval from cloud architectures. In this demo we present a prototype architecture supporting such services by making use of automatically configured multipath service chains implementing network coding based transport solutions over off-the-shelf software defined networking (SDN) components.</p>","0146-4833;01464833","","10.1145/2829988.2790025","","","NFV;SDN;mininet;network coding","","","","","1","","","","","","October 2015","","ACM","ACM Journals & Magazines"
"Analyzing Behavior Specialized Acceleration","T. Nowatzki; K. Sankaralingam","University of Wisconsin - Madison, Madison, WI, USA","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","697","711","<p>Hardware specialization has become a promising paradigm for overcoming the inefficiencies of general purpose microprocessors. Of significant interest are Behavioral Specialized Accelerators (BSAs), which are designed to efficiently execute code with only certain properties, but remain largely configurable or programmable. The most important strength of BSAs -- their ability to target a wide variety of codes -- also makes their interactions and analysis complex, raising the following questions: can multiple BSAs be composed synergistically, what are their interactions with the general purpose core, and what combinations favor which workloads? From a methodological standpoint, BSAs are also challenging, as they each require ISA development, compiler and assembler extensions, and either simulator or RTL models.</p> <p>To study the potential of BSAs, we propose a novel modeling technique called the Transformable Dependence Graph (TDG) - a higher level alternative to the time-consuming traditional compiler+simulator approach, while still enabling detailed microarchitectural models for both general cores and accelerators. We then propose a multi-BSA organization, called ExoCore, which we model and study using the TDG. A design space exploration reveals that an ExoCore organization can push designs beyond the established energy-performance frontiers for general purpose cores. For example, a 2-wide OOO processor with three BSAs matches the performance of a conventional 6-wide OOO core, has 40% lower area, and is 2.6x more energy efficient.</p>","0163-5980;01635980","","10.1145/2954680.2872412","","","accelerators;dependence graphs;modeling;program behaviors;specialization","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"Improving random write performance in homogeneous and heterogeneous erasure-coded drive arrays","N. Jeremic; H. Parzyjegla; G. M√ºhl","University of Rostock, Rostock, Germany","ACM SIGAPP Applied Computing Review","20161111","2015","15","4","31","53","<p>In data storage systems, drive arrays known as RAIDs are often used in order to avoid data loss and to maintain availability in the event of drive failure(s). RAID schemes define various drive array organizations (denoted as RAID levels) that can be used in arrays of hard disk drives (HDDs) and arrays of NAND flash memory solid-state drives (SSDs). For larger drive arrays, using data striping with erasure coding is appealing due to its notably higher space efficiency compared to data replication. However, the main issue of data striping with erasure coding is the performance of random writes smaller than a stripe. This problem is even aggravated if the random access performance characteristics of the deployed device type (HDD or SSD) and device model are not properly considered when choosing the data striping configuration (in particular the stripe unit size).</p> <p>In this article, we provide an analytical model allowing to predict the random write throughput of homogeneous drive arrays as well as of a heterogeneous drive array with code blocks stored on the faster drives. Based on our model, we develop a method to improve the random write throughput in homogeneous drive arrays (comprising only one device type, e.g., HDDs or SSDs) by adapting the data striping configuration to the used device type and model in relation to the workload. Then, based on our previous work, we describe an organization for heterogeneous drive arrays, which is especially suitable for arrays combining HDDs with SSDs, and permits to further increase the random write throughput by storing data blocks on slower and code blocks on faster drives. Finally, we experimentally evaluate our analytical claims and show that random write throughput can indeed be notably increased in drive arrays that use data striping with erasure coding.</p>","1559-6915;15596915","","10.1145/2893706.2893709","","","RAID;data striping;erasure coding;hard disk drives;heterogeneous data storage systems;solid-state drives","","","","","","","","","","","December 2015","","ACM","ACM Journals & Magazines"
"Engineering Quality and Reliability in Technology-Assisted Review","G. V. Cormack; M. R. Grossman","University of Waterloo, Waterloo, ON, Canada","Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval","20161111","2016","","","75","84","<p>The objective of technology-assisted review (""TAR"") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.</p>","","","10.1145/2911451.2911510","","","continuous active learning;e-discovery;electronic discovery;predictive coding;quality;relevance feedback;reliability;systematic review;technology-assisted review;test collections","","","","","","","","","","","17-21 July 2016","","ACM","ACM Conferences"
"Assessing the accuracy of trauma patient prioritization: communication design of the M.I.S.E.R information system protocol and communication channel during crisis communication exchanges","T. A. Avtgis; D. Kappel; E. P. Polack; A. Wilson; J. Knight","Ashland University","Communication Design Quarterly Review","20161111","2015","3","4","85","90","<p>This study sought to investigate the effectiveness of an information exchange protocol (M.I.S.E.R) designed to increase the effectiveness of messages pertaining to rural trauma patients and triage prioritization. Trained coders were randomly assigned to three conditions; audio, transcript, and transcript and audio. Participants coded several hundred actual information exchanges between first responders and medical command operators. Findings confirm the effectiveness of the M.I.S.E.R. information exchange protocol as well as the effectiveness of exchanging crisis messages via two-way radio as compared to having a transcript of the call or both audio recordings and transcripts. Implications for communication design, healthcare practitioners, and effective modes for exchanging crisis communication messages are presented.</p>","2166-1200;21661200","","10.1145/2826972.2826980","","","communication design;crisis communication;first responder communication;health communication;medical communication;patient safety;trauma medicine;triage communication","","","","","","","","","","","August 2015","","ACM","ACM Journals & Magazines"
"Towards dynamic adaptation in broadcasting with hybrid rateless codes","C. Faneca; J. Vieira; A. Z√∫quete; J. Cano; A. Moreira; L. Almeida","University of Aveiro, Aveiro, Portugal","ACM SIGBED Review","20161111","2015","12","3","45","48","<p>There are many situations, such as in training and education, in which there is a frequent need to distribute large files to many clients, e.g., operating system boot images or raw data files. To carry out such distribution efficiently, we use wireless broadcast and a hybrid coding technique that combines forward coding using weak LT Codes with a feedback phase at the end, which allows concluding the process faster with lower computing cost than traditional LT codes. However, for the sake of scalability, in the feedback phase a scheduler bounds the maximum number of clients that can communicate feedback in each given cycle and schedules them. Moreover, using a shorter or longer feedback phase also impacts on the number of simultaneous clients in feedback mode, resulting in more or less impact of the scheduler and more or less effectiveness of the feedback itself. In this short paper we briefly describe a recently developed prototype and we summarize some preliminary results confirming the advantage of using scheduled feedback. Moreover, we discuss the interplay between duration of the feedback phase and clients scheduling as a line for future research in scheduling-coding co-design.</p>","","","10.1145/2815482.2815491","","","LT codes;ad-hoc networks;fountain codes;traffic scheduling;weak-LT codes;wireless broadcast","","","","","","","","","","","June 2015","","ACM","ACM Journals & Magazines"
"RAPID Programming of Pattern-Recognition Processors","K. Angstadt; W. Weimer; K. Skadron","University of Virginia, Charlottesville, VA, USA","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","593","605","<p>We present RAPID, a high-level programming language and combined imperative and declarative model for programming pattern-recognition processors, such as Micron's Automata Processor (AP). The AP is a novel, non-Von Neumann architecture for direct execution of non-deterministic finite automata (NFAs), and has been demonstrated to provide substantial speedup for a variety of data-processing applications. RAPID is clear, maintainable, concise, and efficient both at compile and run time. Language features, such as code abstraction and parallel control structures, map well to pattern-matching problems, providing clarity and maintainability. For generation of efficient runtime code, we present algorithms to convert RAPID programs into finite automata. Further, we introduce a tessellation technique for configuring the AP, which significantly reduces compile time, increases programmer productivity, and improves maintainability. We evaluate five RAPID programs against custom, baseline implementations previously demonstrated to be significantly accelerated by the AP. We find that RAPID programs are much shorter in length, are expressible at a higher level of abstraction than their handcrafted counterparts, and yield generated code that is often more compact. In addition, our tessellation technique for configuring the AP has comparable device utilization to, and results in compilation that is up to four orders of magnitude faster than, current solutions.</p>","0163-5980;01635980","","10.1145/2954680.2872393","","","MISD;accelerators;automata processor","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"Datix: A System for Scalable Network Analytics","D. Sarlis; N. Papailiou; I. Konstantinou; G. Smaragdakis; N. Koziris","CSLab, NTUA, Athens, Greece","ACM SIGCOMM Computer Communication Review","20161111","2015","45","5","21","28","<p>The ever-increasing Internet traffic poses challenges to network operators and administrators that have to analyze large network datasets in a timely manner to make decisions regarding network routing, dimensioning, accountability and security. Network datasets collected at large networks such as Internet Service Providers (ISPs) or Internet Exchange Points (IXPs) can be in the order of Terabytes per hour. Unfortunately, most of the current network analysis approaches are ad-hoc and centralized, and thus not scalable.</p> <p>In this paper, we present Datix, a fully decentralized, open-source analytics system for network traffic data that relies on smart partitioning storage schemes to support fast join algorithms and efficient execution of filtering queries. We outline the architecture and design of Datix and we present the evaluation of Datix using real traces from an operational IXP. Datix is a system that deals with an important problem in the intersection of data management and network monitoring while utilizing state-of-the-art distributed processing engines. In brief, Datix manages to efficiently answer queries within minutes compared to more than 24 hours processing when executing existing Python based code in single node setups. Datix also achieves nearly 70% speedup compared to baseline query implementations of popular big data analytics engines such as Hive and Shark.</p>","0146-4833;01464833","","10.1145/2831347.2831351","","","hadoop;hbase;k-d tree;map-join;mapreduce;sflow","","","","","1","","","","","","October 2015","","ACM","ACM Journals & Magazines"
"How to Build Static Checking Systems Using Orders of Magnitude Less Code","F. Brown; A. N√∂tzli; D. Engler","Stanford University, Stanford , CA, USA","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","143","157","<p>Modern static bug finding tools are complex. They typically consist of hundreds of thousands of lines of code, and most of them are wedded to one language (or even one compiler). This complexity makes the systems hard to understand, hard to debug, and hard to retarget to new languages, thereby dramatically limiting their scope. This paper reduces checking system complexity by addressing a fundamental assumption, the assumption that checkers must depend on a full-blown language specification and compiler front end. Instead, our program checkers are based on drastically incomplete language grammars (""micro-grammars"") that describe only portions of a language relevant to a checker. As a result, our implementation is tiny-roughly 2500 lines of code, about two orders of magnitude smaller than a typical system. We hope that this dramatic increase in simplicity will allow people to use more checkers on more systems in more languages.</p> <p>We implement our approach in Œºchex, a language-agnostic framework for writing static bug checkers. We use it to build micro-grammar based checkers for six languages (C, the C preprocessor, C++, Java, JavaScript, and Dart) and find over 700 errors in real-world projects.</p>","0163-5980;01635980","","10.1145/2954680.2872364","","","bug finding;micro-grammars;parsing;static analysis","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"A ""hitchhiker's"" guide to fast and efficient data reconstruction in erasure-coded data centers","K. V. Rashmi; N. B. Shah; D. Gu; H. Kuang; D. Borthakur; K. Ramchandran","UC Berkeley, Berkeley, USA","ACM SIGCOMM Computer Communication Review","20161111","2014","44","4","331","342","<p>Erasure codes such as Reed-Solomon (RS) codes are being extensively deployed in data centers since they offer significantly higher reliability than data replication methods at much lower storage overheads. These codes however mandate much higher resources with respect to network bandwidth and disk IO during reconstruction of data that is missing or otherwise unavailable. Existing solutions to this problem either demand additional storage space or severely limit the choice of the system parameters. In this paper, we present ""Hitchhiker"", a new erasure-coded storage system that reduces both network traffic and disk IO by around 25% to 45% during reconstruction of missing or otherwise unavailable data, with no additional storage, the same fault tolerance, and arbitrary flexibility in the choice of parameters, as compared to RS-based systems. Hitchhiker 'rides' on top of RS codes, and is based on novel encoding and decoding techniques that will be presented in this paper. We have implemented Hitchhiker in the Hadoop Distributed File System (HDFS). When evaluating various metrics on the data-warehouse cluster in production at Facebook with real-time traffic and workloads, during reconstruction, we observe a 36% reduction in the computation time and a 32% reduction in the data read time, in addition to the 35% reduction in network traffic and disk IO. Hitchhiker can thus reduce the latency of degraded reads and perform faster recovery from failed or decommissioned machines.</p>","0146-4833;01464833","","10.1145/2740070.2626325","","","degraded reads;disk IO;distributed storage;erasure codes;fault tolerance;network traffic;recovery","","","","","13","3","","","","","October 2014","","ACM","ACM Journals & Magazines"
"Scaling up Superoptimization","P. M. Phothilimthana; A. Thakur; R. Bodik; D. Dhurjati","University of California, Berkeley, Berkeley, CA, USA","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","297","310","<p>Developing a code optimizer is challenging, especially for new, idiosyncratic ISAs. Superoptimization can, in principle, discover machine-specific optimizations automatically by searching the space of all instruction sequences. If we can increase the size of code fragments a superoptimizer can optimize, we will be able to discover more optimizations. We develop LENS, a search algorithm that increases the size of code a superoptimizer can synthesize by rapidly pruning away invalid candidate programs. Pruning is achieved by selectively refining the abstraction under which candidates are considered equivalent, only in the promising part of the candidate space. LENS also uses a bidirectional search strategy to prune the candidate space from both forward and backward directions. These pruning strategies allow LENS to solve twice as many benchmarks as existing enumerative search algorithms, while LENS is about 11-times faster.</p> <p>Additionally, we increase the effective size of the superoptimized fragments by relaxing the correctness condition using contexts (surrounding code). Finally, we combine LENS with complementary search techniques into a cooperative superoptimizer, which exploits the stochastic search to make random jumps in a large candidate space, and a symbolic (SAT-solver-based) search to synthesize arbitrary constants. While existing superoptimizers consistently solve 9--16 out of 32 benchmarks, the cooperative superoptimizer solves 29 benchmarks. It can synthesize code fragments that are up to 82% faster than code generated by gcc -O3 from WiBench and MiBench.</p>","0163-5980;01635980","","10.1145/2954680.2872387","","","SMT;program synthesis;superoptimization","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"Cogent: Verifying High-Assurance File System Implementations","S. Amani; A. Hixon; Z. Chen; C. Rizkallah; P. Chubb; L. O'Connor; J. Beeren; Y. Nagashima; J. Lim; T. Sewell; J. Tuong; G. Keller; T. Murray; G. Klein; G. Heiser","Data61 and UNSW, Sydney, Australia","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","175","188","<p>We present an approach to writing and formally verifying high-assurance file-system code in a restricted language called Cogent, supported by a certifying compiler that produces C code, high-level specification of Cogent, and translation correctness proofs. The language is strongly typed and guarantees absence of a number of common file system implementation errors. We show how verification effort is drastically reduced for proving higher-level properties of the file system implementation by reasoning about the generated formal specification rather than its low-level C code. We use the framework to write two Linux file systems, and compare their performance with their native C implementations.</p>","0163-5980;01635980","","10.1145/2954680.2872404","","","co-generation;domain-specific languages;file systems;isabelle/hol;verification","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"M3: A Hardware/Operating-System Co-Design to Tame Heterogeneous Manycores","N. Asmussen; M. V√∂lp; B. N√∂then; H. H√§rtig; G. Fettweis","Technische Universit&#228;t Dresden, Dresden, Germany","ACM SIGOPS Operating Systems Review","20161111","2016","50","2","189","203","<p>In the last decade, the number of available cores increased and heterogeneity grew. In this work, we ask the question whether the design of the current operating systems (OSes) is still appropriate if these trends continue and lead to abundantly available but heterogeneous cores, or whether it forces a fundamental rethinking of how systems are designed. We argue that: 1. hiding heterogeneity behind a common hardware interface unifies, to a large extent, the control and coordination of cores and accelerators in the OS, 2. isolating at the network-on-chip rather than with processor features (like privileged mode, memory management unit, ...), allows running untrusted code on arbitrary cores, and 3. providing OS services via protocols over the network-on-chip, instead of via system calls, makes them accessible to arbitrary types of cores as well.</p> <p>In summary, this turns accelerators into first-class citizens and enables a single and convenient programming environment for all cores without the need to trust any application.</p> <p>In this paper, we introduce network-on-chip-level isolation, present the design of our microkernel-based OS, M3, and the common hardware interface, and evaluate the performance of our prototype in comparison to Linux. A bit surprising, without using accelerators, M3 outperforms Linux in some application-level benchmarks by more than a factor of five.</p>","0163-5980;01635980","","10.1145/2954680.2872371","","","accelerators;capabilities;heterogeneous architectures;on-chip networks;operating systems","","","","","","","","","","","June 2016","","ACM","ACM Journals & Magazines"
"Vidyut: exploiting power line infrastructure for enterprise wireless networks","V. Yenamandra; K. Srinivasan","The Ohio State University, Columbus, OH, USA","ACM SIGCOMM Computer Communication Review","20161111","2014","44","4","595","606","<p>Global synchronization across time and frequency domains significantly benefits wireless communications. Multi-Cell (Network) MIMO, interference alignment solutions, opportunistic routing techniques in ad-hoc networks, OFDMA etc. all necessitate synchronization in either time or frequency domain or both. This paper presents sysname, a system that exploits the easily accessible and ubiquitous power line infrastructure to achieve synchronization in time and frequency domains across nodes distributed beyond a single-collision domain. sysname uses the power lines to transmit a reference frequency tone to which each node locks its frequency. sysname exploits the steady periodicity of delivered power signal itself to synchronize distributed nodes in time.</p> <p>We validate the extent of sysname's synchronization and evaluate its effectiveness. We verify sysname's suitability for wireless applications such as OFDMA and multi-cell MIMO by validating the benefits of global synchronization in an enterprise wireless network. Our experiments show a throughput gain of 8.2x over MegaMIMO, 7x over NemoX and 2.5x over OFDMA systems.</p> <p>Enterprise wireless networks are supported by an Ethernet backbone. Researchers have been exploring techniques over the backbone to enable and assist in improving the performance of the wireless networks. Recent work also showed the benefit of sharing information in the air between the nodes to enable higher performance of the network. Even sharing information as little as synchronization information has been demonstrated to open new avenues (such as MU-MIMO, Physical Network Coding, Opportunistic routing, etc.) for the wireless networks to enhance its performance. However, another medium shared by majority of the nodes in the enterprise network - the power line infrastructure - has been largely left untapped to assist the wireless network. While the power lines are noisy and have a frequency selective transmission characteristic, its uniq- eness is that its range can extend beyond that over air while it is unburdened by switching and other responsibilities of the Ethernet backbone. This paper poses the following question: How best to exploit the opportunity presented by the power lines to further enhance enterprise wireless networks?</p> <p>The key contributions of this paper are the following: Identify and demonstrate the feasibility of utilizing power lines as a medium to achieve synchronization (in time and frequency domains) between nodes in the network; Demonstrate the scalability of this technique by achieving synchronization between nodes beyond the transmission range of any of the individual nodes. The paper presents empirical results pertaining to the accuracy of synchronization and the benefit of the proposed synchronization method to existing distributed wireless techniques by virtue of extension across multiple collision domains.</p>","0146-4833;01464833","","10.1145/2740070.2626329","","","frequency synchronization;network mimo;power line communications;time synchronization;wireless networks","","","","","3","","","","","","October 2014","","ACM","ACM Journals & Magazines"
"Reviewing and Evaluating Existing File Carving Techniques for JPEG Files","E. Alshammary; A. Hadi","","2016 Cybersecurity and Cyberforensics Conference (CCC)","20161020","2016","","","55","59","Digital forensic is the process of collecting, preserving and analyzing the evidence obtained from the digital media, which start from the moment of collecting these evidence. On the other hand, if the file system is corrupted or deleted, then file carving techniques should be established to restore the maximum amount of deleted data, especially, the fragments. File carving is a methodology that helps the investigators to retrieve and acquire the data from unallocated space. There are many carving techniques are used to find different types of file such as (PDF, XML, JPEG etc.). This paper will focus on the JPEG file carving techniques since the JPEG is the most widespread loss compression formats used by digital cameras. The main contribution of this paper is to review the existing techniques for JPEG file carving and evaluate them to identifying their characteristics. Also are classified according to the types of carving techniques used, fragmentation handling issues, and the existence of file system. Additionally, a hybrid method proposed to perform special tasks depending on fragmentation handling issues. To the best of our knowledge, this paper becomes a step forward for researchers interested in carving techniques by helping them finding the best algorithm for carving and obtaining any deleted data in cyberspace.","","","10.1109/CCC.2016.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7600210","Data Recovery;Digital Forensic;File Carving;JPEG Carving","Digital forensics;File systems;Image segmentation;Media;Metadata;Standards;Transform coding","image coding;image forensics;image retrieval","JPEG files;cyberspace;data acquisition;data retrieval;digital cameras;digital forensic;digital media;evidence analysis;evidence collection;evidence preservation;file carving techniques;fragmentation handling issues;hybrid method;loss compression formats","","1","","","","","","2-4 Aug. 2016","","IEEE","IEEE Conferences"
"Sensor-Cloud and Power Line Communication: Recent Developments and Integration","C. Zhu; Y. Huo; V. C. M. Leung; L. T. Yang","","2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)","20161013","2016","","","302","308","Sensor-Cloud (SC) and power line communication (PLC) are attracting growing attention from both academia and industry, by 1) integrating wireless sensor networks and cloud computing and 2) utilizing the power grid infrastructure to perform data transmission, respectively. Aiming to be an enlightening guidance for research about SC and PLC, this paper first reviews SC and PLC, by presenting the recent developments about SC and PLC. Further, this paper explores SC and PLC, by envisioning the applications and advantages of the integration of SC and PLC. We hope our investigations can inspire more research about SC and PLC to make them develop faster and better.","","","10.1109/DASC-PICom-DataCom-CyberSciTec.2016.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588863","Sensor-Cloud;developments;integration;power line communications","Cloud computing;Data communication;MIMO;Mobile communication;Security;Turbo codes;Wireless sensor networks","","","","1","","","","","","8-12 Aug. 2016","","IEEE","IEEE Conferences"
"Advances in Two-Photon Scanning and Scanless Microscopy Technologies for Functional Neural Circuit Imaging","S. R. Schultz; C. S. Copeland; A. J. Foust; P. Quicke; R. Schuck","Center for Neurotechnology and the Department of Bioengineering, Imperial College London, South Kensington, London, U.K.","Proceedings of the IEEE","20161222","2017","105","1","139","157","Recent years have seen substantial developments in technology for imaging neural circuits, raising the prospect of large-scale imaging studies of neural populations involved in information processing, with the potential to lead to step changes in our understanding of brain function and dysfunction. In this paper, we will review some key recent advances: improved fluorophores for single-cell resolution functional neuroimaging using a two-photon microscope; improved approaches to the problem of scanning active circuits; and the prospect of scanless microscopes which overcome some of the bandwidth limitations of current imaging techniques. These advances in technology for experimental neuroscience have in themselves led to technical challenges, such as the need for the development of novel signal processing and data analysis tools in order to make the most of the new experimental tools. We review recent work in some active topics, such as region of interest segmentation algorithms capable of demixing overlapping signals, and new highly accurate algorithms for calcium transient detection. These advances motivate the development of new data analysis tools capable of dealing with spatial or spatiotemporal patterns of neural activity that scale well with pattern size.","0018-9219;00189219","","10.1109/JPROC.2016.2577380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7579213","Acousto-optic;beamlets;calcium imaging;calcium transient detection;cortical circuits;galvanometric scanning;holography;lightsheet microscopy;multiphoton imaging;neural code;neurotechnology","Calcium;Fluorescence;Kinetic theory;Microscopy;Neurons;Proteins","neural nets;optical microscopy;two-photon processes","brain dysfunction;brain function;data analysis tools;fluorophores;functional neural circuit imaging;information processing;region of interest segmentation algorithms;scanless microscopes;scanless microscopy technologies;scanning active circuits;signal processing;single-cell resolution functional neuroimaging;two-photon microscope;two-photon scanning","","","","","","","20160928","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Domain-Specific Video Compression for Long-Term Archiving of Endoscopic Surgery Videos","B. M√ºnzer; K. Schoeffmann; L. B√∂sz√∂rmenyi","Lakeside Labs., Klagenfurt Univ., Klagenfurt, Austria","2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)","20160818","2016","","","312","317","Common lossy video compression methods have already reached a very high performance level and are about to reach their natural limit. However, their compression efficiency is still insufficient for certain domains where a plethora of video data should be archived. One example is the domain of medical endoscopy where entire surgeries are recorded for documentation with high visual quality because it is not possible to anticipate if and when exceptional situations may happen. Eventually, only a fraction of the video footage is of actual relevance for post-operative review. In this paper, we introduce the concept of domain-specific video compression by the example of this very specific domain. We identify several characteristics of endoscopic surgery videos that can be exploited by content-based analysis algorithms to achieve a significantly higher compression efficiency than in current practice. The extensive evaluation shows that the proposed methods massively reduce the data volume without any semantic information loss and thus allow for long-term video archiving of endoscopic surgery videos. Moreover, we expect to stimulate future research in this challenging new field.","","Electronic:978-1-4673-9036-1; POD:978-1-4673-9037-8","10.1109/CBMS.2016.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546009","Domain-specific video compression;content-based video analysis;endoscopic video","Biomedical imaging;Documentation;Endoscopes;Semantics;Surgery;Video compression;Visualization","data compression;endoscopes;information retrieval systems;medical image processing;surgery;video coding","compression efficiency;content-based analysis algorithms;domain-specific video compression;endoscopic surgery videos;long-term video archiving;lossy video compression;medical endoscopy","","","","","","","","20-24 June 2016","","IEEE","IEEE Conferences"
"An Overview of Device-to-Device Communications Technology Components in METIS","G. Fodor; S. Roger; N. Rajatheva; S. B. Slimane; T. Svensson; P. Popovski; J. M. B. Da Silva; S. Ali","Ericsson Research, Stockholm, Sweden","IEEE Access","20170520","2016","4","","3288","3299","As the standardization of network-assisted device-to-device (D2D) communications by the Third Generation Partnership Project progresses, the research community has started to explore the technology potential of new advanced features that will largely impact the performance of 5G networks. For 5G, D2D is becoming an integrative term of emerging technologies that take an advantage of the proximity of communicating entities in licensed and unlicensed spectra. The European 5G research project Mobile and Wireless Communication Enablers for the 2020 Information Society (METIS) has identified advanced D2D as a key enabler for a variety of 5G services, including cellular coverage extension, social proximity, and communicating vehicles. In this paper, we review the METIS D2D technology components in three key areas of proximal communications - network-assisted multi-hop, full-duplex, and multi-antenna D2D communications - and argue that the advantages of properly combining cellular and ad hoc technologies help to meet the challenges of the information society beyond 2020.","","","10.1109/ACCESS.2016.2585188","Anite Telecoms; CNPq Brazilian Research-Support Agency; EU Project P2PSmartest; European Fonds Europ¬øen de D¬øveloppement ¬øconomique et R¬øgional Funds; European Union within the 7th Framework Programme through the Mobile and Wireless Communications Enablers for the Twenty-Twenty Information Society (METIS) Project; Finnish Funding Agency for Technology and Innovation (Tekes); Swedish Foundation for Strategic Research Strategic Mobility SM13-0008 Project Matthew; Wireless@KTH Project BUSE; 10.13039/100004356 - Nokia; 10.13039/501100003329 - Ministerio de Economia y Competitividad, Spain; 10.13039/501100003816 - Huawei Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500139","Device-to-device communications;MIMO systems;cooperative communications;full duplex;network coding;vehicular communications","3GPP;5G mobile communication;Cooperative communication;Electronic mail;Encoding;Intelligent vehicles;MIMO;Machine-to-machine communication;Network coding;Transmitters;Uplink","3G mobile communication;5G mobile communication;antenna arrays;cellular radio;mobile antennas;standardisation","5G networks;European 5G research project;METIS D2D technology components;Mobile and Wireless Communication Enablers for the 2020 Information Society;ad hoc technology;cellular coverage extension;communicating vehicles;device-to-device communication technology components;multiantenna D2D communications;network-assisted D2D communication standardization;network-assisted multihop full-duplex D2D communications;proximal communications;social proximity;third generation partnership project","","6","","27","","","20160627","2016","","IEEE","IEEE Journals & Magazines"
"A Review of Compressive Sensing in Information Security Field","Y. Zhang; L. Y. Zhang; J. Zhou; L. Liu; F. Chen; X. He","Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, School of Electronics and Information Engineering, Southwest University, Chongqing, China","IEEE Access","20170520","2016","4","","2507","2519","The applications of compressive sensing (CS) in the field of information security have captured a great deal of researchers' attention in the past decade. To supply guidance for researchers from a comprehensive perspective, this paper, for the first time, reviews CS in information security field from two aspects: theoretical security and application security. Moreover, the CS applied in image cipher is one of the most widespread applications, as its characteristics of dimensional reduction and random projection can be utilized and integrated into image cryptosystems, which can achieve simultaneous compression and encryption of an image or multiple images. With respect to this application, the basic framework designs and the corresponding analyses are investigated. Specifically, the investigation proceeds from three aspects, namely, image ciphers based on chaos and CS, image ciphers based on optics and CS, and image ciphers based on chaos, optics, and CS. A total of six frameworks are put forward. Meanwhile, their analyses in terms of security, advantages, disadvantages, and so on are presented. At last, we attempt to indicate some other possible application research topics in future.","","","10.1109/ACCESS.2016.2569421","Fundamental Research Funds for the Shenzhen University; Macau Science and Technology Development Fund; Research Committee within the University of Macau; Research Foundation of the Education Department of Jiangxi Province; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473818","Compressive sensing;application security;chaos;image cipher;optics;theoretical security","Chaos theory;Ciphers;Compressed sensing;Computer security;Cryptography;Image processing;Information security;Optical fiber communication","chaos;compressed sensing;cryptography;image coding","CS;application security;chaos;compressive sensing;dimensional reduction;image cipher;image cryptosystems;image encryption;information security field;random projection;theoretical security","","13","","128","","","20160519","2016","","IEEE","IEEE Journals & Magazines"
"DRAT: An Unobtrusive, Scalable Approach to Large Scale Software License Analysis","C. A. Mattmann; J. H. Oh; T. Palsulich; L. J. McGibbney; Y. Gil; V. Ratnakar","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA","2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)","20160307","2015","","","97","101","The Apache Release Audit Tool (RAT) performs software open source license auditing and checking, however RAT fails to successfully audit today's large code bases. Being a natural language processing (NLP) tool and a crawler, RAT marches through a code base, but uses rudimentary black lists and white lists to navigate source code repositories, and often does a poor job of identifying source code versus binary files. In addition RAT produces no incremental output and thus on code bases that themselves are ""Big Data"", RAT could run for e.g., a month and still not provide any status report. We introduce Distributed ""RAT"" or the Distributed Release Audit Tool (DRAT). DRAT overcomes RAT's limitations by leveraging: (1) Apache Tika to automatically detect and classify files in source code repositories and determine what is a binary file, what is source code, what are notes that need skipping, etc. (2) Apache Solr to interactively perform analytics on a code repository and to extract metadata using Apache Tika, and finally (3) Apache OODT to run RAT on per-MIME type (e.g., C/C++, Java, Javascript, etc.) and per configurable K-file sized chunks in a MapReduce workflow. Each Mapper task is an instance of RAT running on a K-file sized per Multipurpose Internet Mail Extensions (MIME) type chunk (split using Tika) and each mapper produces and incremental and intermediate log file, and where the Reducer aggregates the individual log files.","","Electronic:978-1-4673-9775-9; POD:978-1-4673-9776-6","10.1109/ASEW.2015.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426645","DRAT;Open source;Software license auditing","Government;Java;Licenses;Metadata;Open source software;Open systems","meta data;pattern classification;software engineering;software reviews;source code (software)","Apache OODT;Apache Solr;Apache Tika;DRAT;Distributed Release Audit Tool;MapReduce workflow;Multipurpose Internet Mail Extensions;binary file;code repository analytics;file classification;file detection;large scale software license analysis;metadata extraction;per-MIME type;source code repositories","","","","12","","","","9-13 Nov. 2015","","IEEE","IEEE Conferences"
"GRAPEVINE: hybrid cooperative opportunistic routing for challenged wireless networks using fountain coding","A. N. Plymoth; P. Johansson; R. L. Cruz; O. Chipara; W. G. Griswold","Calit2, University of California San Diego, La Jolla, CA, USA","ACM SIGMOBILE Mobile Computing and Communications Review","20160129","2013","17","1","61","70","<p>This paper present Grapevine, a wireless networking protocol designed to be used in challenged environments such an emergency response network. These environments typically experience a lot of noise, interference, disconnections and high mobility resulting in high packet loss rates. As often critical data needs to be disseminated to other nodes it is important to have a protocol that efficiently delivers data under these conditions, but which is also efficient under good conditions. Grapevine uses to fountain coding to opportunistically and cooperatively to efficiently delivers data in high throughput wireless multi hop networks, as well as lossy and delay tolerant networks. Results show that our flooding based protocol is more efficient than traditional protocols in lossy networks both in terms of lower delay and lower overhead.</p>","1559-1662;15591662","","10.1145/2502935.2502947","","","","","","","","","","","","","","January 2013","","ACM","ACM Journals & Magazines"
"Accelerated model-based robustness testing of state machine implementations","P. Heckeler; H. Eichelberger; T. Kropf; J. Ruf; S. Huster; S. Burg; W. Rosenstiel; B. Schlich","University of T&#252;bingen, T&#252;bingen, Germany","ACM SIGAPP Applied Computing Review","20160129","2013","13","3","50","67","<p>In this paper, we present an approach for accelerating model-based robustness testing of state-based software components. We use the Z3 SAT solver to derive executable test cases fully automatically from a state model based on a UML Statechart. The test cases are paths along the Statechart comprising states and transitions. The main advantage of our approach is an accelerated execution of the test cases by refining already reached states as starting points for further test cases using reverse execution. Furthermore, the presented approach is able to check whether a correct target state has been reached or not when executing the path-based test cases using runtime verification and therefore increases the test significance. The runtime verification is done without instrumenting the source code of the component under test.</p>","1559-6915;15596915","","10.1145/2537728.2537733","","","acceleration;model-based testing;robustness;runtime verification","","","","","1","","","","","","September 2013","","ACM","ACM Journals & Magazines"
"Summary-based data-flow analysis that understands regular composite objects and iterators","X. Tang; J. J√§rvi","Texas A&M University, College Station TX","ACM SIGAPP Applied Computing Review","20160129","2012","12","4","36","47","<p>Today's industrial-strength compilers do not take advantage of the semantics of user-defined types and operations when analyzing code involving objects of user-defined types. We show that user-defined types that are both ""regular"" and ""composite"" (roughly corresponding to what is casually known as ""value semantics"") can, however, be analyzed efficiently and effectively. The notion of regularity comes from generic programming and C++. Programmers routinely rely on regularity when reasoning about generic code and manually performing (optimizing) code transformations and rewrites. Stepanov suggests that compilers, too, should take advantage of regularity to expand the opportunities for applying optimizing transformations. This paper exploits the properties of regular composite objects to produce concise procedure summaries for summary-based analyses, thus taking a step towards Stepanov's goal. In addition to regularity and compositeness, we also make our analysis aware of the prevalent ""iterator"" abstraction, which expands the applicability of our approach. We target the C++ language, and use the LLVM framework to implement the analysis.</p>","1559-6915;15596915","","10.1145/2432546.2432549","","","C++;generic programming;program analysis","","","","","","","","","","","December 2012","","ACM","ACM Journals & Magazines"
"Supporting semantic conflict prevention in real-time collaborative programming environments","H. Fan; C. Sun","Nanyang Technological University, Singapore","ACM SIGAPP Applied Computing Review","20160129","2012","12","2","39","52","<p>Real-time collaborative programming environments support multiple programmers to concurrently edit shared source code documents at the same time over communication networks. One of the key challenges in supporting real-time collaborative programming is semantic conflicts, which may occur when multiple programmers are performing concurrent and incompatible programming work in the same source code region or in different source code regions with dependency relationships. Semantic conflicts may result in programming errors that are difficult to detect and costly to resolve. This paper presents in-depth analyses of representative collaborative programming scenarios for understanding the nature and general conditions of semantic conflicts, and proposes a novel <i>Dependency-based Automatic Locking</i> (DAL) approach for supporting semantic conflict prevention in real-time collaborative programming environments. The novelty of the DAL approach lies in its capabilities of supporting automatic and fine-grained locking on selected source code regions with dependency relationships to balance conflict prevention, concurrent work, and programmer convenience in real-time collaborative programming. The DAL approach and technical solutions have been implemented in the <i>CoEclipse</i> prototype system as a proof-of-concept for this emerging technique, which is being continuously extended for further exploration and evaluation.</p>","1559-6915;15596915","","10.1145/2340416.2340420","","","conflict prevention;dependency graph;dependency relationship;locking;locking state maintenance;real-time collaborative programming;semantic conflict;semantic consistency","","","","","1","","","","","","2012","","ACM","ACM Journals & Magazines"
"Tolerating path heterogeneity in multipath TCP with bounded receive buffers","M. Li; A. Lukyanenko; S. Tarkoma; Y. Cui; A. Yl√§-J√§√§ski","Aalto University, Espoo, Finland","ACM SIGMETRICS Performance Evaluation Review","20160129","2013","41","1","375","376","","0163-5999;01635999","","10.1145/2494232.2465750","","","MPTCP;NS-3;TCP;flow control;systematic coding","","","","","","","","","","","June 2013","","ACM","ACM Journals & Magazines"
"Towards optimal error-estimating codes through the lens of Fisher information analysis","N. Hua; A. Lall; B. Li; J. Xu","Georgia Tech, Atlanta, GA, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","1","125","136","<p>Error estimating coding (EEC) has recently been established as an important tool to estimate bit error rates in the transmission of packets over wireless links, with a number of potential applications in wireless networks. In this paper, we present an in-depth study of error estimating codes through the lens of Fisher information analysis and find that the original EEC estimator fails to exploit the information contained in its code to the fullest extent. Motivated by this discovery, we design a new estimator for the original EEC algorithm, which significantly improves the estimation accuracy, and is empirically very close to the Cramer-Rao bound. Following this path, we generalize the EEC algorithm to a new family of algorithms called gEEC generalized EEC. These algorithms can be tuned to hold 25-35% more information with the same overhead, and hence deliver even better estimation accuracy---close to optimal, as evidenced by the Cramer-Rao bound. Our theoretical analysis and assertions are supported by extensive experimental evaluation.</p>","0163-5999;01635999","","10.1145/2318857.2254773","","","Fisher information;error estimating coding","","","","","","","","","","","June 2012","","ACM","ACM Journals & Magazines"
"ByPass: towards an unified transport protocol for the internet","A. Chanda; S. C. Nelson; G. Bhanage; D. Raychaudhuri","WINLAB, Rutgers University","ACM SIGMOBILE Mobile Computing and Communications Review","20160129","2012","16","4","2","3","<p>Due to the limitations in the performance of the transmission control protocol (TCP) over wireless networks, multiple variants and alternatives have been proposed and evaluated. In most cases, these protocols are designed for specific type of networks and they perform well in their best case, for example, HOP performs well in mesh networks but would incur unnecessary overhead in wired networks . For supporting these different protocols across the Internet, all hosts need to have implementation of multiple transport protocols running. The application layer can then ask for a specific transport protocol as required. Since the transport layer is typically built in the operating system of most static and mobile communication end points, most protocols cannot be supported on legacy networks. This paper proposes ByPass, a transport layer protocol which alleviates the need for implementing different transport protocols. We present ByPass as an unified transport layer protocol that can switch modes according to variation in the network type to provide steady performance guarantee. For a network with unstable wireless links, it will switch to a hop-by-hop reliable mode and when the links are steady, it will switch to an end-to-end reliable mode. Using the ByPass framework API, application layer can be completely agnostic of the transport approach thus allowing common application code to be written across different types of network.</p>","1559-1662;15591662","","10.1145/2436196.2436198","","","","","","","","","","","","","","October 2012","","ACM","ACM Journals & Magazines"
"When to stop reviewing documents in eDiscovery cases: the Lit i View quality monitor and endpoint detector","J. Halskov; H. Takeda","UBIC Japan, Konan, Minato-ku Tokyo, Japan","Proceedings of the Fifth International Conference on Management of Emergent Digital EcoSystems","20160129","2013","","","227","232","<p>In this paper, we describe a key issue of the review phase of litigation cases under the Discovery system, namely when to stop reviewing potentially relevant (""responsive"") documents. UBIC's Lit i View software has recently been augmented with a Quality Monitor (patent pending) and a so-called Endpoint Detector (patent pending) which provide sound statistical indications of when document review can safely be stopped while maintaining defensibility in the subsequent trial.</p>","","","10.1145/2536146.2536177","","","Lit i View;big data;data analysis;data management system;eDiscovery;endpoint detector;international litigation support;predictive coding;quality monitor;text mining","","","","","1","","","","","","28-31 Oct. 2013","","ACM","ACM Conferences"
"A first implementation and evaluation of the IEEE 802.11aa group addressed transmission service","P. Salvador; L. Cominardi; F. Gringoli; P. Serrano","Institute IMDEA Networks, University Carlos III de Madrid","ACM SIGCOMM Computer Communication Review","20160129","2014","44","1","35","41","<p>The IEEE 802.11aa Task Group has recently standardized a set of mechanisms to efficiently support video multicasting, namely, the Group Addressed Transmission Service (GATS). In this article, we report the implementation of these mechanisms over commodity hardware, which we make publicly available, and conduct a study to assess their performance under a variety of real-life scenarios. To the best of our knowledge, this is the first experimental assessment of GATS, which is performed along three axes: we report their complexity in terms of lines of code, their effectiveness when delivering video traffic, and their efficiency when utilizing wireless resources. Our results provide key insights on the resulting trade-offs when using each mechanism, and paves the way for new enhancements to deliver video over 802.11 Wireless LANs.</p>","0146-4833;01464833","","10.1145/2567561.2567567","","","802.11aa;WLAN;groupcast;standard","","","","","6","","","","","","January 2014","","ACM","ACM Journals & Magazines"
"Towards SmartFlow: case studies on enhanced programmable forwarding in OpenFlow switches","F. N√©meth; √Å. Stipkovits; B. Sonkoly; A. Guly√°s","Budapest University of Technology and Economics, Budapest, Hungary","ACM SIGCOMM Computer Communication Review","20160129","2012","42","4","85","86","<p>The limited capabilities of the switches renders the implementation of unorthodox routing and forwarding mechanisms as a hard task in OpenFlow. Our high level goal is therefore to inspect the possibilities of slightly smartening up the OpenFlow switches. As a first step in this direction we demonstrate (with Bloom filters, greedy routing and network coding) that a very limited computational capability enables us to natively support experimental technologies while preserving performance. We distribute the demos in source files and as a ready-to-experiment VM image to promote further improvements and evaluations.</p>","0146-4833;01464833","","10.1145/2377677.2377693","","","bloom filters;greedy routing;network coding;openflow;sdn","","","","","4","","","","","","October 2012","","ACM","ACM Journals & Magazines"
"Distributed web app execution with chunks","J. M. Paluska; H. Pham; S. Ward","MIT CSAIL, Cambridge, MA, U.S.A.","ACM SIGMOBILE Mobile Computing and Communications Review","20160129","2012","16","4","10","11","<p>We propose an execution model for web apps that eschews the current need for static partitioning of web apps into client-side JavaScript and server-side ""data center"" code, but rather enables dynamic partitioning by allowing the client and server to share code and migrate computation between each other. Our model abstracts and represents shared code and data as a graph of fixed-sized chunks; our runtime then manages and migrates the graph rather than managing the code and its dependencies directly. We demonstrate our model through PhotoBoss, a suite of web apps for editing high-resolution photos.</p>","1559-1662;15591662","","10.1145/2436196.2436202","","","","","","","","","","","","","","October 2012","","ACM","ACM Journals & Magazines"
"Adaptive multiple description coding and transmission of uncompressed video over 60GHz networks","Z. He; S. Mao","Auburn University, Auburn, AL, USA","ACM SIGMOBILE Mobile Computing and Communications Review","20160129","2014","18","1","14","24","<p>While many cognitive radio (CR) techniques are developed to better utilize the allocated spectrum, the massive unlicensed bandwidth in the 60GHz band also provides great potential for supporting new bandwidth intensive applications. In this paper, we investigate the problem of streaming uncompressed High Definitio (HD) videos over 60GHz networks. We present an adaptive multiple description (MD) coding (MDC) technique based on Priority Encoding Transmission (PET) that exploits the different significanc of the pixel bits, and an interleaving based transmission scheme to combat the bursty losses due to blockage. A nonlinear integer programming problem is formulated and solved with a heuristic approach for determining the sub-optimal coding and transmission parameters. The proposed scheme is adaptive to the dynamic 60GHz link conditions for enhanced video quality. The performance of the proposed scheme is validated with simulations.</p>","1559-1662;15591662","","10.1145/2581555.2581558","","","","","","","","9","","","","","","January 2014","","ACM","ACM Journals & Magazines"
"Efficient and reliable low-power backscatter networks","J. Wang; H. Hassanieh; D. Katabi; P. Indyk","MIT, Cambridge, MA, USA","ACM SIGCOMM Computer Communication Review","20160129","2012","42","4","61","72","<p>There is a long-standing vision of embedding backscatter nodes like RFIDs into everyday objects to build ultra-low power ubiquitous networks. A major problem that has challenged this vision is that backscatter communication is neither reliable nor efficient. Backscatter nodes cannot sense each other, and hence tend to suffer from colliding transmissions. Further, they are ineffective at adapting the bit rate to channel conditions, and thus miss opportunities to increase throughput, or transmit above capacity causing errors.</p> <p> This paper introduces a new approach to backscatter communication. The key idea is to treat all nodes as if they were a single virtual sender. One can then view collisions as a code across the bits transmitted by the nodes. By ensuring only a few nodes collide at any time, we make collisions act as a <i>sparse code</i> and decode them using a new customized compressive sensing algorithm. Further, we can make these collisions act as a <i>rateless code</i> to automatically adapt the bit rate to channel quality --i.e., nodes can keep colliding until the base station has collected enough collisions to decode. Results from a network of backscatter nodes communicating with a USRP backscatter base station demonstrate that the new design produces a 3.5√ó throughput gain, and due to its rateless code, reduces message loss rate in challenging scenarios from 50% to zero.</p>","0146-4833;01464833","","10.1145/2377677.2377685","","","backscatter;compressive sensing;rfid;wireless","","","","","15","1","","","","","October 2012","","ACM","ACM Journals & Magazines"
"An experimental evaluation of IP4-IPV6 IVI translation","A. K. Tsetse; A. L. Wijesinha; R. Karne; A. Loukili; P. Appiah-Kubi","Livingstone College, Salisbury, NC","ACM SIGAPP Applied Computing Review","20160129","2013","13","1","19","27","<p>While IPv6 deployment in the Internet continues to grow slowly at present, the imminent exhaustion of IPv4 addresses will encourage its increased use over the next several years. However, due to the predominance of IPv4 in the Internet, the transition to IPv6 is likely to take a long time. During the transition period, translation mechanisms will enable IPv6 hosts and IPv4 hosts to communicate with each other. For example, translation can be used when a server or application works with IPv4 but not with IPv6, and the effort or cost to modify the code is large. Stateless and stateful translation is the subject of several recent IETF RFCs. We evaluate performance of the new IVI translator, which is viewed as a design for stateless translation by conducting experiments in both LAN and Internet environments using a freely available Linux implementation of IVI. To study the impact of operating system overhead on IVI translation, we implemented the IVI translator on a bare PC that runs applications without an operating system or kernel. Our results based on internal timings in each system show that translating IPv4 packets into IPv6 packets is more expensive than the reverse, and that address mapping is the most expensive IVI operation. We also measured packets per second in the LAN, roundtrip times in the LAN and Internet, IVI overhead for various prefix sizes, TCP connection time, and the delay and throughput over the Internet for various files sizes. While both the Linux and bare PC implementations of IVI have low overhead, a modest performance gain is obtained due to using a bare PC.</p>","1559-6915;15596915","","10.1145/2460136.2460138","","","IPv4-IPv6 transition;IPv6;IVI translation;Linux;bare PC","","","","","2","","","","","","March 2013","","ACM","ACM Journals & Magazines"
"Zooming into radio events by bus snooping","Z. He; T. Voigt","Swedish Institute of Computer Science (SICS), Kista, Sweden","ACM SIGBED Review","20160129","2012","9","3","21","23","<p>In this position paper, we advocate the use of bus snooping to trace radio events. Highly precise and unintrusive, the technique leads to potentially more efficient code and enables more insightful protocol analysis than conventional code instrumentation techniques.</p>","","","10.1145/2367580.2367583","","","","","","","","","","","","","","July 2012","","ACM","ACM Journals & Magazines"
"Measuring energy consumption for short code paths using RAPL","M. H√§hnel; B. D√∂bel; M. V√∂lp; H. H√§rtig","Technische Universit&#228;t Dresden, Dresden, Germany","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","3","13","17","<p>Measuring the energy consumption of software components is a major building block for generating models that allow for energy-aware scheduling, accounting and budgeting. Current measurement techniques focus on coarse-grained measurements of application or system events. However, fine grain adjustments in particular in the operating-system kernel and in application-level servers require power profiles at the level of a single software function. Until recently, this appeared to be impossible due to the lacking fine grain resolution and high costs of measurement equipment. In this paper we report on our experience in using the Running Average Power Limit (RAPL) energy sensors available in recent Intel CPUs for measuring energy consumption of short code paths. We investigate the granularity at which RAPL measurements can be performed and discuss practical obstacles that occur when performing these measurements on complex modern CPUs. Furthermore, we demonstrate how to use the RAPL infrastructure to characterize the energy costs for decoding video slices.</p>","0163-5999;01635999","","10.1145/2425248.2425252","","","RAPL;operating systems;power consumption","","","","","20","","","","","","December 2012","","ACM","ACM Journals & Magazines"
"Data dissemination performance in large-scale sensor networks","T. M. M. Meyfroyt; S. C. Borst; O. J. Boxma; D. Denteneer","Eindhoven University of Technology, Eindhoven, Netherlands","ACM SIGMETRICS Performance Evaluation Review","20160129","2014","42","1","395","406","<p>As the use of wireless sensor networks increases, the need for (energy-)efficient and reliable broadcasting algorithms grows. Ideally, a broadcasting algorithm should have the ability to quickly disseminate data, while keeping the number of transmissions low. In this paper we develop a model describing the message count in large-scale wireless sensor networks. We focus our attention on the popular Trickle algorithm, which has been proposed as a suitable communication protocol for code maintenance and propagation in wireless sensor networks. Besides providing a mathematical analysis of the algorithm, we propose a generalized version of Trickle, with an additional parameter defining the length of a listen-only period. This generalization proves to be useful for optimizing the design and usage of the algorithm. For single-cell networks we show how the message count increases with the size of the network and how this depends on the Trickle parameters. Furthermore, we derive distributions of inter-broadcasting times and investigate their asymptotic behavior. Our results prove conjectures made in the literature concerning the effect of a listen-only period. Additionally, we develop an approximation for the expected number of transmissions in multi-cell networks. All results are validated by simulations.</p>","0163-5999;01635999","","10.1145/2637364.2591981","","","analytical model;gossip protocol;message count;message overhead;trickle algorithm;wireless sensor networks","","","","","","","","","","","June 2014","","ACM","ACM Journals & Magazines"
"What is communication design?","C. Spinuzzi","University of Texas at Austin","Communication Design Quarterly Review","20160129","2012","1","1","8","11","<p>In 1997, I worked with a team to conduct my first qualitative research project, a study of how software developers used code libraries when developing a common codebase (McLellan et al. 1998; Spinuzzi 2001). In particular, I was interested in how developers used inline comments to understand their own and others' code. At two sites, the developers used comments pretty much as you might expect: as notes for interpreting and communicating information about the code. But at the third site, developers essentially ignored the comments. One compared the comments to an approaching car's blinker: it might or might not indicate intent, but you'd be foolish to trust it. Another set his editor to gray out comments so they wouldn't distract him. A third used comments - not to interpret the code, but as landmarks for navigating it. ""If I have 50 lines of code without a comment,"" he told me, ""I get lost. It takes me a while to actually read the code and find out what it's doing. But if I have comments I can separate it into sections, and if I know it's the second section in the function, I can go right to it.""</p>","2166-1200;21661200","","10.1145/2448917.2448919","","","","","","","","","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"IntroPerf: transparent context-sensitive multi-layer performance inference using system stack traces","C. H. Kim; J. Rhee; H. Zhang; N. Arora; G. Jiang; X. Zhang; D. Xu","Purdue University and CERIAS, West Lafayette, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2014","42","1","235","247","<p>Performance bugs are frequently observed in commodity software. While profilers or source code-based tools can be used at development stage where a program is diagnosed in a well-defined environment, many performance bugs survive such a stage and affect production runs. OS kernel-level tracers are commonly used in post-development diagnosis due to their independence from programs and libraries; however, they lack detailed program-specific metrics to reason about performance problems such as function latencies and program contexts. In this paper, we propose a novel performance inference system, called IntroPerf, that generates fine-grained performance information -- like that from application profiling tools -- transparently by leveraging OS tracers that are widely available in most commodity operating systems. With system stack traces as input, IntroPerf enables transparent context-sensitive performance inference, and diagnoses application performance in a multi-layered scope ranging from user functions to the kernel. Evaluated with various performance bugs in multiple open source software projects, IntroPerf automatically ranks potential internal and external root causes of performance bugs with high accuracy without any prior knowledge about or instrumentation on the subject software. Our results show IntroPerf's effectiveness as a lightweight performance introspection tool for post-development diagnosis.</p>","0163-5999;01635999","","10.1145/2637364.2592008","","","context-sensitive performance analysis;performance inference;stack trace analysis","","","","","2","","","","","","June 2014","","ACM","ACM Journals & Magazines"
"CApRI: CAche-conscious data reordering for irregular codes","W. Ding; M. Kandemir","The Pennsylvania State University, University Park, PA, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2014","42","1","477","489","<p>Caches play a critical role in today's computer systems and optimizing their performance has been a critical objective in the last couple of decades. Unfortunately, compared to a plethora of work in software and hardware directed code/data optimizations, much less effort has been spent in understanding the fundamental characteristics of data access patterns exhibited by application programs and their interaction with the underlying cache hardware. Therefore, in general it is hard to reason about cache behavior of a program running on a target system. Motivated by this observation, we first set up a ""locality model"" that can help us determine the theoretical bounds of the cache misses caused by irregular data accesses. We then explain how this locality model can be used for different data locality optimization purposes. After that, based on our model, we propose a data reordering (data layout reorganization) scheme that can be applied after any existing data reordering schemes for irregular applications to improve cache performance by further reducing the cache misses. We evaluate the effectiveness of our scheme using a set of 8 programs with irregular data accesses, and show that it brings significant improvements over the state-of-the-art on two commercial multicore machines.</p>","0163-5999;01635999","","10.1145/2637364.2591992","","","cache;compiler;data locality;irregular application","","","","","","","","","","","June 2014","","ACM","ACM Journals & Magazines"
"Genealogical insights into the facts and fictions of clone removal","M. F. Zibran; R. K. Saha; C. K. Roy; K. A. Schneider","University of Saskatchewan","ACM SIGAPP Applied Computing Review","20160129","2013","13","4","30","42","<p>Clone management has drawn immense interest from the research community in recent years. It is recognized that a deep understanding of how code clones change and are refactored is necessary for devising effective clone management tools and techniques. This paper presents an empirical study based on the clone genealogies from a significant number of releases of nine software systems, to characterize the patterns of clone change and removal in evolving software systems. With a blend of qualitative analysis, quantitative analysis and statistical tests of significance, we address a number of research questions. Our findings reveal insights into the removal of individual clone fragments and provide empirical evidence in support of conventional clone evolution wisdom. The results can be used to devise informed clone management tools and techniques.</p>","1559-6915;15596915","","10.1145/2577554.2577559","","","clone evolution;clone removal;reengineering;refactoring","","","","","","","","","","","December 2013","","ACM","ACM Journals & Magazines"
"Nullspace-based stopping conditions for network-coded transmissions in DTNs","A. Hennessy; A. Gladd; B. Walker","The Laboratory for Telecommunications Sciences","ACM SIGMOBILE Mobile Computing and Communications Review","20160129","2013","17","1","14","21","<p>In a challenged network environment, where end-to-end connectivity may be a rare occurrence, delay-tolerant routing protocols must strike a balance between the increased robustness and reliability that comes with message replication and the resulting high bandwidth and storage overhead. Network coded routing, in which a node combines messages from different sources, has been shown to increase reliability in the presence of link failures with small additional overhead. A drawback of network coded routing is the lack of a natural stopping condition to control the dissemination of data. We describe an enhanced coding router that uses the mathematical structure of the orthogonal complement, or nullspace, as an improved stopping condition to eliminate redundant transmissions, and an additional technique to balance multiple coded data flows. These changes are incorporated into the DTN2 Reference Implementation and evaluated in two types of experiments. In a simple data-mule scenario, our EBR router comes very close to perfect efficiency. In a more complicated scenario with segmented communities and occasional nodes moving between them, our solutions show a drastic improvement in delivery rates.</p>","1559-1662;15591662","","10.1145/2502935.2502938","","","","","","","","","","","","","","January 2013","","ACM","ACM Journals & Magazines"
"Regenerating codes: a system perspective","S. Jiekak; A. M. Kermarrec; N. Le Scouarnec; G. Straub; A. Van Kempen","Technicolor, Rennes, France","ACM SIGOPS Operating Systems Review","20160129","2013","47","2","23","32","<p>The explosion of the amount of data stored in cloud systems calls for more efficient paradigms for redundancy. While replication is widely used to ensure data availability, erasure correcting codes provide a much better trade-off between storage and availability. Regenerating codes are good candidates for they also offer low repair costs in term of network bandwidth. While they have been proven optimal, they are difficult to understand and parameterize. In this paper we provide an analysis of regenerating codes for practitioners to grasp the various trade-offs. More specifically we make two contributions: (i) we study the impact of the parameters by conducting an analysis at the level of the system, rather than at the level of a single device; (ii) we compare the computational costs of various implementations of codes and highlight the most efficient ones. Our goal is to provide system designers with concrete information to help them choose the best parameters and design for regenerating codes.</p>","0163-5980;01635980","","10.1145/2506164.2506170","","","distributed storage systems;regenerating codes","","","","","7","","","","","","July 2013","","ACM","ACM Journals & Magazines"
"Delay tails in MapReduce scheduling","J. Tan; X. Meng; L. Zhang","IBM T. J. Watson Research, Hawthorne, NY, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","1","5","16","<p>MapReduce/Hadoop production clusters exhibit heavy-tailed characteristics for job processing times. These phenomena are resultant of the workload features and the adopted scheduling algorithms. Analytically understanding the delays under different schedulers for MapReduce can facilitate the design and deployment of large Hadoop clusters. The map and reduce tasks of a MapReduce job have fundamental difference and tight dependence between them, complicating the analysis. This also leads to an interesting starvation problem with the widely used Fair Scheduler due to its greedy approach to launching reduce tasks. To address this issue, we design and implement Coupling Scheduler, which gradually launches reduce tasks depending on map task progresses. Real experiments demonstrate improvements to job response times by up to an order of magnitude.</p> <p>Based on extensive measurements and source code investigations, we propose analytical models for the default FIFO and Fair Scheduler as well as our implemented Coupling Scheduler. For a class of heavy-tailed map service time distributions, i.e., regularly varying of index -a, we derive the distribution tail of the job processing delay under the three schedulers, respectively. The default FIFO Scheduler causes the delay to be regularly varying of index -a+1. Interestingly, we discover a criticality phenomenon for Fair Scheduler, the delay under which can change from regularly varying of index -a to -a+1, depending on the maximum number of reduce tasks of a job. Other more subtle behaviors also exist. In contrast, the delay distribution tail under Coupling Scheduler can be one order lower than Fair Scheduler under some conditions, implying a better performance.</p>","0163-5999;01635999","","10.1145/2318857.2254761","","","MapReduce;coupling scheduler;fair scheduler;first in first out;hadoop;heavy-tails;processor sharing","","","","","5","","","","","","June 2012","","ACM","ACM Journals & Magazines"
"mTags: augmenting microkernel messages with lightweight metadata","A. B. de Oliveira; A. Saif Ur Rehman; S. Fischmeister","University of Waterloo, Canada","ACM SIGOPS Operating Systems Review","20160129","2012","46","2","67","79","<p>In this work we propose mTags, an efficient mechanism that augments microkernel interprocess messages with lightweight metadata to enable the development of new, system-wide functionality without requiring modification of the application source code. As such it is well suited for systems with a large legacy code base or third-party applications like phone and tablet applications.</p> <p>We explored mTags in a variety of different contexts in local and distributed system scenarios. For example, we detail use cases in areas including messaging-induced deadlocks and mode propagation. To demonstrate that mTags is technically feasible and practical, we implemented it in a commercial microkernel and executed multiple sets of standard benchmarks on two different computing architectures. The results clearly demonstrate that mTags has only negligible overhead and strong potential for many applications.</p>","0163-5980;01635980","","10.1145/2331576.2331587","","","","","","","","3","","","","","","July 2012","","ACM","ACM Journals & Magazines"
"Spinal codes","J. Perry; P. A. Iannucci; K. E. Fleming; H. Balakrishnan; D. Shah","M.I.T., Cambridge, MA, USA","ACM SIGCOMM Computer Communication Review","20160129","2012","42","4","49","60","<p>Spinal codes are a new class of rateless codes that enable wireless networks to cope with time-varying channel conditions in a natural way, without requiring any explicit bit rate selection. The key idea in the code is the sequential application of a pseudo-random hash function to the message bits to produce a sequence of coded symbols for transmission. This encoding ensures that two input messages that differ in even one bit lead to very different coded sequences after the point at which they differ, providing good resilience to noise and bit errors. To decode spinal codes, this paper develops an approximate maximum-likelihood decoder, called the <i>bubble decoder</i>, which runs in time polynomial in the message size and achieves the Shannon capacity over both additive white Gaussian noise (AWGN) and binary symmetric channel (BSC) models. Experimental results obtained from a software implementation of a linear-time decoder show that spinal codes achieve higher throughput than fixed-rate LDPC codes, rateless Raptor codes, and the layered rateless coding approach of Strider, across a range of channel conditions and message sizes. An early hardware prototype that can decode at 10 Mbits/s in FPGA demonstrates that spinal codes are a practical construction.</p>","0146-4833;01464833","","10.1145/2377677.2377684","","","capacity;channel code;practical decoder;rateless;spinal code;wireless","","","","","3","","","","","","October 2012","","ACM","ACM Journals & Magazines"
"The efficacy of error mitigation techniques for DRAM retention failures: a comparative experimental study","S. Khan; D. Lee; Y. Kim; A. R. Alameldeen; C. Wilkerson; O. Mutlu","Carnegie Mellon University &#38; Intel Labs, Pittsburgh, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2014","42","1","519","532","<p>As DRAM cells continue to shrink, they become more susceptible to retention failures. DRAM cells that permanently exhibit short retention times are fairly easy to identify and repair through the use of memory tests and row and column redundancy. However, the retention time of many cells may vary over time due to a property called <i>Variable Retention Time (VRT)</i>. Since these cells intermittently transition between failing and non-failing states, they are particularly difficult to identify through memory tests alone. In addition, the high temperature packaging process may aggravate this problem as the susceptibility of cells to VRT increases after the assembly of DRAM chips. A promising alternative to manufacture-time testing is to detect and mitigate retention failures after the system has become operational. Such a system would require mechanisms to detect and mitigate retention failures in the field, but would be responsive to retention failures introduced after system assembly and could dramatically reduce the cost of testing, enabling much longer tests than are practical with manufacturer testing equipment.</p> <p>In this paper, we analyze the efficacy of three common error mitigation techniques (memory tests, guardbands, and error correcting codes (ECC)) in real DRAM chips exhibiting both intermittent and permanent retention failures. Our analysis allows us to quantify the efficacy of recent system-level error mitigation mechanisms that build upon these techniques. We revisit prior works in the context of the experimental data we present, showing that our measured results significantly impact these works' conclusions. We find that mitigation techniques that rely on run-time testing alone [38, 27, 50, 26] are unable to ensure reliable operation even after many months of testing. Techniques that incorporate ECC[4, 52], however, can ensure reliable DRAM operation after only a few hours of testing. For example, VS-ECC[4], which couples testing with variabl- strength codes to allocate the strongest codes to the most error-prone memory regions, can ensure reliable operation for 10 years after only 19 minutes of testing. We conclude that the viability of these mitigation techniques depend on efficient online profiling of DRAM performed without disrupting system operation.</p>","0163-5999;01635999","","10.1145/2637364.2592000","","","dram;ecc;error correction;fault tolerance;memory scaling;retention failures;system-level detection and mitigation","","","","","7","","","","","","June 2014","","ACM","ACM Journals & Magazines"
"On the applicability of network coding in wireless sensor networks","T. Voigt; U. Roedig; O. Landsiedel; K. Samarasinghe; M. B. S. Prasad","Swedish Institute of Computer Science (SICS), Kista, Sweden","ACM SIGBED Review","20160129","2012","9","3","46","48","<p>Network coding is a novel concept for improving network capacity. This additional capacity may be used to increase throughput or reliability. Also in wireless networks, network coding has been proposed as a method for improving communication. We present our experience from two studies of applying network coding in realistic wireless sensor networks scenarios. As we show, network coding is not as useful in practical deployments as earlier theoretical work suggested. We discuss limitations and future opportunities for network coding in sensor networks.</p>","","","10.1145/2367580.2367588","","","network coding;wireless sensor networks","","","","","1","","","","","","July 2012","","ACM","ACM Journals & Magazines"
"Buffer overflow patching for C and C++ programs: rule-based approach","H. Shahriar; H. M. Haddad; I. Vaidya","Kennesaw State University, Kennesaw, GA","ACM SIGAPP Applied Computing Review","20160129","2013","13","2","8","19","<p>The presence of buffer overflow (BOF) vulnerabilities in programs hampers essential security objectives such as confidentiality, integrity and availability. In particular, exploitations of BOF might lead to many unwanted consequences including denial of service through program crash, control flow hijacking, and corrupted program state. When BOF vulnerabilities are detected, they need to be patched before the software is redeployed. Source level automatic patching of vulnerabilities has the challenges of finding a set of general rules and consistently applying them without bringing any side effects to intended software. This paper proposes a set of general rules to address the mitigation of BOF vulnerabilities for C/C++ programs. In particular, we developed a set of rules to identify vulnerable code and how to make the code vulnerability free. The proposed rule-based approach addresses both simple (one statement) and complex (multiple statements) forms of code that can be vulnerable to BOF ranging from unsafe library function calls to the pointer usage in control flow structures (loop and conditional statements). We evaluated the proposed approach using two publicly available benchmarks and a number of open source C/C++ applications. The results show that the proposed rules can not only identify previously known BOF vulnerabilities, but also find new vulnerabilities. Moreover, the patching rules impose negligible overhead to the application.</p>","1559-6915;15596915","","10.1145/2505420.2505421","","","buffer overflow;pointer usage;rule-based patching;software vulnerability;unsafe library function calls","","","","","3","","","","","","June 2013","","ACM","ACM Journals & Magazines"
"Scheduling of users with markovian time-varying transmission rates","F. Cecchi; P. Jacko","University of Pisa, Italy; BCAM - Basque Center for Applied Mathematics, Spain, Pisa, Italy","ACM SIGMETRICS Performance Evaluation Review","20160129","2013","41","1","129","140","<p>We address the problem of developing a well-performing and implementable scheduler of users with wireless connection to the base station. The main feature of such real-life systems is that the quality conditions of the user channels are time-varying, which turn into the time-varying transmission rate due to different modulation and coding schemes. We assume that this phenomenon follows a Markovian law and most of the discussion is dedicated to the case of three quality conditions of each user, for which we characterize an optimal index policy and show that threshold policies (of giving higher priority to users with higher transmission rate) are not necessarily optimal. For the general case of arbitrary number of quality conditions we design a scheduler and propose its two practical approximations, and illustrate the performance of the proposed index-based schedulers and existing alternatives in a variety of simulation scenarios.</p>","0163-5999;01635999","","10.1145/2494232.2465550","","","markov decision processes;opportunistic scheduling;performance evaluation;stability;stochastic scheduling;wireless network","","","","","","","","","","","June 2013","","ACM","ACM Journals & Magazines"
"RaptorStream: boosting mobile peer-to-peer streaming with raptor codes","P. M. Eittenberger","University of Bamberg, Bamberg, Germany","ACM SIGCOMM Computer Communication Review","20160129","2012","42","4","291","292","<p>As mobile devices and cellular networks become ubiquitous, first apps for popular P2P video streaming networks emerge. We have observed that when these applications operate in cellular networks, they don't upload video traffic back to other peers. This paper presents a reason for this behavior and proposes a viable solution to exploit the uplink capacity of mobile devices more efficiently. To the best of our knowledge, this paper is the first to propose the usage of Raptor codes to increase the upload throughput of mobile P2P applications.</p>","0146-4833;01464833","","10.1145/2377677.2377736","","","android;mobile p2p streaming;raptor codes","","","","","","","","","","","October 2012","","ACM","ACM Journals & Magazines"
"Error estimating codes for insertion and deletion channels","J. Huang; S. Yang; A. Lall; J. Romberg; J. Xu; C. Lin","Tsinghua University, Beijing, China","ACM SIGMETRICS Performance Evaluation Review","20160129","2014","42","1","381","393","<p>Error estimating codes (EEC) have recently been proposed for measuring the bit error rate (BER) in packets transmitted over wireless links. They however can provide such measurements only when there are no insertion and deletion errors, which could occur in various wireless network environments. In this work, we propose ``idEEC'', the first technique that can do so even in the presence of insertion and deletion errors. We show that idEEC is provable robust under most bit insertion and deletion scenarios, provided insertion/deletion errors occur with much lower probability than bit flipping errors. Our idEEC design can build upon any existing EEC scheme. The basic idea of the idEEC encoding is to divide the packet into a number of segments, each of which is encoded using the underlying EEC scheme. The basic idea of the idEEC decoding is to divide the packet into a few slices in a randomized manner -- each of which may contain several segments -- and then try to identify a slice that has no insertion and deletion errors in it (called a ``clean slice''). Once such a clean slice is found, it is removed from the packet for later processing, and this ``randomized divide and search'' procedure will be iteratively performed on the rest of the packet until no more clean slices can be found. The BER will then be estimated from all the clean slices discovered through all the iterations. A careful analysis of the accuracy guarantees of the idEEC decoding is provided, and the efficacy of idEEC is further validated by simulation experiments.</p>","0163-5999;01635999","","10.1145/2637364.2591976","","","deletion channel;error estimating coding;insertion channel","","","","","","","","","","","June 2014","","ACM","ACM Journals & Magazines"
"A measurement study of google play","N. Viennot; E. Garcia; J. Nieh","Columbia University, New York, NY, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2014","42","1","221","233","<p>Although millions of users download and use third-party Android applications from the Google Play store, little information is known on an aggregated level about these applications. We have built PlayDrone, the first scalable Google Play store crawler, and used it to index and analyze over 1,100,000 applications in the Google Play store on a daily basis, the largest such index of Android applications. PlayDrone leverages various hacking techniques to circumvent Google's roadblocks for indexing Google Play store content, and makes proprietary application sources available, including source code for over 880,000 free applications. We demonstrate the usefulness of PlayDrone in decompiling and analyzing application content by exploring four previously unaddressed issues: the characterization of Google Play application content at large scale and its evolution over time, library usage in applications and its impact on application portability, duplicative application content in Google Play, and the ineffectiveness of OAuth and related service authentication mechanisms resulting in malicious users being able to easily gain unauthorized access to user data and resources on Amazon Web Services and Facebook.</p>","0163-5999;01635999","","10.1145/2637364.2592003","","","android;authentication;clone detection;decompilation;google play;mobile computing;oauth;security","","","","","2","","","","","","June 2014","","ACM","ACM Journals & Magazines"
"SimPal: a design study on a framework for flexible safety-critical software development","J. P. Notander; P. Runeson; M. H√∂st","Lund University, Sweden","ACM SIGAPP Applied Computing Review","20160129","2013","13","4","17","29","<p>This paper presents the findings from a design study on a framework for flexible safety-critical software development, called <i>SimPal</i>. It is an extended version of a paper that was published in SAC'13 Proceedings of the 2013 ACM Symposium on Applied Computing, in which additional details about <i>SimPal</i> as well as a more extensive evaluation of the framework is presented. The objective is to identify necessary quality properties and to learn more about the challenges of realizing frameworks such as SimPal. We approach our research questions by developing a framework and by analysing our experiences from the design and evaluation process. Some necessary quality characteristics has been identified by discussing the ISO25010 <i>quality in use</i> quality model in relation to the problem domain, which were then used to design and evaluate the developed framework. The evaluation was conducted as a design case in which a <i>soft safety controller</i> was developed following the methodology outlined in the paper. We show that our approach, which tries to merge service-oriented practices with model-based development techniques, has potential considering safety-critical software development. However, there are some concerns about run-time performance as well as the ability to qualify the tool for safety-critical development. Based on our results we conclude that the ideas behind the <i>SimPal</i> framework are sound but more work is required to investigate how they can be realized. For the future, we plan on further investigating the code generating capabilities of the modelling tool we are using to see if and how it can be utilised to increase performance. We also plan on adding more features to the framework, for instance coordination and configuration of services, as well as monitoring of messages and system properties.</p>","1559-6915;15596915","","10.1145/2577554.2577558","","","frameworks and tools;real-time systems;safety-critical development;service-oriented computing;verification","","","","","","","","","","","December 2013","","ACM","ACM Journals & Magazines"
"Our troubles with Linux Kernel upgrades and why you should care","A. S. Harji; P. A. Buhr; T. Brecht","University of Waterloo, Waterloo, Canada","ACM SIGOPS Operating Systems Review","20160129","2013","47","2","66","72","<p>Linux and other open-source Unix variants (and their distributors) provide researchers with full-fledged operating systems that are widely used. However, due to their complexity and rapid development, care should be exercised when using these operating systems for performance experiments, especially in systems research. In particular, the size and continual evolution of the Linux code-base makes it difficult to understand, and as a result, decipher and explain the reasons for performance improvements. In addition, the rapid kernel development cycle means that experimental results can be viewed as out of date, or meaningless, very quickly. We demonstrate that this viewpoint is incorrect because kernel changes can and have introduced both bugs and performance degradations.</p> <p>This paper describes some of our experiences using Linux and FreeBSD as platforms for conducting performance evaluations and some performance regressions we have found. Our results show, these performance regressions can be serious (e.g., repeating identical experiments results in large variability in results) and long lived despite having a large negative effect on performance (one problem was present for more than 3 years). Based on these experiences, we argue: it is sometimes reasonable to use an older kernel version, experimental results need careful analysis to explain why a performance effect occurs, and publishing papers validating prior research is essential.</p>","0163-5980;01635980","","10.1145/2506164.2506175","","","","","","","","","","","","","","July 2013","","ACM","ACM Journals & Magazines"
"RAID triple parity","A. Goel; P. Corbett","NetApp Inc.","ACM SIGOPS Operating Systems Review","20160129","2012","46","3","41","49","<p>RAID triple parity (RTP) is a new algorithm for protecting against three-disk failures. It is an extension of the double failure correction Row-Diagonal Parity code. For any number of data disks, RTP uses only three parity disks. This is optimal with respect to the amount of redundant information required and accessed. RTP uses XOR operations and stores all data un-encoded. The algorithm's parity computation complexity is provably optimal. The decoding complexity is also much lower than that of existing comparable codes. This paper also describes a symmetric variant of the algorithm where parity computation is identical to triple reconstruction.</p>","0163-5980;01635980","","10.1145/2421648.2421655","","","RAID recovery;RDP code;disk failure;recovery algorithm","","","","","5","","","","","","December 2012","","ACM","ACM Journals & Magazines"
"Filling the gaps of unused capacity through a fountain coded dissemination of information","G. Parisis; D. Trossen","University of Sussex, Brighton, United Kingdom","ACM SIGMOBILE Mobile Computing and Communications Review","20160129","2014","18","1","46","54","<p>Lowest Cost Denominator Networking (LCDnet) envisions ""breaking the mould of thinking that law of economics should govern connectivity to all"". It brings together a multi-layer resource pooling of communication technologies at several levels to support benevolence in the Internet. One of the proposed levels of resource pooling involves better network and storage utilisation, as promised by Information-centric networking architectures. In this paper we present a transport and resource management approach on top of an informationcentric network that enables efficient, multi-source and multi-path information dissemination as well as in-network caching and mobility support, characteristics that are well desired in the LCDnet context.</p>","1559-1662;15591662","","10.1145/2581555.2581563","","","","","","","","","","","","","","January 2014","","ACM","ACM Journals & Magazines"
"Neighbor-cell assisted error correction for MLC NAND flash memories","Y. Cai; G. Yalcin; O. Mutlu; E. F. Haratsch; O. Unsal; A. Cristal; K. Mai","Carnegie Mellon University, Pittsburgh, PA, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2014","42","1","491","504","<p>Continued scaling of NAND flash memory to smaller process technology nodes decreases its reliability, necessitating more sophisticated mechanisms to correctly read stored data values. To distinguish between different potential stored values, conventional techniques to read data from flash memory employ a single set of reference voltage values, which are determined based on the overall threshold voltage distribution of flash cells. Unfortunately, the phenomenon of program interference, in which a cell's threshold voltage unintentionally changes when a neighboring cell is programmed, makes this conventional approach increasingly inaccurate in determining the values of cells.</p> <p>This paper makes the new empirical observation that identifying the value stored in the immediate-neighbor cell makes it easier to determine the data value stored in the cell that is being read. We provide a detailed statistical and experimental characterization of threshold voltage distribution of flash memory cells <i>conditional upon</i> the immediate-neighbor cell values, and show that such conditional distributions can be used to determine a set of read reference voltages that lead to error rates much lower than when a single set of reference voltage values based on the overall distribution are used. Based on our analyses, we propose a new method for correcting errors in a flash memory page, neighbor-cell assisted correction (NAC). The key idea is to re-read a flash memory page that fails error correction codes (ECC) with the set of read reference voltage values corresponding to the conditional threshold voltage distribution assuming a neighbor cell value and use the re-read values to correct the cells that have neighbors with that value. Our simulations show that NAC effectively improves flash memory lifetime by 33% while having no (at nominal lifetime) or very modest (less than 5% at extended lifetime) performance overhead.</p>","0163-5999;01635999","","10.1145/2637364.2591994","","","ecc;error correction;fault tolerance;nand flash memory;program interference;threshold voltage distribution","","","","","4","","","","","","June 2014","","ACM","ACM Journals & Magazines"
"Waveperf: a benchmark generator for performance evaluation","J. Kriegel; F. Broekaert; A. Pegatoquet; M. Auguin","Thales Communications and Security, Paris, France","ACM SIGBED Review","20160129","2012","9","2","7","11","<p>Multi-core processors are more and more present in the embedded and real-time world. This paper introduces a code generator software applied to the benchmarking of embedded platforms. This solution creates an application runnable on embedded multicore platform and compliant with both POSIX or Xenomai interface. Running the application outputs an execution trace for each thread of the benchmark. It is also used to check the interruption latency and the preemption of real-time platforms.</p>","","","10.1145/2318836.2318837","","","benchmark;code generator;evaluation;multi-core;performance evaluation","","","","","","","","","","","June 2012","","ACM","ACM Journals & Magazines"
"Evaluation of machine-learning protocols for technology-assisted review in electronic discovery","G. V. Cormack; M. R. Grossman","University of Waterloo, Waterloo, ON, Canada","Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval","20160129","2014","","","153","162","<p>Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P<0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P<0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of ""stabilization"" -- determining when training is adequate, and therefore may stop.</p>","","","10.1145/2600428.2609601","","","e-discovery;electronic discovery;predictive coding;technology-assisted review","","","","","1","","","","","","6-11 July 2014","","ACM","ACM Conferences"
"An automatic control interface for network-accessible embedded instruments","G. Smecher; F. Aubin; E. George; T. de Haan; J. Kennedy; M. Dobbs","McGill University, Montreal","ACM SIGBED Review","20160129","2012","9","2","23","27","<p>We describe a metaprogrammed control interface and support library for network-accessible embedded systems. Together, this project permits functions written in standard C code to be exposed via a network interface expressed in JSON. In turn, this JSON interface mates with a Python library that provides a high-level, user-friendly, and expressive development environment.</p> <p>This control interface removes the need to explicitly code interactions at the Python and network layers. As a result, the volume of error-prone and redundant hand-written code (e.g. for error-checking and validation) is vastly reduced.</p>","","","10.1145/2318836.2318840","","","embedded Linux;field-programmable gate arrays (FPGAs);metaprogramming;remote procedure call (RPC)","","","","","","","","","","","June 2012","","ACM","ACM Journals & Magazines"
"Predictive modeling and analysis of OP2 on distributed memory GPU clusters","G. R. Mudalige; M. B. Giles; C. Bertolli; P. H. J. Kelly","Oxford e-Research Centre, University of Oxford","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","2","61","67","<p>OP2 is an ""active"" library framework for the development and solution of unstructured mesh based applications. It aims to decouple the scientific specification of an application from its parallel implementation to achieve code longevity and near-optimal performance through re-targeting the backend to different multi-core/many-core hardware. This paper presents a predictive performance analysis and benchmarking study of OP2 on heterogeneous cluster systems. We first present the design of a new OP2 back-end that enables the execution of applications on distributed memory clusters, and benchmark its performance during the solution of a 1.5M and 26M edge-based CFD application written using OP2. Benchmark systems include a large-scale CrayXE6 system and an Intel Westmere/InfiniBand cluster. We then apply performance modeling to predict the application's performance on an NVIDIA Tesla C2070 based GPU cluster, enabling us to compare OP2's performance capabilities on emerging distributed memory heterogeneous systems. Results illustrate the performance benefits that can be gained through many-core solutions both on single-node and heterogeneous configurations in comparison to traditional homogeneous cluster systems for this class of applications. </p>","0163-5999;01635999","","10.1145/2381056.2381072","","","GPU;OP2;performance modeling;unstructured mesh","","","","","2","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"Congestion control meets medium access: throughput, delay, and complexity","S. Bodas; D. Shah; D. Wischik","Massachusetts Institute of Technology, Cambridge, MA, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","1","399","400","<p>This paper looks at the problem of designing medium access algorithm for wireless networks with the objective of providing high throughput and low delay performance to the users, while requiring only a modest computational effort at the transmitters and receivers. Additive inter-user interference at the receivers is an important physical layer characteristic of wireless networks. Today's Wi-Fi networks are based upon the abstraction of physical layer where inter-user interference is considered as noise leading to the 'collision' model in which users are required to co-ordinate their transmissions through Carrier Sensing Multiple Access (CSMA)-based schemes to avoid interference. This, in turn, leads to an inherent performance trade-off [1]: it is impossible to obtain high throughput and low delay by means of low complexity medium access algorithm (unless P=NP). As the main result, we establish that this trade-off is primarily due to treating interference as noise in the current wireless architecture. Concretely, we develop a simple medium access algorithm that allows for simultaneous transmissions of users to the same receiver by performing joint decoding at receivers, over time. For a receiver to be able to decode multiple transmissions quickly enough, we develop appropriate congestion control where each transmitter maintains a ""window"" of undecoded transmitted data that is adjusted based upon the ""feedback"" from the receiver. In summary, this provides an efficient, low complexity ""online"" code operating at varying rate, and the system as a whole experiences only small amount of delay (including decoding time) while operating at high throughput.</p>","0163-5999;01635999","","10.1145/2318857.2254812","","","complexity;congestion control;delay;medium access;throughput","","","","","","","","","","","June 2012","","ACM","ACM Journals & Magazines"
"ADOK: a minimal object oriented real-time operating system in C++","S. Benedetto; G. Lipari","Scuola Superiore Sant'Anna","ACM SIGBED Review","20160129","2014","11","1","74","79","<p>Most embedded software is currently developed using the C programming language, even though its low level of abstraction requires a lot of effort to the programmer. The C++ language is a better choice because: it raises the level of abstraction; it is strongly typed, so it prevents many common programming mistakes; it can be made as efficient as C through fine-grained customisation of memory mechanisms; it can be easily adapted to domain-specific needs. In addition, recent compilers have grown in maturity and performance, and the new standard considerably improves the language by introducing new concepts and an easier syntax.</p> <p>In this paper we present ADOK, a minimal Real-Time Operating System entirely written in C++ with the exception of a few lines of assembler code. It directly offers a C++ interface to the developer, and it provides a flexible scheduling framework which allows the developer to customise the scheduling to its needs. In particular, we implement a two-level scheduler based on Earliest Deadline First, the Stack Resource Policy protocol for sharing resources and support for mode changes. We demonstrate through examples and a small case-study that ADOK can substantially improve productivity without sacrificing on performance.</p>","","","10.1145/2597457.2597468","","","systems","","","","","","","","","","","February 2014","","ACM","ACM Journals & Magazines"
"MultiNet: usable and secure WiFi device association","A. Brown; R. Mortier; T. Rodden","University of Nottingham, Nottingham, United Kingdom","ACM SIGCOMM Computer Communication Review","20160129","2012","42","4","275","276","<p>This demo presents <i>MultiNet</i>, a novel method for joining devices to a domestic Wi-Fi network. MultiNet dynamically reconfigures the network to accept each device, rather than configuring each device to fit the network as is the norm. It does so by assuming that each device is pre-configured with a cryptographically generated WPA2 network SSID/passphrase pair, and then providing a lightweight interaction through which the user creates a new network for each device. This approach makes securely adding devices to a wireless network straightforward without compromising security or burdening the user, and maintaining backward compatibility with existing deployed standards and protocols.</p> <p>The demo deploys a MultiNet Access Point (AP) and a number of Wi-Fi enabled consumer devices to allow viewers to dynamically construct and deconstruct the network via the MultiNet controller currently implemented as an app on an Android phone (Figure 1). The code for MultiNet is publicly available under open-source licenses.</p>","0146-4833;01464833","","10.1145/2377677.2377728","","","usable security; domestic environments; 802.11; infrastructure intervention","","","","","","","","","","","October 2012","","ACM","ACM Journals & Magazines"
"Ipb-frame adaptive mapping mechanism for video transmission over IEEE 802.11e WLANs","X. W. Yao; W. L. Wang; S. H. Yang; Y. F. Cen; X. M. Yao; T. Q. Pan","Zhejiang University of Technology, Hangzhou, China","ACM SIGCOMM Computer Communication Review","20160129","2014","44","2","5","12","<p>This paper proposed an IPB-frame Adaptive Mapping Mechanism (AMM) to improve the video transmission quality over IEEE 802.11e Wireless Local Area Networks (WLANs). Based on the frame structure of hierarchical coding technology, the probability of each frame allocated to the most appropriate Access Category (AC) was dynamically updated according to its importance and traffic load of each AC. Simulation results showed the superior performance of the proposed AMM by comparing with three other existing mechanisms in terms of three objective metrics.</p>","0146-4833;01464833","","10.1145/2602204.2602206","","","802.11e;hierarchical video coding;mapping mechanism;video transmission;wlans","","","","","2","","","","","","April 2014","","ACM","ACM Journals & Magazines"
"Optimizing matrix transposes using a POWER7 cache model and explicit prefetching","G. Mateescu; G. H. Bauer; R. A. Fiedler","Ecole Polytechnique F&#233;d&#233;rale de Lausanne, Lausanne, Switzerland","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","2","68","73","<p>We consider the problem of efficiently computing matrix transposes on the POWER7 architecture. We develop a matrix transpose algorithm that uses cache blocking, cache prefetching and data alignment. We model the POWER7 data cache and memory concurrency and use the model to predict the memory throughput of the proposed matrix transpose algorithm. The performance of our matrix transpose algorithm is up to five times higher than that of the dgetmo routine of the Engineering and Scientific Subroutine Library and is 2.5 times higher than that of the code generated by compiler-inserted prefetching. Numerical experiments indicate a good agreement between the predicted and the measured memory throughput. </p>","0163-5999;01635999","","10.1145/2381056.2381073","","","POWER7;cache;matrix transpose;prefetching","","","","","1","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"Auto-generation of communication benchmark traces","V. Deshpande; X. Wu; F. Mueller","North Carolina State University, Raleigh, NC, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","2","99","105","<p>Benchmarks are essential for evaluating HPC hardware and software for petascale machines and beyond. But benchmark creation is a tedious manual process. As a result, benchmarks tend to lag behind the development of complex scientific codes. Our work automates the creation of communication benchmarks. Given an MPI application, we utilize ScalaTrace, a lossless and scalable framework to trace communication operations and execution time while abstracting away the computations. A single trace file that reflects the behavior of all nodes is subsequently expanded to C source code by a novel code generator. This resulting benchmark code is compact, portable, human-readable, and accurately reflects the original application's communication characteristics and performance. Experimental results demonstrate that generated source code of benchmarks preserves both the communication patterns and the run-time behavior of the original application. Such automatically generated benchmarks not only shorten the transition from application development to benchmark extraction but also facilitate code obfuscation, which is essential for benchmark extraction from commercial and restricted applications. </p>","0163-5999;01635999","","10.1145/2381056.2381078","","","ScalaTrace;communication benchmark;performance;trace compression","","","","","1","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"An empirical study on clone stability","M. Mondal; C. K. Roy; K. A. Schneider","University of Saskatchewan, Canada","ACM SIGAPP Applied Computing Review","20160129","2012","12","3","20","36","<p>Code cloning is a controversial software engineering practice due to contradictory claims regarding its effect on software maintenance. Code stability is a recently introduced measurement technique that has been used to determine the impact of code cloning by quantifying the changeability of a code region. Although most existing stability analysis studies agree that cloned code is more stable than non-cloned code, the studies have two major flaws: (i) each study only considered a single stability measurement (e.g., lines of code changed, frequency of change, age of change); and, (ii) only a small number of subject systems were analyzed and these were of limited variety.</p> <p>In this paper, we present a comprehensive empirical study on code stability using four different stability measuring methods. We use a recently introduced hybrid clone detection tool, NiCAD, to detect the clones and analyze their stability in different dimensions: by clone type, by measuring method, by programming language, and by system size and age. Our in-depth investigation on 12 diverse subject systems written in three programming languages considering three types of clones reveals that: (i) cloned code is generally less stable than non-cloned code, and more specifically both Type-1 and Type-2 clones show higher instability than Type-3 clones; (ii) clones in both Java and C systems exhibit higher instability compared to the clones in C# systems; (iii) a system's development strategy might play a key role in defining its comparative code stability scenario; and, (iv) cloned and non-cloned regions of a subject system do not follow any consistent change pattern.</p>","1559-6915;15596915","","10.1145/2387358.2387360","","","changeability;code stability;modification frequency;overall instability;software clones;types of clones","","","","","10","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"Attack tolerant architecture for big data file systems","B. B. Madan; M. Banik","Old Dominion University, Norfolk, VA","ACM SIGMETRICS Performance Evaluation Review","20160129","2014","41","4","65","69","<p>Data driven decisions derived from big data have become critical in many application domains, fueling the demand for collection, transportation, storage and processing of massive volumes of data. Such applications have made data a valuable resource that needs to be provided appropriate security. High value associated with big data sets has rendered big data storage systems attractive targets for cyber attackers, whose goal is to compromise the Confidentiality, Integrity and Availability of data and information. Common defense strategy for protecting cyber assets has been to first take preventive measures, and if these fail, detecting intrusions and finally recovery. Unfortunately, attackers have developed tremendous technical sophistication to defeat most defensive mechanisms. Alternative strategy is to design architectures which are intrinsically attack tolerant. This paper describes a technique that involves eliminating single point of security failures through fragmentation, coding, dispersion and reassembly. It is shown that this technique can be successfully applied to routing, networked storage systems, and big data file systems to make them attack tolerant.</p>","0163-5999;01635999","","10.1145/2627534.2627556","","","attack tolerance;big-data security;data availability;data confidentiality;data integrity;secure storage","","","","","","","","","","","March 2014","","ACM","ACM Journals & Magazines"
"BlackjackBench: portable hardware characterization","A. Danalis; P. Luszczek; G. Marin; J. S. Vetter; J. Dongarra","University of Tennessee, Knoxville, TN, USA","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","2","74","79","<p>DARPA's AACE project aimed to develop Architecture Aware Compiler Environments that automatically characterizes the hardware and optimizes the application codes accordingly. We present the BlackjackBench -- a suite of portable benchmarks that automate system characterization, plus statistical analysis techniques for interpreting the results. The BlackjackBench discovers the effective sizes and speeds of the hardware environment rather than the often unattainable peak values. We aim at hardware characteristics that can be observed by running standard C codes. We characterize the memory hierarchy, including cache sharing and NUMA characteristics of the system, properties of the processing cores affecting instruction execution speed, and the length of the OS scheduler time slot. We show how they all could potentially interfere with each other and how established classification and statistical analysis techniques reduce experimental noise and aid automatic interpretation of results. </p>","0163-5999;01635999","","10.1145/2381056.2381074","","","hardware characterization;micro-benchmarks;statistical analysis","","","","","1","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"Towards autotuning by alternating communication methods","A. Tineo; S. R. Alam; T. C. Schulthess","Swiss National Supercomputing Centre, Switzerland","ACM SIGMETRICS Performance Evaluation Review","20160129","2012","40","2","80","85","<p>Interconnects in emerging high performance computing systems feature hardware support for one-sided, asynchronous communication and global address space programming models in order to improve parallel efficiency and productivity by allowing communication and computation overlap and outof- order delivery. In practice though, complex interactions between the software stack and the communication hardware make it challenging to obtain optimum performance for a full application expressed with a one-sided programming paradigm. Here, we present a proof-of-concept study for an autotuning framework that instantiates hybrid kernels based on refactored codes using available communication libraries or languages on a Cray XE6 and a SGI Altix UV 1000. We validate our approach by improving performance for bandwidth- and latency-bound kernels of interest in quantum physics and astrophysics by up to 35% and 80% respectively. </p>","0163-5999;01635999","","10.1145/2381056.2381075","","","PGAS;autotuning;one-sided communication","","","","","","","","","","","September 2012","","ACM","ACM Journals & Magazines"
"A performance evaluation tool for hybrid and dynamic distributed systems","A. E. Silva Freitas; R. J. de Ara√∫jo Mac√™do","Federal Institute of Bahia (IFBA), Salvador, BA, Brazil and Federal University of Bahia (UFBA), Salvador, BA, Brazil","ACM SIGOPS Operating Systems Review","20160129","2014","48","1","11","18","<p>Distributed systems are usually modeled by a set of distributed processes spread over a number of networked computers. Such processes communicate and synchronize themselves by message passing through communication channels. Processes and communication channels can be characterized by synchronous or asynchronous timeliness behavior, according to the characteristics of underlying systems (operating system and communication sub-system). Unlike conventional distributed systems, the timeliness characteristics of dynamic and hybrid distributed systems may vary over time, according to the availability of resources and occurrence of failures. Such systems are becoming common today because of the increasing diversity and heterogeneity of computer networks and associated devices. Due to their high complexity, these systems are difficult to test or verify. In this paper, we introduce a novel simulation tool for such environments, where distinct fault models and timeliness properties can be dynamically assigned to processes and communication channels. Such a tool is meant not only for protocol evaluation but also for prototyping, allowing code reuse in real applications.</p>","0163-5980;01635980","","10.1145/2626401.2626404","","","","","","","","","","","","","","January 2014","","ACM","ACM Journals & Magazines"
"Enhancing X3DOM declarative 3D with rigid body physics support","A. Stamoulias; A. G. Malamos; M. Zampoglou; D. Brutzman","Technological Educational Institute of Crete, Heraklion, Greece, GR","Proceedings of the Nineteenth International ACM Conference on 3D Web Technologies","20160129","2014","","","99","107","<p>Given that physics can be fundamental for realistic and interactive Web3D applications, a number of JavaScript versions of physics engines have been introduced during the past years. This paper presents the implementation of the rigid body physics component, as defined by the X3D specification, in the X3DOM environment, and the creation of dynamic 3D interactive worlds. We briefly review the state of the art in current technologies for Web3D graphics, including HTML5, WebGL and X3D, and then explore the significance of physics engines in building realistic Web3D worlds. We include a comprehensive review of JavaScript physics engine libraries, and proceed to summarize the significance of our implementation while presenting in detail the methodology followed. The results obtained so far from our cross-browser experiments demonstrate that real-time interactive scenes with hundreds of rigid bodies can be constructed and operate with acceptable frame rates, while the allowing the user to maintain the scene control.</p>","","","10.1145/2628588.2628602","","","Ammo.js;HTML5;Web3D;WebGL;X3D;X3DOM;bullet physics;constraint;interactive 3D;physics engines;real-time;rigid body","","","","","","","","","","","8-10 Aug. 2014","","ACM","ACM Conferences"
"A review on recent phishing attacks in Internet","Lakhita; S. Yadav; B. Bohra; Pooja","Dept. of Comput. Sci., Maharishi Arvind Coll. of Eng. & Res. Center, Jaipur, India","2015 International Conference on Green Computing and Internet of Things (ICGCIoT)","20160114","2015","","","1312","1315","The development of internet comes with the other domain that is cyber-crime. The record and intelligently can be exposed to a user of illegal activity so that it has become important to make the technology reliable. Phishing techniques include domain of email messages. Phishing emails have hosted such a phishing website, where a click on the URL or the malware code as executing some actions to perform is socially engineered messages. Lexically analyzing the URLs can enhance the performance and help to differentiate between the original email and the phishing URL. As assessed in this study, in addition to textual analysis of phishing URL, email classification is successful and results in a highly precise anti phishing.","","Electronic:978-1-4673-7910-6; POD:978-1-4673-7911-3; USB:978-1-4673-7909-0","10.1109/ICGCIoT.2015.7380669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380669","Phishing Attacks;Security;Tabnapping","Browsers;Companies;Electronic mail;Internet;Malware;Uniform resource locators","Internet;Web sites;computer crime;invasive software;text analysis;unsolicited e-mail","Internet;URL;cyber-crime;email classification;email messages;illegal activity;malware code;phishing Website;phishing attacks;phishing emails;socially engineered messages;textual analysis","","","","8","","","","8-10 Oct. 2015","","IEEE","IEEE Conferences"
"A Survey on High Mobility Wireless Communications: Challenges, Opportunities and Solutions","J. Wu; P. Fan","University of Arkansas, Fayetteville, AR, USA","IEEE Access","20170520","2016","4","","450","476","Providing reliable broadband wireless communications in high mobility environments, such as high-speed railway systems, remains one of the main challenges faced by the development of the next generation wireless systems. This paper provides a systematic review of high mobility communications. We first summarize a list of key challenges and opportunities in high mobility communication systems, then provide comprehensive reviews of techniques that can address these challenges and utilize the unique opportunities. The review covers a wide spectrum of communication operations, including the accurate modeling of high mobility channels, the transceiver structures that can exploit the properties of high mobility environments, the signal processing techniques that can harvest the benefits (e.g., Doppler diversity) and mitigate the impairments (e.g., carrier frequency offset, intercarrier interference, channel estimation errors) in high mobility systems, and the mobility management and network architectures that are designed specifically for high mobility systems. The survey focuses primarily on physical layer operations, which are affected the most by the mobile environment, with some additional discussions on higher layer operations, such as handover management and control-plane/user-plane decoupling, which are essential to high mobility operations. Future research directions on high mobility communications are summarized at the end of this paper.","","","10.1109/ACCESS.2016.2518085","111 Project; Fundamental Research Funds for the Central Universities; National Basic Research Program of China (973 Program); National Science Foundation of China (NSFC); 10.13039/100000001 - National Science Foundation of US; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383229","CFO;Doppler diversity;High mobility communications;ICI;fast time-varying fading;mobility management","Channel estimation;Doppler effect;Fading channels;Handover;Mobility management;OFDM;Wireless communication","broadband networks;mobile radio;mobility management (mobile radio);radio spectrum management;radio transceivers;signal processing;telecommunication network reliability","broadband wireless communication reliability;communication operation wide spectrum;high mobility wireless communication system environment;mobile environment;mobility management;network architecture;signal processing technique;transceiver structure","","16","","223","","","20160114","2016","","IEEE","IEEE Journals & Magazines"
"A Comparative Review of Steganalysis Techniques","T. Qian; S. Manoharan","Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand","2015 2nd International Conference on Information Science and Security (ICISS)","20160107","2015","","","1","4","While Cryptography is the science of secure communication, Steganography is the art of covert communication. Steganography has a long history, but modern steganography uses digital carriers such as electronic text, disk space, network packets, digital audio and images to hide secrets. Steganalysis, the countermeasure to steganography, is designed to detect and analyse the hidden data disseminated using steganography. This paper reviews and compares three steganalysis techniques: the Histogram Characteristic Function (HCF) technique, the Regular-Singular analysis (RS) technique, and Raw Quick Pair (RQP) technique. The paper analyses the performance, characteristics, and limitations of these three techniques, supported by empirical evidence.","","Electronic:978-1-4673-8611-1; POD:978-1-4673-8612-8","10.1109/ICISSEC.2015.7370963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7370963","","Detectors;Gray-scale;Histograms;Image coding;Image color analysis;Q-factor;Transform coding","cryptography;steganography","HCF technique;RQP technique;RS technique;covert communication;cryptography;digital audio;digital carriers;digital images;disk space;electronic text;histogram characteristic function;network packets;raw quick pair;regular-singular analysis;secret data hiding;secure communication;steganalysis techniques;steganography","","","","12","","","","14-16 Dec. 2015","","IEEE","IEEE Conferences"
"Supervised Image Classification Using Deep Convolutional Wavelets Network","S. Hassairi; R. Ejbali; M. Zaied","REGIM-Lab.: Res. Groups in Intell., Machines Univ. of Sfax, Sfax, Tunisia","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","20160107","2015","","","265","271","This paper gives a review of the deep learning history and proposes a new approach to supervised image classification by the combination of two techniques of learning: the wavelet network and the deep learning. This new approach consists of performing the classification of one class versus all the other classes of the dataset by the reconstruction of a convolutional deep neural wavelet network. This network is obtained using a series of stacked auto-encoders and a linear classifier. The experimental test of our approach performed on ""COIL-100"" dataset demonstrates that our model is remarkably efficient for image classification compared to a known classifier.","1082-3409;10823409","Electronic:978-1-5090-0163-7; USB:978-1-5090-0162-0","10.1109/ICTAI.2015.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372145","Convolutional Neural Network;Deep learning;Stacked AutoEncoders;Wavelet Network","Approximation methods;Biological neural networks;Convolutional codes;Machine learning;Neurons;Training","image classification;wavelet neural nets","COIL-100 dataset;deep convolutional wavelet network;linear classifier;one class classification;stacked auto-encoders;supervised image classification","","4","","40","","","","9-11 Nov. 2015","","IEEE","IEEE Conferences"
"Multithreaded signal preprocessing approach for inertial sensors of smartphone","S. Kaghyan; H. Sarukhanyan","Coding and Signal Processing Department, Institute for Informatics and Automation Problems of NAS RA, Yerevan, Armenia","2015 Computer Science and Information Technologies (CSIT)","20151217","2015","","","85","89","Cell phones and other mobile devices become the part of human culture and change activity and lifestyle patterns. Mobile phone technology continuously evolves and incorporates more and more sensors for enabling advanced applications. Latest generations of smart phones incorporate GPS and WLAN location finding modules, vision cameras, microphones, accelerometers, temperature sensors etc. The availability of these sensors in mass-market communication devices creates exciting new opportunities for data mining applications. Particularly healthcare applications exploiting build-in sensors are very promising. This work reviews different aspects of human activity recognition, including review of state-of-the-art, implementation and algorithmic aspects.","","Electronic:978-1-4673-7562-7; POD:978-1-4673-7563-4","10.1109/CSITechnol.2015.7358256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358256","Activity recognition;feature extraction;mobile computing;signal preprocessing;smartphone sensors","Accelerometers;Classification algorithms;Data processing;Instruction sets;Mobile handsets;Sensors;Servers","Global Positioning System;accelerometers;data mining;health care;microphone arrays;microphones;mobile computing;multi-threading;smart phones;temperature sensors;wireless LAN","GPS;WLAN;accelerometer;cell phone;data mining application;healthcare application;human activity recognition;inertial sensor;location finding module;mass-market communication device;microphones;mobile device;mobile phone technology;multithreaded signal preprocessing approach;smartphone;temperature sensor;vision camera","","","","17","","","","Sept. 28 2015-Oct. 2 2015","","IEEE","IEEE Conferences"
"A Review on Synergistic Learning","C. Li; Y. Li","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Access","20170520","2016","4","","119","134","In neuroscience, it is widely believed that learning and memory are primarily based on synaptic plasticity which is a neural mechanism that modifies the strength of connections between neurons. As a counterpart in machine learning, the modification of connection strength (weight) endows artificial neural networks with a powerful learning capability to solve various problems. Independent of modification for synaptic strength, recent experimental results have revealed that a single neuron also has the ability to change its intrinsic excitability to fit the synaptic input. This mechanism is referred to as neuronal intrinsic plasticity (IP) in the literature. Computational learning rules for IP have been developed based on the hypothesis of information maximization with a stable response level. With the discovery of this novel plasticity mechanism, a series of studies has focused on how IP plays a role in biological neural systems and how they benefit the learning performance of artificial neural networks. In this review, corresponding research on synergies between IP and synaptic plasticity mechanisms is presented in both the computational modeling of biological neural systems and the applications of artificial neural networks, and this combination in artificial learning systems is defined as synergistic learning.","","","10.1109/ACCESS.2015.2509005","National Program for Special Support of Eminent Professionals; 10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358047","Intrinsic plasticity;artificial neural networks;homeostasis;machine learning;sparse coding;synaptic plasticity;synergistic learning","Computational modeling;Hebbian theory;IP networks;Learning systems;Neuroscience;Synergistic learning","learning (artificial intelligence);neural nets","artificial learning systems;artificial neural networks;biological neural systems;machine learning;neural mechanism;neuronal intrinsic plasticity;synaptic plasticity;synergistic learning","","2","","111","","","20151217","2016","","IEEE","IEEE Journals & Magazines"
"Social media interaction and analytics for enhanced educational experiences","G. Palaiokrassas; A. Voulodimos; K. Konstanteli; N. Vretos; D. S. Osborne; E. Chatzi; P. Daras; T. Varvarigou","National Technical University of Athens","IEEE Internet Computing","","2015","Early Access","Early Access","1","1","In this paper we present an innovative system based on social media and high quality multimedia content delivery designed to enhance the educational experiences of students in cultural centers and museums. The system brings together students that are viewing an educational movie with a panel of experts that may be geographically dispersed, and engages them in an exciting social media experience via which they can interact with each other as well as with the remote experts. We employ appropriate methodologies for monitoring and analysing social network activity in order to extract valuable information from the social network activity and use it to seamlessly annotate in real-time the presented educational movie using anthropocentric semantic extraction based on MPEG-7. In this way, the multimedia content of the presented movie can be easily reviewed by multiple geographically dispersed individuals, and impoved afterwards according to the received feedback, thus maximising the benefit for students, teachers and content producers alike.","1089-7801;10897801","","10.1109/MIC.2015.141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325169","","Facebook;Media;Motion pictures;Multimedia communication;Real-time systems;Transform coding;Twitter","","","","","","","","","20151111","","","IEEE","IEEE Early Access Articles"
"The Hitchhikers Guide to Sharing Graph Data","M. Roughan; J. Tuke","ARC Centre of Excellence for Math. & Stat. Frontiers, Univ. of Adelaide, Adelaide, SA, Australia","2015 3rd International Conference on Future Internet of Things and Cloud","20151026","2015","","","435","442","A graph is used to represent data in which the relationships between the objects in the data are at least as important as the objects themselves. Over the last two decades nearly a hundred file formats have been proposed or used to provide portable access to such data. This paper seeks to review these formats, and provide some insight to both reduce the ongoing creation of unnecessary formats, and guide the development of new formats where needed.","","Electronic:978-1-4673-8103-1; POD:978-1-4673-8104-8","10.1109/FiCloud.2015.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300850","GML;Graph;Graph ML;XML;data exchange;data representation;database;network","Databases;Documentation;Encoding;Image coding;Software;Sparse matrices;XML","data structures;graph theory","data representation;graph data sharing;hitchhikers guide","","","","19","","","","24-26 Aug. 2015","","IEEE","IEEE Conferences"
"Video Encoding and Streaming Mechanisms in IoT Low Power Networks","E. G. Pereira; R. Pereira","Dept. of Comput., Edge Hill Univ., Ormskirk, UK","2015 3rd International Conference on Future Internet of Things and Cloud","20151026","2015","","","357","362","Low power networks form a fundamental supporting communication platform for various ""things"" that help in realising the Internet-of-Things (IoT) vision of better, smarter, interconnected world. The IoT paradigm is an emerging area that comes with a number of challenges that requires rethinking of how conventional communication and computational mechanisms can be adapted for this new model where the Internet does not only connect end users but also physical entities, also known as smart devices. While the low power networks provide energy optimised solution for Machine-to-Machine (M2M) communications and interaction between the smart entities, there are also some concerns around the conventional video encoding and streaming mechanisms suitability in such networks. In this paper we review widely adopted video encoders such as MPEG and H.264 and assess their suitability for the low power networks within different video streaming scenarios. In previous work we considered video steaming in low power networks related challenges and elicited a number of possible recommendations based on specific requirements associated with IoT. In this paper we present a discussion of applications where video streaming in IoT low power networks will play an important part, thus highlighting the need for adequate mechanisms to facilitate the deployment of these applications. Also, we present further discussions on the previously identified issues and provide some evaluation results assessing the viability of our recommendations. Furthermore, we discuss how the high error rate associated with wireless communications channels, combined with the low power networked resources, dictate different requirements and solutions for achieving acceptable quality of service.","","Electronic:978-1-4673-8103-1; POD:978-1-4673-8104-8","10.1109/FiCloud.2015.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300839","Compression;H264 encoding;Streaming;Wireless Communication","Bit rate;Encoding;Internet;Quantization (signal);Scalability;Standards;Streaming media","Internet of Things;power aware computing;quality of service;video codecs;video coding;video streaming","H.264;Internet-of-Things;IoT low power networks;MPEG;low power networked resources;machine-to-machine communications;quality of service;smart devices;streaming mechanisms;video encoders;video encoding","","","","27","","","","24-26 Aug. 2015","","IEEE","IEEE Conferences"
"Objective Quality Metrics in Correlation with Subjective Quality Metrics for Steganography","R. Wazirali; S. Slehat; Z. Chaczko; G. Borowik; L. Carri√≥n","Fac. of Eng. & Inf. Technol., Univ. of Technol., Sydney, Sydney, NSW, Australia","2015 Asia-Pacific Conference on Computer Aided System Engineering","20151005","2015","","","238","245","The main goal of hiding data is to conceal the very existence of the hidden information, therefore there is a significant demand for steganographic approaches that can ensure imperceptibility of such infromation. However, there is a limited corresponding evaluation parameters available. Most of the studies use the Peak Signal to Noise Ratio (PSNR) as a metric for imperceptibility evaluation, although it could provide less accurate results than the Human Visual System (HVS) evaluation. This paper provides a review of the existent evaluation metrics that are used to assess the quality of steganography. The examination of the correlation between the existing objective and subjective metrics is also conducted. Pixel differences metrics have a poor correlation with the subjective metrics, hence the HSV based metrics have better correlation than pixel metrics.","","Electronic:978-1-4799-7588-4; POD:978-1-4799-7589-1","10.1109/APCASE.2015.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7287026","","Correlation;Distortion;Distortion measurement;Image quality;PSNR;Visualization","data encapsulation;image coding;steganography","HSV based metrics;HVS;PSNR;data hiding;evaluation metrics;evaluation parameters;human visual system evaluation;imperceptibility evaluation;objective quality metrics;peak signal to noise ratio;pixel metrics;steganographic approaches;steganography;subjective quality metrics","","","","9","","","","14-16 July 2015","","IEEE","IEEE Conferences"
"Review of Big Data Storage Based on DNA Computing","H. A. Hakami; Z. Chaczko; A. Kale","Fac. of Eng. & Inf. Technol., Univ. of Technol., Sydney, NSW, Australia","2015 Asia-Pacific Conference on Computer Aided System Engineering","20151005","2015","","","113","117","There is a need of noteworthy scaling down in the information approached may be saved in the most recent decade. Delicate and advanced version hard paper duplicate which helps in two ways that they increased the effectiveness from claiming data management but also improved the distribution of entrance of information. On engineered DNA, it may be a chance to view the late improvement on the possibility about data capacity. Similarly in this way we have figured out how leap forward engineering could dramatically change the lifestyle out of our information capacity. This topic ' Big Data Storage based DNA' is described from the first research to newer one, their advantages and disadvantages, their techniques and how it will become a practice in the future. We also propose an approach is proposed as simple method to store data into DNA. The experiment work is done to validate the proposed approach result clearly show advantages merits of proposed method.","","Electronic:978-1-4799-7588-4; POD:978-1-4799-7589-1","10.1109/APCASE.2015.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7287004","Coding theory;DNA;DNA Computing;DNA cloud;Encoding;Storage Based DNA","Big data;Computers;DNA;DNA computing;Encoding;Matrix converters;Memory","Big Data;biocomputing;storage management","DNA computing;big data storage;data management;engineered DNA","","","","13","","","","14-16 July 2015","","IEEE","IEEE Conferences"
"Remote Sensing Image Compression: A Review","S. Zhou; C. Deng; B. Zhao; Y. Xia; Q. Li; Z. Chen","Sch. of Inf. & Electron., Beijing Inst. of Technol., Beijing, China","2015 IEEE International Conference on Multimedia Big Data","20150713","2015","","","406","410","With the increasing spatial and temporal resolutions of acquired remote sensing (RS) images, effective image compression is becoming more and more important. RS image compression technologies have been extensively studied in the past a few decades, and various algorithms have been developed accordingly. In this paper, we provide an overview of practically deployed RS image compression approaches, including predictive coding and transform coding approaches that have been adopted in different satellite systems. In addition, some newly derived RS image compression methods are discussed, with highlights on the new trends of the on-going design and developments of RS image compression.","","CD-ROM:978-1-4799-8687-3; Electronic:978-1-4799-8688-0; POD:978-1-4799-8689-7","10.1109/BigMM.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153923","Remote sensing;compressed sensing;image compression;predictive coding;task-driven coding;transform","Discrete cosine transforms;Discrete wavelet transforms;Image coding;Remote sensing;Satellites;Standards;Transform coding","data compression;image coding;remote sensing;transform coding","predictive coding;remote sensing image compression;satellite systems;spatial resolutions;temporal resolutions;transform coding","","0","1","38","","","","20-22 April 2015","","IEEE","IEEE Conferences"
"A Light-Weight Permutation Based Method for Data Privacy in Mobile Cloud Computing","M. Bahrami; M. Singhal","Cloud Lab., Univ. of California, Merced, Merced, CA, USA","2015 3rd IEEE International Conference on Mobile Cloud Computing, Services, and Engineering","20150625","2015","","","189","198","Cloud computing paradigm provides virtual IT infrastructures with a set of resources that are shared with multi-tenant users. Data Privacy is one of the major challenges when users outsource their data to a cloud computing system. Privacy can be violated by the cloud vendor, vendor's authorized users, other cloud users, unauthorized users, or external malicious entities. Encryption is one of the solutions to protect and maintain privacy of cloud-stored data. However, encryption methods are complex and expensive for mobile devices. In this paper, we propose a new light-weight method for mobile clients to store data on one or multiple clouds by using pseudo-random permutation based on chaos systems. The proposed method can be used in the client mobile devices to store data in the cloud(s) without using cloud computing resources for encryption to maintain user's privacy. We consider JPEG image format as a case study to present and evaluate the proposed method. Our experimental results show that the proposed method achieve superior performance compared to over encryption methods, such as AES and encryption on JPEG encoders while protecting the mobile user data privacy. We review major security attack scenarios against the proposed method that shows the level of security.","","Electronic:978-1-4799-8977-5; POD:978-1-4799-8978-2","10.1109/MobileCloud.2015.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130886","Chaos System;Data storage;JPEG encryption;Mobile Cloud Computing;Permutation;Privacy","Chaos;Cloud computing;Data privacy;Encryption;Mobile communication;Mobile handsets;Transform coding","authorisation;cloud computing;cryptography;data privacy;image coding;mobile computing;mobile handsets","AES;JPEG encoders;JPEG image format;cloud computing paradigm;cloud users;cloud vendor;cloud-stored data;data privacy;encryption methods;external malicious entities;light-weight method;light-weight permutation based method;mobile clients;mobile cloud computing;mobile devices;privacy maintain;security attack scenarios;unauthorized users;vendor authorized users;virtual IT infrastructures","","8","","24","","","","March 30 2015-April 3 2015","","IEEE","IEEE Conferences"
"Out-of-band emission reduction and a unified framework for precoded OFDM","X. Huang; J. A. Zhang; Y. J. Guo","University of Technology, Sydney","IEEE Communications Magazine","20150610","2015","53","6","151","159","OFDM has been regarded as a promising candidate for use in cognitive radio systems with dynamic spectrum reuse capability. However, conventional OFDM has significant OOBE, which can cause severe interference to systems operating in adjacent frequency bands. In addition to conventional techniques such as spectral shaping filtering, guard band insertion, and time domain windowing, new OOBE reduction techniques, including cancellation carrier and spectral precoding, have been proposed in recent years. This article reviews various OOBE reduction techniques and proposes a generalized low-complexity OOBE reduction framework for discrete Fourier transform precoded OFDM. With the allocation of explicit frequency domain cancellation subcarriers and data domain cancellation symbols, the proposed framework enables various configurations to achieve significant OOBE reduction with low implementation complexity, and provides flexibility in balancing OOBE reduction and other performance metrics such as peak-to-average power ratio.","0163-6804;01636804","","10.1109/MCOM.2015.7120032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120032","","Channel coding;Complexity theory;Discrete Fourier transforms;Frequency-domain analysis;OFDM;Peak to average power ratio;Receivers;Time-domain analysis","OFDM modulation;cognitive radio;discrete Fourier transforms;frequency-domain analysis;interference suppression;precoding;radio spectrum management;radiofrequency interference","OOBE reduction technique;cognitive radio system;data domain cancellation symbol allocation;discrete Fourier transform precoded OFDM;dynamic spectrum reuse capability;explicit frequency domain cancellation subcarrier allocation;orthogonal frequency division multiplexing;out-of-band emission reduction;unified framework","","9","1","13","","","","June 2015","","IEEE","IEEE Journals & Magazines"
"A Survey Of Architectural Approaches for Data Compression in Cache and Main Memory Systems","S. Mittal; J. S. Vetter","Future Technologies Group, Oak Ridge, TN","IEEE Transactions on Parallel and Distributed Systems","20160407","2016","27","5","1524","1536","As the number of cores on a chip increases and key applications become even more data-intensive, memory systems in modern processors have to deal with increasingly large amount of data. In face of such challenges, data compression presents as a promising approach to increase effective memory system capacity and also provide performance and energy advantages. This paper presents a survey of techniques for using compression in cache and main memory systems. It also classifies the techniques based on key parameters to highlight their similarities and differences. It discusses compression in CPUs and GPUs, conventional and non-volatile memory (NVM) systems, and 2D and 3D memory systems. We hope that this survey will help the researchers in gaining insight into the potential role of compression approach in memory components of future extreme-scale systems.","1045-9219;10459219","","10.1109/TPDS.2015.2435788","10.13039/100000015 - U.S. Department of Energy; 10.13039/100000993 - UT-Battelle; 10.13039/100004673 - LLC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7110612","3D memory;Review;cache;classification;compaction;compression;data redundancy;extreme-scale computing systems;main memory;non-volatile memory","Bandwidth;Compression algorithms;Image coding;Memory management;Nonvolatile memory;Program processors;Redundancy","cache storage;data compression;graphics processing units;random-access storage","2D memory systems;3D memory systems;CPUs;GPUs;architectural approach;cache systems;data compression;data-intensive memory systems;extreme-scale systems;main memory systems;memory components;nonvolatile memory systems","","5","","94","","","20150520","May 1 2016","","IEEE","IEEE Journals & Magazines"
"CoGI: Towards Compressing Genomes as an Image","X. Xie; S. Zhou; J. Guan","Fudan University, Shanghai, China","IEEE/ACM Transactions on Computational Biology and Bioinformatics","20151208","2015","12","6","1275","1285","Genomic science is now facing an explosive increase of data thanks to the fast development of sequencing technology. This situation poses serious challenges to genomic data storage and transferring. It is desirable to compress data to reduce storage and transferring cost, and thus to boost data distribution and utilization efficiency. Up to now, a number of algorithms / tools have been developed for compressing genomic sequences. Unlike the existing algorithms, most of which treat genomes as one-dimensional text strings and compress them based on dictionaries or probability models, this paper proposes a novel approach called CoGI (the abbreviation of Compressing Genomes as an Image) for genome compression, which transforms the genomic sequences to a two-dimensional binary image (or bitmap), then applies a rectangular partition coding algorithm to compress the binary image. CoGI can be used as either a reference-based compressor or a reference-free compressor. For the former, we develop two entropy-based algorithms to select a proper reference genome. Performance evaluation is conducted on various genomes. Experimental results show that the reference-based CoGI significantly outperforms two state-of-the-art reference-based genome compressors GReEn and RLZ-opt in both compression ratio and compression efficiency. It also achieves comparable compression ratio but two orders of magnitude higher compression efficiency in comparison with XM-one state-of-the-art reference-free genome compressor. Furthermore, our approach performs much better than Gzip-a general-purpose and widely-used compressor, in both compression speed and compression ratio. So, CoGI can serve as an effective and practical genome compressor. The source code and other related documents of CoGI are available at: http://admis.fudan.edu.cn/projects/cogi.htm.","1545-5963;15455963","","10.1109/TCBB.2015.2430331","10.13039/501100001809 - National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102721","Genomics, genomes compression, reference-based compression, sequence matrixization, rectangular partition coding, entropy coding","Bioinformatics;Computational biology;Encoding;Entropy;Genomics;Image coding;Partitioning algorithms","DNA;biological techniques;genomics;molecular biophysics;probability;reviews","CoGI;RLZ-opt;compressing genomic sequences;data distribution;entropy-based algorithms;genomic data storage;genomic data transferring;high compression efficiency;one-dimensional text strings;performance evaluation;probability models;rectangular partition coding algorithm;sequencing technology;state-of-the-art reference-based genome compressors GReEn;state-of-the-art reference-free genome compressor;two-dimensional binary imaging","Algorithms;Base Sequence;Chromosome Mapping;Data Compression;Database Management Systems;Databases, Genetic;Genome;Molecular Sequence Data;Sequence Analysis, DNA","3","","35","","","20150506","Nov.-Dec. 1 2015","","IEEE","IEEE Journals & Magazines"
"MPEG-UD Standard Enabling Responsive Remote User Interface","H. C. Bae; M. U. Kim; K. Yoon","Sch. of Comput. Sci. & Eng., Konkuk Univ., Seoul, South Korea","2014 International Conference on Information Science & Applications (ICISA)","20140708","2014","","","1","2","Various personalized recommendation services are provided through internet for users with different screen sizes and various sensors. It is also expected to be many competing service providers for similar recommendation service, and the users are expected to have the chances to select one of these services. However, the user related information as well as the environmental information around the user are not well reflected in the recommendation services, as the information types that the users may vary from user to user. If the user information and the environmental information are clearly defined and classified, the recommendation services may be able to provider broader services to the customers. Further, if the interfaces of the recommendation services are standardized, the users may have more choices as multiple recommendation services are using the same interfaces. This paper briefly reviewed the MPEG-UD standard, which is a standard for interfaces for recommendation services, using user and context information, and presented the concept of the Responsive Remote User Interface using MPEG-UD.","2162-9048;21629048","Electronic:978-1-4799-4441-5; POD:978-1-4799-4440-8","10.1109/ICISA.2014.6847431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6847431","","Context;Educational institutions;Engines;Internet;Standards;Transform coding;User interfaces","recommender systems;user interfaces;video coding","Internet;MPEG user description;MPEG-UD standard;context information;environmental information;information types;personalized recommendation services;recommendation service interfaces;responsive remote user interface;service providers;user information;user related information","","0","","2","","","","6-9 May 2014","","IEEE","IEEE Conferences"
"Exploring the Challenges of MP3 Audio Steganography","M. S. Atoum; S. Ibrahimn; G. Sulong; A. Zeki; A. Abubakar","Fac. of Comput., Univ. Teknol. Malaysia, Skudai, Malaysia","2013 International Conference on Advanced Computer Science Applications and Technologies","20140619","2013","","","156","161","This paper presents the issues and challenges faced for steganographic techniques that uses MP3 (Moving Picture Experts Group-1 ""MPEG-1"" Audio Layer 3) files as the cover or carrier file. The problem of steganographic techniques for MP3 is variety of MP3 states, where it exists at higher or lower bit rates, with higher or lower resulting quality. These have profound effects on strength of techniques that will be used for a different state of MP3 files. Several techniques have already been proposed for embedding data in MP3 files using steganography systems. Unfortunately, there are lacks of general emergence research on the issues and the challenges faced with the MP3 file in terms of steganography. Some techniques for embedding information in MP3 files were reviewed, and recommendations are being proposed for the best strategy of hide information in MP3 files and the possibility of finding other new techniques hiding information in MP3 files.","","Electronic:978-1-4799-2758-6; POD:978-1-4799-2759-3","10.1109/ACSAT.2013.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6836567","Embedding bits;Extracting bit;MP3 Compression;MP3 file;Steganography","Cryptography;Digital audio players;Encoding;Internet;Robustness;Standards;Transform coding","audio signal processing;steganography","MP3 files;MPEG-1 audio layer 3 files;Moving Picture Experts Group-1;audio steganography;general emergence research;information hiding","","3","","38","","","","23-24 Dec. 2013","","IEEE","IEEE Conferences"
"Image Encryption Using Chaotic Maps: A Survey","P. R. Sankpal; P. A. Vijaya","Dept. of TCE, BNMIT, Bangalore, India","2014 Fifth International Conference on Signal and Image Processing","20140331","2014","","","102","107","As the exchange of data over the open networks and Internet is rapidly growing, security of the data becomes a major concern. One possible solution to this problem is to encrypt the data. The data can be text, image, audio, video etc.. In today's world most of the multimedia applications involve images. Earlier image encryption techniques like AES,DES,RSA etc. exhibit low levels of security and also weak anti attack ability. This problem was overcome by using chaos based cryptography. The chaotic systems are very sensitive to initial conditions and control parameters which make them suitable for image encryption. Many works have been done in the field of chaos based image encryption. In this survey paper an attempt has been made to review the aspects and approaches of the design used for image encryption.","","Electronic:978-0-7695-5100-5; POD:978-1-4799-1394-7","10.1109/ICSIP.2014.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754860","Image;analysis;chaotic maps;cryptography;encryption;security","Chaotic communication;Ciphers;Encryption;Streaming media","Internet;chaos;cryptography;image coding;multimedia computing","AES;DES;Internet;RSA;chaotic maps;cryptography;data exchange;data security;image encryption;multimedia applications;weak antiattack ability","","6","","14","","","","8-10 Jan. 2014","","IEEE","IEEE Conferences"
"Computational Implications of Lognormally Distributed Synaptic Weights","J. n. Teramae; T. Fukai","Graduate School of Information Science and Technology, Osaka University, Suita, Osaka, Japan","Proceedings of the IEEE","20140325","2014","102","4","500","512","The connectivity structure of neural networks has significant implications for neural information processing, and much experimental effort has been made to clarify the structure of neural networks in the brain, i.e., both graph structure and weight structure of synaptic connections. A traditional view of neural information processing suggests that neurons compute in a highly parallel and distributed manner, in which the cooperation of many weak synaptic inputs is necessary to activate a single neuron. Recent experiments, however, have shown that not all synapses are weak in cortical circuits, but some synapses are extremely strong (several tens of times larger than the average weight). In fact, the weights of excitatory synapses between cortical excitatory neurons often obey a lognormal distribution with a long tail of strong synapses. Here, we review some of our important and recent works on computation with sparsely distributed synaptic weights and discuss the possible implications of this synaptic principle for neural computation by spiking neurons. We demonstrate that internal noise emerges from long-tailed distributions of synaptic weights to produce stochastic resonance effect in the reverberating synaptic pathways constituted by strong synapses. We show a spike-timing-dependent plasticity rule and other mechanisms that produce such weight distributions. A possible hardware realization of lognormally connected networks is also shown.","0018-9219;00189219","","10.1109/JPROC.2014.2306254","; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6767040","Associative memory (AM);feedforward networks;network connectivity;neural dynamics;neuromorphic engineering;principal component analysis (PCA);recurrent networks;sparse coding;spike sequence;spike-timing-dependent plasticity (STDP);stochastic resonance","Biological neural networks;Integrated circuit modeling;Memory;Neurons;Noise measurement;Principal component analysis;Stability analysis;Stochastic resonance;Stochastic systems","bioelectric potentials;brain;log normal distribution;medical computing;neural nets;neurophysiology;stochastic processes","brain;computational implications;connectivity structure;cortical circuits;cortical excitatory neurons;distributed computation;excitatory synapse weights;graph structure;hardware realization;internal noise;lognormal distribution;long-tailed distributions;neural computation;neural information processing;neural network structure;parallel computation;reverberating synaptic pathways;single neuron;sparsely distributed synaptic weights;spike-timing-dependent plasticity rule;spiking neurons;stochastic resonance effect;strong synapses;synaptic connection;synaptic principle;weak synaptic inputs;weight distribution;weight structure","","7","","56","","","20140314","April 2014","","IEEE","IEEE Journals & Magazines"
"Review: Performance evolution of different detection techniques in V-BLAST","P. P. Jariwala; P. Lapsiwala","Dept. of Electron. & Commun., Sarvajanik Coll. of Eng. & Technol., Surat, India","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","20140130","2013","","","1","5","Multiple Input Multiple Output (MIMO) channels can offer high capacity to wireless systems and the capacity increases linearly with the number of antennas. There are many schemes that can be applied to MIMO systems such as space time block codes, space time trellis codes, and the Vertical Bell Labs Space-Time Architecture (V-BLAST). We study the general MIMO system, the general V-BLAST structure with Zero-Forcing (ZF) and Minimum Mean Square Estimation (MMSE) detectors. Based on bit error rate, we show the performance of these receiver schemes in flat fading channels in MatLab.","","Electronic:978-1-4799-3926-8; POD:978-1-4799-3927-5","10.1109/ICCCNT.2013.6726555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726555","BER;Minimum Mean Square Error (MMSE) and Zero Forcing (ZF);Multiple Input Multiple Output (MIMO);Rayleigh fading channel;SNR","Bit error rate;MIMO;Modulation;Receiving antennas;Transmitting antennas","MIMO systems;error statistics;fading channels;least mean squares methods;radio receivers;signal detection;space-time block codes;trellis codes","MIMO systems;MMSE;MatLab;V-BLAST detection;ZF;bit error rate;flat fading channels;minimum mean square estimation detectors;multiple input multiple output channels;performance evolution;space time block codes;space time trellis codes;vertical Bell Labs space-time architecture;wireless systems;zero forcing detectors","","0","","8","","","","4-6 July 2013","","IEEE","IEEE Conferences"
"Compact Descriptors for Visual Search","L. Y. Duan; J. Lin; J. Chen; T. Huang; W. Gao","Peking Univ., Beijing, China","IEEE MultiMedia","20140722","2014","21","3","30","40","To ensure application interoperability in visual object search technologies, the MPEG Working Group has made great efforts in standardizing visual search technologies. Moreover, extraction and transmission of compact descriptors are valuable for next-generation, mobile, visual search applications. This article reviews the significant progress of MPEG Compact Descriptors for Visual Search (CDVS) in standardizing technologies that will enable efficient and interoperable design of visual search applications. In addition, the article presents the location search and recognition oriented data collection and benchmark under the MPEG CDVS evaluation framework.","1070-986X;1070986X","","10.1109/MMUL.2013.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6701302","compact descriptors;location recognition;multimedia;standard;visual search","Detectors;Encoding;Interoperability;Pipelines;Transform coding;Vectors;Visualization","data compression;image coding;image retrieval","MPEG CDVS evaluation framework;MPEG Working Group;compact descriptor extraction;compact descriptor transmission;compact descriptors for visual search;visual object search technologies","","22","","19","","","20140102","July-Sept. 2014","","IEEE","IEEE Journals & Magazines"
"Inkling: An Executable Paper System for Reviewing Scientific Applications","D. G. Castleberry; S. R. Brandt; F. L√∂ffler","Center for Comput. & Technol., Louisiana State Univ., Baton Rouge, LA, USA","2013 International Conference on Social Computing","20140102","2013","","","917","922","This paper details Inkling, a generalized executable paper system for generating hypermedia. Whereas a traditional paper has static content derived from the data, i.e. tables, charts, graphs, and animations, the executable paper dynamically generates these using an underlying code and editable input parameters specified in the paper itself. By use of a language which may be seamlessly incorporated into the paper text and made transparent to the reader or reviewer, the system allows for ease of both use and validation. Novel in our system is (1)generality, in that it provides a generic coupling between the paper-generating infrastructure and the backend science code, (2) a minimalist text-based human-readable input format which abstracts algorithms from the reader and reviewer, (3) out-of-order dependency-based execution, which allows the author to chain outputs to inputs, and (4) a scheme for building a database of author-contributed codes which may be easily shared, reused and referenced.","","Electronic:978-0-7695-5137-1; POD:978-1-4799-1519-4","10.1109/SocialCom.2013.142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693439","publication activities;software reviews","Abstracts;Educational institutions;Heuristic algorithms;Scientific computing;Software;Software algorithms;Space exploration","hypermedia;publishing;social networking (online);software reviews;text analysis","Inkling;author-contributed codes;backend science code;editable input parameters;executable paper system;generalized executable paper system;generic coupling;minimalist text-based human-readable input format;out-of-order dependency-based execution;paper-generating infrastructure;scientific application reviewing","","0","","10","","","","8-14 Sept. 2013","","IEEE","IEEE Conferences"
"A Comprehensive Literature Review of File Carving","R. Poisel; S. Tjoa","Inst. of IT Security Res., St. Poelten Univ. of Appl. Sci., St. Polten, Austria","2013 International Conference on Availability, Reliability and Security","20131107","2013","","","475","484","File carving is a recovery technique allowing file recovery without knowledge about contextual information such as file system metadata. Due to recent advancements in research, file carving has become an essential technique for both general data recovery and digital forensics investigations. During the last few years a considerable amount of publications has been published on the topic of file carving. Out of around 130 publications in this field we selected 70 key papers with major contributions to the topic in order to identify potential fields of future research activities. The first contribution of this paper is a survey on state-of-the-art literature supporting researchers and practitioners in gaining a comprehensive view on the progress in file carving research. In addition to that, the second major contribution of this paper is a (preliminary) file carving ontology. The purpose of the ontology presented within this paper is to push forward recovery approaches that are based on knowledge bases processible by computer systems.","","Electronic:978-0-7695-5008-4; POD:978-1-4799-1097-7","10.1109/ARES.2013.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657278","Classification;Data Recovery;Digital Forensics;File;File Carving;Fragment;Knowledge Base;Ontology","Accuracy;Computers;Digital forensics;Ontologies;Support vector machines;Transform coding","digital forensics;knowledge based systems;meta data;ontologies (artificial intelligence)","digital forensics investigations;file carving ontology;file recovery;file system metadata;forward recovery approaches;general data recovery;knowledge bases;literature review;recovery technique","","6","","80","","","","2-6 Sept. 2013","","IEEE","IEEE Conferences"
"Review efforts reduction by partitioning of static analysis warnings","T. B. Muske; A. Baid; T. Sanas","TRDDC, 54-B, Hadapsar Industrial Estate, Pune, MH, 411013, India","2013 IEEE 13th International Working Conference on Source Code Analysis and Manipulation (SCAM)","20131028","2013","","","106","115","Static analysis has been successfully employed in software verification, however the number of generated warnings and cost incurred in their manual review is a major concern. In this paper we present a novel idea to reduce manual review efforts by identifying redundancy in this review process. We propose two partitioning techniques to identify redundant warnings - 1) partitioning of the warnings with each partition having one leader warning such that if the leader is a false positive, so are all the warnings in its partition which need not be reviewed and 2) further partitioning the leader warnings based on similarity of the modification points of variables referred to in their expressions. The second technique makes the review process faster by identifying further redundancies and it also makes the reviewing of a warning easier due to the associated information of modification points. Empirical results obtained with these grouping techniques indicate that, on an average, 60% of warnings are redundant in the review context and skipping their review would lead to a reduction of 50-60% in manual review efforts.","","Electronic:978-1-4673-5739-5; POD:978-1-4673-5738-8","10.1109/SCAM.2013.6648191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648191","Analysis Warnings;Data Flow Analysis;False Positives;Review of Warnings;Static Analysis","Arrays;Conferences;Equations;Indexes;Manuals;Maximum likelihood estimation;Redundancy","data flow analysis;program verification","data flow analysis;generated warnings;grouping techniques;leader warnings;modification points similarity;redundant warning identification;review efforts reduction;software verification;static analysis warnings partitioning","","8","","19","","","","22-23 Sept. 2013","","IEEE","IEEE Conferences"
"Teaching developer skills in the first software engineering course","V. Rajlich","Department of Computer Science, Wayne State University, Detroit, MI 48202, U.S.A.","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","1109","1116","Both employers and graduate schools expect computer science graduates to be able to work as developers on software projects. Software engineering courses present the opportunity in the curriculum to learn the relevant skills. This paper presents our experience from Wayne State University and reviews challenges and constraints that we faced while trying to teach these skills. In our first software engineering course, we teach the iterative software development that includes practices of software change, summarized in the phased model of software change. The required resources for our software engineering course are comparable to the other computer science courses. The students - while working in teams - are graded based on their individual contribution to the team effort rather than on the work of the other team members, which improves the fairness of the grading and considerably lessens the stress for the best students in the course. Our students have expressed a high level of satisfaction, and in a survey, they indicated that the skills that they learned in the course are highly applicable to their careers.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606661","First software engineering course;actualization;concept location;developer role;evolutionary-iterative-agile development;impact analysis;open source;phased model of software change;project technologies;realistic code;refactoring","Educational institutions;Portfolios;Software;Software engineering;Unified modeling language","computer science education;educational courses;educational institutions;software engineering;teaching","Wayne State University;computer science courses;developer skills teaching;first software engineering course;iterative software development;phased software change model;software projects","","5","","28","","","","18-26 May 2013","","IEEE","IEEE Conferences"
"Visualizing Software Metrics with Service-Oriented Mining Software Repository for Reviewing Personal Process","S. Yasutaka; S. Matsumoto; S. Saiki; M. Nakamura","Grad. Sch. of Syst. Inf., Kobe Univ., Kobe, Japan","2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","20130916","2013","","","549","554","We have proposed a framework named SO-MSR: service-oriented mining software repository, which applied service oriented architecture to MSR. Following the SO-MSR, we have developed a web service, named MetricsWebAPI, for metrics calculation from a variety of software repositories and a variety source codes. In this paper, we develop and propose Metrics Viewer, which is client of Metrics Viewer and is a web application to support personal process improvement. Metrics Viewer provides an interactive user interface for repository file exploring. Moreover the Metrics Viewer visualizes change of source code metrics to support overhead view of personal process. End user can improve their development activities based on software repository data without MSR specific knowledge by using Metrics Viewer. We have conducted a pilot study to evaluate the effect of proposed system for personal process improvement.","","Electronic:978-0-7695-5005-3; POD:978-1-4799-0371-9","10.1109/SNPD.2013.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598518","MSR;MetricsViewer;SO-MSR;Service-Oriented Architecture;Software Metrics;Visualization","Data mining;History;Software;Software metrics;Visualization;Writing","Web services;application program interfaces;data mining;data visualisation;graphical user interfaces;interactive systems;personal computing;service-oriented architecture;software metrics;software process improvement","MetricsViewer;MetricsWebAPI;SO-MSR;Web application;Web service;interactive user interface;personal process improvement;repository file exploration;service-oriented architecture;service-oriented mining software repository;software metrics visualization;source code metrics","","0","","7","","","","1-3 July 2013","","IEEE","IEEE Conferences"
"Analysis of Field Data on Web Security Vulnerabilities","J. Fonseca; N. Seixas; M. Vieira; H. Madeira","Centre for Inf. & Syst., Univ. of Coimbra, Coimbra, Portugal","IEEE Transactions on Dependable and Secure Computing","20140409","2014","11","2","89","100","Most web applications have critical bugs (faults) affecting their security, which makes them vulnerable to attacks by hackers and organized crime. To prevent these security problems from occurring it is of utmost importance to understand the typical software faults. This paper contributes to this body of knowledge by presenting a field study on two of the most widely spread and critical web application vulnerabilities: SQL Injection and XSS. It analyzes the source code of security patches of widely used Web applications written in weak and strong typed languages. Results show that only a small subset of software fault types, affecting a restricted collection of statements, is related to security. To understand how these vulnerabilities are really exploited by hackers, this paper also presents an analysis of the source code of the scripts used to attack them. The outcomes of this study can be used to train software developers and code inspectors in the detection of such faults and are also the foundation for the research of realistic vulnerability and attack injectors that can be used to assess security mechanisms, such as intrusion detection systems, vulnerability scanners, and static code analyzers.","1545-5971;15455971","","10.1109/TDSC.2013.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6589556","Internet applications;Security;languages;review and evaluation","Awards activities;Blogs;Internet;Java;Security;Software","Internet;SQL;security of data;software fault tolerance;source code (software)","SQL injection;Web application vulnerabilities;Web security vulnerabilities;XSS;attack injectors;code inspectors;field data analysis;intrusion detection systems;realistic vulnerability;security mechanisms;security patches;software faults;source code;static code analyzers;vulnerability scanners","","8","","56","","","20130906","March-April 2014","","IEEE","IEEE Journals & Magazines"
"Macho: A failure model-oriented adaptive cache architecture to enable near-threshold voltage scaling","T. Mahmood; S. Kim; S. Hong","Department of Information & Communication Engineering, Korea Advanced Institute of Science & Technology, Korea","2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)","20130603","2013","","","532","541","Recent interest in CMOS voltage scaling has produced a class of cache architectures which tolerate parametric SRAM failures at low voltage by substituting faulty words of one cache line with healthy words of another line. These caches rely on the fault maps (which grow reciprocally with smaller word sizes) for fault identification. Therefore, the benefits of cache voltage scaling must be rigorously investigated against the cost of their fault map overheads, especially in large caches. This paper reviews the word substitution caches and develops their parametric failure model. Our developed model leads to a non-intrusive and reconfigurable cache (Macho) which can be locally optimized (based on local fault density) by two graph-based algorithms. Specifically, our adaptive matching algorithm increases effective cache capacity by dynamically concentrating healthy cache blocks into active cache sets. Macho enables voltage scaling down to 400mV by tolerating high SRAM-failure rates (‚â• 1%) and achieves better energy reduction (44%) than other substitution caches with similar area overheads.","1530-0897;15300897","Electronic:978-1-4673-5587-2; POD:978-1-4673-5585-8","10.1109/HPCA.2013.6522347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522347","","Adaptation models;Circuit faults;Error correction codes;Fault tolerance;Fault tolerant systems;Mathematical model;Random access memory","CMOS memory circuits;SRAM chips;cache storage;failure analysis;graph theory;memory architecture;pattern matching;power aware computing;reconfigurable architectures","CMOS voltage scaling;Macho;SRAM-failure rates;adaptive matching algorithm;cache capacity;cache voltage scaling;energy reduction;failure model-oriented adaptive cache architecture;fault identification;fault maps;faulty words;graph-based algorithms;healthy cache blocks;local fault density;near-threshold voltage scaling;nonintrusive cache;parametric SRAM failures;reconfigurable cache;word substitution caches","","9","","20","","","","23-27 Feb. 2013","","IEEE","IEEE Conferences"
"Learning to Relate Images","R. Memisevic","University of Montreal, Montreal","IEEE Transactions on Pattern Analysis and Machine Intelligence","20130617","2013","35","8","1829","1846","A fundamental operation in many vision tasks, including motion understanding, stereopsis, visual odometry, or invariant recognition, is establishing correspondences between images or between images and data from other modalities. Recently, there has been increasing interest in learning to infer correspondences from data using relational, spatiotemporal, and bilinear variants of deep learning methods. These methods use multiplicative interactions between pixels or between features to represent correlation patterns across multiple images. In this paper, we review the recent work on relational feature learning, and we provide an analysis of the role that multiplicative interactions play in learning to encode relations. We also discuss how square-pooling and complex cell models can be viewed as a way to represent multiplicative interactions and thereby as a way to encode relations.","0162-8828;01628828","","10.1109/TPAMI.2013.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475945","Learning image relations;complex cells;energy models;mapping units;spatiotemporal features","Computational modeling;Image recognition;Learning systems;Logic gates;Mathematical model;Standards;Training","computer vision;correlation methods;image coding;inference mechanisms;learning (artificial intelligence);spatiotemporal phenomena","bilinear deep-learning method;complex cell model;correlation pattern representation;image features;image pixels;inference framework;multiplicative interaction representation;relation encoding;relational deep-learning method;relational feature learning;spatiotemporal deep-learning method;square-pooling model;vision tasks","Algorithms;Artificial Intelligence;Humans;Pattern Recognition, Automated;Pattern Recognition, Visual","22","","65","","","20130307","Aug. 2013","","IEEE","IEEE Journals & Magazines"
"Automated architectural reviews with Semmle","K. De Schutter","Certipost, Erembodegem, Belgium","2012 28th IEEE International Conference on Software Maintenance (ICSM)","20130110","2012","","","557","565","Keeping code at a high level of quality and in sync with the defined architecture is not a trivial matter when you are participating in a competitive market in which you would rather provide solutions today than tomorrow. Code checkers help, but most are too generic to be of real value and so end up being ignored. Customisable code checkers can provide real benefit at reasonable cost. By encoding company specific (or even project specific) rules, actual results become much more relevant. With the additional integration of UML diagrams and specifications into the code checker, it even becomes possible to manage the gap between the actual code and the architecture from day to day at reasonable cost.","1063-6773;10636773","Electronic:978-1-4673-2312-3; POD:978-1-4673-2313-0","10.1109/ICSM.2012.6405320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405320","","Computer architecture;Conferences;Guidelines;Software maintenance;Standards;Unified modeling language;XML","Unified Modeling Language;program verification;software architecture;software quality","Semmle;UML diagrams;automated architectural reviews;competitive market;customisable code checkers","","0","","19","","","","23-28 Sept. 2012","","IEEE","IEEE Conferences"
"Standards-Based Architectures for Content Management","S. Llorente; E. Rodr√≠guez; J. Delgado; V. Torres-Padrosa","Univ. Politec. de Catalunya, Barcelona, Spain","IEEE MultiMedia","20131122","2013","20","4","62","72","Standards-based middleware architectures for content management are suitable for a range of business scenarios. In this context, the authors review the MPEG-M standard and the MIPAMS standards-based architecture. They describe a selection of relevant deployment scenarios, from content licensing to authorization-based content access control, including a specific case for mobile scenarios. They illustrate each of the scenarios with real MIPAMS implementations developed in several research projects and under contracts within the industry.","1070-986X;1070986X","","10.1109/MMUL.2012.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365164","content management;mobile devices;multimedia;standards-based architectures","Computer architecture;Content management;Licenses;Middleware;Multimedia communication;Standards;Transform coding","authorisation;content management;middleware;mobile computing;service-oriented architecture;standards","MIPAMS standards;MPEG-M standard;authorization-based content access control;business scenarios;content licensing;content management;deployment scenarios;mobile scenarios;standards-based middleware architectures","","0","","18","","","20121129","Oct.-Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Strider: automatic rate adaptation and collision handling","A. Gudipati; S. Katti","Stanford University, Stanford, CA, USA","ACM SIGCOMM Computer Communication Review","20121018","2011","41","4","158","169","<p>This paper presents the design, implementation and evaluation of Strider, a system that automatically achieves almost the optimal rate adaptation without incurring any overhead. The key component in Strider is a novel code that has two important properties: it is <i>rateless and collision-resilient</i>. First, in time-varying wireless channels, Strider's rateless code allows a sender to effectively achieve almost the optimal bitrate, without knowing how the channel state varies. Second, Strider's collision-resilient code allows a receiver to decode both packets from collisions, and achieves the same throughput as the collision-free scheduler. We show via theoretical analysis that Strider achieves Shannon capacity for Gaussian channels, and our empirical evaluation shows that Strider outperforms SoftRate, a state of the art rate adaptation technique by 70% in mobile scenarios and by upto 2.8√ó in contention scenarios.</p>","0146-4833;01464833","","10.1145/2043164.2018455","","","collision decoding;hidden terminals;rate adaptation;rateless coding","","","","","20","1","","","","","August 2011","","ACM","ACM Journals & Magazines"
"Finding protocol manipulation attacks","N. Kothari; R. Mahajan; T. Millstein; R. Govindan; M. Musuvathi","University of Southern California, Los Angeles, CA, USA","ACM SIGCOMM Computer Communication Review","20121018","2011","41","4","26","37","<p>We develop a method to help discover manipulation attacks in protocol implementations. In these attacks, adversaries induce honest nodes to exhibit undesirable behaviors by misrepresenting their intent or network conditions. Our method is based on a novel combination of static analysis with symbolic execution and dynamic analysis with concrete execution. The former finds code paths that are likely vulnerable, and the latter emulates adversarial actions that lead to effective attacks. Our method is precise (i.e., no false positives) and we show that it scales to complex protocol implementations. We apply it to four diverse protocols, including TCP, the 802.11 MAC, ECN, and SCTP, and show that it is able to find all manipulation attacks that have been previously reported for these protocols. We also find a previously unreported attack for SCTP. This attack is a variant of a TCP attack but must be mounted differently in SCTP because of subtle semantic differences between the two protocols.</p>","0146-4833;01464833","","10.1145/2043164.2018440","","","manipulation attacks;symbolic execution","","","","","5","","","","","","August 2011","","ACM","ACM Journals & Magazines"
"What's the difference?: efficient set reconciliation without prior context","D. Eppstein; M. T. Goodrich; F. Uyeda; G. Varghese","University of California at Irvine, Irvine, CA, USA","ACM SIGCOMM Computer Communication Review","20121018","2011","41","4","218","229","<p>We describe a synopsis structure, the Difference Digest, that allows two nodes to compute the elements belonging to the set difference in a single round with communication overhead proportional to the <i>size of the difference</i> times the logarithm of the keyspace. While set reconciliation can be done efficiently using logs, logs require overhead for every update and scale poorly when multiple users are to be reconciled. By contrast, our abstraction assumes no prior context and is useful in networking and distributed systems applications such as trading blocks in a peer-to-peer network, and synchronizing link-state databases after a partition.</p> <p>Our basic set-reconciliation method has a similarity with the peeling algorithm used in Tornado codes [6], which is not surprising, as there is an intimate connection between set difference and coding. Beyond set reconciliation, an essential component in our Difference Digest is a new estimator for the size of the set difference that outperforms min-wise sketches [3] for small set differences.</p> <p>Our experiments show that the Difference Digest is more efficient than prior approaches such as Approximate Reconciliation Trees [5] and Characteristic Polynomial Interpolation [17]. We use Difference Digests to implement a generic KeyDiff service in Linux that runs over TCP and returns the sets of keys that differ between machines.</p>","0146-4833;01464833","","10.1145/2043164.2018462","","","difference digest;invertible bloom filter;set difference","","","","","1","5","","","","","August 2011","","ACM","ACM Journals & Magazines"
"GLADE: a scalable framework for efficient analytics","F. Rusu; A. Dobra","University of California, Merced, Merced, CA","ACM SIGOPS Operating Systems Review","20121018","2012","46","1","12","18","<p>In this paper we introduce GLADE, a scalable distributed framework for large scale data analytics. GLADE consists of a simple user-interface to define Generalized Linear Aggregates (GLA), the fundamental abstraction at the core of GLADE, and a distributed runtime environment that executes GLAs by using parallelism extensively.</p> <p>GLAs are derived from User-Defined Aggregates (UDA), a relational database extension that allows the user to add specialized aggregates to be executed inside the query processor. GLAs extend the UDA interface with methods to Serialize/Deserialize the state of the aggregate required for distributed computation. As a significant departure from UDAs which can be invoked only through SQL, GLAs give the user direct access to the state of the aggregate, thus allowing for the computation of significantly more complex aggregate functions.</p> <p>GLADE runtime is an execution engine optimized for the GLA computation. The runtime takes the user-defined GLA code, compiles it inside the engine, and executes it right near the data by taking advantage of parallelism both inside a single machine as well as across a cluster of computers. This results in maximum possible execution time performance (all our experimental tasks are I/O-bound) and linear scaleup.</p>","0163-5980;01635980","","10.1145/2146382.2146386","","","","","","","","5","","","","","","January 2012","","ACM","ACM Journals & Magazines"
"MPLS-TE and MPLS VPNS with openflow","A. R. Sharafat; S. Das; G. Parulkar; N. McKeown","Stanford University, Stanford, CA, USA","ACM SIGCOMM Computer Communication Review","20121018","2011","41","4","452","453","<p>We demonstrate MPLS Traffic Engineering (MPLS-TE) and MPLS-based Virtual Private Networks (MPLS VPNs) using OpenFlow [1] and NOX [6]. The demonstration is the outcome of an engineering experiment to answer the following questions: How hard is it to implement a complex control plane on top of a network controller such as NOX? Does the global vantage point in NOX make the implementation easier than the traditional method of implementing it on every switch, embedded in the data plane? We implemented every major feature of MPLS-TE and MPLS-VPN in just 2,000 lines of code, compared to much larger lines of code in the more traditional approach, such as Quagga-MPLS. Because NOX maintains a consistent, up-to-date topology map, the MPLS control plane features are quite simple to implement. And its simplicity makes it easy to extend: We have easily added several new features; something a network operator could do to customize their network to meet their customers' needs.</p> <p>The demo consists of two parts: MPLS-TE services and then MPLS VPN driven by a GUI.</p>","0146-4833;01464833","","10.1145/2043164.2018516","","","mpls;mpls-te;openflow;traffic engineering;vpn","","","","","9","3","","","","","August 2011","","ACM","ACM Journals & Magazines"
"SEEP: exploiting symbolic execution for energy-aware programming","T. H√∂nig; C. Eibel; R. Kapitza; W. Schr√∂der-Preikschat","Friedrich-Alexander University Erlangen-Nuremberg","ACM SIGOPS Operating Systems Review","20121018","2011","45","3","58","62","<p>In recent years, there has been a rapid evolution of energyaware computing systems (e.g., mobile devices, wireless sensor nodes), as still rising system complexity and increasing user demands make energy a permanently scarce resource. While static and dynamic optimizations for energy-aware execution have been explored massively, writing energyefficient programs in the first place has only received limited attention. This paper proposes SEEP, a framework which exploits symbolic execution and platform-specific energy profiles to provide the basis for <i>energy-aware programming</i>. More specifically, the framework provides developers with information about the energy demand of their code at hand, even for the invocation of library functions and in settings with multiple possibly strongly heterogeneous target platforms. This equips developers with the necessary knowledge to take energy demand into account during the task of writing programs.</p>","0163-5980;01635980","","10.1145/2094091.2094106","","","","","","","","2","","","","","","December 2011","","ACM","ACM Journals & Magazines"
"Soft error benchmarking of L2 caches with PARMA","J. Suh; M. Manoochehri; M. Annavaram; M. Dubois","University of Southern California, Los Angeles, CA, USA","ACM SIGMETRICS Performance Evaluation Review","20121018","2011","39","1","85","96","<p>The amount of charge stored in an SRAM cell shrinks rapidly with each technology generation thus increasingly exposing caches to soft errors. Benchmarking the FIT rate of caches due to soft errors is critical to evaluate the relative merits of a plethora of protection schemes that are being proposed to protect against soft errors. The benchmarking of cache reliability introduces a unique challenge as compared to internal processor storage structures, such as the load/store queue. In the case of internal processor structures the time a data bit resides in the structure is so short that it is generally safe to assume that no more than one soft error strike can occur. Thus the reliability of such structures is overwhelmingly dominated by single bit errors. By contrast, a memory block may reside for millions of cycles in a last level cache. In this case it is important to consider the impact of the spatial and temporal distribution of multiple errors within the lifetime of a cache block in the presence of error protection.</p> <p>This paper introduces a unified reliability benchmarking framework called PARMA (Precise Analytical Reliability Model for Architecture). PARMA is a rigorous analytical framework that accurately accounts for the distribution of multiple errors to measure the failure rate under any protection scheme. In a single simulation run PARMA provides a precise FIT rate (expected number of failures in one billion hours) measurement for storage structures where the effect of multiple errors cannot be neglected. We have implemented the PARMA framework on top of a cycle-accurate out-of-order processor simulator (sim-outorder) to benchmark L2 cache failure rates for a set of CPU 2000 benchmarks. The effectiveness of three protection schemes are compared in terms of L2 cache FIT rate: parity, word-level Single Error Correcting Double Error Detecting (SECDED) code and block-level SECDED.</p> <p>Exploiting the accuracy of PARMA, we demonstrate that current te- hniques to evaluate cache FIT rates in the presence of SECDED, such as accelerated fault injection simulations and first-principle derivations based on Architectural Vulnerability Factor (AVF), can overestimate FIT rates by vast amounts. Based on the insights gained during this research we also introduce a new approximate analytical model that can quickly and more accurately estimate cache FIT rate in the presence of SECDED.</p>","0163-5999;01635999","","10.1145/2007116.2007127","","","cache;reliability;soft error","","","","","0","7","","","","","June 2011","","ACM","ACM Journals & Magazines"
"On the use of code mobility mechanisms in real-time systems","L. L. Ferreira; L. Nogueira","CISTER Research Centre/School of Engineering of the Polytechnic Institute of Porto, Porto, Portugal","ACM SIGBED Review","20121018","2011","8","4","16","21","<p>Applications with soft real-time requirements can benefit from code mobility mechanisms, as long as those mechanisms support the timing and Quality of Service requirements of applications. In this paper, a generic model for code mobility mechanisms is presented. The proposed model gives system designers the necessary tools to perform a statistical timing analysis on the execution of the mobility mechanisms that can be used to determine the impact of code mobility in distributed real-time applications.</p>","","","10.1145/2095256.2095258","","","code mobility;distributed embedded systems;mobile systems;quality of service;real-time systems","","","","","1","","","","","","December 2011","","ACM","ACM Journals & Magazines"
"jMarkov package: a stochastic modeling tool","M. Cote; G. Riano; R. Akhavan-Tabatabaei; J. F. Perez; A. Sarmiento; J. Goez","Universidad de los Andes, Bogota D.C., Colombia","ACM SIGMETRICS Performance Evaluation Review","20121018","2012","39","4","48","48","<p>When analyzing real life stochastic systems in most cases is easier, cheaper and more effective to use analytical models rather than studying the physical system or a simulation model of it. The stochastic modeling is a powerful tool that helps the analysis and optimization of stochastic systems.</p> <p>However the use of stochastic modeling is not widely spread in today's industries and among practitioners. This lack of acceptance is caused by two main reasons the first being the curse of dimensionality, which is defined by the number of states required to describe a system. This number grows exponentially as the size of the system increases. The second reason is the lack of user-friendly and efficient software packages that allow the modeling of the problem without involving the user with the implementation of the solution algorithms to solve it.</p> <p>The curse of dimensionality is a constant problem that has been addressed by different approaches through time, but it is not intended within the scope of our work; our focus is on the latter issue. We propose a generic solver that enables the user to focus on modeling without getting involved in the complexity required by the solution methods.</p> <p>We design an object oriented framework for stochastic modeling with four components namely, jMarkov which models Markov Chains, jQBD which models Quasi Birth and Death Processes, jPhase which models Phase Types Distributions and jMDP which models Markov Decision Processes. We concentrate all our effort on creating a software that allows the user to model any kind of system like a Markov Chain, QBD or MDP with fairly basic knowledge of programming. To this end we separate the modeling part from the solution algorithms; therefore the user only needs to mathematically model the problem and the software will do the rest. However, we leave the package with the possibility that experienced users can code their own solution algorithms; this is done since the package onl- contains the most common algorithms found in the literature.</p> <p>The software does not use external plain files like '.txt' or '.dat' written with specific commands, but rather it is based on OOP (Object Oriented Programming). The main advantages of it include implementation in Java framework, which allows the computational representation of the model to be very similar to its mathematical representation such that it would become natural to pass from one to another. Also the program possesses the usual characteristics of Java such as the use of inheritance and abstraction. Finally, Java is a high level computational language so the user doesn't need to be concerned about technical problems.</p>","0163-5999;01635999","","10.1145/2185395.2185439","","","","","","","","0","","","","","","April 2012","","ACM","ACM Journals & Magazines"
"HipG: parallel processing of large-scale graphs","E. Krepska; T. Kielmann; W. Fokkink; H. Bal","VU University Amsterdam, Amsterdam, Netherlands","ACM SIGOPS Operating Systems Review","20121018","2011","45","2","3","13","<p>Distributed processing of real-world graphs is challenging due to their size and the inherent irregular structure of graph computations. We present HipG, a distributed framework that facilitates programming parallel graph algorithms by composing the parallel application automatically from the user-defined pieces of sequential work on graph nodes. To make the user code high-level, the framework provides a unified interface to executing methods on local and non-local graph nodes and an abstraction of exclusive execution. The graph computations are managed by logical objects called synchronizers, which we used, for example, to implement distributed divide-and-conquer decomposition into strongly connected components. The code written in HipG is independent of a particular graph representation, to the point that the graph can be created on-the-fly, i.e. by the algorithm that computes on this graph, which we used to implement a distributed model checker. HipG programs are in general short and elegant; they achieve good portability, memory utilization, and performance.</p>","0163-5980;01635980","","10.1145/2007183.2007185","","","","","","","","4","","","","","","July 2011","","ACM","ACM Journals & Magazines"
"Supporting novel home network management interfaces with openflow and NOX","R. Mortier; B. Bedwell; K. Glover; T. Lodge; T. Rodden; C. Rotsos; A. W. Moore; A. Koliousis; J. Sventek","University of Nottingham, Nottingham, United Kingdom","ACM SIGCOMM Computer Communication Review","20121018","2011","41","4","464","465","<p>The Homework project has examined redesign of existing home network infrastructures to better support the needs and requirements of actual home users. Integrating results from several ethnographic studies, we have designed and built a home networking platform providing detailed per-flow measurement and management capabilities supporting several novel management interfaces. This demo specifically shows these new visualization and control interfaces (1), and describes the broader benefits of taking an integrated view of the networking infrastructure, realised through our router's augmented measurement and control APIs (2).</p> <p>Aspects of this work have been published: the Homework Database in Internet Management (IM) 2011 [3] and implications of the ethnographic results are to appear at the SIGCOMM W-MUST workshop 2011 [2]. Separate, more detailed expositions of the interface elements and system performance and implications are currently under submission at other venues. A partial code release is already available and we anticipate fuller public beta release by Q4 2011.</p>","0146-4833;01464833","","10.1145/2043164.2018523","","","dhcp;home networks;network management;nox;openflow","","","","","1","","","","","","August 2011","","ACM","ACM Journals & Magazines"
"Poster: on the capacity delay error tradeoff of source coding","R. L√ºbben; M. Fidler","Institute of Communications Technology, Leibniz Universit&#228;t Hannover","ACM SIGMETRICS Performance Evaluation Review","20121018","2011","39","2","72","72","<p>In this note we present a statistical, non-equilibrium envelope model that, based on Legendre transforms, characterizes source coders and transmission channels by their capacity-delay-error-tradeoff. The model is proven to have the desirable property of additivity, that allows analyzing coders and channels separately. We present an example for Huffman coding. More can be found in the report [2].</p>","0163-5999;01635999","","10.1145/2034832.2034856","","","","","","","","1","","","","","","September 2011","","ACM","ACM Journals & Magazines"
"Multicasting MDC videos to receivers with different screen resolution","R. Gandhi; D. Koutsonikolas; Y. C. Hu","Purdue University","ACM SIGMETRICS Performance Evaluation Review","20121018","2011","39","3","122","124","<p>A primary challenge in multicasting video in a wireless LAN has been to deal with the client diversity in terms of channel diversity: clients may have different channel characteristics and hence receive different numbers of packet transmissions from the AP. Various schemes exploiting layered video coding schemes such as MRC and MDC have been proposed to address this problem. With the advent of smartphones, a new form of client heterogeneity, that different portable devices have different screen resolution and hence desire different numbers of layers, has become increasingly prominent. In this paper, we propose a practical transmission strategy selection method that takes into consideration this new form of client diversity and show it can increase the number of layers received by clients by up to 62%, compared to a scheme that is oblivious of the client screen resolution diversity.</p>","0163-5999;01635999","","10.1145/2160803.2160878","","","MDC;WiFi;client diversity;streaming media","","","","","0","","","","","","December 2011","","ACM","ACM Journals & Magazines"
"Finding resource-release omission faults in Linux","S. Saha; J. Lawall; G. Muller","LIP6-Regal","ACM SIGOPS Operating Systems Review","20121018","2011","45","3","5","9","<p>The management of the releasing of allocated resources is a continual problem in ensuring the robustness of systems code. Missing resource-releasing operations lead to memory leaks and deadlocks. A number of approaches have been proposed to detect such problems, but they often have a high rate of false positives, or focus only on commonly used functions. In this paper we observe that resource-releasing operations are often found in error-handling code, and that the choice of resource-releasing operation may depend on the context in which it is to be used. We propose an approach to finding resource-release omission faults in C code that takes into account these issues. We use our approach to find over 100 faults in the drivers directory of Linux 2.6.34, with a false positive rate of only 16%, well below the 30% that has been found to be acceptable to developers.</p>","0163-5980;01635980","","10.1145/2094091.2094094","","","","","","","","0","1","","","","","December 2011","","ACM","ACM Journals & Magazines"
"Performance driven multi-objective distributed scheduling for parallel computations","A. Narang; A. Srivastava; N. P. K. Katta; R. K. Shyamasundar","IBM Research - India, New Delhi","ACM SIGOPS Operating Systems Review","20121018","2011","45","2","14","27","<p>With the advent of many-core architectures and strong need for Petascale (and Exascale) performance in scientific domains and industry analytics, efficient scheduling of parallel computations for higher productivity and performance has become very important. Further, movement of massive amounts (Terabytes to Petabytes) of data is very expensive, which necessitates affinity driven computations. Therefore, distributed scheduling of parallel computations on multiple places 1 needs to optimize multiple performance objectives: follow affinity maximally and ensure efficient space, time and message complexity. Simultaneous consideration of these objectives makes distributed scheduling a particularly challenging problem. In addition, parallel computations have data dependent execution patterns which requires online scheduling to effectively optimize the computation orchestration as it unfolds.</p> <p>This paper presents an online algorithm for affinity driven distributed scheduling of multi-place 2 parallel computations. To optimize multiple performance objectives simultaneously, our algorithm uses a low time and message complexity mechanism for ensuring affinity and a randomized work-stealing mechanism within places for load balancing. Theoretical analysis of the expected and probabilistic lower and upper bounds on time and message complexity of this algorithm has been provided. On multi-core clusters such as Blue Gene/P (MPP architecture) and Intel multicore cluster, we demonstrate performance close to the custom MPI+Pthreads code. Further, strong, weak and data (increasing input data size) scalability have been demonstrated on multi-core clusters. Using well known benchmarks, we demonstrate 16% to 30% performance gain as compared to Cilk [6] on multi-core Intel Xeon 5570 (NUMA) architecture. Detailed experimental analysis illustrates efficient space (main memory) utilization as well. To the best of our knowledge, this is the first time multi-objective affinity driven- distributed scheduling algorithm has been designed, theoretically analyzed and experimentally evaluated in a multi-place setup for multi-core cluster architectures.</p>","0163-5980;01635980","","10.1145/2007183.2007186","","","","","","","","1","","","","","","July 2011","","ACM","ACM Journals & Magazines"
"A core language for executable models of cyber physical systems: work in progress report","W. Taha; P. Brauner; R. Cartwright; V. Gaspes; A. Ames; A. Chapoutot","Halmstad University, Halmstad, Sweden and Rice University, Houston, TX","ACM SIGBED Review","20121018","2011","8","2","39","43","<p>Recently we showed that an expressive class of mathematical equations can be automatically translated into simulation codes. Focusing on the expressivity of equations on continuous functions, this work considered only minimal interaction with discrete behaviors and only a static number of statically connected components. However, the interaction between continuous and hybrid components in many cyber physical domains is highly coupled, and such systems are often highly dynamic in both respects. This paper gives an overview of a proposed core language for capturing executable hybrid models of highly dynamic cyber physical systems.</p>","","","10.1145/2000367.2000376","","","cyber-physical systems;modeling;simulation","","","","","1","","","","","","June 2011","","ACM","ACM Journals & Magazines"
"Designing a testbed for large-scale distributed systems","C. Leng; M. Lehn; R. Rehner; A. Buchmann","Technische Universit&#228;t Darmstadt, Darmstadt, Germany","ACM SIGCOMM Computer Communication Review","20121018","2011","41","4","400","401","<p>Different evaluation methods for distributed systems like prototyping, simulation and emulation have different tradeoffs. We present a testbed for Internet applications that supports real-network prototypes and multiple simulators with unchanged application code. To ensure maximum portability between runtimes, a compact but flexible system interface is defined.</p>","0146-4833;01464833","","10.1145/2043164.2018488","","","event-based simulator;network simulator;peer-to-peer","","","","","0","","","","","","August 2011","","ACM","ACM Journals & Magazines"
"Configuration coverage in the analysis of large-scale system software","R. Tartler; D. Lohmann; C. Dietrich; C. Egger; J. Sincero","Friedrich-Alexander University Erlangen-Nuremberg, Germany","ACM SIGOPS Operating Systems Review","20121018","2011","45","3","10","14","<p>System software, especially operating systems, tends to be highly configurable. Like every complex piece of software, a considerable amount of bugs in the implementation has to be expected. In order to improve the general code quality, tools for static analysis provide means to check for source code defects without having to run actual test cases on real hardware. Still, for proper type checking a specific configuration is required so that all header include paths are available and all types are properly resolved.</p> <p>In order to find as many bugs as possible, usually a ""full configuration"" is used for the check. However, mainly because of alternative blocks in form of #else-blocks, a single configuration is insufficient to achieve full coverage. In this paper, we present a metric for configuration coverage (CC) and explain the challenges for (properly) calculating it. Furthermore, we present an efficient approach for determining a sufficiently small set of configurations that achieve (nearly) full coverage and evaluate it on a recent Linux kernel version.</p>","0163-5980;01635980","","10.1145/2094091.2094095","","","","","","","","3","","","","","","December 2011","","ACM","ACM Journals & Magazines"
"High confidence embedded software design: a quadrotor helicopter case study","Z. Zhang; J. Porter; N. Kottenstette; X. Koutsoukos; J. Sztipanovits","Vanderbilt University, Nashville, TN","ACM SIGBED Review","20121018","2011","8","2","44","47","<p>Traditional design methodology is not suitable for high-confidence embedded software due to the lack of a formal semantic model for software analysis, automatic code generation, and often designed embedded software is hard to reuse. In order to automatically generate high-confidence and reusable embedded software, we propose a TLM-centric, platform-based, time-triggered and component-oriented method. We use this new method to generate the control software for a quadrotor helicopter.</p>","","","10.1145/2000367.2000377","","","computer aided software engineering;digital control;embedded software;graphical models;real-time systems","","","","","0","","","","","","June 2011","","ACM","ACM Journals & Magazines"
"DEFCAM: A design and evaluation framework for defect-tolerant cache memories","H. Lee; S. Cho; B. R. Childers","University of Pittsburgh","ACM Transactions on Architecture and Code Optimization (TACO)","20121018","2011","8","3","1","29","<p>Advances in deep submicron technology call for a careful review of existing cache designs and design practices in terms of yield, area, and performance. This article presents a Design and Evaluation Framework for defect-tolerant Cache Memories (DEFCAM), which enables processor architects to consider yield, area, and performance together in a unified framework. Since there is a complex, changing trade-off among these metrics depending on the technology, the cache organization, and the yield enhancement scheme employed, such a design flow is invaluable to processor architects when they assess a design and explore the design space quickly at an early stage. We develop a complete framework supporting the proposed DEFCAM design flow, from injecting defects into a wafer to evaluating program performance of individual processors on the wafer. Using DEFCAM, interesting interactions between architectural, organizational, and layout/defect related parameters can be easily evaluated. Moreover, we propose practical set remapping schemes to contain hard faults in cache memory. In a set remapping scheme, accesses that would go to an unusable faulty set are directed to a sound set. Case studies are presented to demonstrate the effectiveness of the proposed design flow and developed tools. Experimental results show that a set remapping is the most efficient fault covering method among prevailing strategies.</p>","1544-3566;15443566","","10.1145/2019608.2019616","","","Process Variations;Yield","","","","","2","","","","","","October 2011","","ACM","ACM Journals & Magazines"
"WiSec 2011 demo: demonstrating self-contained on-node counter measures for various jamming attacks in WSN","S. Ortmann; P. Langend√∂rfer; S. Kornemann","IHP, Im Technologiepark 25, D-15236 Frankfurt (Oder), Germany","ACM SIGMOBILE Mobile Computing and Communications Review","20121018","2011","15","3","39","40","<p>This paper shortly introduces our real-time jamming detection approach which can be executed on standard wireless sensor nodes. The benefits are that no thresholds need to be defined since it detects jamming based on deviations of the Received Signal Strength Indication (RSSI) and the fact that for doing so it needs only 422 Bytes of memory including execution code and stored RSSI values. Our mock-up demonstrator visualises how various attacks of permanent, periodic and random jamming effect RSSI values and how the sensor nodes independently of the location of the jammer reliably indicate ongoing jamming.</p>","1559-1662;15591662","","10.1145/2073290.2073299","","","","","","","","1","","","","","","July 2011","","ACM","ACM Journals & Magazines"
"A method for Hensel code overflow detection","X. Li; C. Lu; J. A. Sjogren","Towson University, Towson, MD","ACM SIGAPP Applied Computing Review","20121018","2012","12","1","6","11","<p>Hensel code was originally defined by Krishnamurthy, Rao and Subramanian [1], which was developed from the <i>P</i>--adic number system first proposed by Hensel in 1900s. The purpose was to realize exact computation for rational numbers. The Hensel code arithmetic has been well developed [1, 2, 3], but the problem of detecting Hensel code overflow and underflow has not been properly addressed [3]. In this paper, we proposed a method for Hensel code overflow detection. The method can realize overflow detection by the prime <i>p</i> and the Hensel code itself. Using this method, a few digits of the Hensel code will be sacrificed.</p>","1559-6915;15596915","","10.1145/2188379.2188380","","","Hensel code;algorithm;error-free computing;overflow;truncation error","","","","","3","","","","","","Spring 2012","","ACM","ACM Journals & Magazines"
"Experience building non-functional requirement models of a complex industrial architecture (abstracts only)","D. D. Gouv√™a; C. d. A. Assis D. Muniz; G. A. Pinto; A. Avritzer; R. M. Meri Le√§o; E. d. S. e Silva; M. C. Diniz; L. Berardinelli; J. C. B. Leite; D. Moss√©; Y. Cai; M. Dalton; L. Kapova; A. Koziolek","","ACM SIGMETRICS Performance Evaluation Review","20121018","2011","39","3","11","11","<p>In this paper, we report on our experience with the application of validated models to assess performance, reliability, and adaptability of a complex mission critical system that is being developed to dynamically monitor and control the position of an oil-drilling platform. We present real-time modeling results that show that all tasks are schedulable. We performed stochastic analysis of the distribution of tasks execution time as a function of the number of system interfaces. We report on the variability of task execution times for the expected system configurations. In addition, we have executed a system library for an important task inside the performance model simulator. We report on the measured algorithm convergence as a function of the number of vessel thrusters. We have also studied the system architecture adaptability by comparing the documented system architecture and the implemented source code. We report on the adaptability findings and the recommendations we were able to provide to the system's architect. Finally, we have developed models of hardware and software reliability. We report on hardware reliability results based on the evaluation of the system architecture. As a topic for future work, we report on an approach that we recommend be applied to evaluate the system under study software reliability.</p>","0163-5999;01635999","","10.1145/2160803.2160809","","","","","","","","0","","","","","","December 2011","","ACM","ACM Journals & Magazines"
"SocialMesh: Can networks of meshed smartphones ensure public access to twitter during an attack?","D. Srikrishna; R. Krishnamoorthy","Tropos Networks","IEEE Communications Magazine","20120606","2012","50","6","99","105","We explore whether public data networks called SocialMesh - formed automatically from smartphone radio-routers and adapting to changing RF propagation and interference without relying on managed cellular infrastructure - can also be designed to overcome any and all countermeasures mounted by an attacker and reliably support social applications such as Twitter, Facebook, and Google. We rigorously examine the architecture, techniques and algorithms that make decentralized or ""ad-hoc,"" ultrawideband networks work in the presence of hostile jamming or protocol attacks. We illustrate the RF link-budget using 3-d graphs to show how interference affects the network performance as these networks become heavily utilized or jammed. Using low-cost hardware, self-organizing routing software, and uncoordinated/distributed CDMA we show how they can retain adequate system capacity even under heavy load from users or interference from jammers. Open standardization and field trials of SocialMesh networks will further clarify SocialMesh security and performance by opening up the design specifications to international review. Open-source design specifications for SocialMesh will be updated at http://www.socialmesh.org.","0163-6804;01636804","","10.1109/MCOM.2012.6211492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211492","","Interference;Peer to peer computing;Radio frequency;Receivers;Routing protocols;Smart phones;Transmitters","code division multiple access;radiofrequency interference;radiowave propagation;security of data;smart phones;social networking (online);telecommunication network routing","3d graphs;Facebook;Google;RF interference;RF link-budget;RF propagation;SocialMesh networks;SocialMesh security;Twitter;ad hoc ultrawideband network;decentralized ultrawideband network;hostile jamming;meshed smartphones;open-source design specifications;protocol attacks;public access;public data networks;self-organizing routing software;smartphone radio-routers;uncoordinated-distributed CDMA","","3","","12","","","","June 2012","","IEEE","IEEE Journals & Magazines"
"When the Software Goes Beyond its Requirements -- A Software Security Perspective","J. C. Liou","Dept. of Comput. Sci., Kean Univ., Union, NJ, USA","2012 Ninth International Conference on Information Technology - New Generations","20120531","2012","","","403","408","Evidences from current events have shown that, in addition to virus and hacker attacks, many software systems have been embedded with ""agents"" that pose security threats such as allowing someone to ""invade"" into computers with such software installed. This will eventually grow into a more serious problem when Cluster and Cloud Computing becomes popular. As this is an area that few have been exploring, we discuss in this paper the issue of software security breaches resulting from embedded sleeping agents. We also investigate some patterns of embedded sleeping agents utilized in software industry. In addition, we review these patterns and propose a security model that identifies different scenarios. This security model will provide a foundation for further study on how to detect and prevent such patterns from becoming security breaches.","","Electronic:978-0-7695-4654-4; POD:978-1-4673-0798-7","10.1109/ITNG.2012.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6209207","Software security;security code;software agents;software assurance;software security model","Computers;Industries;Production;Programming;Security;Software systems","cloud computing;security of data;software agents;software quality","cloud computing;cluster computing;embedded sleeping agent;security threat;software security breaches","","0","3","10","","","","16-18 April 2012","","IEEE","IEEE Conferences"
"Social Television: Enabling Technologies and Architectures","M. J. Montpetit; M. Medard","Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA, USA","Proceedings of the IEEE","20120510","2012","100","Special Centennial Issue","1395","1399","In this paper, we review recent networking developments that will help create the next-generation social television experiences. These include revisiting the way networks are created and using social connectivity to drive physical connectivity and network virtualization. Multipath dissemination and reduction of interruptions will provide better quality of experience. Content protection and privacy are also essential to enable social commentary and metadata applications and will be briefly introduced. Examples of potential applications and results of field trials are also included.","0018-9219;00189219","","10.1109/JPROC.2012.2189804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6179504","Network coding (NC);network on demand;social networking;social television","Electrical engineering;Encoding;Network coding;Streaming media;TV;Watches;Wireless networks","data privacy;digital television;next generation networks;social networking (online);virtualisation","content protection;metadata;multipath dissemination;network virtualization;next-generation social television experiences;physical connectivity;quality of experience;social commentary;social connectivity","","9","","12","","","20120406","May 2012","","IEEE","IEEE Journals & Magazines"
"Vision algorithms for automated census of animals","C. J. Cohen; D. Haanpaa; S. Rowe; J. P. Zott","Cybernet Systems Corporation, Ann Arbor, MI, USA","2011 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","20120403","2011","","","1","5","Numerous military bases have a requirement, based on the Sikes Act, to maintain the base's natural environment while still meeting military mission objectives. One method used to accomplish this is by working toward the goal of achieving habitat and species sustainability. One difficulty is that there is currently no adequate baseline of the ecosystem; specifically, a critical need is the detection, identification, and tracking of animals on Federal and State endangered lists 24 hours a day. For instance, the U.S. Fish and Wildlife Service lists 130 animals as either endangered or threatened, including the desert tortoise, the Mohave ground squirrel, various species of fox, jaguar, mountain beaver, and wolf. In order to even begin to form an appropriate natural environmental baseline, the location and movements of these animals must be acquired, recorded, and made available for review. To this end, we detail technology and machine vision algorithms that can be used to recognize, track, record, and annotate sightings of these animals. We present the methods used, results of our work, current challenges, and future approaches we are taking with our research.","1550-5219;15505219","Electronic:978-1-4673-0216-6; POD:978-1-4673-0215-9","10.1109/AIPR.2011.6176371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176371","animal census;endangered species;machine vision;tracking","Animals;Cameras;Databases;Image coding;Streaming media;Surveillance","computer vision;military computing;object recognition;object tracking;software architecture;sustainable development","Federal endangered list;Mohave ground squirrel;Sikes Act;State endangered list;US Fish and Wildlife Service list;United States;animal sighting annotation;animal sighting recognition;animal sighting record;animal sighting tracking;automated animal census;desert tortoise;ecosystem;fox;habitat sustainability;jaguar;machine vision algorithm;military base;military mission objective;mountain beaver;species sustainability;vision algorithm;wolf","","1","","7","","","","11-13 Oct. 2011","","IEEE","IEEE Conferences"
"Reversible video stream anonymization for video surveillance systems based on pixels relocation and watermarking","J. Cichowski; A. Czyzewski","Gdansk University of Technology, Multimedia Systems Department, Narutowicza 11/12 80-233 Poland","2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)","20120116","2011","","","1971","1977","A method of reversible video image regions of interest anonymization for applications in video surveillance systems is described. A short introduction to the anonymization procedures is presented together with the explanation of its relation to visual surveillance. A short review of state of the art of sensitive data protection in media is included. An approach to reversible Region of Interest (ROI) hiding in video is presented, utilizing a new relocation algorithm for hashing and a watermarking technique for extra data embedding. Implemented application is described, and results obtained using it are reported. Future work and possible improvements to introduced algorithms are discussed.","","Electronic:978-1-4673-0063-6; POD:978-1-4673-0062-9","10.1109/ICCVW.2011.6130490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130490","","Data mining;Decoding;Image coding;Robustness;Streaming media;Transform coding;Watermarking","computer network security;cryptography;file organisation;video streaming;video surveillance;video watermarking","data embedding;data protection;hashing technique;pixels relocation;region of interest hiding;reversible video image regions;reversible video stream anonymization;video surveillance systems;visual surveillance;watermarking technique","","8","","6","","","","6-13 Nov. 2011","","IEEE","IEEE Conferences"
"Similarity Calculation with Length Delimiting Dictionary Distance","A. Burkovski; S. Klenk; G. Heidemann","Dept. for Intell. Syst., Univ. of Stuttgart, Stuttgart, Germany","2011 IEEE 23rd International Conference on Tools with Artificial Intelligence","20111215","2011","","","856","864","The Normalized Compression Distance (NCD) has gained considerable interest in pattern recognition as a similarity measure applicable to unstructured data of very different domains, such as text, DNA sequences, or images. NCD uses existing compression programs such as gzip to compute similarity between objects. NCD has unique features: It does not require any prior knowledge, data preprocessing, feature extraction, domain adaptation or any parameter settings. Further, the NCD can be applied to symbolic data and raw signals alike. In this paper we decompose the NCD and introduce a method to measure compression-based similarity without the need to use compression. The Length Delimiting Dictionary Distance (LD<sup>3</sup>) takes the one component essential in compression methods, the dictionary generation, and strips the NCD of all dispensable components. The LD<sup>3</sup> performs ""compression based pattern recognition without compression"", keeping all of the above benefits of the NCD while achieving better speed and recognition rates. We first review the NCD, introduce LD<sup>3</sup> as the ""essence"" of NCD, and evaluate the LD<sup>3</sup> based on language tree experiments, authorship recognition, and genome phylogeny data.","1082-3409;10823409","Electronic:978-0-7695-4956-7; POD:978-1-4577-2068-0","10.1109/ICTAI.2011.133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6103424","dictionary-based compression;normalized compression distance;parameter-free data mining;pattern recognition;similarity metric","Complexity theory;Compression algorithms;Compressors;Dictionaries;Image coding;Measurement;Pattern recognition","data mining;dictionaries;pattern recognition;trees (mathematics)","NCD;compression-based similarity;feature extraction;genome phylogeny data;language tree experiments;length delimiting dictionary distance;normalized compression distance;parameter-free data mining;pattern recognition","","2","","34","","","","7-9 Nov. 2011","","IEEE","IEEE Conferences"
"Lossless Compression of Hyperspectral Imagery","R. Pizzolante","Dipt. di Inf. ed Applicazioni R. M. Capocelli, Univ. degli Studi di Salerno, Fisciano, Italy","2011 First International Conference on Data Compression, Communications and Processing","20111027","2011","","","157","162","In this paper we review the Spectral oriented Least SQuares (SLSQ) algorithm : an efficient and low complexity algorithm for Hyper spectral Image loss less compression, presented in [2]. Subsequently, we consider two important measures : Pearson's Correlation and Bhattacharyya distance and describe a band ordering approach based on this distances. Finally, we report experimental results achieved with a Java-based implementation of SLSQ on data cubes acquired by NASA JPL's Airborne Visible/Infrared Imaging Spectrometer (AVIRIS).","","Electronic:978-0-7695-4528-8; POD:978-1-4577-1458-0","10.1109/CCP.2011.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061018","3D data;Lossless compression;band ordering;hyperspectral images;image compression;remote sensing","Context;Correlation;Hyperspectral imaging;Image coding;Lakes;Moon","image coding;least squares approximations;visible spectrometers","AVIRIS;Bhattacharyya distance;Java;Pearson correlation;airborne visible/infrared imaging spectrometer;band ordering;data cubes;hyper spectral image lossless compression;hyperspectral imagery;low complexity algorithm;spectral oriented least squares algorithm","","0","","13","","","","21-24 June 2011","","IEEE","IEEE Conferences"
"Development of a Video Streaming Module for Moodle","K. Watanabe; T. Umezu; M. Otani","Dept. of Inf. Sci., Saga Univ., Saga, Japan","2011 International Conference on Complex, Intelligent, and Software Intensive Systems","20110818","2011","","","634","638","We have developed a video streaming module for Moodle. This module adds three functions into Moodle. The one is an uploading function for recorded video files to Moodle. The second is a video format converting function. This function can convert from 14 major video formats to Flash format, automatically. The third is a video playing function. This module works as an activity module in Moodle. We have recorded lectures of a subject through one semester. We have provided recorded videos as a material for lecture reviewing of students via Moodle. Results of a questionnaire show the videos are good materials for review.","","Electronic:978-0-7695-4373-4; POD:978-1-61284-709-2","10.1109/CISIS.2011.105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989051","FFmpeg;Flash Video;Moodle;Video Streaming","Education;Media;Software;Streaming media;Transform coding;Watches","video recording;video streaming","Moodle;flash format;recorded video files;uploading function;video format converting function;video playing function;video streaming module","","1","","6","","","","June 30 2011-July 2 2011","","IEEE","IEEE Conferences"
"Cloudifying source code repositories: how much does it cost?","M. Siegenthaler; H. Weatherspoon","Cornell University","ACM SIGOPS Operating Systems Review","20110708","2010","44","2","24","28","<p>Cloud computing provides us with general purpose storage and server hosting platforms at a reasonable price. We explore the possibility of tapping these resources for the purpose of hosting source code repositories for individual projects as well as entire open source communities. An analysis of storage costs is presented, and a complete hosting solution is built and evaluated as a proof-of-concept.</p>","0163-5980;01635980","","10.1145/1773912.1773919","","","","","","","","1","","","","","","April 2010","","ACM","ACM Journals & Magazines"
"Why panic()?: improving reliability with restartable file systems","S. Sundararaman; S. Subramanian; A. Rajimwale; A. C. Arpaci-Dusseau; R. H. Arpaci-Dusseau; M. M. Swift","University of Wisconsin, Madison","ACM SIGOPS Operating Systems Review","20110708","2010","44","1","25","29","<p>The file system is one of the most critical components of the operating system. Almost all applications running in the operating system require file systems to be available for their proper operation. Though file-system availability is critical in many cases, very little work has been done on tolerating file system crashes. In this paper, we propose Membrane, a set of changes to the operating system to support restartable file systems. Membrane allows an operating system to tolerate a broad class of file system failures and does so while remaining transparent to running applications; upon failure, the file system restarts, its state is restored, and pending application requests are serviced as if no failure had occurred. Our initial evaluation ofMembrane with ext2 shows thatMembrane induces little performance overhead and can tolerate a wide range of file system crashes. More critically, Membrane does so with few changes to ext2, thus improving robustness to crashes without mandating intrusive changes to existing filesystem code.</p>","0163-5980;01635980","","10.1145/1740390.1740397","","","","","","","","0","","","","","","January 2010","","ACM","ACM Journals & Magazines"
"Fived: a service-based architecture implementation to innovate at the endpoints","D. J. Capelis; D. D. E. Long","University of California, Santa Cruz, Santa Cruz, CA, USA","ACM SIGCOMM Computer Communication Review","20110708","2010","40","4","419","420","<p>Security functions such as access control, encryption and authentication are typically left up to applications on the modern Internet. There is no unified system to implement these critical features. The access control that does exist on the network doesn't integrate well with user authentication systems, so access control decisions are based on the network location of a computer rather than the privilege level of its user. Just about every layer of the Internet provides optional encryption, yet most data on the Internet continues to be sent in the clear. Application developers routinely make mistakes in security critical code leading to bugs that manifest in worms, malware or provide a doorway for actively malicious attackers. We propose a unified session layer that integrates trustworthiness features into the core of the network. This would reverse the fortunes of security on the Internet and lead us toward a safer, more secure global network.</p>","0146-4833;01464833","","10.1145/1851275.1851240","","","fived;network architecture;network design;session layer;sessions","","","","","0","","","","","","October 2010","","ACM","ACM Journals & Magazines"
"Wireless network coding and concurrent MAC: are these approaches complementary?","B. Ni; N. Santhapuri; S. Nelakuditi","University of South Carolina","ACM SIGMOBILE Mobile Computing and Communications Review","20110708","2010","14","1","19","21","","1559-1662;15591662","","10.1145/1837188.1837196","","","","","","","","0","","","","","","January 2010","","ACM","ACM Journals & Magazines"
"Visual and algorithmic tooling for system trace analysis: a case study","W. De Pauw; S. Heisig","IBM T.J. Watson Research Center, Hawthorne, NY","ACM SIGOPS Operating Systems Review","20110708","2010","44","1","97","102","<p>Despite advances in the application of automated statistical and machine learning techniques to system log and trace data there will always be a need for human analysis of machine traces, because trace information on unstable systems may be incomplete, or incorrect. In addition, false positives from automated analysis will not likely disappear, and remediation measures and candidate fix tests will need to be evaluated. We present Zinsight, a visual and analytic tool that supports performance analysts and debugging, using large event traces to understand complex systems. This tool enables analysts to quickly create and manipulate high-level structural representations linked with statistical analysis derived from the underlying event trace data. The original raw trace is annotated with module names and a domain specific database is incorporated to relate software functions to module names. Navigable sequence context graph views present automatically extracted execution flow patterns from arbitrarily definable sets of events and are linked to frequency, distribution, and response time views. The goal is to reduce the cognitive and computational load on the analyst while providing answers to the most natural questions in a problem determination session. We present a case study of the tool in use on field problems from the recently shipped (late 2008) IBM z10 mainframe. As a result of the industry trend toward higher parallelism and memory latency, many issues were encountered with legacy code. The tool was applied successfully to diagnose these problems.</p>","0163-5980;01635980","","10.1145/1740390.1740412","","","pattern extraction;problem determination;trace analysis;visualization","","","","","2","","","","","","January 2010","","ACM","ACM Journals & Magazines"
"Towards architecture independent metrics for multicore performance analysis","M. Kulkarni; V. Pai; D. Schuff","Purdue University","ACM SIGMETRICS Performance Evaluation Review","20110708","2010","38","3","10","14","<p>The prevalence of multicore architectures has made the performance analysis of multithreaded applications an intriguing area of inquiry. An understanding of locality effects and communication behavior can provide programmers with valuable information about performance bottlenecks and opportunities for optimization. Unfortunately, most performance analyses are architecture dependent, and hence insights gleaned from an application's behavior on one platform may not apply when the application is run on another. In this position paper, we argue that what is needed are architecture independent metrics that characterize the behavior of an application in a system-agnostic manner. Such metrics will allow a program's performance to be analyzed across a range of architectures without incurring the overhead of repeated profiling and analysis. We propose two specific analyses: multicore-aware reuse distance, which captures the locality properties of an application and communication analysis, which exposes the structure of communication in an application. We also discuss a number of applications of these analyses, in the domains of optimization, code restructuring and performance modeling.</p>","0163-5999;01635999","","10.1145/1925019.1925022","","","","","","","","1","","","","","","December 2010","","ACM","ACM Journals & Magazines"
"The 2nd workshop on active internet measurements (AIMS-2) report","k. claffy; E. Aben; J. Auge; R. Beverly; F. Bustamante; B. Donnet; T. Friedman; M. Fomenkov; P. Haga; M. Luckie; Y. Shavitt","University of California, San Diego, La Jolla, CA, USA","ACM SIGCOMM Computer Communication Review","20110708","2010","40","5","53","58","<p>On February 8-10, 2010, CAIDA hosted the second Workshop on Active Internet Measurements (AIMS-2) as part of our series of Internet Statistics and Metrics Analysis (ISMA) workshops. The goals of this workshop were to further our understanding of the potential and limitations of active measurement research and infrastructure in the wide-area Internet, and to promote cooperative solutions and coordinated strategies to addressing future data needs of the network and security research communities. The three-day workshop included presentations, group discussion and analysis, and focused interaction between participating researchers, operators, and policymakers from all over the world. This report describes the motivation and findings of the workshop, and reviews progress on recommendations developed at the 1st Active Internet Measurements Workshop in 2009 [18]. Slides from the workshop presentations are available at [9].</p>","0146-4833;01464833","","10.1145/1880153.1880162","","","active measurement;codes of ethics;codes of good practice;management techniques;measurement techniques;validation","","","","","0","","","","","","October 2010","","ACM","ACM Journals & Magazines"
"On coding concurrent transmissions in wireless networks","S. Lakshmanan; C. L. Tsao; R. Sivakumar","Georgia Institute of Technology, Atlanta, GA, USA","ACM SIGMOBILE Mobile Computing and Communications Review","20110708","2010","14","2","4","6","<p>In this paper we identify a specific class of collisions called asymmetric collisions where the nature of collisions is different at the receivers of the colliding signals. We show that, with appropriate handling, asymmetric collisions allow the receiver to decode its intended reception successfully. Due to the natural combining of signals from multiple senders, the received symbol can be represented as a function of the transmitted symbols fc. We identify this property of concurrent coded wireless transmissions and propose a solution called collision coding to leverage such collisions.</p>","1559-1662;15591662","","10.1145/1854219.1854222","","","newsletter","","","","","1","","","","","","April 2010","","ACM","ACM Journals & Magazines"
"Efficient error estimating coding: feasibility and applications","B. Chen; Z. Zhou; Y. Zhao; H. Yu","National University of Singapore, Singapore, Singapore","ACM SIGCOMM Computer Communication Review","20110708","2010","40","4","3","14","<p>Motivated by recent emerging systems that can leverage partially correct packets in wireless networks, this paper investigates the novel concept of error estimating codes (EEC). Without correcting the errors in the packet, EEC enables the receiver of the packet to estimate the packet's bit error rate, which is perhaps the most important meta-information of a partially correct packet. Our EEC algorithm provides provable estimation quality, with rather low redundancy and computational overhead. To demonstrate the utility of EEC, we exploit and implement EEC in two wireless network applications, Wi-Fi rate adaptation and real-time video streaming. Our real-world experiments show that these applications can significantly benefit from EEC.</p>","0146-4833;01464833","","10.1145/1851275.1851186","","","bit error rate;error correcting coding;error estimating coding;partial packet;partially correct packet","","","","","9","","","","","","October 2010","","ACM","ACM Journals & Magazines"
"Filet-o-fish: practical and dependable domain-specific languages for OS development","P. E. Dagand; A. Baumann; T. Roscoe","ENS Cachan-Bretagne, France","ACM SIGOPS Operating Systems Review","20110708","2010","43","4","35","39","<p>We address a persistent problem with using domain-specific languages to write operating systems: the effort of implementing, checking, and debugging the DSL usually outweighs any of its benefits. Because these DSLs generate C by templated string concatenation, they are tedious to write, fragile, and incompatible with automated verification tools.</p> <p>We present Filet-o-Fish (FoF), a semantic language to ease DSL construction. Building a DSL using FoF consists of safely composing semantically-rich building blocks. This has several advantages: input files for the DSL are formal specifications of the system's functionality, automated testing of the DSL is possible via existing tools, and we can prove that the C code generated by a given DSL respects the semantics expected by the developer. Early experience has been good: FoF is in daily use as part of the tool chain of the Barrelfish multicore OS, which makes extensive use of domain-specific languages to generate low-level OS code. We have found that the ability to rapidly generate DSLs we can rely on has changed how we have designed the OS.</p>","0163-5980;01635980","","10.1145/1713254.1713263","","","","","","","","0","","","","","","January 2010","","ACM","ACM Journals & Magazines"
"Challenges and opportunities for efficient computing with FAWN","V. Vasudevan; D. G. Andersen; M. Kaminsky; J. Franklin; M. A. Kozuch; I. Moraru; P. Pillai; L. Tan","Carnegie Mellon University","ACM SIGOPS Operating Systems Review","20110708","2011","45","1","34","44","<p>This paper presents the architecture and motivation for a clusterbased, many-core computing architecture for energy-efficient, dataintensive computing. FAWN, a Fast Array of Wimpy Nodes, consists of a large number of slower but efficient nodes coupled with low-power storage. We present the computing trends that motivate a FAWN-like approach, for CPU, memory, and storage. We follow with a set of microbenchmarks to explore under what workloads these FAWN nodes perform well (or perform poorly), and briefly examine scenarios in which both code and algorithms may need to be re-designed or optimized to perform well on an efficient platform. We conclude with an outline of the longer-term implications of FAWN that lead us to select a tightly integrated stacked chip and-memory architecture for future FAWN development.</p>","0163-5980;01635980","","10.1145/1945023.1945029","","","cluster computing;design;energy efficiency;flash;measurement;performance","","","","","2","","","","","","January 2011","","ACM","ACM Journals & Magazines"
"Reliability analysis of deduplicated and erasure-coded storage","X. Li; M. Lillibridge; M. Uysal","Hewlett-Packard Laboratories, Palo Alto, CA","ACM SIGMETRICS Performance Evaluation Review","20110708","2010","38","3","4","9","<p>Space efficiency and data reliability are two primary concerns for modern storage systems. Chunk-based deduplication, which breaks up data objects into single-instance chunks that can be shared across objects, is an effective method for saving storage space. However, deduplication affects data reliability because an object's constituent chunks are often spread across a large number of disks, potentially decreasing the object's reliability. Therefore, an important problem in deduplicated storage is how to achieve space efficiency yet maintain each object's original reliability. In this paper, we present initial results on the reliability analysis of HP-KVS, a deduplicated key-value store that allows each object to specify its own reliability level and that uses software erasure coding for data reliability. The combination of deduplication and erasure coding gives rise to several interesting research problems. We show how to compare the reliability of erasure codes with different parameters and how to analyze the reliability of a big data object given its constituent parts' reliabilities. We also present a method for system designers to determine under what conditions deduplication will save space for erasure-coded data.</p>","0163-5999;01635999","","10.1145/1925019.1925021","","","","","","","","6","6","","","","","December 2010","","ACM","ACM Journals & Magazines"
"Performance analysis of the OP2 framework on many-core architectures","M. B. Giles; G. R. Mudalige; Z. Sharif; G. Markall; P. H. J. Kelly","University of Oxford","ACM SIGMETRICS Performance Evaluation Review","20110708","2011","38","4","9","15","<p>We present a performance analysis and benchmarking study of the OP2 ""active"" library, which provides an abstraction framework for the solution of parallel unstructured mesh applications. OP2 aims to decouple the scientific specification of the application from its parallel implementation, achieving code longevity and near-optimal performance through re-targeting the back-end to different hardware.</p> <p>Runtime performance results are presented for a representative unstructured mesh application written using OP2 on a variety of many-core processor systems, including the traditional X86 architectures from Intel (Xeon based on the older Penryn and current Nehalem micro-architectures) and GPU offerings from NVIDIA (GTX260, Tesla C2050). Our analysis demonstrates the contrasting performance between the use of CPU (OpenMP) and GPU (CUDA) parallel implementations for the solution on an industrial sized unstructured mesh consisting of about 1.5 million edges.</p> <p>Results show the significance of choosing the correct partition and thread-block configuration, the factors limiting the GPU performance and insights into optimizations for improved performance.</p>","0163-5999;01635999","","10.1145/1964218.1964221","","","CFD;CUDA;GPU;OP2;OpenMP;performance;unstructured mesh applications","","","","","8","","","","","","March 2011","","ACM","ACM Journals & Magazines"
"An asynchronous multi-relay cooperation protocol exploiting rateless codes","X. Wang; W. Chen; Z. Cao","Tsinghua University, Beijing, China","ACM SIGMOBILE Mobile Computing and Communications Review","20110708","2010","14","2","1","3","<p>Multi-relay cooperation, which can highly improve the link quality, is shown to be a promising approach for emerging wireless networks. This paper proposes an Asynchronous Multi-Relay Cooperation (AMRC) protocol using rateless codes in order to enhance the bandwidth efficiency of multi-relay transmissions. The protocol performance is analyzed and optimized so as to achieve the maximal end-to-end throughput.</p>","1559-1662;15591662","","10.1145/1854219.1854221","","","newsletter","","","","","1","","","","","","April 2010","","ACM","ACM Journals & Magazines"
"Techniques and tools for implementing IEEE 754 floating-point arithmetic on VLIW integer processors","C. P. Jeannerod; C. Mouilleron; J. M. Muller; G. Revy; C. Bertin; J. Jourdan-Lu; H. Knochel; C. Monat","Universit&#233; de Lyon, France","Proceedings of the 4th International Workshop on Parallel and Symbolic Computation","20110708","2010","","","1","9","<p>Recently, some high-performance IEEE 754 single precision floating-point software has been designed, which aims at best exploiting some features (integer arithmetic, parallelism) of the STMicroelectronics ST200 Very Long Instruction Word (VLIW) processor. We review here the techniques and software tools used or developed for this design and its implementation, and how they allowed very high instruction-level parallelism (ILP) exposure. Those key points include a hierarchical description of function evaluation algorithms, the exploitation of the standard encoding of floating-point data, the automatic generation of fast and accurate polynomial evaluation schemes, and some compiler optimizations.</p>","","","10.1145/1837210.1837212","","","C software implementation;IEEE 754;VLIW processor;binary floating-point arithmetic;code generation;correct rounding;instruction-level parallelism;polynomial evaluation","","","","","3","","","","","","21-23 July 2010","","ACM","ACM Conferences"
"Should we worry about memory loss?","O. Perks; S. D. Hammond; S. J. Pennycook; S. A. Jarvis","University of Warwick, UK","ACM SIGMETRICS Performance Evaluation Review","20110708","2011","38","4","69","74","<p>In recent years the High Performance Computing (HPC) industry has benefited from the development of higher density multi-core processors. With recent chips capable of executing up to 32 tasks in parallel, this rate of growth also shows no sign of slowing. Alongside the development of denser micro-processors has been the considerably more modest rate of improvement in random access memory (RAM). The effect has been that the available memory-per-core has reduced and current projections suggest that this is set to reduce still further.</p> <p>In this paper we present three studies into the use and measurement of memory in parallel applications; our aim is to capture, understand and, if possible, reduce the memory-per-core needed by complete multi-component applications. First, we present benchmarked memory usage and runtimes of a six scientific benchmarks, which represent algorithms that are common to a host of production-grade codes. Memory usage of each benchmark is measured and reported for a variety of compiler toolkits, and we show >30% variation in memory high-water mark requirements between compilers. Second, we utilise this benchmark data combined with runtime data, to simulate via the Maui scheduler simulator, the effect on a multi-science workow if memory-per-core is reduced from 1.5GB-per-core to only 256MB. Finally, we present initial results from a new memory profiling tool currently in development at the University of Warwick. This tool is applied to a finite-element benchmark and is able to map high-water-mark memory allocations to individual program functions. This demonstrates a lightweight and accurate method of identifying potential memory problems, a technique we expect to become commonplace as memory capacities decrease.</p>","0163-5999;01635999","","10.1145/1964218.1964230","","","memory;multi-core;simulation;tracing;workow","","","","","0","","","","","","March 2011","","ACM","ACM Journals & Magazines"
"Tool release: gathering 802.11n traces with channel state information","D. Halperin; W. Hu; A. Sheth; D. Wetherall","University of Washington, Seattle, WA, USA","ACM SIGCOMM Computer Communication Review","20110708","2011","41","1","53","53","<p>We are pleased to announce the release of a tool that records detailed measurements of the wireless channel along with received 802.11 packet traces. It runs on a commodity 802.11n NIC, and records Channel State Information (CSI) based on the 802.11 standard. Unlike Receive Signal Strength Indicator (RSSI) values, which merely capture the total power received at the listener, the CSI contains information about the channel between sender and receiver at the level of individual data subcarriers, for each pair of transmit and receive antennas.</p> <p>Our toolkit uses the Intel WiFi Link 5300 wireless NIC with 3 antennas. It works on up-to-date Linux operating systems: in our testbed we use Ubuntu 10.04 LTS with the 2.6.36 kernel. The measurement setup comprises our customized versions of Intel's close-source firmware and open-source iwlwifi wireless driver, userspace tools to enable these measurements, access point functionality for controlling both ends of the link, and Matlab (or Octave) scripts for data analysis. We are releasing the binary of the modified firmware, and the source code to all the other components.</p>","0146-4833;01464833","","10.1145/1925861.1925870","","","802.11n;channel state information;csi;mimo","","","","","82","","","","","","January 2011","","ACM","ACM Journals & Magazines"
"Code-partitioning gossip","L. Princehouse; K. Birman","Cornell University","ACM SIGOPS Operating Systems Review","20110708","2010","43","4","40","44","<p>Code-Partitioning Gossip (CPG) is a novel technique to facilitate implementation and analysis of gossip protocols. A gossip exchange is a pair-wise transaction between two nodes; a gossip system executes an endless sequence of exchanges between nodes chosen by a randomized procedure. Using CPG, the effects of a gossip exchange are succinctly defined by a single function that atomically updates a pair of node states based on their previous values. This function is automatically partitioned via program slicing into executable code for the roles of gossip-initiator and gossip-recipient, and networking code is added automatically. CPG may have concrete benefits for protocol analysis and authoring composite gossip protocols.</p>","0163-5980;01635980","","10.1145/1713254.1713264","","","gossip protocol program slicing code partitioning","","","","","1","","","","","","January 2010","","ACM","ACM Journals & Magazines"
"Architecture optimisation with currawong","N. FitzRoy-Dale; I. Kuz; G. Heiser","NICTA and the University of New South Wales, Sydney, Australia","ACM SIGCOMM Computer Communication Review","20110708","2011","41","1","115","119","<p>We describe Currawong, a tool to perform <i>system software architecture optimisation</i>. Currawong is an extensible tool which applies optimisations at the point where an application invokes framework or library code. Currawong does not require source code to perform optimisations, effectively decoupling the relationship between compilation and optimisation. We show, through examples written for the popular Android smartphone platform, that Currawong is capable of significant performance improvement to existing applications.</p>","0146-4833;01464833","","10.1145/1925861.1925880","","","android;binary rewriting;optimisation;optimization;prolog","","","","","0","","","","","","January 2011","","ACM","ACM Journals & Magazines"
"Log-based architectures: using multicore to help software behave correctly","S. Chen; P. B. Gibbons; M. Kozuch; T. C. Mowry","Intel Labs Pittsburgh","ACM SIGOPS Operating Systems Review","20110708","2011","45","1","84","91","<p>While application performance and power-efficiency are both important, application correctness is even more important. In other words, if the application is misbehaving, it is little consolation that it is doing so quickly or power-efficiently. In the Log-Based Architectures (LBA) project, we are focusing on a challenging source of application misbehavior: software bugs, including obscure bugs that only cause problems during security attacks. To help detect and fix software bugs, we have been exploring techniques for accelerating dynamic program monitoring tools, which we call ""lifeguards"". Lifeguards are typically written today using dynamic binary instrumentation frameworks such as Valgrind or Pin. Due to the overheads of binary instrumentation, lifeguards that require instructiongrain information typically experience 30X-100X slowdowns, and hence it is only practical to use them during explicit debug cycles. The goal in the LBA project is to reduce these overheads to the point where lifeguards can run continuously on deployed code. To accomplish this, we propose hardware mechanisms to create a dynamic log of instruction-level events in the monitored application and stream this information to one or more software lifeguards running on separate cores on the same multicore processor. In this paper, we highlight techniques and features of LBA that reduce the slowdown to just 2%--51% for sequential programs and 28%--51% for parallel programs.</p>","0163-5980;01635980","","10.1145/1945023.1945034","","","lifeguards;log-based architectures;parallel monitoring;program monitoring;software bugs","","","","","1","1","","","","","January 2011","","ACM","ACM Journals & Magazines"
"Designing coded feedback for efficient network coding based opportunistic routing","D. Koutsonikolas; C. C. Wang; Y. C. Hu","Purdue University, West Lafayette, IN","ACM SIGMOBILE Mobile Computing and Communications Review","20110708","2010","14","1","10","12","<p>A fundamental challenge faced by network coding (NC)-based opportunistic routing (OR) protocols is determining how many coded packets each forwarding node (FN) should transmit. Existing protocols address this challenge by combining NC with offline loss rate based heuristics to eliminate the overhead of feedback exchange, often at the cost of reduced performance in dynamic wireless environments. In this paper, we present a novel solution to this challenge through the design of the CCACK protocol. CCACK introduces an online Cumulative Coded ACKnowledgment scheme which allows the protocol to sustain high performance in dynamic wireless environments, with practically zero overhead.</p>","1559-1662;15591662","","10.1145/1837188.1837193","","","","","","","","0","","","","","","January 2010","","ACM","ACM Journals & Magazines"
"A study of secure deployment of wireless technology in the medical fields","N. Turab; S. Aljawarneh; S. Masadeh","Isra University, Amman, Jordan","Proceedings of the 1st International Conference on Intelligent Semantic Web-Services and Applications","20110708","2010","","","1","4","<p>Wireless Local Area Networks (WLANs) offer the organizations and users many benefits such as mobility, increased productivity and low cost of installation. This paper presents a proposal of deploying WLAN technology in hospitals. It starts with a brief review of the Health Level 7 (HL7), which is used to transfer medical records and data. In addition, a proposal of hospital network that makes use of a new Wi-Fi protocol, called Wi-Fi Protected Setup (WPS). The WPS is used to setup WLAN in easy and secure manner that meets the different requirements of hospitals, and the HL7 standard security requirements.</p>","","","10.1145/1874590.1874615","","","IEEE 802.11i;counter mode with cipher block chaining message authentication code protocol (CCMP) and wi-fi protected setup (WPS);health level (HL7);temporal key integrity protocol","","","","","1","","","","","","14-16 June 2010","","ACM","ACM Conferences"
"Optimal recovery of single disk failure in RDP code storage systems","L. Xiang; Y. Xu; J. C. S. Lui; Q. Chang","University of Science and Technology of China, Hefei, China","ACM SIGMETRICS Performance Evaluation Review","20110708","2010","38","1","119","130","<p>Modern storage systems use thousands of inexpensive disks to meet the storage requirement of applications. To enhance the data availability, some form of redundancy is used. For example, conventional RAID-5 systems provide data availability for single disk failure only, while recent advanced coding techniques such as row-diagonal parity (RDP) can provide data availability with up to two disk failures. To reduce the probability of data unavailability, whenever a single disk fails, disk recovery (or rebuild) will be carried out. We show that conventional recovery scheme of RDP code for a single disk failure is inefficient and suboptimal. In this paper, we propose an optimal and efficient disk recovery scheme, Row-Diagonal Optimal Recovery (RDOR), for single disk failure of RDP code that has the following properties: (1) it is read optimal in the sense that it issues the smallest number of disk reads to recover the failed disk; (2) it has the load balancing property that all surviving disks will be subjected to the same amount of additional workload in rebuilding the failed disk. We carefully explore the design state space and theoretically show the optimality of RDOR. We carry out performance evaluation to quantify the merits of RDOR on some widely used disks.</p>","0163-5999;01635999","","10.1145/1811099.1811054","","","RDP code;disk failure;raid recovery;recovery algorithm","","","","","10","","","","","","June 2010","","ACM","ACM Journals & Magazines"
"Elon: enabling efficient and long-term reprogramming for wireless sensor networks","W. Dong; Y. Liu; X. Wu; L. Gu; C. Chen","Zhejiang University, Hangzhou, China","ACM SIGMETRICS Performance Evaluation Review","20110708","2010","38","1","49","60","<p>We present a new mechanism called Elon for enabling efficient and long-term reprogramming in wireless sensor networks. Elon reduces the transferred code size significantly by introducing the concept of replaceable component. It avoids the cost of hardware reboot with a novel software reboot mechanism. Moreover, it significantly prolongs the reprogramming lifetime by avoiding flash writes for TelosB nodes. Experimental results show that Elon transfers up to 120--389 times less information than Deluge, and 18-42 times less information than Stream. The software reboot mechanism that Elon applies reduces the rebooting cost by 50.4%-53.87% in terms of beacon packets, and 56.83% in terms of unsynchronized nodes. In addition, Elon prolongs the reprogramming lifetime by a factor of 2.3.</p>","0163-5999;01635999","","10.1145/1811099.1811046","","","component;reboot;reprogramming;wireless sensor network","","","","","1","","","","","","June 2010","","ACM","ACM Journals & Magazines"
"SoftCast: one-size-fits-all wireless video","S. Jakubczak; D. Katabi","Massachusetts Institute of Technology, Cambridge, MA, USA","ACM SIGCOMM Computer Communication Review","20110708","2010","40","4","449","450","<p>The focus of this demonstration is the performance of streaming video over the mobile wireless channel. We compare two schemes: the standard approach to video which transmits H.264/AVC-encoded stream over 802.11-like PHY, and SoftCast -- a clean-slate design for wireless video where the source transmits one video stream that each receiver decodes to a video quality commensurate with its specific instantaneous channel quality.</p>","0146-4833;01464833","","10.1145/1851275.1851257","","","joint source-channel coding;scalable video communications;wireless networks","","","","","9","1","","","","","October 2010","","ACM","ACM Journals & Magazines"
"A one year empirical study of student programming bugs","R. C. Bryce; A. Cooley; A. Hansen; N. Hayrapetyan","Utah State University","2010 IEEE Frontiers in Education Conference (FIE)","20101223","2010","","","F1G-1","F1G-7","Students in introductory Computer Science courses often have difficulty with coding and problem solving which results in bugs. These bugs cause both student frustration and attrition of many of our CS majors. In this work, we seek to understand the problems that students believe they cannot solve on their own and for which they ask tutors for assistance. We collect and analyze 450 bugs that were brought to our tutor lab by our CS1 and CS2 students over a one year period. The results show that approximately 22% of the problems are due to problem solving skills, while the remaining problems involve a combination of logic and syntax problems for specific topics in the courses.","0190-5848;01905848","Electronic:978-1-4244-6262-9; POD:978-1-4244-6261-2","10.1109/FIE.2010.5673143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5673143","CS1;CS2;programming bugs","Book reviews;Computer bugs;Problem-solving;Programming;Switches;Syntactics","computer science education;educational courses;problem solving;program debugging","introductory computer science courses;logic problems;problem solving;student frustration;student programming bugs;syntax problems","","2","","6","","","","27-30 Oct. 2010","","IEEE","IEEE Conferences"
"Reading 1D Barcodes with Mobile Phones Using Deformable Templates","O. Gallo; R. Manduchi","University of California, Santa Cruz, Santa Cruz","IEEE Transactions on Pattern Analysis and Machine Intelligence","20111020","2011","33","9","1834","1843","Camera cellphones have become ubiquitous, thus opening a plethora of opportunities for mobile vision applications. For instance, they can enable users to access reviews or price comparisons for a product from a picture of its barcode while still in the store. Barcode reading needs to be robust to challenging conditions such as blur, noise, low resolution, or low-quality camera lenses, all of which are extremely common. Surprisingly, even state-of-the-art barcode reading algorithms fail when some of these factors come into play. One reason resides in the early commitment strategy that virtually all existing algorithms adopt: The image is first binarized and then only the binary data are processed. We propose a new approach to barcode decoding that bypasses binarization. Our technique relies on deformable templates and exploits all of the gray-level information of each pixel. Due to our parameterization of these templates, we can efficiently perform maximum likelihood estimation independently on each digit and enforce spatial coherence in a subsequent step. We show by way of experiments on challenging UPC-A barcode images from five different databases that our approach outperforms competing algorithms. Implemented on a Nokia N95 phone, our algorithm can localize and decode a barcode on a VGA image (640 √ó 480, JPEG compressed) in an average time of 400-500 ms.","0162-8828;01628828","","10.1109/TPAMI.2010.229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674056","Barcodes;UPC-A;deformable templates.;mobile devices","Approximation algorithms;Cameras;Cellular phones;Computer vision;Decoding;Image edge detection;Image segmentation;Visualization","cameras;computer vision;decoding;image coding;mark scanning equipment;mobile computing;mobile handsets;photographic lenses","1D barcode reading;Nokia N95 phone;UPC-A barcode image;VGA image;barcode decoding;binary data;bypasses binarization;camera cellphone;camera lens;deformable template;gray-level information;maximum likelihood estimation;mobile phone;mobile vision application;spatial coherence","1","13","3","14","","","20101223","Sept. 2011","","IEEE","IEEE Journals & Magazines"
"MPEG Application in Electronic Records and Document Management","F. Yu; F. Dan; W. Dandong","Sch. of Comput. Sci., Southwest Pet. Univ., Chengdu, China","2010 International Conference on Management of e-Commerce and e-Government","20101111","2010","","","411","414","Since electronic records and document management progressively become a key issue in knowledge management, the MPEG has occupied the video records and documents' domain as rich information resources and evidences for law & medical, E-learning aids and so on. It is significant to apply MPEG in ERDM. In this paper, a practical system model and architecture has been proposed for ERDM system. XML template of MPEG was already preliminary for system development. A few critical issues of ERDM are discussed as well.","","Electronic:978-0-7695-4245-4; POD:978-1-4244-8507-9","10.1109/ICMeCG.2010.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5628743","ERDM;MPEG;XML;electronic records and document management;multimedia","Book reviews;Encoding;Indexing;MPEG 7 Standard;Media;Streaming media;Transform coding","XML;knowledge management;multimedia computing;video databases;video retrieval","ERDM system;MPEG application;XML;document management;electronic record management;knowledge management;video document;video record","","1","","13","","","","23-24 Oct. 2010","","IEEE","IEEE Conferences"
"Development and Application of Land-Use Planning Management Information System Based on ArcGIS","H. b. Zhang; S. x. Zhao","Inst. of Surveying & Land Info Eng., Henan Polytech. Univ. (HPU), Jiaozuo, China","2010 International Forum on Information Technology and Applications","20101111","2010","3","","64","67","With the startup of new round of planning, our land-use planning has entered a new stage of development. It is meaningful to build the land-use planning management information system, promote the scientific level of the land-use planning, improve the efficiency of daily work, strengthen the criterion of land-use planning management. The paper made a detailed operation-analysis according to the character and the requirement of land-use planning management, and it detailed design the system framework, modular functions and workflow adopt to the approach of modularization. Moreover, one step forward on data collection, data organization and coding process were discussed in this dissertation, a land-use planning database model which incorporates space database and property database was established. By using the software of ArcGIS, the land-use planning management information system based on GIS was finished, and makes prospect for its application.","","Electronic:978-1-4244-7622-0; POD:978-1-4244-7621-3","10.1109/IFITA.2010.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634719","Database;GIS;Information system;Land-use planning","Book reviews;Graphics;Land use planning;Management information systems;Planning;Spatial databases","geographic information systems;land use planning;management information systems;operations research","ArcGIS;data collection;data organization;land use planning management information system","","0","","9","","","","16-18 July 2010","","IEEE","IEEE Conferences"
"A Review on Establishment of E-democracy in China","M. Lu-yan; Z. Xiaoning","Sch. of Political Sci. & Public Adm., Univ. of Electron. Sci. & Technol., Chengdu, China","2010 International Conference on E-Business and E-Government","20100930","2010","","","429","431","E-democracy is a new form of democracy in the internet age. E-democracy plays active roles in democratic construction in China. However, the information asymmetries, the inadequate of government administration, imperfectness of relevant laws, and the absence of internet ethics, all restrained the building of E-democracy. Therefore, great efforts should be made to strengthen development of information system, strengthen political civilization development and improve government operation mechanism. In addition, it is quite essential to speed up internet legislation and perfect the system of internet ethical codes.","","Electronic:978-1-4244-6647-4; POD:978-1-4244-6646-7","10.1109/ICEE.2010.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590625","e-democracy;estabilish;government","Construction industry;Electronic government;Ethics;Information systems;Internet;Law","Internet;ethical aspects;government","China;Internet ethical codes;Internet ethics;Internet legislation;democratic construction;e-democracy;government administration;government operation;information asymmetries;information system;political civilization development","","0","","6","","","","7-9 May 2010","","IEEE","IEEE Conferences"
"Research on the Copy Detection Algorithm for Source Code Based on Program Organizational Structure Tree Matching","Q. Junping; S. Yongxi; S. Hongfeng","Sch. of Inf. Eng., Inner Mongolia Univ. of Technol., Hohhot, China","2010 International Conference on E-Business and E-Government","20100930","2010","","","4461","4464","Code plagiarism is an ubiquitous phenomenon in the teaching of Programming Language. A large number of source code can be automatically detected and uses the similarity value to determine whether the copy is present. It can greatly improve the efficiency of teachers and promote teaching quality. A algorithm is provided that firstly match program organizational structure tree and then process the methods of program to calculate the similarity value. It not only applies to process-oriented programming languages but also applies to object-oriented programming language.","","Electronic:978-1-4244-6647-4; POD:978-1-4244-6646-7","10.1109/ICEE.2010.1120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590938","Copy Detection for Source Code;Longest Common Subsequence;Organizational Structure Tree;Similarity;Token String","Book reviews;Cloning;Computer languages;Educational institutions;Plagiarism;Software measurement","computer science education;object-oriented languages;object-oriented programming;source coding;teacher training;ubiquitous computing","copy detection algorithm;object-oriented programming language;process-oriented programming languages;program organizational structure tree matching;programming language teaching;source code;teaching quality;ubiquitous phenomenon","","0","","6","","","","7-9 May 2010","","IEEE","IEEE Conferences"
"Architecture conformance checking of multi-language applications","R. Rahimi; R. Khosravi","School of Electrical and Computer Engineering, University of Tehran, North Karegar Avenue, Iran","ACS/IEEE International Conference on Computer Systems and Applications - AICCSA 2010","20100927","2010","","","1","8","As the development in a software project goes on, the structure of the implemented code diverges from the intended architecture. To prevent this, architecture conformance methods are used to check if the source code complies with the architecture. In the development of today's enterprise applications, general-purpose programming languages are used along with a number of domain specific languages. So, there is a need for a conformance checking method to support multi-language source artifacts. We present a model-based approach for checking cross-language architecture conformance rules. Our method is extensible, in the sense that it is independent of the specific set of languages used in the project.","2161-5322;21615322","Electronic:978-1-4244-7717-3; POD:978-1-4244-7716-6","10.1109/AICCSA.2010.5587025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587025","","Book reviews","conformance testing;software architecture;software development management","architecture conformance checking;cross-language architecture conformance rules;enterprise applications;general-purpose programming languages;multilanguage applications;multilanguage source artifacts;software project development;source code checking","","1","","28","","","","16-19 May 2010","","IEEE","IEEE Conferences"
"Using Feedback Tags and Sentiment Analysis to Generate Sharable Learning Resources Investigating Automated Sentiment Analysis of Feedback Tags in a Programming Course","S. Cummins; L. Burd; A. Hatch","Sch. of Eng. & Comput. Sci., Durham Univ., Durham, UK","2010 10th IEEE International Conference on Advanced Learning Technologies","20100916","2010","","","653","657","This paper demonstrates how sentiment analysis can be used to identify differences in how students and staff perceive the opinions contained in feedback for programming work. The feedback considered in this paper is conceptually different in that it is given in the form of tags that when associated with a fragment of source code can be considered as a sharable learning resource. The research presented investigates the differences in perception of whether feedback is positive, negative or neutral according to students and examiners. This paper also investigates the adequacy of an automated sentiment analysis engine with a view that sentiment information when combined with the feedback tag and source code may create a more informative sharable learning resource. This paper describes the investigatory technique and presents the initial results. Results indicate that there are important differences between the sentiment of feedback perceived by students and examiners. This paper highlights the benefit of including sentiment data along with feedback.","2161-3761;21613761","Electronic:978-1-4244-7145-4; POD:978-1-4244-7144-7","10.1109/ICALT.2010.186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5572602","Feedback;Programming;Sentiment Analysis;Tagging","Book reviews;Education;Humans;Programming profession;Tag clouds","computer aided instruction;computer science education;educational courses;feedback;groupware","feedback tags;investigatory technique;learning resources;programming course;sentiment analysis","","2","","9","","","","5-7 July 2010","","IEEE","IEEE Conferences"
"ESL design and multi-core validation using the System-on-Chip Environment","W. Chen; X. Han; R. D√∂mer","Center for Embedded Computer Systems University of California, Irvine, USA","2010 IEEE International High Level Design Validation and Test Workshop (HLDVT)","20100628","2010","","","142","147","Design at the Electronic System-Level (ESL) tackles the increasing complexity of embedded systems by raising the level of abstraction in system specification and modeling. Aiming at an automated top-down synthesis flow, effective ESL design frameworks are needed in transforming and refining the highlevel design models until a satisfactory multi-processor system-on-chip (MPSoC) implementation is reached. In this paper, we provide an overview of the System-on-Chip Environment (SCE), a SpecC-based ESL framework for heterogeneous MPSoC design. Our SCE framework has been shown effective for its designer-controlled top-down refinement-based design methodology. After reviewing the SCE design flow, this paper highlights our recent extension of the SCE simulation engine to support multi-core parallel simulation for fast validation of large MPSoC designs. We demonstrate the benefits of the parallel simulation using a case study on a H.264 video decoder application.","1552-6674;15526674","Electronic:978-1-4244-7806-4; POD:978-1-4244-7805-7","10.1109/HLDVT.2010.5496646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5496646","","Application software;Application specific processors;Computational modeling;Computer architecture;Design methodology;Embedded computing;Embedded system;Engines;Hardware;System-on-a-chip","embedded systems;system-on-chip;video coding","H.264 video decoder application;SCE simulation engine;SpecC-based ESL framework;automated top-down synthesis flow;designer-controlled top-down refinement-based design methodology;electronic system-level design;embedded systems;heterogeneous MPSoC design;multicore parallel simulation;multicore validation;multiprocessor system-on-chip implementation;system specification;system-on-chip environment","","2","5","21","","","","10-12 June 2010","","IEEE","IEEE Conferences"
"Software-as-a-Service and Versionology: Towards Innovative Service Differentiation","Y. Badr; G. Caplat","INSA-Lyon, Lyon, France","2010 24th IEEE International Conference on Advanced Information Networking and Applications","20100601","2010","","","237","243","Version control plays an important role in software industries to manage changes and code development. It is also recognized as embedded parts in various software types such as Content Management Systems, Wikis and Word processors. The emergence of Software-as-a-Service (SaaS) and Cloud computing stimulate the provision of online services via the Internet. Traditional version control systems still provide control over changes to source codes in software engineering, but they are expected to play a major role in service innovation. Incremental changes in service provisions increase value, productivity and consumer satisfaction. From a business perspective, the challenge arises when the revision control helps in to decide whether a new version of a SaaS application should be released and define metrics to determine that users should migrate to a new version after several minor or major revisions. Based on the versionology theory, we introduce a metric-based system to evaluate the distance between successive versions and extrapolate users' resistance to change and their capacity to integrate a new version. We also conducted experiments to demonstrate the usefulness of this approach in service differentiation.","1550-445X;1550445X","Electronic:978-1-4244-6696-2; POD:978-1-4244-6695-5","10.1109/AINA.2010.131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474701","Complexity Measures;Design Concepts;Evolutionary Prototyping;Evolving Internet applications;Review and Evaluation;Service Computing;Software-as-a-Service;Version control","Cloud computing;Computer industry;Content management;Control systems;Disaster management;Electrical equipment industry;Embedded software;Industrial control;Software development management;Web and internet services","Internet;configuration management;software engineering","Internet;SaaS application;Wikis;cloud computing;code development;consumer satisfaction;content management systems;innovative service differentiation;metric based system;online services;revision control;software engineering;software industries;software-as-a-service;version control;versionology theory;word processors","","5","","15","","","","20-23 April 2010","","IEEE","IEEE Conferences"
"Notice of Retraction<BR>A Research and Design of Decimal Floating Multiplier Based on FPGA","G. Yixiong; D. Jun; L. Na; Y. Jun","Sch. of Inf. Sci. & Eng., Yunnan Univ., Kunming, China","2010 Third International Conference on Knowledge Discovery and Data Mining","20100318","2010","","","314","319","Notice of Retraction<BR><BR>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<BR><BR>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<BR><BR>This article is based on the new standard, uses the FPGA device opening, adopts FPGA technique of EDA to build 64 bits decimal floating multiplier model. And in this article, we mainly use DPD codec and BCD new codec and Signed-Digit radix-5 to process coefficient. At last, we use Decimal 32:2 CSA algorithm to process partial product. This effectively increases the computing speed and accuracy. As the new standard revision and widespread application of the decimal floating-point multiplication operations, this design has a certain practical significance in the medical and financial sectors, as well as image processing technology.","","Electronic:978-1-4244-5398-6; POD:978-1-4244-5397-9","10.1109/WKDD.2010.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432613","BCD new codec;DPD codec;Decimal 32:2 CSA;Decimal floating multiplier;Signed-Digit radix-5","Application specific integrated circuits;Biomedical imaging;Codecs;Data mining;Design methodology;Electronic design automation and methodology;Field programmable gate arrays;H infinity control;Image coding;Information science","codecs;field programmable gate arrays;floating point arithmetic;logic design","BCD codec;CSA algorithm;DPD codec;decimal floating multiplier;field programmable gate array;partial product processing;signed-digit radix-5","","0","","7","","","","9-10 Jan. 2010","","IEEE","IEEE Conferences"
"Efficient Inventory Management by Leveraging RFID in Service Organizations","O. E. Cakici; H. Groenevelt; A. Seidmann","Simon Sch. of Bus., Univ. of Rochester, Rochester, NY, USA","2010 43rd Hawaii International Conference on System Sciences","20100311","2010","","","1","11","RFID provides real-time tracking, resulting in two additional benefits as an information technology. When inventory is inaccurate and real-time tracking is not available organizations have to use a periodic review policy. When inventory is accurate and realtime tracking is available, they may switch to a continuous review policy. Based on a case study in a radiology practice, we compare the operational and economic differences between a system that uses barcode technology and periodic review, and one that uses RFID technology and continuous review. While the first switch from barcode to RFID is a technology improvement providing automatic counting, the second switch from periodic to continuous review is a process innovation. We measure the value of automatic counting, process innovation, and the total of the two, (the value of RFID). We also explain how these benefits change with service level, lead time, demand, etc.","1530-1605;15301605","Electronic:978-1-4244-5510-2; POD:978-1-4244-5509-6","10.1109/HICSS.2010.173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428603","","Costs;Drugs;Hospitals;Inventory management;Pharmaceutical technology;Radiofrequency identification;Radiology;Real time systems;Switches;Technological innovation","bar codes;inventory management;radiofrequency identification;tracking","RFID;automatic counting;barcode technology;information technology;inventory management;process innovation;real time tracking;service organizations","","1","","29","","","","5-8 Jan. 2010","","IEEE","IEEE Conferences"
"Unmasking Your Software's Ethical Risks","D. Gotterbarn; K. W. Miller","East Tennessee State Univ., Johnson City, TN, USA","IEEE Software","20091231","2010","27","1","12","13","It's difficult to fully address all our professional obligations as software engineers. Our training focuses on avoiding technical failures, but unfortunately our systems sometimes have unintended consequences. We need to develop products to avoid unintended negative impacts on society, people, and the environment. Professional responsibility requires that we identify the morally salient features of a situation. Some issues are relatively easy to spot; for example, we shouldn't lie to clients, we shouldn't bribe inspectors, and we should respect people's privacy. But some ethical and social risks are harder to recognize. Even developers with the best intentions have walked into ethical traps. When we study technical problems, we apply the project's constraints and priorities to find acceptable possible solutions and choose among them. Here are four suggestions for considering ethical constraints during that process, they are: look for human values in technical decision; identifying who will be affected; examining how stakeholders' right and obligation will be affectedl; and reviewing relevant professional standards to help identify issues.","0740-7459;07407459","","10.1109/MS.2010.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370758","Ethics;ethics code;risks;stakeholder rights","Humans;Privacy","ethical aspects;software engineering","human value;professional obligation;professional responsibility;projects constraint;relevant professional standard;software engineer;software ethical risk;technical decision;unintended negative impact prevention","","1","","3","","","","Jan.-Feb. 2010","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction: Special Section on Adaptive Hardware and Systems","K. Benkrid; D. Keymeulen; U. D. Patel; D. Merodio-Codinachs","The University of Edinburgh, School of Engineering King&#8217;s Buildings, Faraday Building, Mayfield Road, Edinburgh EH9 3JL, Scotland, United Kingdom","IEEE Transactions on Computers","20130627","2013","62","8","1478","1480","This special section of IEEE Transactions on Computers presents some of the latest research developments in the field of adaptive hardware and systems. The creation of this section was motivated by lively discussions held at the annual NASA/ESA Adaptive Hardware and Systems (AHS) conference, which showed a need for such special section at a top ranked journal. At the end of a rigorous review process, ten papers were selected for publication from a set of high quality submissions consisting of regular papers and extended papers from the AHS 2012 conference proceedings. The articles are then briefly described.","0018-9340;00189340","","10.1109/TC.2013.135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547973","","Hardware;Self-organizing networks;Special issues and sections","","","","0","","","","","","Aug. 2013","","IEEE","IEEE Journals & Magazines"
"A New Year","G. Taubin","Brown University","IEEE Computer Graphics and Applications","20111222","2012","32","1","6","6","Editor in Chief Gabriel Taubin discusses the Meet the Editors panel discussion he participated in at VisWeek and IEEE Computer Graphics and Applications' new best article award. He encourages people to submit full-length articles for peer review as well as shorter articles for CG&A's departments.","0272-1716;02721716","","10.1109/MCG.2012.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111344","Advanced Graphics Technology;Applications;Education;Graphically Speaking;Graphics Math and Code;Tools and Products;Tutorials;VisWeek;Visualization Viewpoints;best article award;editor in chief;meet the editors","","","","","0","","","","","","Jan.-Feb. 2012","","IEEE","IEEE Journals & Magazines"
"Report on ACES 2011: The 27th International Review of Progress in Applied Computational Electromagnetics March 27‚Äì31,2,011, Williamsburg, Virginia","","","IEEE Antennas and Propagation Magazine","20110926","2011","53","3","178","181","The Applied Computational Electromagnetics Society (ACES) provides a forum for issues relevant to numerical modeling in applied electromagnetics. The primary focus of ACES is on computational techniques, electromagnetics modeling software, and applications. The Applied Computational Electromagnetics Society offers the ACES annual symposium, publications, code user groups, benchmark-problem solution workshops, short courses, software demonstrations, and other activities. In 2010, the ACES conference was held in Tampere, Finland. This year, ACES 2011 was held in Williamsburg, Virginia, USA. Dr. C. J. Reddy of EM Software and Systems (USA) Inc./Applied EM Inc. and Mr. Erik Vedeler of NASA Langley were the General Chairs of the conference. Prof. Atef Elsherbeni of the University of Mississippi was the Technical Program Chair.","1045-9243;10459243","","10.1109/MAP.2011.6028447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6028447","","","","","","0","","","","","","June 2011","","IEEE","IEEE Journals & Magazines"
"Control & Dynamics: How They Are Applied in Power System Microgrids [Book Review]","G. Sheble","","IEEE Power and Energy Magazine","20180418","2018","16","3","98","100","This book explores how power system control of the smart grid is impacting the power industry. The effects of renewable energy development are increasing at a time when transmission investments are decreasing. In many places, the structure of the power grid is returning to the more isolated city regions that were the norm before high-voltage transmission linked larger geographic areas. Today‚Äôs transmission system is stressed by an increasing lack of inertia, poor frequency control, and the need for improved voltage control. The smart grid is an outgrowth of this evolution as a result of communications systems having low cost and increased capabilities. Competitive markets, such as transactive energy, further complicate the operation and planning of the system. Students and industry practitioners alike would be well served by reading this book which covers all aspects of dynamic simulation and control under the assumption that readers are familiar with steady-state behavior, which is not discussed in this text. The book has two goals: 1) to show how to model generation by fully considering the increased use of power electronics for converter control and coordination and 2) to introduce consensus control for interarea oscillations. The author‚Äôs goal is to provide a bridge between traditional control and microgrid control. That goal is fully achieved. The reader learns by example problems and solutions with the provided MATLAB code.","1540-7977;15407977","","10.1109/MPE.2018.2798760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8340906","","Book reviews;Mathematical model;Matlab;Microgrids;Power system control;Power system planning;Power system stability;Renewable energy sources;Voltage control","","","","","","","","","","May-June 2018","","IEEE","IEEE Journals & Magazines"
"Introduction to the Issue on Light Field Image Processing","Y. Liu; L. Fang; D. Gutierrez; Q. Wang; J. Yu; F. Wu","Max Planck Institute for Informatik, Tsinghua University, Saarbr&#x00FC;cken, Beijing, GermanyChina","IEEE Journal of Selected Topics in Signal Processing","20171026","2017","11","7","923","925","Light field technology still suffers from crucial problems such as low spatial resolution, poor reconstructed depth quality, or large data storage requirements. Similarly, human interaction with four-dimensional data remains a challenging problem. Therefore, light field research is still in need of better theories and methods, targeting for more efficient data capture, analysis, processing, and storage. To this end, this special issue aims at providing readers with the latest developments and emerging technologies that aim to advance the field, and in the end make light fields more practical. For this purpose, 23 papers have been selected during the review process from 43 submissions. These are briefly summarized.","1932-4553;19324553","","10.1109/JSTSP.2017.2759458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8085236","","Image coding;Image reconstruction;Light field image processing;Light fields;Spatial resolution;Special issues and sections;Three-dimensional displays","","","","","","","","","","Oct. 2017","","IEEE","IEEE Journals & Magazines"
"IEEE Journal of Selected Topics in Quantum Electronics Topic Codes and Topics","","","IEEE Journal of Selected Topics in Quantum Electronics","20170605","2017","23","3","C4","C4","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.","1077-260X;1077260X","","10.1109/JSTQE.2017.2648678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938625","","","","","","","","","","","","May-June 2017","","IEEE","IEEE Journals & Magazines"
"Book review","A. Orlandi","","IEEE Electromagnetic Compatibility Magazine","20170519","2017","6","1","38","39","EMC engineers often simulate electromagnetic fields in order to gain a better understanding of how they interact with some device, circuit, or system. Higher-order Techniques in Computational Electromagnetics is a highly specialized book about basis functions for expanding electromagnetic fields and currents within numerical solution procedures. Its focus is primarily on basis functions developed by the authors and published in a series of journal articles during the past 20 years. While numerical techniques are in widespread use today, almost all are based on ""low-order"" techniques that place a limit on accuracy for a given computational cost. The authors' basis functions enable a better computational efficiency when implemented into a computer code.","2162-2264;21622264","","10.1109/MEMC.2017.7931980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931980","","Book reviews;Computational efficiency;Computional electromagnetics;Electromagnetic compatibility;Electromagnetic fields","","","","","","","","","","First Quarter 2017","","IEEE","IEEE Journals & Magazines"
"Principles of System Identification: Theory and Practice [Bookshelf]","W. R. Cluett","","IEEE Control Systems","20170316","2017","37","2","181","184","The book is up-to-date in the field of system identification. It contains a large amount of code and references to the Matlab System Identification Toolbox. With the book being so thick, it may seem overwhelming to contemplate using it as a textbook for a single, introductory, one-semester course on system identification. However, in the preface, the author does provide some helpful suggestions on which sections to use for a beginner course and which to use for a more advanced course. Nevertheless, the book will serve as an excellent reference for students and instructors alike looking to learn about the field. I make this latter statement because what the author has bravely attempted to do with this book is to create a onestop resource for system identification. The focus of the book is on discrete-time, linear time invariant, open-loop identification. The structure of the book is hierarchical but also very accessible at different places, depending on the reader‚Äôs background. The book contains 26 chapters divided into five parts.","1066-033X;1066033X","","10.1109/MCS.2016.2643262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879889","","Adaptive control;Book reviews;Data models;Estimation;Mathematical model;Predictive models;Stochastic processes;System identification","","","","","","","","","","April 2017","","IEEE","IEEE Journals & Magazines"
"Robust and Adaptive Model Predictive Control of Nonlinear Systems [Bookshelf]","B. Ding","School of Electronic and Information Engineering, Xi&#8217;an Jiaotong University, Shaanxi, 710049, P. R. China","IEEE Control Systems","20170118","2017","37","1","125","127","This book provides a comprehensive study of nonlinear adaptive robust model predictive control (MPC). Chapters 2‚Äì5 present a framework for the analysis and synthesis of nonlinear robust MPC. This framework includes the treatment of robustness, computation methods, and performance improvement. Chapters 6‚Äì7 show how to develop the basic ideas for the design and analysis of the nonlinear adaptive robust MPC. One of the key techniques is the set-based approach, in which the internal model identifier allows the MPC to compensate for future changes in the parameter estimates and uncertainty associated with the unknown model parameters. Chapters 8‚Äì12 illustrate how to implement the synthesis approaches for nonlinear adaptive robust MPC, and a robust adaptive economic MPC is also proposed. This text also gives a finite-time identification method, which can be used to estimate the unknown parameters in finite time, provided a persistence of excitation (PE) condition is satisfied. This identification method is particularly effective in the online implementation of MPC. The early chapters study continuous-time systems, and Chapters 13‚Äì14 extend the set-based estimation and robust adaptive MPC to discrete-time problems. While adaptive robust MPC is an improvement on robust MPC, this book shows that feedback MPC can be used to improve the open-loop MPC. At each sampling instant, a sequence of parameter estimates can be performed/invoked to improve the control performance. Economic MPC is also incorporated so as to improve the control performance in a broader way. This book is intended for someone learning functions of a complex variable and who enjoys using Matlab. It will enhance the experience of learning complex-variable theory and will strengthen the knowledge of someone already trained in this branch of advanced calculus. Supplying students with a bridge between the functions of complex-variable theory and Matlab, this supplemental text enables inst- uctors to easily add a Matlab component to their complex-variables courses. The book shows students how Matlab can be a powerful learning aid in such staples of complex-variable theory as conformal mapping, infinite series, contour integration, and Laplace and Fourier transforms. In addition to Matlab programming problems, the text includes many examples in each chapter along with Matlab code.","1066-033X;1066033X","","10.1109/MCS.2016.2621463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823076","","Adaptation models;Adaptive systems;Book reviews;MATLAB;Nonlinear systems;Predictive control","","","","","","","","","","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"Book interview [Reviews Book Interview]","N. Smith","","Engineering & Technology","20161018","2015","10","9","92","93","Steve Silberman doesn't mince his words when it comes to describing the relationship between autism and the world of engineering. ""If you've ever wondered why some of the techno-whizz kids in the computer department seem a bit odd, then it's because they are. Technology provides a pretty good work environment for some people who are on the autism spectrum."" Early on in his new book 'Neurotribes', Silberman notes that he once received a call from a manager at Microsoft, who told him: ""all of my top debuggers have Asperger's syndrome. They can hold hundreds of lines of code in their head as a visual image. They look for the flaws in the pattern, and that's where the bugs are.""","1750-9637;17509637","","10.1049/et.2015.0933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7587312","","","","","","","","","","","","October 2015","","IET","IET Journals & Magazines"
"Fundamentals of Digital Communications [Book/Software Reviews]","J. Chu","Kennesaw State University","IEEE Microwave Magazine","20160905","2016","17","10","78","78","This book provides the fundamentals of and basic design techniques for digital communications in a simple, easy-to-understand format. All the text discussions in the book are very brief and simple, chapters are short, and explanations are direct, providing a good summary of each subject. The text covers all important topics related to digital communications, such as radio beams, satellites, optic fibers, radar, and mobile wireless systems. Geared toward engineering students, the book's 13 chapters include a complete analysis of the structures used for emission or reception technology and provide a complete set of approaches to be used for digital communication design. The text consists of four parts: an introduction to digital communication basics; statistics communication and cover sampling methods, analog-to-digital conversion, quantization error, coding, probability of detection, and Nyquist criteria; digital communications on the carriers' frequency; and various exercises and problems, with each problem followed by a detailed solution, providing a most welcome exercise format for all students. This book is designed for master's level engineering courses, and students would need prerequisite undergraduate courses on basic digital communication.","1527-3342;15273342","","10.1109/MMM.2016.2589458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560741","","Adaptive optics;Book reviews;Digital communication;Optical fibers","","","","","","","","","","Oct. 2016","","IEEE","IEEE Journals & Magazines"
"Small and Short-Range Radar Systems [Book Review]","L. Gurel","ABAKUS Computing Technologies, Ankara, 06800, Turkey","IEEE Antennas and Propagation Magazine","20160804","2016","58","4","102","103","The initial pages of this book contain a series of color pictures. The average page contains multiple pictures or tables, compressing the text to a minimum and rendering the book pleasant to read. The first flip through the pages will captivate the readers and compel them to regard the short text under an interesting image, even though this may happen somewhere in the middle of the book on a completely random page. The author is maintaining a companion website where various demonstration videos related to the book chapters can be found, watched, enjoyed, and, in some cases, mimicked to help build one's own radar. Similarly, several MATLAB scripts are provided by the author on his website so that interested readers can try the radar processing algorithms and demonstrations outlined in the book. Following an introductory chapter on the general subject of radars, the book is divided into two main parts: Part I contains six chapters on short-range radar systems, and Part II is devoted to applications. Police Doppler radar and motion sensors, automotive radar, and through-wall radar are the applications covered in Part II.","1045-9243;10459243","","10.1109/MAP.2016.2569429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533614","","Book reviews;Doppler radar;Image coding;Image color analysis;Radar applications;Rendering (computer graphics);Video communication","","","","","","","","","","Aug. 2016","","IEEE","IEEE Journals & Magazines"
"Higher-Order Techniques in Computational Electromagnetics [Book Review]","L. Gurel","ABAKUS Computing Technologies, Ankara, 06800, Turkey","IEEE Antennas and Propagation Magazine","20160601","2016","58","3","106","107","This book caters to the needs of serious researchers, programmers, and scientists in the field of computational electromagnetics (CEM). The authors are both top-notch experts on this subject. With almost 400 pages of encyclopedic information, this is a full-size book containing a lot of rigorous and useful information. It is a good reference book and will be kept on a conveniently accessible shelf of every active CEM researcher‚Äôs bookcase. The title of the book alludes to CEM, but the information contained in the book may be useful in many other computational disciplines. Graglia and Peterson extensively worked on and published the core matter of this book for at least two decades; there is no question that they are the authoritative voices in this field, the book provides a comprehensive summary of the research performed by them. Because the authors have been drawing their applications from the CEM field, they titled the book accordingly. Nevertheless, the high-order techniques discussed spill over to other disciplines as well. The book contains several tables on interpolatory vector bases and hierarchical bases in an effort to present comprehensive and readily available systematized information to CEM programmers for rapid implementation of the higher-order basis functions described. The intended audience of this book is primarily CEM researchers and programmers and focuses on the high-level concepts pertaining to the inner workings of numerical solutions of integral and differential equations. As such, it is not an introductory text but may be used as a graduate-level textbook in specialized CEM courses concentrating on the higher-order discretizations of integral and/or differential equations. This text is particularly useful to those researchers who work with detailed mathematical formulations, numerical analysis, and actual coding.","1045-9243;10459243","","10.1109/MAP.2016.2541603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482886","","Book reviews;Computational electromagnetics;Differential equations;Encoding;Higher order statistics;Integral equations;Numerical analysis","","","","","","","","","","June 2016","","IEEE","IEEE Journals & Magazines"
"Health informatics and winning the war on cancer [review of ""guide to health informatics"" (coiera, e.; 2015)","P. King; R. L. Magin","","IEEE Pulse","20160512","2016","7","3","58","58","This 32-chapter, eight-section text can serve as a comprehensive introduction to the field of health informatics. With the accompanying e-book, the reader/user can easily access relevant, web-linked literature via the heavily web-based chapter references. The eight sections of the text cover 1) basic concepts, 2) informatics skills, 3) information systems in health care, 4) guideline- and protocol-based systems, 5) communication systems in health care, 6) language, coding, and classification, 7) clinical decision support and analytics, and 8) specialized applications for health informatics. Each chapter opens with relevant quotations, and each is logically structured and well diagrammed. Sidebar elaborations assist the novice reader in useful terminology, and ‚Äúbox‚Äù sections serve to give examples of items discussed. Each chapter concludes with a listing of ‚Äúdiscussion points‚Äù for the chapter and a chapter summary. References (by chapter) are placed at the end of the text, preceded by a fairly comprehensive glossary.","2154-2287;21542287","","10.1109/MPUL.2016.2538485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469474","","Bioinformatics;Book reviews;Informatics","","","","","","","","","","May-June 2016","","IEEE","IEEE Journals & Magazines"
"Low-Frequency Electromagnetic Modeling for Electrical and Biological Systems Using MATLAB [Book Reviews]","P. King; V. Iyer","","IEEE Pulse","20160314","2016","7","2","69","69","The role of increasingly powerful computers in the modeling and simulation domain has resulted in great advancements in the fields of wireless communications, medicine, and space technology to name a few. In The authors of this book start from the fundamental equations that govern low-frequency electromagnetic phenomenon and go through each stage of solving such problems by striking a balance between mathematical rigor and actual implementation in code. The use of MATLAB makes the advanced concepts discussed in the book immediately testable through experiments. The book pays close attention to various applications in an electrical and biological system that are of immediate relevance in today‚Äôs world. The use of state-of-the-art human phantom meshes, especially from the Visible Human Project (VHP) of the U.S. National Library of Medicine, makes this text singular in its field. The text is systematic and very well-organized in presenting the various topics on low-frequency electromagnetic. It also should be known that the first part of this text presents the mathematical theory behind low-frequency electromagnetic modeling and follows it with the topic of meshing. The text starts with the basics of meshing and builds it up in an easy-to-read manner with plenty of illustrations.","2154-2287;21542287","","10.1109/MPUL.2015.2513732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7432062","","Biological system modeling;Biological systems;Book reviews;Electromagnetic modeling;Engineering education;MATLAB;Mathematical model","","","","","","","","","","March-April 2016","","IEEE","IEEE Journals & Magazines"
"Mobile Point-of-Care Monitors and Diagnostic Device Design [Book Reviews]","P. H. King","","IEEE Pulse","20160311","2016","7","2","70","70","The first section of the text gives an overview of sensors and systems involved in the field. Presents a great introduction to what follows, covering lab-on-a-chip technologies and their use with consumer electronic devices (CEDs) (aka smartphones, etc.) with and without modification of same. Several examples and discussions of sensing technologies are given, and the chapter concludes with 82 references, completing this overview. The next four chapters give specific information on the chapter authors‚Äô current efforts involving lab-on-a-cell-phone, the phone oximeter, transepidermal waterloss sensors, and ultrasound imaging system design. Three chapters comprise the ‚ÄúInformation Processing and Implementation‚Äù section. Chapter 6 covers the application of CEDs for blood-smear analysis for mobile malaria diagnosis. It is a good overview of current efforts and the roadblocks to a widespread implementation. Chapter 7 covers usability engineering for mobile point-of-care devices and addresses U.S. Food and Drug Administration (FDA) and International Electrotechnical Commission usability codes; furthermore, it could serve as a good generic chapter for design considerations for this field. Chapter 8 (‚ÄúTranslating Sensor Technology into the Medical Device Environment‚Äù) concludes the text with a nice overview of the FDA approval process and discusses some example technologies. Taken as a whole, this text could be a useful introduction to the field of point-of-care technologies for the student or professional considering development of globally useful, inexpensive diagnostic technologies for the betterment of health care worldwide. While the primary base device for much of this text is the smartphone, the text is generic enough in approach that other areas of endeavor could be attempted.","2154-2287;21542287","","10.1109/MPUL.2016.2532222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7432084","","Book reviews;Consumer electronics;Mobile communication;Sensor systems;Smart phones","","","","","","","","","","March-April 2016","","IEEE","IEEE Journals & Magazines"
"[Title page]","","","2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE)","20150713","2015","","","1","1","The following topics are dealt with: vibration signal based monitoring; mechanical microdrilling; rule based inflectional urdu stemmer usal; rule based derivational urdu stemmer usal; fuzzy logic controller; heat exchanger temperature process; text dependent speaker recognition; MFCC; SBC; multikeyword based sorted querying; encrypted cloud data; communication understandability enhancement; GSD; parsing; input power quality; switched reluctance motor drive; externally powered upper limb prostheses; program test data generation; launch vehicle optimal trajectory generation; misalignment fault detection; induction motors; current signature analysis; vibration signature analysis; wind power plants; vortex induced vibration; mechanical structure modal analysis; machining parameter optimization; diesel engines; high speed nonvolatile NEMS memory devices; image fusion; RGB color space; LUV color space; offline English character recognition; human skin detection; tumor boundary extraction; MR images; OdiaBraille; text transcription; shadow detection; YIQ color models; color aerial images; moving object segmentation; image data deduplication; iris recognition; two-stage series connected thermoelectric generator; education information system; cyclone separator CFD simulation; imperfect debugging; vulnerability discovery model; stochastic differential equation; cloud data access; attribute based encryption; agile SCRUM framework; PID controller optimisation; hybrid watermarking technique; privacy preservation; vertical partitioned medical database; power amplifier; software reliability growth modeling; cochlear implantation; cellular towers; feedforward neural networks; MBSOM; agent based semantic ontology matching; phonetic word identification; test case selection; MANET security issues; online movie data classification; modified LEACH protocol; mobile ad hoc networks; virtual machine introspection; task scheduling; cluster computing; image compression; green cloud computin- ; critical health data transmission system; irreversible regenerative Brayton cycle; task set based adaptive round robin scheduling; database security; heterogeneous online social networks; aspect oriented systems; IP network; MPLS network; DBSCAN algorithm; VANET; self-organizing feature map; image segmentation; enzyme classification; wireless sensor networks; energy smart routing protocol; adaptive gateway discovery mechanism; heuristic job scheduling; AODV based congestion control protocol; expert system; home appliances; relay node based heer protocol; data storage; TORA security; data aggregation; low energy adaptive stable energy efficient protocol; fuzzy logic based clustering algorithm; hybrid evolutionary MPLS tunneling algorithm; English mobile teaching; eigenvector centrality; genetic algorithms; data mining; heart disease prediction; lossless data compression; reconfigurable ring resonator; triple band stacked patch antenna; energy based spectrum sensing; cognitive radio networks; FPGA; knowledge representation; multiband microstrip antenna; Web indexing; HTML priority system; Web cache recommender system; e-learning; IT skill learning for visual impaired; user review data analysis; software up-gradation model; software testing; Web crawlers; secret key watermarking; WAV audio file; SRM drive; ZETA converter; fractional PID tuning; medical image reconstruction; speech recognition system; video authentication; digital forensics; content based image retrieval; image classification; hybrid wavelet transform; facial feature extraction; RBSD adder; smart home environment; generalized discrete time model; We Chat marketing; foreign language learning; carbon dioxide emission mitigation; power generation; smartphone storage enhancement; and virtualization.","","CD-ROM:978-1-4799-8432-9; Electronic:978-1-4799-8433-6; POD:978-1-4799-8434-3","10.1109/ABLAZE.2015.7155050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155050","","","Brayton cycle;IP networks;adders;aspect-oriented programming;audio watermarking;biomedical MRI;cardiology;character recognition;cloud computing;cochlear implants;cognitive radio;computational fluid dynamics;computer science education;content-based retrieval;cryptography;cyclone separators;data analysis;data compression;data mining;data privacy;diesel engines;differential equations;digital forensics;domestic appliances;drilling;educational administrative data processing;eigenvalues and eigenfunctions;enzymes;expert systems;face recognition;fault diagnosis;feature extraction;feedforward neural nets;field programmable gate arrays;fuzzy control;genetic algorithms;grammars;green computing;handicapped aids;heat exchangers;home computing;image classification;image coding;image colour analysis;image fusion;image reconstruction;image retrieval;image segmentation;image watermarking;indexing;induction motors;internetworking;iris recognition;knowledge representation;linguistics;medical image processing;microstrip antennas;mobile learning;modal analysis;nanoelectromechanical devices;object detection;ontologies (artificial intelligence);pattern clustering;power amplifiers;power supply quality;program debugging;program testing;radio spectrum management;recommender systems;reluctance motor drives;resonators;routing protocols;scheduling;self-organising feature maps;social networking (online);software reliability;speaker recognition;speech processing;storage management;telecommunication congestion control;thermoelectric conversion;three-term control;trajectory optimisation (aerospace);tumours;vehicular ad hoc networks;vibrations;video signal processing;virtual machines;virtualisation;wavelet transforms;wind power plants;wireless sensor networks","AODV based congestion control protocol;DBSCAN algorithm;English mobile teaching;FPGA;GSD;HTML priority system;IP network;IT skill learning for visual impaired;LUV color space;MANET security issues;MBSOM;MFCC;MPLS network;MR images;OdiaBraille;PID controller optimisation;RBSD adder;RGB color space;SBC;SRM drive;TORA security;VANET;WAV audio file;We Chat marketing;Web cache recommender system;Web crawlers;Web indexing;YIQ color models;ZETA converter;adaptive gateway discovery mechanism;agent based semantic ontology matching;agile SCRUM framework;aspect oriented systems;attribute based encryption;carbon dioxide emission mitigation;cellular towers;cloud data access;cluster computing;cochlear implantation;cognitive radio networks;color aerial images;communication understandability enhancement;content based image retrieval;critical health data transmission system;current signature analysis;cyclone separator CFD simulation;data aggregation;data mining;data storage;database security;diesel engines;digital forensics;e-learning;education information system;eigenvector centrality;encrypted cloud data;energy based spectrum sensing;energy smart routing protocol;enzyme classification;expert system;externally powered upper limb prostheses;facial feature extraction;feedforward neural networks;foreign language learning;fractional PID tuning;fuzzy logic based clustering algorithm;fuzzy logic controller;generalized discrete time model;genetic algorithms;green cloud computing;heart disease prediction;heat exchanger temperature process;heterogeneous online social networks;heuristic job scheduling;high speed nonvolatile NEMS memory devices;home appliances;human skin detection;hybrid evolutionary MPLS tunneling algorithm;hybrid watermarking technique;hybrid wavelet transform;image classification;image compression;image data deduplication;image fusion;image segmentation;imperfect debugging;induction motors;input power quality;iris recognition;irreversible regenerative Brayton cycle;knowledge representation;launch vehicle optimal trajectory generation;lossless data compression;low energy adaptive stable energy efficient protocol;machining parameter optimization;mechanical microdrilling;mechanical structure modal analysis;medical image reconstruction;misalignment fault detection;mobile ad hoc networks;modified LEACH protocol;moving object segmentation;multiband microstrip antenna;multikeyword based sorted querying;offline English character recognition;online movie data classification;parsing;phonetic word identification;power amplifier;power generation;privacy preservation;program test data generation;reconfigurable ring resonator;relay node based heer protocol;rule based derivational urdu stemmer usal;rule based inflectional urdu stemmer usal;secret key watermarking;self-organizing feature map;shadow detection;smart home environment;smartphone storage enhancement;software reliability growth modeling;software testing;software up-gradation model;speech recognition system;stochastic differential equation;switched reluctance motor drive;task scheduling;task set based adaptive round robin scheduling;test case selection;text dependent speaker recognition;text transcription;triple band stacked patch antenna;tumor boundary extraction;two-stage series connected thermoelectric generator;user review data analysis;vertical partitioned medical database;vibration signal based monitoring;vibration signature analysis;video authentication;virtual machine introspection;virtualization;vortex induced vibration;vulnerability discovery model;wind power plants;wireless sensor networks","","0","","","","","","25-27 Feb. 2015","","IEEE","IEEE Conferences"
"Microwave Radar and Radiometric Remote Sensing [Book Reviews]","J. Richards","","IEEE Geoscience and Remote Sensing Magazine","20150324","2015","3","1","51","52","This is an exceptional book. In fact, it is more than that???it is 1000 pages of printed matter supplemented by an extensive collection of electronic materials that include images, Matlab code, interactive examples, instructor assistance and other resources, all held on a dedicated home page at the University of Michigan. Although its principal impact derives from the combination of the printed and electronic matter, the book can stand alone if necessary, particularly for the practitioner. But for the student the electronic material is essential and multiplies the value of the book severalfold. This is because some of the problems set at the end of each chapter use the electronic resources very effectively. These exercises add substantially to the utility of the book as a senior level text, especially since solutions are available on the home page for the instructor. It is hard to overstate the value added by the material on the home page; but it is essential that it be maintained over the life of the book. The publication effectively blends a textbook with a reference manual, and a treatment for the systems designer designer with a course for the remote sensing applications specialist. Its coverage is broad and deep; its completeness means that the reader has little need to consult other books for introductory or supplementary material. Nevertheless adequate referencing is provided to more detailed treatments when relevant. The lead and contributing authors are the authorities in imaging radar and radiometry.","2473-2397;24732397","","10.1109/MGRS.2015.2398391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7067059","","Book reviews;Radar imaging;Remote sensing","","","","0","","","","","","March 2015","","IEEE","IEEE Journals & Magazines"
"Object Detection and Recognition in Digital Images: Theory and Practice [Book News]","R. Zurawski","","IEEE Industrial Electronics Magazine","20150318","2015","9","1","93","94","This book seeks to explain the concepts of modern computer vision methods, blending theory with practical examples. The detailed mathematical derivations contained in this book help in understanding the key concepts. Real-life examples, backed up by details of code implementations, help in bringing the theory to real applications. The book focuses on methods developed by the author over the past few years, with a majority of examples coming from the automotive industry. Some mathematical background, as well as basic programming skills, is expected from the reader. The book has more than 550 pages and consists of five main chapters and an appendix.","1932-4529;19324529","","10.1109/MIE.2014.2388012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7063347","","Automotive engineering;Book reviews;Computer vision;Pattern recognition","","","","0","","","","","","March 2015","","IEEE","IEEE Journals & Magazines"
"A different perspective on scientific programming [review of ""annotated algorithms in python; with applications in physics, biology, and finance"" (di pierro, m.; 2013)]","S. Weppner","Eckerd College","Computing in Science & Engineering","20150204","2015","17","1","6","7","This is an unconventional computer science book, as it???s not a textbook, tutorial, or reference. It???s an attempt to categorize the algorithms of quantitative programming and then make further remarks using a number of examples from each category. The objective, I would presume, is to strengthen the ability of the reader to recognize that their own programming challenges can be reduced to a combination of approximate and statistical approaches that must be pragmatic (so that the time length of execution is reasonable). The coding illustrations demonstrate common techniques for the ordering of algorithms. At the same time, it introduces the reader to Python, a language which is growing in popularity among engineers and scientists. The book excels as an introduction to the Python language for experienced programmers. It has an excellent chapter on parallel processing, an impressive random number generator discussion, and some fine biology and finance examples.","1521-9615;15219615","","10.1109/MCSE.2015.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030301","Monte Carlo simulations;Python;algorithm analysis;algorithm design;computational finance;numerical algorithms;parallel algorithms;scientific computing","Algorithm design and analysis;Approximation algorithms;Book reviews;Computer languages;Data structures;Formal languages;Pragmatics","","","","0","","2","","","","Jan.-Feb. 2015","","IEEE","IEEE Journals & Magazines"
"Forging a Close Relationship with Multimedia Communities","W. Zeng; Z. Liu; E. Steinbach","University of Missouri and Microsoft Research Asia","IEEE MultiMedia","20141103","2014","21","4","14","15","In May 2014, the authors of the top 26 papers from the IEEE International Conference on Multimedia &amp; Expo (ICME) 2014 were invited to submit extended versions of their papers to this fast track special issue. After a rigorous peer-review process, eight of those submissions were accepted for this special issue, now titled ""Hot Topics in Multimedia Research."" This is just the beginning of a close collaboration between MM and major multimedia conferences.","1070-986X;1070986X","","10.1109/MMUL.2014.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945277","ICME;distributed video coding;gaze estimation;image quality assessment;learning algorithm;location propagation;multimedia;multimedia research;object tracking;visual analysis","","","","","0","","","","","","Oct.-Dec. 2014","","IEEE","IEEE Journals & Magazines"
"Applied Mathematics: Methods and Matlab","B. G. Cook","Oak Ridge National Laboratory","Computing in Science & Engineering","20140819","2014","16","4","6","7","The book Methods of Applied Mathematics for Engineers and Scientists by Thomas B. Co presents a range of topics in applied mathematics suitable for teaching a graduate student-level course or an entry-level reference volume for the professional. The introduction of Matlab notation side-by-side with theoretical background and the inclusion of many companion example codes in this text will prove particularly useful to those already using or wishing to learn Matlab.","1521-9615;15219615","","10.1109/MCSE.2014.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6879728","Matlab;applied mathematics;computational science;scientific computing","Book reviews;MATLAB;Mathematical models;Matrix theory;Numerical models;Partial differential equations;Vectors","","","","0","","","","","","July-Aug. 2014","","IEEE","IEEE Journals & Magazines"
"Color Communication Green (review of ""Green Radio Communication Networks"" (Hossain, E., Eds, et al; 2012) [Book/Software Reviews]","A. Riddle","M/ACOM Technology Solutions","IEEE Microwave Magazine","20130128","2013","14","1","164","165","This book begins with a very high level overview of the tradeoffs between things like spectral efficiency, energy efficiency, bandwidth, and power. The book ends with a chapter describing some projects such as Energy Aware Radio and neTwork tecHnologies (EARTH); test beds; and consortiums. In between there are chapters on energy harvesting, energy consumption models, comparing coding for efficiency, and modulation methods. As with any book involving networks at a high level, there are lots of acronyms. The claim is made that the energy consumed by our wireless network can easily be reduced by two thirds. The case is also made that some form of energy harvesting such as solar or wind powered microturbines has promise and would add to the overall efficiency. Many of the chapters focus on media access control, cooperative efficiency, multiple nodes and hop networks, and metrics for measuring efficiency while considering the quality of service. This book contains contributions from almost 60 scientists and engineers in academia and industry. Green Radio Communication Networks is an excellent snapshot of the state of green in wireless networking, and it is a nice overview of who is doing what and where in the field.","1527-3342;15273342","","10.1109/MMM.2012.2226544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6421099","","Book reviews;Energy efficiency;Green design;Product design;Smart phones;Telecommunication equipment","","","","0","","","","","","Jan.-Feb. 2013","","IEEE","IEEE Journals & Magazines"
"Ensemble Methods: Foundations and Algorithms [Book Review]","F. Schwenker","Ulm University, GERMANY","IEEE Computational Intelligence Magazine","20130115","2013","8","1","77","79","This monograph is a valuable contribution to theoretical and practical ensemble learning. The material is very well-presented, preliminaries and basic knowledge are discussed in detail, many illustrations and pseudo-code tables help to understand the facts of this interesting field of research. The book will become a helpful tool for practitioners working in the field of machine learning or pattern recognition as well as for students of engineering or computer sciences at the graduate and postgraduate level.","1556-603X;1556603X","","10.1109/MCI.2012.2228600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6410720","","Algorithm design and analysis;Book reviews;Classification algorithms;Clustering algorithms;Machine learning algorithms;Supervised learning","","","","1","","","","","","Feb. 2013","","IEEE","IEEE Journals & Magazines"
"Digital Image Processing and Analysis: Human and Computer Applications with CVIPtools, 2nd Edition (Umbaugh, S.; 2011) [Book Reviews]","P. H. King","","IEEE Pulse","20120719","2012","3","4","84","85","The author presents the book on digital image and analysis that has four sections and thirteen chapters, which is written at a junior-year or above level and used as a basis for advanced studies involving images. The first section deals with introduction and overview of digital image processes and analysis (Chapter 1), and computer imaging systems (Chapter 2). Section 2 contains digital image analysis and computer vision that includes chapters: 3 (introduction of digital image analysis); 4- image segmentation and line and edge analysis; 5- discrete transform (Fourier and other), and filtering; and 6- feature analysis and pattern classification. Section 3 covers digital image processing and human vision, and consists of chapters: 7- human vision and perception and heavily relates to the usefulness of image processing and the effects of noise reduction; 8- overview of image enhancement techniques; 9- image restoration; and 10- image compression with discussion of entropy and information. And section 4 deals with program and application development with CVIPtools (computer vision and image processing (CVIP)), and has the following chapters: 11- basic use of the CD software; 12- application development; and 13- libraries.","2154-2287;21542287","","10.1109/MPUL.2012.2196843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244928","","Biomedical image processing;Book reviews;Computer vision;Digital images;Neuroimaging","computer vision;data compression;discrete Fourier transforms;image coding;image enhancement;image processing;image segmentation;libraries;pattern classification;visual perception","CD software;CVIPtools;application development;computer application;computer imaging systems;computer vision;digital image analysis;digital image processing;discrete transform;edge analysis;entropy;feature analysis;human perception;human vision;image compression;image enhancement technique;image restoration;image segmentation;libraries;line analysis;noise reduction;pattern classification","","0","","","","","","July 2012","","IEEE","IEEE Journals & Magazines"
"Wireless Communications"" (Molisch, A.; 2011) [Book review]","S. Misra","","IEEE Wireless Communications","20120223","2012","19","1","5","5","In the last two decades, wireless communications has become a vast field of application and research consisting of subdomains, such as cellular networks, satellite networks, wireless mesh networks, cognitive radio networks, and wireless ad hoc and sensor networks. The common theme for all these networks is the shared wireless medium; however, they differ on a host of other aspects, such as coding techniques, antenna design, and protocols.","1536-1284;15361284","","10.1109/MWC.2012.6155869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6155869","","Ad hoc networks;Book reviews;Cognitive radio;Wideband;Wireless sensor networks","cellular radio;cognitive radio;satellite communication;wireless mesh networks;wireless sensor networks","cellular networks;cognitive radio networks;satellite networks;wireless ad hoc networks;wireless communications;wireless mesh networks;wireless sensor networks","","0","","","","","","February 2012","","IEEE","IEEE Journals & Magazines"
"Fuzzy Networks for Complex Systems: A Modular Rule Base Approach (Gegov, A.; 2010) [Book Review]","R. R. John","De Montfort University, UK","IEEE Computational Intelligence Magazine","20120116","2012","7","1","76","77","The book ??Fuzzy Networks for Complex Systems?? by Alexander Gegov appears in the Springer Studies in Fuzziness and Soft Computing Series. Fuzzy networks as described here are networks where the nodes are fuzzy rule bases and there are connections between the nodes, for example inputting the outputs from one fuzzy system to another. In this book we get a complete description of the approach including the formal underpinning, practical examples, case studies and Matlab code to implement aspects of fuzzy networks.","1556-603X;1556603X","","10.1109/MCI.2011.2176777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132216","","Book reviews;Complex networks;Fuzzy logic;MATLAB","","","","0","","","","","","Feb. 2012","","IEEE","IEEE Journals & Magazines"
"Book reviews (2 books reviewed)","M. Wagrowski; M. Leszczuk","","IEEE Communications Magazine","20110505","2011","49","5","24","25","The following books are reviewed: Wireless Communications, Second Edition (Molisch, A.F.; 2011); and The Essential Guide to Video Processing (Bovik, A., Eds.; 2009)","0163-6804;01636804","","10.1109/MCOM.2011.5762792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762792","","Book reviews;MIMO;Multiaccess communication;Streaming media;Transform coding;Video coding;Video communication;Wireless communication","","","","0","","","","","","May 2011","","IEEE","IEEE Journals & Magazines"
"Reviews [review of ""A Vast Machine: Computer Models, Climate Data, and the Politics of Global Warming"" (Edwards, P.N.; 2010) and ""The Last Good War"" (Wonnacott, P.; 2007)]","R. Eardley-Pryor; K. Smillie","","IEEE Annals of the History of Computing","20110303","2011","33","1","83","85","Two books are reviewed: ""A Vast Machine: Computer Models, Climate Data, and the Politics of Global Warming"" (Edwards, P.N.; 2010) and ""The Last Good War"" (Wonnacott, P.; 2007).","1058-6180;10586180","","10.1109/MAHC.2011.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5723079","","Atmospheric measurements;Book reviews;Codes;Cryptography;Encoding;Global warming;Information analysis;Meteorology;Military communication;Modeling","","","","0","","3","","","","Jan. 2011","","IEEE","IEEE Journals & Magazines"
"On the Dot: The Speck that Changed the World (Humez, A. and Humez, N.; 2008) [Book Review]","E. A. Malone","","IEEE Transactions on Professional Communication","20110222","2011","54","1","100","101","This book covers most of the uses of the dot in history, including its use for bulleted lists, in an ellipsis, and in codes, musical notation, mathematics, and computers. While those who like trivia may enjoy the book, those looking for something more focused and scholarly will have to sift through many pages of pointless information to find that speck of gold.","0361-1434;03611434","","10.1109/TPC.2010.2099850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5719053","Editing;history;punctuation;trivia","Book reviews;Communication symbols;History","","","","0","","2","","","","March 2011","","IEEE","IEEE Journals & Magazines"
"Concurrent checking for logic [review of ""New Methods of Concurrent Checking (Goessel, M., et al; 2008)]","S. Davidson","Sun Microsystems","IEEE Design & Test of Computers","20100518","2010","27","3","80","81","This is a review of New Methods of Concurrent Checking (by Michael Goessel, Vitaly Ocheretny, Egor Sogomonyan, and Daniel Marienfeld).","0740-7475;07407475","","10.1109/MDT.2010.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5465129","concurrent checking;self-checking logic","Aging;Books;Circuit faults;Circuit testing;Error correction;Error correction codes;Fault detection;Logic;Redundancy;System testing","","","","0","","","","","","May-June 2010","","IEEE","IEEE Journals & Magazines"
"Recognition of 2009 <emphasis emphasistype=""smcaps"">Transactions</emphasis> and <emphasis emphasistype=""italic"">Magazine</emphasis> Papers Reviewers","","","IEEE Transactions on Industry Applications","20100318","2010","46","2","555","561","Lists the reviews who contributed to IEEE Transactions on Industry Applications and IEEE Industry Applications Magazine in 2009.","0093-9994;00939994","","10.1109/TIA.2010.2040657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5434655","","Code standards;Electronics industry;Gas industry;Industrial electronics;Industrial power systems;Industry applications;Paper technology;Power engineering and energy;Power systems;Technological innovation","","","","0","","","","","","March-april 2010","","IEEE","IEEE Journals & Magazines"
